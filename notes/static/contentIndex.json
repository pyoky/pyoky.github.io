{"(Article)-Equality-of-What-q":{"title":"(Article) Equality of What?","links":["utilitarianism","Utility","Efficiency-(Statistics)","Microeconomics"],"tags":["article","Economics/Game-Theory","Philosophy/Political-Philosophy"],"content":"(DevonThink) Sen,Equality\n\nEquity is about asking “what should be equal?”\n\nLibertarians: Equality of Rights\nUtilitarians: Equal Weighting of Utility\nProgressive Taxation: Equality of Need Satisfaction\netc.\n\n\n⇒ The question then becomes: “Equality of What?”\n\nPursuit of equality in one domain often damages another domain\n\n⇒ arguments are often in the form of “Inequality in space A is acceptable, because it increases equality in space B.”\n\n\nCommon framing is between equality and liberty\n\ne.g. left: equality in needs. right: liberty and freedom\nThis is the wrong framing because there are lots of other candidates anyways\n\n\n\n\nAlternative framing: Efficiency (Statistics)\n\nEfficiency of utility ⇒ Pareto Optimum\nEfficiency of liberty ⇒ Libertarian Government\netc.\n\n\n"},"(Article)-Magic-Ink---Information-Software-and-the-Graphical-Interface":{"title":"(Article) Magic Ink - Information Software and the Graphical Interface","links":[],"tags":["Design","Computing/Human-Interface"],"content":"Notes on Magic Ink: Information Software and the Graphical Interface\n\n\n                  \n                  What You See is All You Get (WYSIAYG) \n                  \n                \n\n\nUI design is:\n\nHow information is presented: Graphic Design\n\nPersonal accounting, Health app visualization\n\n\nHow to manipulate and create: Industrial design\n\nPhotoshop, Illustrator\n\n\n\n\nUsers mostly want to learn something, not create something.\n⇒ Graphic design &gt; Industrial design.\n⇒ How information is presented &gt; How to create new things\nThus, paper graphic design is not only relevant, but is a baseline\n\n&gt; If a person is in the mood for a movie, what questions might she have?\n&gt; - What movies are showing today, at which times?\n&gt; - What movies are showing around a particular time?\n&gt; - Where are they showing?\n&gt; - What are they about?\n&gt; - Are they good?\n&gt; \n&gt; Consider this redesign.\n\n![[Pasted image 20230801000258.png]]\n- Timeline visualizes and invites time comparisons\n- &quot;What is showing&quot; is boldly presented\n- Cinemas are [[Color as an Extra Dimension|color-coded for easy comparison]]\n\nContext-Sensitive Graphics §\n\nDigital interfaces are better than paper because information is context-sensitive.\n\nIt can link to more information\n(instead of having to search paper)\nIt can present data in different ways according to what the user needs\n(instead of being static data on paper)\n\n\n\nInteractivity is Bad §\n\nFor all information software, interaction == navigation in data-space\n\nContextual software should already know what user wants\nNavigation is effortful and bad.\n\n\nTo reduce manipulation…\n\nContext-Sensitive Specialized Controls:\nTight Feedback loops (immediate results)\n\n\n\n"},"(Book)-The-Case-for-Economic-Democracy":{"title":"The Case for Economic Democracy","links":[],"tags":["Economics/Game-Theory","Philosophy/Political-Philosophy"],"content":""},"(HowTo)-Cost-Minimization":{"title":"(HowTo) Cost Minimization","links":["Profit-Maximization","Constrained-Optimization"],"tags":["Economics/Micro-Economics"],"content":"See also Profit Maximization\nLong Run Cost Minimization §\nminl,k​ C=wl+rk such that x=f(x)\n\nLagrangian Optimization\nOptimal when Isoquant is tangent to the isocost\n\nHigher isoquant is not always better! it just means you’re producing more\n\n\n\n\n\n                  \n                  Warning \n                  \n                \nBeware when MC is monotonically decreasing. This means that the firm will produce zero or infinity, which is means it is a special case which you need to analyze separately.\n\n"},"(Philosopher)-Alexandre-Kojeve":{"title":"(Philosopher) Alexandre Kojeve","links":[],"tags":["People","Philosophy"],"content":"\nAlexandre Kojève (koh-ZHEV, French: [alɛksɑ̃dʁ kɔʒɛv]; 28 April 1902–4 June 1968) was a Russian-born French philosopher and statesman whose philosophical seminars had an immense influence on 20th-century French philosophy, particularly via his integration of Hegelian concepts into twentieth-century continental philosophy. As a statesman in the French government, he was instrumental in the formation of the European Union.\nWikipedia\n"},"(Philosopher)-Edmund-Husserl":{"title":"(Philosopher) Edmund Husserl","links":["Phenomenology"],"tags":["People"],"content":"See Phenomenology."},"(Philosopher)-G.-W.-F.-Hegel":{"title":"(Philosopher) G. W. F. Hegel","links":["(Book)-Phenomenology-of-Spirit"],"tags":["People","Philosophy"],"content":"\nGeorg Wilhelm Friedrich Hegel (; German: [ˈɡeːɔʁk ˈvɪlhɛlm ˈfʁiːdʁɪç ˈheːɡl̩]; 27 August 1770–14 November 1831) was a German philosopher and one of the most influential figures of German idealism and 19th-century philosophy.\nBorn in 1770 in Stuttgart, Holy Roman Empire, during the transitional period between the Enlightenment and the Romantic movement in the Germanic regions of Europe, Hegel lived through and was influenced by the French Revolution and the Napoleonic wars. His fame rests chiefly upon (Book) Phenomenology of Spirit, The Science of Logic, his teleological account of history, and his lectures at the University of Berlin on topics from his Encyclopedia of the Philosophical Sciences.\nGuided by the Delphic imperative to “know thyself,” Hegel presents free self-determination as the essence of humankind–a conclusion from his 1806–07 Phenomenology that he claims is further verified by the systematic account of the interdependence of logic, nature, and spirit in his later Encyclopedia. He asserts that the Logic at once preserves and overcomes the dualisms of the material and the mental–that is, it accounts for both the continuity and difference marking of the domains of nature and culture–as a metaphysically necessary and coherent “identity of identity and non-identity.”\nWikipedia\n"},"(Philosopher)-Martin-Heidegger":{"title":"(Philosopher) Martin Heidegger","links":["Phenomenology"],"tags":["People"],"content":"See Phenomenology."},"10-09-Notes":{"title":"10-09 Notes","links":["Society's-Scripts","Ground-Truth","Emergent-Phenomena","Validating-Emotions","Gender-is-a-performance","Optimistic-Nihilism","tags/Computing","tags/Economics","tags/Literature","Phenomenology","emergent-phenomena","tags/sociability","tags/Sociability/Dating","Communication","Solipcism","Honesty"],"tags":["Journal-Entry","Computing","Economics","Literature","sociability","Sociability/Dating"],"content":"\nSociety’s Scripts of gender are grounded on biology but unfounded. Emergence of diverse modes of social interaction necessitate better ways of interaction\nYour Validating Emotions: your desire to be percieved as the cute girl, to perform that role (Gender is a performance) is built upon\n\nYour history in korea and japan\nyour past relationships\n\n\nOptimistic Nihilism. The problem is that you have it all figured out. Don’t be humble, don’t shy away from facing the void that there is not much else out there. This is all there is, and that’s okay. Facing the void, saying that that’s okay, and then to build something from it.\nBuilding:\n\n\n#Computing as a method of building the mechanized world\n\n\n#Economics as the elegant dance of these emergently rational parts\n\n\n#Literature as the observation, the phenomelogical stance of taking the grand experience in its entirety (Oh me! oh life!)\n\n\nPhysics as the futile goal of a bygone era of human excellence, humanism that trumped it all, that rationality could conquer the forces of all emergent phenomena. And we couldn’t.\n\n\n#sociability and#Sociability/Dating as a construction of a Communication channel between humans\n\nCommunication is humanity’s resolution with Solipcism. Fundamental Misunderstanding can be resolved through Communication\n\n\n\nYour brain as a linear machine that cannot inherently focus on the graph, the whole picture in a visual, direct way but only in an intuitive way.\n\n\n\nMethodollogical issues\n\nThe importance of being purely honest with your own emotions\n\n\n\n"},"581-Exam-Topics":{"title":"581 Exam Topics","links":[],"tags":["Courses"],"content":"Chapter 2: Time Value of Money §\n\n Simple vs compound interest\n From compound interest to continuous compounding\n Formulas for PV and FV of an annuity\n Amortization (payment towards interest and principal)\n PV and FV of Bonds\n Stock valuation (DDM)\n\nChapter 3: Portfolio Theory §\n\n 2-securities: finding the global minimum variance portfolio using 1variable calculus\n Cases for ρ = 0, ρ = ±1\n N-securities: finding the global minimum variance portfolio using Lagrange\n Understanding matrices V and H, quantities A, B, and C\n\nChapter 4: Capital Market Theory §\n\n Understanding CML and SML lines\n Finding the sharp ratio and equation of the CML\n Deriving weight and risk-return of the Market Portfolio\n Deriving the formula for the Beta\n\nChapter 5: Binomial Tree Models §\n\n Understanding the definition of a general binomial tree\n Deriving formulas for u_n, d_n, and p_n for the CRR tree\n From Binomial Tree to the Lognormal Model\n\nChapter 6: Stochastic Calculus §\n\n Definition of SBM and GBM\n Intuition behind SBM: the symmetric random walk\n Properties of SBM in relation to the random walk\n Properties of the Lognormal Model for security prices\n Definition of the stochastic integral\n Using Ito’s formula in simple cases\n\nChapter 7&amp;8: Derivatives and the BSM Model §\n\n Terminal payoff diagrams for Call and Put\n Put- []Call parity\n Binomial tree model for call options\n From Binomial tree model to the BSM formula\n Deriving the BSM PDE\n Understanding the BSM formula for Call and Put\n"},"Abstraction":{"title":"Abstraction","links":["Moore’s-law","Abstraction","Pipelining","Branching-(Computer-Science)","Instruction-Set"],"tags":["Computing/Computer-Architecture"],"content":"1.2. 8 Great Ideas from Computer Science §\n\nMoore’s law\nAbstraction\nOptimize for the Common Case\nParallelism → thruput increase\nPipelining → thuput increase\nPrediction\nMemory Heirarchy\nRedundancy\n\nAbstraction: Application Software &gt; Systems Software &gt; Hardware.\nAbstraction: High Level Language &gt; Assembly Language &gt; Binary Machine Language\nComponents of a Computer §\nComponents of a Computer: Input/Output, Memory, Datapath, Control. Datapath &amp; Control are sometimes combined as the CPU. Examples of I/O Devices: LCD displays (with frame buffers as pixels), touchscreens, etc.\nMemory is built from DRAM (Dynamic Random Access Memory) (↔ Sequential random access memory, which is old technology). Memory Heirarchy: Main Memory (DRAM) &gt; Seconday Memory (magnetic disks, flash memory).\nInstruction Set Architecture (=architecture) is the information developers need to develop a proper binary machine language program, including I/O, memory layout, etc. ABI (Application Binary Interface) is the combination of the instruction set and the OS interface.\n\nTransistor: a electric switch\nVISI (Very Large-Scale Integrated Circuit): ICs with millions and billions of trasnsistors\n(Moore’s law predicts the geometric increase in the number of transistors per area.)\n\nManufacturing process of ICs\nSilcon Ingot ⇒ Wafers ⇒ (processing steps) ⇒ Patterned Wafers ⇒ Diced Wafers (Dies) ⇒ Packaged Dies (patterned wafers, diced wafers and packaged wafers are each tested.)\n\nDefects can occur in each testing process. Yield measures the percentage of good dies from total # of dies on wafer.\nCost of manufactoring, due to defects, is generally proportional to the square of each die area. Which means it’s impractical to design very big dies.\n\nParallelism §\nImprovement of response time fo CPUs began to slow around 2002. Since then cpu architects started to use multicore processors to augment performance. But since programmers had to rewrite programs for them it was hard to adopt—due to the overhead of communication &amp; synchronization.\n1.9. Real Stuff: Benchmarking the Intel Core I7 §\nSPEC (System Performance Evaluation Cooperative) released standard benmarks for common programs. Types: integer benchmark / floating point benchmark. They give a score called a SPECratio per program which is calculated by: SPECratio=execution timereference time​ where reference time is from a reference processor and execution time is from the processor being benchmarked. The higher the better. Overall SPECratio of the CPU is a geometric mean of all programs’ SPECratios.\nSPEC also released a SPECpower power consumption benchmark. It measures average power consumption and ssj_ops (server side Java operations per second), at load increments from 10%, 20%, …, 100%. The final number given is:\noverall ssj_ops per watt=∑i=010​poweri​∑i=110​ssj_opsi​​"},"Ackerman-Function":{"title":"Ackerman Function","links":["Limits-of-Math-and-Computing"],"tags":["Computing/Algorithms","Math"],"content":"def. Ackerman Function. The Ackerman function satisfies the following recurrence relations:\n\nA(k,n)=A(k−1,A(k,n−1))\nA(k,1)=2\nA(1,n)=2n\n\n\nExample:\n\nA(2,n)=A(1,A(2,n−1))=⋯=A(1,A(1,…A(2,1)))=2n\nA(3,n)=2↑n\n\n\n\nProperties §\n\nGrows really fast\nAppears in Limits of Math and Computing\n"},"Agents-of-the-Macro-Economy":{"title":"Agents of the Macro Economy","links":["Profit-Function","Cobb-Douglas-Utility"],"tags":["Economics/Macro-Economics"],"content":"Agents §\n\n\n                  \n                  We’re doing static analysis; i.e. it’s very LR or very SR. \n                  \n                \n\nNumerous agents of each sector are grouped into a representative agent.\n\n\nRepresentative household\nObjective: maximize utility over consumption (C) and Leisure (L) within the utility function\n→ Labor Supply SN and # of laborers N\n→ Goods Demanded D\n\n\nRepresentative Firm\nObjective: maximuze profit (π) over capital (K) and Labor (N) within the production function\n→ Labor Demand DN\n→ Goods Supplied S\n\n\nGovernment\nRole: collect taxes and pay transfers (NT); spend remaining on the goods market G\n\n\nHousehold Optimization §\n\nFor households the price of consumption goods and the price of leisure [=wage] is an exogenous variable; i.e. it cannot be controlled\nHouseholds have a time endownment of h which they can use for leisure (L) or consumption (C)\n\nThe Household Utility Function u(C,L)\nAssumptions:\n\n∂C∂u​,∂L∂u​&gt;0 …i.e. More consumption/leisure is always better\n∂C2∂2u​,∂K2∂2u​&lt;0\n…i.e. the Marginal Utility always diminishes\n∃∂C∂u​\n[= utility function is differentiable] …i.e. C,L are somewhat substitutable\n→ MRS=∂u/∂L∂u/∂C​∣uˉ​\n\nHousehold Budget Constraint\nwN+π−NT=C\n\nw is the representative wage\nN is the hours of labor a HH chooses\nπ is profit or dividends if the HH is also an owner of a firm\nNT is the net transfers (+transfers-taxes)\nC is consumption (in units: # of goods)\n\n\nFirm Optimization §\n\nThe representative firm employs Factors of Production (K,N) to produce YS=f(K,N) output\nProfit of the firm is π=f(K,N)−wN−rK\nThe firm is the average firm of the whole economy, which is unlike a microecon firm in many ways\n\nThe Firm Production Function f(z,K,N)\nAssumptions:\n\nConstant Returns to Scale (M&amp;A is useless)\nxλf(z,K,N)=f(z,λK,λN)\n→ This is probable, since output as a whole in general has CRS\nMore capital or more labor always means more output\n∂K∂Y​,∂N∂Y​&gt;0, equivalently MPK​,MPN​&gt;0\n→ Probable; throw another pencil into the economy, it’ll produce more\nDiminishing Marginal Product\n∂K2∂2Y​,∂K2∂2Y​&lt;0\nMarginal product of capital increase with more labor, and vice versa\n∂K∂N∂Y​&gt;0 [= positive cross-derivative]\n\nProfit Function\n∂K∂π​=∂K∂f​−r=0\n∂N∂π​=∂N∂f​−w=0\n↑ Must satisfy both conditions\n\n⇒ The Cobb-Douglas Production Function satisfies all these assumptions:\nf(z,K,N)=zKαN1−α where  0&lt;α&lt;1\n\nProfit π=zKαN1−α−wN−rK\nCapital Demand, ∂K∂π​=0 at z⋅α(NK​)α−1=r\nLabor Demand, ∂N∂π​=0 at z⋅(1−α)(NK​)α=w\n\nGovernment Plans §\nThe government’s goals are more vague, threefold:\n\nAllocative: resources are used in certain proportions\nDistributive: income, wealth is distributed at tolerable equalities\nStabilization: business cycle is smoothed out\n\nAutomatic stabilizers; e.g. unemployment benefits\ngovernment purchases (G)\n\n\n"},"Algorithm-Problem-Tips":{"title":"Algorithm Problem Tips","links":["Scratchpad","Problem-Sets","Python-Common-Operations","Time-Complexity","Priority-Queue","Hash-Table","Dynamic-Programming","Sliding-Window-Technique"],"tags":["Computing/Algorithms","Computing/Data-Structures","Meta-Learning"],"content":"During Practice §\n\nThere are only two ways to store data structures: as arrays (sequential storage) or as linked lists (linked storage).\n\n\nFirst of all, it should be clear that data structures are tools, and algorithms are methods to solve specific problems with appropriate tools.\n\n\nUse a plain-text editor as Scratchpad\nRecord new techniques/algorithms\nIt’s okay to look at solutions after a certain period of trying.\nUse syntactic sugar &amp; libraries. Python has lots.\nComment above every line while coding\n\nDuring Testing §\n\nBrute Force → Better Data Structures/Algorithms → Even better time complexity\n\nAlways ask: can reduce Time Complexity even further?\n\nExponential is never good. Polynomial is not enough. Linear time!\n\n\nPriority Queue / Hashmap / Dynamic Programming / Sliding Window Technique\nTry thinking: What property doesn’t change? What variable is constant?\nTry thinking: Invert the search; find the contrapositive\n\n\nAre you {python}returning?\nEdge cases\n\ninfinite loops are probably not it\n⇒ make the while loops conditioned on index boundaries\n\n\nTest the program yourself on a text editor\nTry to defeat the problem (find very weird test inputs)\n"},"Algorithm":{"title":"Algorithm","links":["Complexity","Mathematical-Proof","Mathematical-Induction","Proof-by-contradiction","Testing-Principle"],"tags":["Computing","Math"],"content":"Loose definition\n\n\n                  \n                  Note \n                  \n                \nAn algorithm…\n\nfinishes in finite number of steps\nits answer is consistent\nit always gives the correct answer\n\n\nAlgorithm Design Principles §\n\nDesign\n\nWhat to compute\nHow it computes what we want it to compute\n\n\nCorrectness\n\nWhy does it compute what we want it to compute\n\n\nAnalysis &amp; Efficiency\n\nHow much resources does it use? (Complexity)\n\n\n\nMost algorithms follow the following two standard models of computation\n\nRecursion\nIteration\n\nVerifying Correctness §\n\n\n                  \n                  Warning \n                  \n                \nThere is no way to find out if an algorithm is correct.\n\n\nMathematical Proof\n\nDirect Proof\n\nMathematical Induction\n\n⇒ Recursive algorithms looks similar to proof by induction\n\n\n\n\nIndirect Proof [=Proof by contradiction]\n\n\nTest experimentally\n"},"Alienation":{"title":"Alienation","links":["Proletatriat-(Marxism)","Alienation","Species-being"],"tags":["Humanities/Marxism"],"content":"Commoditization of human labor §\n\nTrading labor\n\n\nDifferences of age and sex have no longer any distinctive social validity for the working class. All are instruments of labour, more or less expensive to use, according to their age and sex.\n\n⇒ The wage-laborer is homogenized into a single indistinguishable set\nEstrangement/Alienation §\n\nAlienation of the Commodity, the production\n\n\nIt is the same in religion. The more man puts into God, the less he retains within himself. The worker places his life in the object; but now it no longer belongs to him, but to the object. […] not only that his labour becomes an object, an external existence, but that it exists outside him, independently of him and alien to him, and begins to confront him as an autonomous power; that the life which he has bestowed on the object confronts him as hostile and alien.\n\n\nAlienation of Human Creativity\n\n\nSo if the product of labour is alienation, production itself must be active alienation, the alienation of activity, the activity of alienation.\n\n\nHence the worker feels him­ self only when he is not working; when he is working he does not feel himself. […] It is therefore not the satisfaction of a need but a mere means to satisfy needs outside itself. Its alien character is clearly demonstrated by the fact that as soon as no physical or other compulsion exists it is shunned like the plague.\n\n\nJust as in religion the spontaneous activity of the human imagination, the human brain and the human heart detaches itself from the individual and reappears as the alien activity of a god or of a devil, so the activity of the worker is not his own spontaneous activity. It belongs to another, it is a loss of his self.\n\n⇒ Thus ultimately alienation from the Species-being"},"Amartya-Sen":{"title":"Amartya Sen","links":["Human-Development-Index","(Article)-Equality-of-What-q","Martha-Nussbaum"],"tags":["People"],"content":"\nDeveloper of the Human Development Index\nAuthor of (Article) Equality of What?\nFrequent interactions with Martha Nussbaum\n"},"Anarcho-Syndicalism":{"title":"Anarcho-Syndicalism","links":["Noam-Chomsky","Decentralization","Capital-(Marxism)","Karl-Marx"],"tags":["Philosophy/Political-Philosophy","Economics/Game-Theory"],"content":"Noam Chomsky\n\nAnarcho-: Decentralization of power\nSyndicalism: Worker’s union’s ownership of Means of Production\nA realistic alternative to Marxism\n"},"Annuity":{"title":"Annuity","links":["Proof.png","Future-Value-Calculations"],"tags":["Economics/Finance"],"content":"Annuities are…\n\nMoney you borrowed: mortgages, etc. ⇒ pay regularly to pay off large amount\nMoney you lent: social security credit, etc. ⇒ give large money, get regular payments later\n\nAmortization is the same thing but from a different perspective\n⇒ You’re spreading a one-time cost into multiple years. Formulae are same if the recurring payments are of the same amount.\nTypes of Annuities\n\nOrdinary Annuity: payment occurs at the end of each period\nSimple Ordinary Annuity:\nCurrent balance=Prev. balance+Interest on previous balance−Payment\n\nFormulae for Constant Payment P §\n\nn: Term; time from first payment to last payment (in years)\nAn​: Present value of annuity of n terms.\nSi​: Total payment accrued after i periods (not a present value!)\ny=1+kr​ for formula simplification\n\n\nPreset value of annuity (=An​) formula. (see below for more)\n\n(i.e. the principal loan amount, mortgage loan amount, etc.)\n\n\n\nAn​:=(1+r/k)P​+(1+r/k)2P​+…(1+r/k)nP​\n\nPayment accrued after all n periods (=Sn​ =Future value of annuity after all payments made)\n(Demonstration, use geometric sum formula)\n\nSn​​=P+(1+kr​)P+(1+kr​)2P+⋯+(1+kr​)n−1⋅P=r/k(1+r/k)n−1​⋅P=y−1yn−1​⋅P​…(0)​​\n\nSimplified formula of PV using 1. and 2.\n\nAn​​=(1+r/k)nSn​​=r/k1−(1+r/k)−n​⋅P=y−11−y−n​⋅P​time valuefrom (0)​​\n\nRemaining balance after i periods (=Bi​)\n\nBi​​=(1+kr​)n−1(1+kr​)n−(1+kr​)i​An​=yn−1yn−yi​An​​​​\nPortion of the i-th payment Pi​ that goes to…\nPrincipal\nBi​=kr​yn−1yi−1​An​\n\nTotal Principal ⇒ ∑i=1n​Bi​=An​\n\nInterest\n\nEvery period, interest is fully paid.\nInterest is applied to the remaining balance B (not to be confused with B)\n\nIi​=kr​Bi−1​=kr​yn−1yn−yi−1​An​\n\nTotal Interest ⇒ ∑i=1n​Ii​=nP−An​\n\n⇒ Thus the total payment: ∑i=1n​Pi​=nP\n\nConsider k-periodic compounding (=k times every year)\nCost of Loan: ignoring the time value of money, sum of all interest payments nP−An​\nAs montly payment P increases, number of payment periods n decays exponentially.\n\n\nFormulae for Variable Interest Rates §\nVariables are same as above, except:\n\nPi​: payment after period i\nri​: interest rate during period i\n\n\nPresent value of annuity (An​)\n\nAn​​=l=1∑n​∏j=1l​(1+krj​​)Pl​​=∏j=1n​(1+krj​​)Sn​​​​"},"Approximating-Distributions":{"title":"Approximating Distributions","links":["Normal-Distribution","Poisson-Distribution"],"tags":["Math/Probability"],"content":"thm. A Normal Distribution with μ=np,σ=np(1−p)​ will approximate a Binomial distribution with n trials and success probability p when np&gt;1\nP(a≤X≤b)=k=a∑b​P(X=k)≃∫ab​fnormal​(μ=np,σ=np(1−p)​)dx\nREMARK. A Normal Distribution is equivalent to the following standard normal distribution:\n∫a−0.5b+0.5​Normal(μ,σ)dx=∫a+0.5b+0.5​σ2π​e−21​(σx−μ​)2​dx=∫σa−0.5−μ​σb+0.5−μ​​Normal(0,1)du=∫σa−0.5−μ​σb+0.5−μ​​2π​e−x2/2​dx\nwhere the factor of +0.5 to the integrand’s range is the continuity correction.\nthm. A Poisson Distribution with parameters λ=np will approximate a Binomial distribution with n trials and success probability p when np≤1.\nX∼B(n,p)⇒P(X=k)=k!λke−λ​=k!(np)ke−(np)​\nthm. A Binomial Distrubition with parameters p=NK​ will approxmiate a Hypergeometric Distribution when N&gt;&gt;n\nX∼Binomial(n,p)⇓P(X=k)≃(kn​)(NK​)k(NN−K​)n−k​​\nx1​×x2​∫f(x1​,x2​)dx"},"Approximation-Algorithm":{"title":"Approximation Algorithm","links":[],"tags":["Computing/Algorithms"],"content":"def. Approximation Factor. α(n)-level approximation. For optimization problem π, ALG is a α-level optimization algorithm iff for input I of size n:\nmax[OPT(I)ALG(I)​,ALG(I)OPT(I)​]=α(n)\n\nSmaller is better!\nConstant approximation factor: if approximation factor doesn’t depend on n, ALG is a constant α-level approximation\n"},"Artificial-Needs":{"title":"Artificial Needs","links":["private-property","abstraction"],"tags":["Humanities/Marxism"],"content":"Normally it’s human nature from which arises naturally the needs, and capital should be appropriated for\n\nUnder the system of private property their significance is reversed. Each person speculates on creating a new need in the other, with the aim of forcing him to make a new sacrifice, placing him in a new dependence and seducing him into a new kind of enjoyment and hence into economic ruin.\n\n\nMan becomes ever poorer as a man, and needs ever more money if he is to achieve mastery over the hostile being.\n\nand this extends to all aspects of human life\n\nMoreover, the worker has no more than a precarious right to live in it, for it is for him an alien power that can be daily withdrawn and from which, should he fail to pay, he can be evicted at any time. He actually has to pay for this mortuary.\n\n…ultimately our lives are deprived of natural human needs\n\nworker. Light, air, etc. - the simplest animal cleanliness - ceases to be a need for man. […] Dirt - this pollution and putrefaction of man, the sewage of civilization - becomes an element o f life for him.\n\n…and captial even justified this:\n\nThe fact that the multiplication of needs and of the means of fulfilling them gives rise to a lack of needs and of means is proved by the political economist:\n\n\nBy reducing the worker’s needs to the paltriest minimum necessary to maintain his physical existence and by reducing his activity to the most abstract mechanical movement.\nBy taking as his standard - his universal standard, in the sense that it applies to the mass of men - the worst possible state of privation which life (existence) can know.\n\n\n\n\n…and ultimately “saving” or “not feeling” is now a virtue:\n\nPolitical economy, this science of wealth, is therefore at the same time the science of denial, of starvation, of saving, and it actually goes so far as to save man the need for fresh air or physical exercise.\n\n\nThe less you eat, drink, buy books, go to the theatre, go dancing, go drinking, think, love, theorize, sing, paint, fence, etc., the more you save and the greater will become that treasure which neither moths nor maggots can consume - your capital The less you are, the less you give expression to your life, the more you have, the greater is your alienated life and the more you store up of your estranged life.\n\nMoney, Specifically §\n\nincreases. The need for money is for that reason the real need created by the modem economic system, and the only need it creates.\n\nabstraction above the thing:\n\nJust as it reduces everything to its own form of abstraction, so it reduces itself in the course of its own movement to something quantitative.\n\nand thus this requires its production\n\nthis is manifested partly in the fact that the expansion of production and needs becomes the inventive and ever calculating slave of inhuman, refined, unnatural and imaginary appetites - for private property does not know how to transform crude need into human need. Its idealism is fantasy, caprice and infatuation.\n"},"Ascending-Auction":{"title":"Ascending Auction","links":["Combinatorial-Auction"],"tags":["Economics/Game-Theory"],"content":"Motivation. VCG Auction is DSIC and Welfare-maximizing. But it’s also very complicated to implement, since you have to ask for all valuations vi​ for all subsets of items X.\nInstead, let us focus on simpler, one-item-per-person matching situations."},"Assets-(Finance)":{"title":"Assets (Finance)","links":["Equity","Commodities","Derivatives-(Finance)","Futures","Real-Estate"],"tags":["Economics/Finance"],"content":"Assets are anything that’s tradable.\n\nEquity\nCommodities\nDerivatives (Finance)\n\nFutures\n\n\nReal Estate\n\n\n\n                  \n                  Note \n                  \n                \nA debt for one person is an asset for the lender.\n"},"Atomic":{"title":"Atomic","links":["(Article)-Evergreen-notes"],"tags":["Computing","Meta-Learning"],"content":"Atomicity means indivisibility. Useful in a variety of contexts:\n\nProgramming &amp; Concurrency. atomic operations cannot have any operation happen between it during the operation i.e. it cannot be paused until completion\n(Article) Evergreen notes should be atomic for better, densely connected notes.\n"},"Atsugi":{"title":"Atsugi","links":[],"tags":["Computing"],"content":"AWS John the Ripper Instance §\nKey name is `ssh-key`\nLogin as User `ec2-user` \nHamonikr Vs ff.sh §\nSetting a custom resolution via cmdline\nChanging cinnamon desktop wallpaper from the terminal\nAtsugi Linode VM §\nManagement Console\nIP: 172.104.89.111 (ports 22, 8080 open for sshd)\nDomain: atsugi.pyoky.me\nusers: root (pw: oniichan) \n\nUsing a keyboard with Astugi (Phone) is immensly beneficial.\nAtsugi Moving to Chrome §\nenv LANGUAGE=en_US /usr/bin/chromium-browser --password-store=basic\nConfig is in ~/.config/chromium (This is the one you have to zip.)"},"Attention-is-Currency":{"title":"Attention is Currency","links":["Nudging","Stream-of-Content"],"tags":["Computing/Internet"],"content":"Your attention is the currency of the modern, corporate internet.\n\nIt is valued, monetized, and traded (as with your personal information)\nChoose how to give attention\nBuild in Friction to keep yourself from being consumed by the wealth of information.\nThere is always a Stream of Content. Choose what is important and stands out. Quality &gt; Quantity.\n"},"Auction-Theory":{"title":"Auction Theory","links":[],"tags":["Economics/Game-Theory"],"content":"def. Sealed Bid Auction. An auction is a game where there are a certain number of buyers, buyers each have their value, vi​ and bid, bi​. The auction process takes in the bids of all buyers b, and outputs the allocation x(b) and price p​(b).\n\nx=⟨x1​,…,xi​,…,xn​⟩ where xi​ is the allocation for buyer i\np​=⟨p1​,…,pi​,…,pn​⟩ where pi​ is the price charged for buyer i\ne.g. if there is one item to be sold, and the i-th player gets it, then:\n\nx=⟨0,…,0,1,0,…,0⟩\n\\vec{p}= \\langle 0,\\dots,0,\\9.37,0,\\dots0 \\rangle$\n\n\nNobody except i knowns their true value: vi​\n\ndef. Quasilinear Utility Model. Each bidder, after the auction process finishes, has utility and welfare:\nutilityμi​(b)​​=welfare=valuevi​xi​(b)​​−pi​(b)\nProperties of an Auction §\nMotivation. An auction’s goal is to:\n\nMake sure each person tells the truth, i.e. vi​=bi​\nMaximize total utility, i.e. ∑μi​\nTherefore, we want an auction that will satisfy the following two properties.\n\ndef. Dominant Strategy Incentive Compatibility (DISC). Intuition: all bidders are incentivized to tell the truth. An auction is DISC iff:\n∀i,∀b,regardless of b−i​,utility for telling truthμi​(vi​,b−i​)​​≥utility for lyingμi​(bi​,b−i​)​​\ndef. Welfare Maximizing (=Socially Efficient). An auction is welfare-maximizing iff Given all bidders are truthful, then it maximizes ∑∀i​vi​xi​(b)\nTypes of Auction Games §\nMotivation. A first-price auction (=give the highest bidder the item, then charge them their bid) is a bad idea because, assuming the highest bidder was truthful (vi​=bi​) then their utility is vi​−bi​=0. Thus this is not DISC and not a good auction. It’s also very hard to reason about. Therefore we have…\ngame. Second Price Auction. Give to the highest bidder; charge them the second highest price.\n\nAn ascending bid auction (=an e-bay auction) is actually a second price auction if you think about it. The last two bidders (v1​,b1​)≥(v2​,b2​) will increase their bids continually until they reach b2​, and then bidder 1 bids just slightly above (b1​=v2​+δ) to get the item.\n\nthm. Second Price Auction is Dominant Strategy Incentive Compatible.\nproof. There are only two cases where a bidder may lie.\n\nCase one: a bidder lies to reduce the price of the item. But pi​(b) does not depend on v. Thus this is impossible\nCase two: a bidder lies to get the item. But lying to get the item results in negative utility. ◆\n\ngame. Sponsored Search Auction. This is a different game layout. Imagine a search engine selling advertisement slots.\n\n\nThere are k ad slots, with click-thru rate, largest to smallest: α1​,…,αk​\nThere are n bidders, for whom the value-per-click is v1​,…,vn​. Each bids bi​ to get any slot.\nThen, if bidder i is assigned slot l and charged pi​(b) \n\nAllocation matrix is x and xil​=1\nprice matrix is p and pil​ is the price charged\n\n\n\nthm. Myerson’s Lemma. A DSIC (=truthful) auction satisfies the following properties.\n\nxi​(bi​) is a monotonically increasing function\nGiven b−i​, the optimal price pi​(bi​)=bi​xi​(bi​)−∫0bi​​xi​(z)dz\nThe implication goes both ways.\n\nProof.\nProperty 1. Allocation is monotonic w.r.t. bid amount. (=bid more, get more)\nproof. By definition of DSIC, both of the following holds:\n\nreports bi​, actual value vi​: vi​xi​(vi​)−pi​(vi​)≥vi​xi​(bi​)−pi​(bi​) ⇒ vi​[xi​(vi​)−xi​(bi​)]≥pi​(vi​)−pi​(bi​)\nreports vi​, actual value bi​: bi​xi​(bi​)−pi​(bi​)≥bi​xi​(vi​)−pi​(vi​) ⇒ pi​(vi​)−pi​(bi​)≥bi​[xi​(vi​)−xi​(bi​)]\n\nThis is an argument from the fact that for any b,v DSIC guara ntees it’s the best possible\nFrom the two inequalities we get\n\n\n\nvi​[xi​(vi​)−xi​(bi​)]≥⋯≥bi​[xi​(vi​)−xi​(bi​)]​​\nThus (vi​−bi​)[xi​(vi​)−xi​(bi​)]≥0 ■\nIntuition. vi​−bi​ and xi​(vi​)−xi​(bi​) have the same signs. Which means xi​ is monotonically increasing.\n\nProperty 2. Optimal Price.\nFrom the two inequalities in property 1 we get\nvi​[xi​(vi​)−xi​(bi​)]≥pi​(vi​)−pi​(bi​)≥bi​[xi​(vi​)−xi​(bi​)]\nWe take the limit by pushing vi​,bi​→v\nvxi′​(v)≥pi′​(v)≥vxi′​(v)\nThus vxi′​(v)=pi′​(v). And integrating both sides\nvxi′​(v)=pi′​(v)​⟹∫0v​z⋅xi′​(z)dz=∫0v​pi′​(z)dx⟹∫0v​zd(xi​(z))=pi​(v)⟹pi​(v)=v∫0v​1d(xi​(z))−∫0v​1⋅xi​(z)dz⟹pi​(v)=vxi​(v)−∫0v​xi​(z)dz​substitute zintegration by parts​​\n■\nIntuition. The geometric intuition for this is that the price for the allocated person is the rectangle minus the area under the graph, i.e. the blue region.\n"},"BEM-Method":{"title":"BEM Method","links":[],"tags":["Computing"],"content":"…for naming CSS selectors.\nblock__name-of-content__status--on { ; }\n// e.g.\ndiv-input__first-name__input--enabled { ; }\nhttps://www.devbridge.com/articles/implementing-clean-css-bem-method/"},"Balance-Sheet":{"title":"Balance Sheet","links":[],"tags":["Economics/Finance"],"content":"The Balance Sheet is a table showing all the firm’s assets and liabilities.\nItems on a balance sheet:\n\n\nCash &amp; Equivalents: highly liquid assets on hand by a firm\nRecieveables &amp; Payables: unofficial debts, e.g., regular grocery purchases, etc.\nInventories: Items not sold\nProperty, Plant, &amp; Equipment (PP&amp;E), i.e. Durable capital assdets\nStockholder’s Equity: money recieved by purchasers of stock\n\nProperties.\n\nAssets and liabilities must always sum to zero.\ncounts book value, not market value ⇒ hard to measure the “value” of a firm\nsoft skills (e.g. brand image, reputation) are not reflected (the market may better reflect intangibles).\n"},"Balance-of-Payments":{"title":"Balance of Payments","links":[],"tags":["Economics/Macro-Economics"],"content":"Trade balance"},"Banker's-Rule":{"title":"Banker's Rule","links":[],"tags":["Economics/Finance"],"content":"\n“Exact Time”: number of days from t0​\n“Ordinary interest”: 1 year=360 days,1 month=30 days\n"},"Bankrolling":{"title":"Bankrolling","links":[],"tags":["Economics/Finance"],"content":"\nBankrolling refers to the act of providing financial support or funding to a person, organization, or project. It involves supplying the necessary monetary resources to enable the individual or entity to carry out their activities or achieve their goals. Bankrolling often implies providing substantial financial backing or resources to ensure the success or sustainability of the supported venture.\n"},"Base-&-Superstructure":{"title":"Base & Superstructure","links":["Phenomenology"],"tags":["Humanities/Marxism"],"content":"Phenomenology"},"Basis-Points":{"title":"Basis Points","links":[],"tags":["Economics/Finance"],"content":"100bp=1%"},"Beige-Book":{"title":"Beige Book","links":["Federal-Reserve"],"tags":["Economics/Finance"],"content":"The Beige Book is a report published by the Federal Reserve that provides qualitative information on current economic conditions across the 12 Federal Reserve Districts. It is published eight times per year and is used by policymakers, economists, and investors to gain insights into the state of the economy. The report relies on information gathered from various sources, including surveys of businesses and interviews with key contacts in each district."},"Bernouilli-Distribution":{"title":"Bernouilli Distribution","links":[],"tags":["Math/Statistics"],"content":"\n\n                  \n                  Note \n                  \n                \nAn experiment with two outcomes, success and failure, with probability p and (1−p) respectively.\n\n\nE(X)=p\nVar(X)=pq\n"},"Bertrand-Price-Competition":{"title":"Bertrand Price Competition","links":[],"tags":["Economics/Micro-Economics"],"content":"Firms will choose price as the strategic variable.\n\nMarket Demand x(p)=A−bp where A,b are constants\nSimultaneous game\nFirm with lower price captures all of demand\n\nCase: Constant and Same Marginal Cost §\n\nMarginal Cost is constant for both firms: c(x)=cx, MC=c\nBoth firms will attempt to undercut each other.\n\nBRC1​:p1​(p2​)=max(p2​−ϵ,MC)\nBRC2​:p2​(p1​)=max(p1​−ϵ,MC)\nNE={p1​=c,p2​=c} where both firms have zero profit\n\n\n\n\nCase: Different but Constant Marginal Cost §\n\nMarginal Cost is constant, but different for both firms\n\nc1​(x)=c1​x, c2​(x)=c2​x, c1​&lt;c2​\n\n\nThen…\n\nBRC1​:p1​(p2​)=max(p2​−ϵ,c1​)\nBRC2​:p2​(p1​)=max(p1​−ϵ,c2​)\nNE={p1​=c2​−ϵ,p2​=c2​}\n\n&amp; firm 1 can set undercut the price because they still have c1​&lt;c2​−ϵ\n\n\nFirm 1 will make positive profit; firm 2 will make zero profit.\n\n\n\nCase: Sequential Move §\n\nChange assumption: Dynamic game; firm 1 sets price first, then firm 2 sets price. (c1​&lt;c2​)\n\nThen, there cannot be a best response curve. Instead, consider the following subgame perfect Nash equilibirum:\n\n\n\n12​:p1​=c2​−ϵ:p2​={min(p2M​,p2​−ϵ)c2​​if c2​&lt;p1​if c2​≥p1​​(can undercut price)(can’t b/c MC)​​​\n- The following is a non-subgame perfect Nash equilibirum. The non-credible threat is when firm 2 threatens &quot;I&#039;m just gonna lose money if you don&#039;t listen.&quot;. Given a constant $k$, \n\n12​:p1​=p2M​+k:p2​={p2M​0​if p1​ sets what I wantif p1​ isn’t what I want​​​\n\nDifferent order: Dynamic game; firm 2 sets price first, then firm 1 sets price. (c1​&lt;c2​ still assumed)\n\nThen, SPNE\n\n\n\n21​:p2​=c2​:p1​(p2​)={min(p1M​,p2​−ϵ)c1​​if p2​&gt;c1​else p2​&lt;c1​​​​\nIf Product Can Be Differentiated §\nWhen we assume that products can be differentiated (=diversified), then we can improve on the Bertrand model;.\n\nConsumers’ preferences are uniformly distributed across differentiation space R\nFirms can produce a product with a certain value P of differentiated characteristic P∈R\nA consumer with preference C will pay a cost when consuming P which is proportional to the distance Cost=∣P−C∣\n\n\nAssume there exists a possibility of differentiation space (∃R∈[0,1])\nHowever firms are not allowed change product characteristics.\nThe only strategic variable is price.\n\n\n\nFirst consider firm 1’s BRC:\n\nIf firm 1 gives away products for free p1​=0\n\n…firm 2 can still charge a price since some consumers will prefer their products\n…and p2​&gt;MC because firm 1 will exit otherwise.\n\n\nIf firm 2 charges a price p1​&gt;0\n\n…firm 2 can charge an even higher price since some consumers will prefer their products\n\n\nTherefore the BRC is a positively sloping curve\n\n\nFirm 2’s BRC is symmetrical:\n\n→ Equilibrium is at the intersection of the two BRCs, and at this point p1∗​=p2∗​.\n→ Equilibirum price is above marginal cost, unlike pure Bertrand Price Competition\n\n\n\n\n\n                  \n                  Info \n                  \n                \n\nDifferentiation “softens” price competition. Therefore firms want more differentiation\nDetermining Firm 2’s Best Response Curve: §\n\n\nwhen firm 1’s price &lt; MC, firm 2’s best response is set price at MC\nwhen firm 2’s price &gt; MC, firm 2’s best response is the undercut firm 1’s price by a very small amount (ϵ)\n\n\nFirm 2’s BRC is symmetric\n→ Firms’ prices will unravel to both pricing to p = MC (Bertrand price = MC)\n→ At p=MC, market will purchase 2xM, and firms produce xM each.\n\np_{1}&amp;=max[MC_{2}-\\epsilon,MC_{1}] \\\\\np_{2}&amp;=max[MC_{1}-\\epsilon,MC_{2}]\n\\end{align}\nHotelling Model §\n\nProducts can be differentiated by one characteristic variable which ranges from 0 to 1.\nDifferentiation is the only strategic variable; i.e. price, etc. cannot be changed.\n\n\n\n\n                  \n                  Tip \n                  \n                \nThe graph and its corresponding explaination in the textbook is wrong; the following is the correct explaination.\n\nThinking about Firm 1’s BRC:\n\nwhen firm 2 produces a product with characteristic &lt; 0.5, then firm 1 will produce a product that is just closer to 0.5 [=larger] to capture more consumers\nwhen firm 2 produces a product with characteristic &gt; 0.5, then firm 1 will produce a product that is just closer to 0.5 [=smaller] to capture more consumers\nwhen firm 2 produces a product with characteristic = 0.5, then firm 1 will produce a product with characteristic = 0.5.\n\n\n\n                  \n                  Info \n                  \n                \nIn a single axis (1-dim. differentiation space)\n\n\n→ Firm 2’s BRC is symmetrical; this unravels to the equilibrium where both firms produce product with characteristic = 0.5\nCircle Model §\nThe circle model combines the Price Competition and Differentiation.\n\nThis is a sequential game where firms will enter [first move] and then it will compete with price [second move]\n\nFirms who enter incurs entry cost (an economic cost) of FC when they enter.\n\n\nPrice and differentiation are the two strategic variables.\n\nProduct differentiation space is around a circle whose circumference is 1.\n\n\n\nThen the following conclusions:\n\n\n                  \n                  Info \n                  \n                \n\nEach firm is respresented as a point on the circle.\n\n\nFirms will enter as long as FC&gt;π …①\nFirms compete with their immediate neighbors\n→ Firms will space themselves out around the circle\n\nThe number of firms entering N and the equilibrium price given that N firms have entered p are determined in the following order:\n\nFC↦N∗ where N∝FC1​ (∵ ①)\nN∗↦p∗ where p∝N1​ but p&gt;MC. Specifically:\n\n"},"Besset-Correction":{"title":"Besset Correction","links":["Bias-(Statistics)"],"tags":["Math/Statistics"],"content":"Review and intuition why we divide by n-1 for the unbiased sample | Khan Academy - YouTube\nNaiive sample variance estimator:\ns^2=n∑i=1n​(xi​−xˉ)​\n⇒ Sample variance is biased.\nBesset Corrected sample variance estimator:\ns^2=n−1∑i=1n​(xi​−xˉ)​\n\nLarger than the naiive sample estimator\n"},"Beveridge-Curve":{"title":"Beveridge Curve","links":["Unemployment"],"tags":["Economics/Macro-Economics"],"content":"Beveridge Curve §\n:= description of the inverse correlation between Unemployment and job openings\n\n\n\nIt may shift due to: increased in unemployment benefits, participation rate changes, etc.\nBeveridge curve\n\nThe curve, named after William Beveridge, is hyperbolic-shaped and slopes downward, as a higher rate of unemployment normally occurs with a lower rate of vacancies.\n\n\n\nThe more inwards, the more efficient the labor market is.\n\ni.e. the matchmaking between employer and employee is done well.\n\n\n"},"Bias-(Statistics)":{"title":"Bias (Statistics)","links":[],"tags":["Math/Statistics"],"content":"\n\n                  \n                  Note \n                  \n                \nBias is the difference between the expected value [=mean] of an estimator and the ground truth.\n\nBias[θ^]=E[θ^]−θ0​\nthm. Linear Transformation Preserves Bias. If θ^ is unbiased, g(θ^) is unbiased if g is a linear transformation.\n→ For estimators without bias, we can compute the precision to evaluate how good it is."},"Big-Oh-Notation":{"title":"Big-Oh Notation","links":[],"tags":["Math/Calculus","Computing/Algorithms"],"content":"Alternative formulation of parts of calculus.\ndef. Big-Oh Notation. for functions f,g, we say f(n)=O(g(n)) iff:\n\nDefinition 1:\n\n∃c∈R+ ∀n∈N+,f(n)≤c⋅g(n)\n\nDefinition 2:\n\nn→∞lim​sup(f(n))&lt;∞\n\n⇒ Intuitive Understanding: f=O(g)≈f&lt;g\n\ndef. Big-Omega Notation. For functions f,g we say f=Ω(g) iff g=O(f)\ndef. Big-Theta Notation\nf=O(g) and f=Ω(g)⟹f=Θ(g)"},"Binomial-Distribution":{"title":"Binomial Distribution","links":[],"tags":["Math/Common-Distributions"],"content":"Binomial Distribution §\ndef. Binomial Distribution. For random variable X which denotes the number of successes within n trials with success probability p:\nXP(X=k)​∼Binom(n,p)=(kn​)⋅pk⋅(1−p)n−k​\n\nE[X]=np\nVar[X]=n⋅p(1−p)\n"},"Binomial-Theorem":{"title":"Binomial Theorem","links":[],"tags":["Math"],"content":"(x+y)n=k=0∑n​(kn​)xn−kyk=k=0∑n​(kn​)xkyn−k."},"Binomial-Tree-Model-of-Security-Pricing":{"title":"Binomial Tree Model of Security Pricing","links":["Bernouilli-Distribution","Binomial-Distribution","Dividend-Discount-Model","Central-Limit-Theorem"],"tags":["Economics/Finance"],"content":"\nModel Definition §\n\nConsider time interval [t0​,tf​]\n\ntotal time τ=tf​−t0​\nn intervals\nhn​: duration of one interval (=ntf​−t0​​)\n\n\nSt​: price of security at time t.\n\nS0​ is given as a constant\n\n\nGross return Sj−1​Sj​​\n\nIt’s called gross because its in the form of 1.xx or 0.xx, not Sj−1​Sj​−Sj−1​​\nIs defined as a Bernouilli Distribution:\n\n\n\nSj−1​Sj​​={un​dn​​with probability pn​with probability (1−pn​)​(stock uptick)(stock downtick)​\n\nN: number of upticks. Is a random variable with Binomial Distribution N∼Binom(n,pn​)\n\nP(N=k)=(kn​) pnk​ (1−pn​)n−k\n\n\nSample Space Ωn​\n\nΩ={U,D}\nΩ2​={UU,UD,DU,DD}\nΩ3​={UUU,UUD,…,DDD}\nΩn​ is for n-period binomial tree\nPr(ω):=pnN(ω)​(1−pn​)N(ω)\n\n\nFinal Price Sn​=S0​ unk​ dnn−k​\n\nSn​ is a random variable. P(Sn​=S0​ unk​ dnn−k​)=P(N=k)\ni.e. probability that price will be equal to there being k upticks is the probability that there will be k upticks. (no shit)\n\n\nExpectation of final price: E(Sn​)=S0​(pn​un​+(1−pn​)dn​)n\nTo find number of upticks k from final price Sn​ we use\n\nk=ln(S0​dnSn​​)/ln(du​)\n\nDividends: D(tj−1​,tj​)=qS(tj−1​)hn​\n\nSee Dividend Discount Model for dividends in non-binomial tree model\nTotal Return (incl. dividends): R(tj−1​,tj​)=S(tj​)S(tj​)−S(tj−1​)+D(tj−1​,tj​)​=Rj​+qhn​\nCapital Gains Return (excl. dividends): Ri​:=S0​S1​−S0​​\n(Dividends Return qhn​)\n\n\n\nAssumptions &amp; Definitions §\n\nLog returns: Yn,j​:=ln(Sj−1​Sj​​) Is also a random variable\n\nLog normal is another indicator of how well the stock is performing\nIt is similar in value to the percentage return\nSee What does the average log-return value of a stock mean? - Personal Finance &amp; Money Stack Exchange for the intuition\n\n\nun​dn​=1 i.e. the stock ticking up then down is same as no movement at all\nInstantaneous Rate of Return\n\nm=limn→∞​hn​E[R(t0​,t1​)]​\n\n\nDrift: Instantaneous Expected Log-Return\n\nμ=limn→∞​Yn,j​=limn→∞​hn​E[lnS0​S1​​]​\n\n\nLog Variance: Instantaneous Variance of Log-Return = Volatility (σ)\n\nσ2=limn→∞​σn2​=limn→∞​hn​Var[lnS0​S1​​]​\n\n\nDividend Rate q\n\nLemmas §\n\nμn​hn​=pn​lnun​+(1−p)lndn​\nσn2​hn​=pn​(1−pn​)[lndn​un​​]2\nμ=m−q−2σ2​\n(Proofs in notes)\n\nthm. Parameter Triple. Given a security with μ,σ2,m,q we determine that:\n\nun​≈eσhn​​\ndn​≈e−σhn​​\npn​≈21​(1+σμ​hn​​)=un​−dn​e(m−q)hn​−dn​​\n\nContinuous Time Model §\nModel Definition §\nInstead of assuming an uptick-downtick gross return is a Bernouilli Distribution, we instead think of the log returns. (Use Lindenberg CLT)\nSn​(t)​=S0​exp(lnS0​Sn​​)=S0​exp(j=1∑n​lnSj−1​Sj​​)=S0​exp(Yn,j​)=S0​exp(μt+σt⋅Z)​Lindenberg CLTY∼N(nμn​hn​,nσn2​hn​)Z∼N(0,1)​​\n⇒ Thus we have\nln(S0​St​​)∼N(μt,σ2t)\nProperties\n\nS(t) is a lognormal random variable\n\ni.e. S(t)=S0​eμt+σt​Zt where Z is the standard normal random variable\n\n\nln(St−1​St​​) are i.i.d.\n"},"Bivariate-Ordinary-Least-Squares-Regression":{"title":"Bivariate Ordinary Least Squares Regression","links":["Mean-Squared-Error"],"tags":["Math/Statistics"],"content":"Motivation. Let’s say that there is a relationship between GDP per capita and life expectancy. Maybe god has declared a perfect formula describing this relationship:\nGDP Per Capita=200⋅Life Expectancy+1000+Noise\nWhile we humans may never truly know the parameters of the formula, 200 and 1000, we can still make a good guess about it. Therefore, assuming this is a linear relationship, we have the Bivariate Ordinary Least Squares Model.\nYi​=β0​+β1​Xi​+ϵi​\nwhere (X1​,Y1​),…,(Xn​,Yn​) are observations (=regressors). The OLS algorithm will minimize the squared sum of residuals:\nminβ1​^​,β0​^​​i=1∑N​ϵi​^​2\nwhere ϵi​^​:=Yi​−=Y^(β0​^​+β1​^​Xi​)​​. Note the square.\nthm. Parameter OLS Estimator for N observations (Xi​,Yi​)\nβ1​^​β0​^​​:=∑i=1N​(Xi​−Xˉ)2∑i=1N​(Xi​−Xˉ)(Yi​−Yˉ)​:=σX2​σXY​​=ρXY​σX​σY​​:=Yˉ−β1​^​Xˉ​​\nProperties.\n\nPredictor: Yi​^​=β0​^​+β1​^​Xi​\nResidual: ϵ^:=Yi​−Y^ is the estimator for the error term, i.e. how good the predictor is.\n\nN1​∑iN​ϵi​^​=0 i.e. the mean of residuals is zero in OLS algorithm.\n\n\nOLS results in assuming that X is exogenous, i.e. ρX,ϵ​=0.\n\nρ\n\n\nRegression Variance: σ^2=N−k∑i=1N​ϵi​^​2​=N−k∑i=1N​(Yi​−Yˉ)2​ where k is the number of parameters (k=2 in this case)\n\nbasically the Mean Squared Error. The lower the better.\nσ^ is also the standard error of the residuals.\nIn Stata, σ^ is called the Root MSE\n\n\n\nEvaluation of Estimators.\n\nMean of β1​^​: E(β1​^​)=β1​+ρX,ϵ​σX​σϵ​​\n\nThus bias is ρX,ϵ​σX​σϵ​​\nIf ρX,ϵ​=0 then exogenous (this is the definition of exogeniety)\nIf ρX,ϵ​&gt;0 then there’s some 3rd factor positively correlated with X, thus bias is positive.\nIf ρX,ϵ​&lt;0 then v.v.\n&amp; Thus the bias characterizes exogeniety\n\n\nVariance of β1​^​: Var(β1​^​)=N⋅Var(X)σ^2​\n\nThis is also called precision\nVar(β1​^​)​ is also called standard error of β1​^​.\n! For random variables, Var(X)​ is called standard deviation. For estimator random variables, it is called standard error. An abuse of terminology.\n\n\n\nOther Data when Regression is Run §\nThere are a bunch of other variables that may matter, that is not included in the above core set of variables of the regression. Here are a few:\ndef. Coefficient of Determination (R-Squared). Intuition: The proportion of the variation in Y^ that can be determined from X. We define it as:\nR2:=1−SStot​SSres​​\nwhere\n\nSSres​:=∑i​ϵi​ residual sum of squares\nSStot​:=∑i​(Yi​^​−Yˉ)2 total sum of squares\n\nExample. A R2 of 0.67 means that around 67% of the variation in Y^ is explained by X. Therefore, the higher it is, the better the regression is (=has more explanatory power).\nRemark. It can also be shown that:\nR2=ρY,Y^2​\nMeasurement Error §\nMotivation. There are measurement errors in every data; it can be both in the independent variable X or the dependent variable Y. We can characterize measurement error (in this case a measurement error in X) as:\nXi​=Xi∗​+vi​\nwhere vi​ is randomly distributed with E(vi​)=0 and std. dev. σv​, which is the measurement error. In this case, the regression in the model Yi​^​=β0​^​+β1​^​Xi​+ϵ^i​ will also change, into:\nβ1​^​=n→∞​β1​σv2​+σX∗2​σX∗2​​\nWe can extract a couple of facts from this relationship:\n\nAttenuation bias: the greater the σv​, the closer β1​^​ is to zero.\nif σv​=0 then β1​^​=β1​, i.e. no measurement error\n\nAlternatively, if there is a measurement error in Yi​, then:\n\nMeasurement error: Yi​=Yi∗​+vi​ where E(vi​)=0 and std. dev. σv​\nError term vi​ is absorbed by error term ϵi​^​\nThis will increase σ^ (variance of regression) and thus Var(β1​^​), but does not bias the estimator.\n\nIssues to Watch out for §\nHeteroskedasticity is when the variance of data is different for some subsets of data than other subsets. ⇒ Use heteroskedatic-consistent standard errors by using robust in stata. This does not affect value of β1​^​ (how!) and does not bias it.\nAutocorrelation often occurs in time series data where the error terms are sticky. For example, attendance at a NY yankees game will be sticky, because people who watched a good year will probably come back next year, even if the yankees aren’t as good as the year before."},"Black-Scholes-Merton-Derivative-Pricing-Formula":{"title":"Black-Scholes-Merton Derivative Pricing Formula","links":["Stochastic-Process","Dividend-Discount-Model","Interest-Rate"],"tags":["Economics/Finance"],"content":"thm. BSM Derivative Pricing. Generalized pricing of any derivative. Using different constraints, one can price any derivative.\n∂S∂f​+2σ2S2​∂S2∂2f​+(r−q)S∂S∂f​−rf=0\nwhere…\n\nf[S(t),t] is the price of the derivative at time t\nS(t) is a Geometric Brownian Motion s.t. dS=(m−q)S⋅dt+σS⋅dB(t) with B(t) a standard brownian motion\n\nσ is the scale of process S\nμ is the drift of process S\nm is the rate of return of process S\nq is the dividend rate (any dividend is immediately reinvested)\n\n\nr is the Discount Rate\n"},"Bond-Price":{"title":"Bond Price","links":["Bonds-(Finance)","Treasury"],"tags":["Economics/Finance"],"content":"thm. The price of a bond is the following:\nB(n)=ry​/21−(1+ry​/2)−n​C+(1+2ry​​)nM​\nwhere…\n\nB(n) is the current market price of the bond\nn=2τ is the periods left to maturity (where τ is the years left to maturity)\n\nnot periods passed! periods left\n\n\nM is the Principal amount\n\nNormally use 100or1000\n\n\nC is the per-period (=semi-annual) coupon payment\n\nnormally, coupon is quoted at annual coupon rate rC​ (in percent) so C=2rC​​M\n\n\nry​ (or y) is the yield to maturity (≈discount rate)\n\nr is the current yield. r=B(n)2C​\n\n\nn is the number of coupon payments (semi-annual for Treasury bonds)\n\nThe Three Rates of Bonds\n\nry​: Yield to Maturity—“What’s the return rate for the coupons if I hold until maturity?”\nr: Current Yield—“Coupon rate, but at the current bond price”\nrC​: Coupon Rate—“Coupon rate (at principal price).”\n\n\nPrice and rates:\n\nTrades at premium B(n)&gt;M ⇔ ry​&lt;r&lt;rC​\nTrades at discount B(n)&lt;M ⇔ ry​&gt;r&gt;rC​\n\n\n\nObserve…\n\n\nB(n)∝yield1​.\n\n\nlimn→0​B(n)=M\n\n\nCausality: Exogenous factors → Δyield → Δprice.\n\n\n\nIn a portfolio of bonds…\n\n\nMarket Value=100Price of par​×# of pars\nThe causality of the variables of a bond. The market’s expected rate of return of a bond is the yield. With this yield we can calculate its price.\nPrice—Yield Curve (mathematical) §\nPrice and Yield are inversly correlated → Price-yield curve looks like this:\n\nDuration is the gradient of this price-yield function:\nDuration:=dydP​\nDollar Duration (DV01) is the change in price due to 1% change in yield [=$1 change in yield per par]\nDV01:=±1%⋅yΔP​\n\nDollar Duration can be only used to approximate price changes for small changes in yield (~50bp)\nDuration is used to estimate risk (Price sensitivity against yield = DV01 risk\nDollar Duration is quoted as an absolute value. (We all know that it’s mathematically negative)\nDV01 Risk:= Δ1%yieldΔMarket Value​ = “Δ market value for 1% yield change” = 100#par​⋅DV01\nDV01 has no relationship with volatility\n\n\n\n                  \n                  Example calculation \n                  \n                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime to MaturityDV01Yield Vol.e.g. change in yieldchange in price2 year$1.810%p20bp$0.3610 year$7.25%p10bp$0.72\nYield Curve (emperical) §\n\n⇒ Yield ∝ Maturity, because the farther away the cash flows are, the more risky it is [∵ uncertainty]\n\nInverted Yield Curve: Bond investors think a recession is looming → Treasury will increase rates\nLeft intercept is the fed rates &amp; short-mat bonds → high movement\nright side is long-mat bonds → low movement\n\nRisk &amp; Volatility §\ndef. Price[rate of return] volatility = SD[Price] (in $)\ndef. Yield volatility = SD[Yield] (in %p)\ndef. DV01 Risk = Change in MV per change in 1% yield\n\nYield Vol &gt; RoR Vol ← mathematical relationship (price formula)\nMaturity ∝ Price Vol.\n"},"Bonds-(Finance)":{"title":"Bonds (Finance)","links":["Bond-Price","Tradable-Inflation-Protected-Securities-(TIPS)","Treasury","Investment-Bank"],"tags":["Economics/Finance"],"content":"\nSee Bond Price for details on pricing.\nTradable Inflation-Protected Securities (TIPS) are inflation protected bonds.\n\nHow Bonds Work §\n\n\nLender lends money to the government (usually US Treasury)\nCoupon payments pay regularly:\n\nSemi-annually for US Treasury\nAnnually for other countries\n\n\nPrincipal (=par, base) amount is paid at the maturity;\nTime To Maturity (TTM) is time from now to when the principal pays\nYield\n\nCurrent Yield\nYield to Maturity: Expectation Hypothesis defines yield to maturity as the expected rate of return if held til maturity.\n\n\n\nCash Flow Structure of a Bond §\nGovernments are the borrowers of money in this transaction.\n\nGovernment “issues” bonds regularly, which is\n\nPrimary Market: Treasury sells bonds (millions of $ worth) mostly to private Investment Banks\nSecondary Market: Investment banks sell smaller chunks to private investors / bond holders sell to each other\n\n\nGovernments pay coupon and principal through\n\nTax revenue\nIssuing more bonds\n\n\n\nBonds are an important part of a portfolio because…\n\nAre near-riskless investments as they generate stable cash flows. The only risks are:\ninflation risk—when the government doesn’t have the money, they’ll just print more money. This causes inflation which devalues the bonds.\nAre very liquid, since many people are willing to buy bonds in most scenarios, even in recessions.\n"},"Boolean-Decision-Problem":{"title":"Boolean Decision Problem","links":[],"tags":["Computing/Algorithms"],"content":"Q. CNF Satisfiability Problem. (=CNF-SAT =Circuit Satisfiability) Given a conjunctive normal form, is there a way to set the variables to True or False such that the formula evaluates to True?\n(x1​∨¬x2​∨x3​)∧(¬x1​∨x4​)∧(x2​∨¬x3​∨x5​∨¬x6​)\n\nLiteral: xi​ or xi​ˉ​\nClause: (x1​∨x2​∨x3​ˉ​) a set of or-connected literals\n\nthm. Cooke-Levin Theorem. The Boolean Satisfiability problem is NP-complete.\n\nThe first problem to be determined to be NP-complete.\n\nQ. 3-Satisfiability Problem. (3-SAT)\n\nThe CNF-SAT problem, but every set of ORs has only three variables\n3-SAT ≤p​ 4-SAT\n\nalg. Reduction CNF-SAT ≤p​ 3-SAT\n\nIdea: a∨b≡(a∨c)∩(b∨cˉ)……(1)\n\n\nConstruction\n\n∃{x1​…xn​} variables that satisfies Q:q1​…qm​ where qi​ is a clause\nQ′:q1′​…qm′′​ constructed to satisfy (1).\n\nm′&gt;m (obviously)\nNew variables v1​…vk​ thus Q has variables {x1​…xn​,v1​…,vk​}\n\n\n\n\nCNF-SAT ⇒ 3-SAT\n\nlet X1​…Xn​∈{0,1} that satisfies Q. Then these values will satisfy Q′ regardless of vi​ ’s values, so choose them arbitrarily. These values satisfy Q′\n\n\n3-SAT ⇒ CNF-SAT\n\nlet X1​…Xn​,V1​…Vn​∈{0,1} that satisfies Q′. Then X1​…Xn​ will satisfy Q.\n\n\nThus proven.\n\nQ. DNF Satisfiability Problem. Given a disjunctive normal form, is there a way to set the variables to True or False such that the formula evaluates to True?\n\nExists a Polynomial time algorithm\nOnly one clause need be satisfied\n\nQ. Tautology. Given a Boolean formula Q, is Q always True regardless of what the variables are set to?\n\nThe verification procedure for True itself is NP-Complete, and it is exactly the CNF Satisfiability problem\nThe verification procedure for False is also NP-hard\nThe whole problem is not in NP (and is Co-NP Complete)\n"},"Branching-(Computer-Science)":{"title":"Branching (Computer Science)","links":[],"tags":["Computing/Computer-Architecture"],"content":"\n20% of instructions are branches\n30% of branches take an extra cycle\naverage CPI is 1+0.2*(0*.*3***1) = 1.06 cycles\n"},"Budget-Lines":{"title":"Budget Lines","links":["Screen_Shot_2022-09-01_at_15.34.06.png","Screen_Shot_2022-09-01_at_15.31.59.png"],"tags":["Economics/Micro-Economics"],"content":"\n⇒ Similar to the Possibility Frontier (PPF), but like a “consumption possibility frontier”.\n\n\n                  \n                  Info \n                  \n                \n\nYou will often plot all other consumed goods into one combined good, as the **“ofotherconsumption”∗∗withunit.\nI=p1​x1​+p2​x2​—Equation of the budget constraint\n\ny-intercept: p2​I​ → if you buy only x2​\nx-intercept: p1​I​ → if you buy only x1​\ngradient: −p2​p1​​\n\n→ Opportunity cost of one unit of x1​ in terms of x2​.\nnegative, since for additional consumption of x1​ you lose your ability to consume x2​.\n\n\n\nChange in Budget Lines §\n\nExogenous change in income I\nChnage in price p1​ or p2​ Graph\n\nWhen price of good x1​ decreases (i.e. p1​ decreases) intercept p2​I​ stays the same, gradient p2​p1​​ increases. Vice versa when p2​ changes.\n⇒ Real income changes\n\n\nEndowments\n\nEndowments are a particular basket of goods you get for free; it’s a single point in the graph—in this case, (5 pants, 10 shirts) is the endowment.\n\n\nProgressive Taxes\nInter-temporal Budgets\n"},"Buffett-Indicator":{"title":"Buffett Indicator","links":[],"tags":["Economics/Finance"],"content":"Buffett Indicator Valuation Model\n\nThe Buffett Indicator (aka, Buffett Index, or Buffett Ratio) is the ratio of the total United States stock market to GDP.\n\nGDPStock Market ∑MCAP​\n\n\n\n1: Stock is overvalued → Bubble\n\n\n&lt;1: Stock is undervalued → Buy Now\n"},"CAPM-Model":{"title":"CAPM Model","links":["Security-(Finance)","Regression","Risk-(Finance)","Leverage","Portfolio-Theory","Market-Beta","Measuring-Security-Performance","Constrained-Optimization"],"tags":["Economics/Finance"],"content":"Gentle Introduction §\ndef. The Capital Asset Pricing Model (CAPM) is a method of predicting the returns of an asset, by using regression analysis between it and the market’s returns.\n\nThe regression line has the equation:\nri​−rf​=α+β⋅(rm​−rf​)\nData correlates between market excess return (rm​−rf​) and asset excess return (=ri​−rf​). The statistics produces the following data:\n\nCoefficient of Determination [=R-value] 0≤R2≤1, measures the goodness of fit. Bigger means the model is a better fit.\nStandard deviations [=Risk (Finance)] σm​,σi​\nCorrelation coefficient −1&lt;ρ&lt;1, measures the degree of correlation between the two variables\nBeta [=slope ≈1] β:=σm​ρi,m​⋅σi​​, which measures the sensitivity of one variable to another. It doesn’t deviate much from 1.\nAlpha [=intercept ≈1], which measures the excess return against all odds (of the prediction of the CAPM model.) It should, theoretically, always be zero. But it is not (See Jenson’s Alpha below.)\n\nIf we plot riskless assets with a riskful asset:\n\n\nrf​ is the riskless return, rp​ is the riskful asset’s return, and w is the weight of riskful assets in portfolio.\nσp​ is the volatility for the riskful asset, and σf​ the volatility of riskless asset, is zero by definition\nGradient of the blue line is σp​rp​−rf​​ ← this is a constant. Thus the blue frontier is linear.\n\n⇒ With riskless assets, you can short-sell the riskless asset to go beyond the risk/return:\n\n(2) is achieved by short-selling the risk-free asset and purchasing more of the riskfull asset.\n\nThis is also called Leverage.\nThis means that the weight on the risk-free asset is negative\n\n…and the weight on the risk-full asset is &gt;1\n\n\nIf the risk-free rate ever eclipses the risk-ful rate, disaster insues. (This is what happened during the financial crisis of 2008; banks were highly levered.)\n\nCombination of Risk-ful—Risk-free &amp; Risk-ful—Risk-ful §\n\n\n\n                  \n                  6~7%.\n                  \n                \n\n\nNote that the axes are not market returns, but **excess returns**. This means usually the y-intercept will be zero, unless as we later discuss $\\alpha &gt;0$.\n\nSensitivity vs. Asset Return. If the data shows the asset’s return is higher, then the sensitivity should increase.\n\n\nM is when the market is measuring against itself.\n\n\nRisks. In the CAPM model, there are two types of risks we care about.\n\nSystematic Risk [=market risk, beta risk]. This is the risk due to the market recessions.\n\nDiversification cannot solve beta risk.\nHigh beta implies higher returns in good times, worse returns in bad times.\n\n\nIdiosyncratic Risk. This is the risk due to individual firms’ characteristics (tech, organization, etc.)\n\nIdiosyncratic risk ⟶n→∞​0 i.e. diversification solves this\nUncorrelated with market risk.\n\n\n\n\n\n                  \n                  Industries that do well despite a recession: Pharmacuticals, Consumer necessities, Defense industries. \n                  \n                \n\nTechnical Reference §\nUse terminology from Portfolio Theory.\n\nMarket Beta: correlation of portfolio with market.\nCapital Allocation Line (CAL)\n\nSecurity (σi​,μi​)\nRisk-free security (0,μf​)\nEquation of Possible Portfolios (=Capital Allocation Line): μp​=σi​μi​−μf​​σp​+μf​\n\nThis is a line ⇒ Capital Allocation Line.\nSlope: σi​μi​−μf​​=Portfolio RiskExcess Return​=Sharpe Ratio\nHigher slope (=higher Measuring Security Performance) is better (more return per unit risk)\n\n\n\n\nCapital Market Line (CML)\n\nMarket Security (σi​,μM​)\nRisk-free security (0,μf​)\n\n\n\nInvestor Utility Function\n\nRed: risk-seeking\nBlack: risk-neutral\nBlue: risk-averse\n\n\n\n\nConstrained Optimization problem maxμ,σ​E[u(Rp​)] s.t. g(μ,σ)=k\n⇒ to get optimal portfolio Rp∗​ \n\n\n\n\n"},"CS-250-Architecture":{"title":"CS 250 Architecture","links":["Abstraction"],"tags":["Courses"],"content":"\nAbstraction\n"},"CS-316-Database-Systems":{"title":"CS 316 Database Systems","links":["Database-Management-System","Relational-Algebra","Entity-Relationship-Model","Database-Design-Theory","SQL-Basics","SQL-Constraints","Extensible-Markup-Language","Document-Type-Definition","XPath-and-XQuery","MongoDB-Reference","Physical-Data-Organization","Database-Indexing","SQL-Query-Processing-Algorithms","SQL-Query-Optimization","SQL-Transaction-Guarantees"],"tags":["Courses"],"content":"Theoretical Foundations §\n\nDatabase Management System\nRelational Algebra\nEntity-Relationship Model\nDatabase Design Theory\n\nSerializable Query Language §\n\nSQL Basics\nSQL Constraints\n\nNoSQL Databases §\n\nExtensible Markup Language (XML)\nDocument Type Definition\nXPath and XQuery\nMongoDB Reference\n\nQuery Processing §\n\nPhysical Data Organization\nDatabase Indexing\nSQL Query Processing Algorithms\nSQL Query Optimization\nSQL Transaction Guarantees\n"},"CS-330-Advanced-Algorithms":{"title":"CS 330 Advanced Algorithms","links":["CS330-HW03","CS330-HW04","CS330-HW05","CS330-HW07","CS330-HW08","CS330-HW10","Time-Complexity","Recurrence-Relation","Peak-Element","Undominated-Points","Sorting-Algorithms","Priority-Queue","Parallel-Algorithms","Matrix-Multiplication","Inner-Product","Dynamic-Programming","Longest-Common-Sequence","Knapsack-Problem","Matrix-Chain-Multiplication","Depth-First-Search","Directed-Graph","Directed-Acyclical-Graph","Shortest-Path","Greedy-Algorithm","Scheduling-Problem","Huffman-Text-Compression-Algorithm","Minimal-Spanning-Tree-Problem","Ackerman-Function","Maximum-Flow-Problem","Computational-Tractability","Boolean-Decision-Problem","Common-Graph-Problems","Subset-Sum","Linear-Programming","Approximation-Algorithm","Set-Cover","Traveling-Salesperson-Problem","K-Clustering-Problem","Hashing-Algorithms"],"tags":["Courses"],"content":"Homeworks\n\n\nCS330 HW03\n\n\nCS330 HW04\n\n\nCS330 HW05\n\n\nCS330 HW07\n\n\nCS330 HW08\n\n\nCS330 HW10\n\n\nAsymptotic Analysis\n\nRecurrence Relation\n\n\n\nDivide and Conquer\n\nPeak Element\nUndominated Points\nQuick Sort\n\nTournament Tree\n\n\nMerge Sort\n\n\n\nParallel Algorithms\n\nParallel Sum\nMatrix Multiplication\n\nParallel Inner Product\n\n\nParallel Merge Sort\n\n\n\nDynamic Programming\n\nFibonacci Sequence\nLongest Common Sequence\nKnapsack Problem\nMatrix Chain Multiplication: increasing in gaps\n\n\n\nDepth First Search\n\nReachability and Connectivity\nDirected Graph\n\nPreand Post-time\nStrongly Connected Component algorithm (Kosaraju’s)\nCycle detection → DAG\nTopological Sort of DAG\n\n\n\n\n\nShortest Path\n\nSingle Source Shortest Path\n\nBFS Shortest Path (vertex creation, alarm clock method)\nDijkstra’s\nA*\nBellman-Ford\n\n\nAll-Source Shortest Path: Floyd-Warshall\n\n\n\nGreedy Algorithm\n\nScheduling Problem\nHuffman Text Compression Algorithm\nMinimal Spanning Tree Problem\nAckerman Function\n\n\n\nMaximum Flow Problem (=Min-cut, Edge matching)\n\nMin Cut\nEdge Matching\n\n\n\nComputational Tractability\n\nPolynomial Reduction\nBoolean Decision Problem\n\nCooke-Levin Theorem\nCNF-SAT ≤p​ 3-SAT\nVertex Cover ≤p​ Subset Sum\nSubset Sum ≤p​ Knapsack Problem\n\n\nCommon Graph Problems\n\nIndependent Set\nClique\nVertex Cover\nTriangle Cover\n\n\nLinear Programming\n\n\n\nApproximation Algorithms\n\nLoad Balancing\nSet Cover\nTraveling Salesperson Problem\n\nChristofides Approximation (1.5-level)\n\n\nK-Clustering Problem\n\nK-Max: Maximum distance to closest center\nK-Means: Mean per center\n\nLlyod’s Approximation (2-level)\nK-Means++ Approximation (&lt;2-level)\n\n\n\n\n\n\n\nHashing Algorithms\n\nUniformity\nUniversality\nLinear Congruence Hashing\nMultiply Shift Binary Hashing\n\n\n"},"CS-334-Formal-Languages":{"title":"CS 334 Formal Languages","links":["Finite-Automata","Regular-Expressions","Regular-Grammar","Regular-Languages","Pushdown-Automata","Context-Free-Grammar","Context-Free-Language","Turing-Machine","Recursively-Enumerable-Languages","Unrestricted-Grammar","Formal-Grammar","Formal-Languages","Chompsky-Heirarchy","Greibach-Normal-Form","Chompsky-Normal-Form"],"tags":["Courses"],"content":"Finite Automata §\n\nFinite Automata\nRegular Expressions\nRegular Grammar\nRegular Languages\n\nPushdown Autotmata §\n\nPushdown Automata\nContext-Free Grammar\nContext-Free Language\n\nTuring Machines §\n\nTuring Machine\nRecursively Enumerable Languages\nUnrestricted Grammar\n\nBackground Knowledge §\n\nFormal Grammar, Formal Languages\nChompsky Heirarchy\nGreibach Normal Form, Chompsky Normal Form\n"},"CS-535-Algorithmic-Game-Theory":{"title":"CS 535 Algorithmic Game Theory","links":["Game-Theory","Equilibria-in-Game-Theory","Potential-Game","Traffic-Routing","No-Regret-Dynamics","Auction-Theory","Revenue-Maximizing-Auctions","Optimal-Stopping-Problem","Combinatorial-Auction","Ascending-Auction"],"tags":["Courses"],"content":"Games, Learning, and Dynamics §\n\nGame Theory\n\nEquilibria in Game Theory\n\n\nPotential Games\n\nTraffic Routing\n\n\nNo-Regret Dynamics\n\nAuctions §\n\nAuction Theory\nRevenue-Maximizing Auctions\nOptimal Stopping Problem\nCombinatorial Auction\nAscending Auction\n"},"CS-Course-Requirements-(1-left)":{"title":"CS Course Requirements (1 left)","links":[],"tags":["Courses"],"content":"Bachelor of Science (BS) Degree\nCourse Substitutions for CS Majors or Minors\nGraduation with Distinction\nPrerequisites (Done) §\n\nOne of the following introductory COMPSCI courses or equivalent:\n\nCOMPSCI 101L (Introduction to Computer Science)\nCOMPSCI 102 (Interdisciplinary Introduction to Computer Science)\nCOMPSCI 116 (Foundations of Data Science)\n\n\nMATH 111L (Introductory Calculus I) or equivalent\nMATH 112L (Introductory Calculus II) or equivalent\n\nBase Requirements (Done) §\n\nCOMPSCI 201 (Data Structures and Algorithms)\n==COMPSCI 230 (Discrete Math for Computer Science) see substitutions==\nCOMPSCI 250 (Computer Architecture)\nCOMPSCI 330 (Design &amp; Analysis of Algorithms)\n\nOne Of the following COMPSCI Courses on Systems: (Done) §\n\nCOMPSCI 310 (Introduction to Operating Systems) or 510 (Advanced Operating Systems)\nCOMPSCI 316 (Introduction to Databases) or 516 (Database Systems)\nCOMPSCI 350 (Digital Systems, cross-listed as ECE 350) or 550 (Advanced Computer Architecture, cross-listed as ECE 552)\nCOMPSCI 351 (Computer Security) or 551 (Advanced Computer Security)*\nCOMPSCI 356 (Computer Network Architecture) or 514 (Computer Networks)\n\nTwo Courses in MATH/STA: (Done) §\n\nOne STA course at or above STA 111**, including the cross-listed MATH 230\nOne of MATH 202, 216, 218, or 221***\n\nFive Electives at 200-level or Higher (beyond Those Counted towards the Requirements above): (1 left) §\n\n\nThree COMPSCI courses that are not independent study courses\n\nCS 334 Formal Languages\nCS 545 Algorithmic game theory\n??\n\n\n\nTwo in COMPSCI (independent study possible), MATH, STA, or a related area approved by the Director of Undergraduate Studies\n\nMath 212 (this is allowed, see below)\nSTAT 432 Stat. Inference &amp; Learning\n\n\n\nBoth courses have been offered as a 290 and 590 course with the same name, and will satisfy this requirement.\n\nSTA 111 will not be offered after Summer 2020. We recommend you take STA 199 or higher.\n\n**MATH 212 does not count towards this requirement, but can count towards an elective."},"CS330-HW03":{"title":"CS330 HW03","links":[],"tags":["Courses"],"content":"Problem 2 §\nSubproblem (a) §\nIdea: a majority element must be majority in either of the two halves of an array\nAlgorithm:\n# array to check\nA[1..n]\n \nfunction Count(l, r, elem)\n\t# base case\n\tif l = r\n\t\treturn 1\n \n\t# divide array into two, and count there\n\tm = ceiling((l+r)*0.5)\n\tl_count = PCount(l, m, elem)\n\tr_count = PCount(m+1, r, elem)\n \n\t# return the sum of left and right counts\n\treturn l_count + r_count\n \nfunction MajorityElem(l,r)\n\t# base case\n\tif l = r\n\t\treturn A[l] \n \n\t# check left, right half majority\n\tm = ceiling((l+r)*0.5)\n\tleft_majority = MajorityElem(l, m)\n\tright_majority = MajorityElem(m+1, r)\n \n\t## check consensus\n \n\t# check no consensus first; if one is null return the non-null\n\tif left_majority == NULL and right_majority != NULL:\n\t\treturn right_majority\n\tif left_majority != NULL and right_majority == NULL:\n\t\treturn left_majority\n \n\t# there cannot be a both no consensus case\n \n\t# consensus agrees\n\tif left_majority == right_majority:\n\t\treturn left_majority\n\t\n\t# consensus disagrees\n\telse if left_majority != right_majority:\n \n\t\t# count the majority\n\t\tleft_count = Count(l, r, left_majority)\n\t\tright_count = Count(l, r, right_majority)\n\t\t\n\t\t# if tied, no consensus\n\t\tif left_count == right_count\n\t\t\treturn NULL\n\t\t\n\t\t# winner exists, return the winner\n\t\treturn left_count &gt; right_count ? left_majority : right_majority\n\t\t\n\nCount function: T(n)≤2T(2n​)+O(1)=O(n)\nMajorityElement function: T(n)≤2T(2n​)+2⋅O(n)\n\nFirst term: recursive calls\nSecond term: calls Count\nSolution: T(n)=O(nlogn) (same recurrence relation as mergesort)\n\n\nCorrectness (Brief argument):\n\nBase case: in n=1 the element is majority element\nIH: Assume MajorityElement(k/2) is correct.\nIS:\n\nIf one of them has no majority but the other ones does, the one with majority must be the true majority because of an element exists in more than half of the total array it must be that in one of the halves it will be the majority.\nIf both halves have a majority, then the one that has more counts in the total array wins\nThere cannot be a both halves no-majority case as explained in (1)\n\n\n\n\n\nSubproblem (b) §\nParallel version of algorithm\n# array to check\nA[1..n]\n \nfunction PCount(l, r, elem)\n\t# base case\n\tif l = r\n\t\treturn 1\n \n\t# divide array into two, and count there\n\tm = ceiling((l+r)*0.5)\n\tspawn l_count = PCount(l, m, elem)\n\tspawn r_count = PCount(m+1, r, elem)\n\tsync\n\t\n\t# return the sum of left and right counts\n\treturn l_count + r_count\n \ndef MajorityElem(l,r)\n\t# base case\n\tif l = r\n\t\treturn A[l] \n \n\t# check left, right half majority\n\tm = ceiling((l+r)*0.5)\n\tparallel left_majority = MajorityElem(l, m)\n\tparallel right_majority = MajorityElem(m+1, r)\n \n\t## check consensus\n \n\t# check no consensus first; if one is null return the non-null\n\tif left_majority == NULL and right_majority != NULL:\n\t\treturn right_majority\n\tif left_majority != NULL and right_majority == NULL:\n\t\treturn left_majority\n \n\t# there cannot be a both no consensus case\n \n\t# consensus agrees\n\tif left_majority == right_majority:\n\t\treturn left_majority\n\t\n\t# consensus disagrees\n\telse if left_majority != right_majority:\n\t\t# count the majority\n\t\tleft_count = PCount(l, r, left_majority)\n\t\tright_count = PCount(l, r, right_majority)\n\t\t\t\t\n\t\t# if tied, no consensus\n\t\tif left_count == right_count\n\t\t\treturn NULL\n\t\t\n\t\t# winner exists, return the winner\n\t\treturn left_count &gt; right_count ? left_majority : right_majority\n \n\nPCount analysis\n\nSpan: T∞​(n)≤T(2n​)+O(1)=O(logn)\nWork: W∞​(n)≤2T(2n​)+O(1)=O(n)\nmajority element determination can be run in parallel\n\n\nSpan: T∞​(n)≤T(2n​)+O(logn)=O(log2n)\n\nFirst term: parallel recursive call\nSecond term: compare the majority\nSolution\n\nUsing the master theorem, we see logb​a=log2​1=0 and the for the residual term ccrit​=0.\nThis is the second case from which we infer T∞​(n)=O(log2n)\n\n\n\n\nWork: W∞​≤2T(2n​)+O(n)=O(nlogn)\n\nProblem 3 §\nfunction PCountAndArrange(l,r) =&gt; int\n\t# base case; if zero, return count 1; else return count 0\n\tif l=r\n\t\treturn A[l] == 0 ? 1 : 0\n \n\t# recursive call &amp; count\n\tm = ceiling((l+r)*0.5)\n\tk = spawn PCountAndArrange(l, m)\n\tn = spawn PCountAndArrange(m+1, r)\n\tsync\n\tzerocount = k + n\n \n\t# rearrange the left half zeros with right half numbers\n\tparallel for i = m-k+1 .. m\n\t\tB[i+k] = A[i]\n\tparallel for i = m .. m+k\n\t\tB[i-k] = A[i]\n\tparallel for i = m-k+1 .. m+k\n\t\tA[i] = B[i]\n \n\t# return the count\n\treturn zerocount\n\t\n\nProcessor count: O(n)\n\nPCountAndArrange recursive calls take O(n) processors\nrearrange takes O(n) processors for each parallel for loop\n\n\nSpan: T∞​(n)≤T(2n​)+O(1)=O(logn)\nWork: W∞​(n)=2T(2n​)+O(n)=O(n)\n\nFirst term for recursive call\nSecond term for rearranging\n\n\n\nProblem 4 §\n# A[(int, int)] is the structure of the array\n \nfunction FindMaximal()\n\tmax_y = -Infinity\n\t\n\t\n\t# Sort the points in decreasing order by x-coordinate\n\tReverseMergeSort(A[])\n\tmax_y = A[0]\n\tresult = [A[0]]\n\t\n\t# traverse in reverse x direction (right to left)\n\tfor p in A\n\t\t# if y is bigger than anything seen before it&#039;s maximal\n\t\tif p.y &gt; max_y then\n\t\t\tresult.append(p)\n\t\t\tmax_y = p.y\n\t\t\t\n\treturn result\n\nReverseMergeSort O(nlogn)\nFindMaximal T(n)=O(n)+O(nlogn)=O(nlogn)\nCorrectness (Brief argument):\n\nThe rightmost element must be the maximal element\nIf the current element has a higher y value than any other element on the right side of it, it must be a maximum\nIf the current element has a lower y value than another point on the right of it, it is dominated by that point and thus cannot be a maximum\n\n\n\n# A[(int, int)] is the structure of the array\n \nfunction isDominant((x1,y1),(x2,y2))\n\t# first is dominant\n\tif (x1 &gt;= x2 and y1 &gt; y2) or (x1 &gt; x2 and y1 &gt;= y2)\n\t\treturn 1\n\t# second is dominant\n\tif (x2 &gt;= x1 and y2 &gt; y1) or (x2 &gt; x1 and y2 &gt;= y1)\n\t\treturn -1\n\t# none are dominant\n\telse\n\t\treturn 0\n\t\t\n \nfunction PSortedSubtract(A[1..n],B[1..m])\n\tparallel for i = 1..n\n\t\tif BinarySearch[A[i],B] == False\n\t\t\tC.append(A[i])\n \nfunction PFindDoms(l, r)\n\t# base case\n\tif l=r\n\t\treturn l\n\t\n\t# get indicies of domannt points from left and right half\n\tm = ceiling((l+r)*0.5)\n\tldoms = spawn PFindDoms(l,m)\n\trdoms = spawn PFindDoms(m+1,r)\n\tsync\n \n\tnondoms = []\n\tparallel for i = l..m\n\t\tparallel for j= m+1..r\n\t\t\t# either one is dominant drop the non-dominant\n\t\t\tif isDominant[A[i],A[j]] == 1\n\t\t\t\tnondoms.add(j)\n\t\t\tif isDominant[A[i],A[j]] == -1\n\t\t\t\tnondoms.add(i)\n\t\t\t\t\n\tPMergeSort(nondoms)\n\t\n\treturn SortedSubstract((ldoms + rdoms), nondoms)\n \nfunction FindMaximal()\n\t# mergesort based on the x values of the array\n\tXMergeSort(A)\n\treturn FindDoms(l, r)\n \n\nisDominant span: O(1)\nPSortedSubtract\n\nspan: T∞​(n)=O(logn)\nwork: W∞​(n)=O(nlogn)\n\n\nPFindDoms\n\nspan: T(n)=T(2n​)+O(1)+O(log3n)+O(logn)=O(log4n)\n\nfirst term: parallel recursion\nsecond term: parallel comparison of dominant points\nthird term: PMergeSort\nfourth term: SortedSubtract\nSecond case using master theorem: O(log4n) time.\n\n\n\n\n"},"CS330-HW04":{"title":"CS330 HW04","links":[],"tags":["Courses"],"content":"Problem 2 §\nfunction VC(v)\n\t# including v\n\tin = 1\n\tfor child c of v:\n\t\tin += min(c.in, c.ex)\n\t\n\t# not including v\n\tex = 0\n\tfor child c of v:\n\t\tex += c.in\n\t\t\n\tv.in = in\n\tv.ex = ex\n\t\n\treturn min(v.in, v.ex)\n\nBase case: for a leaf node leaf, leaf.in = 1, leaf.ex = 0\nIH: for node v with leaves u1..uk, assume ui.in, ui.ex is correct\nIS: for node v,\n\nCase including: VC including v does not require u1..uk to be included, but including it may be more minimal. IH guarantees ui.in, ui.ex is correct so take the minimum\nCase excluding: VC excluding v requires all of u1..uk to be included, so in that case sum all the ui.in values to guarantee counting all cases where children are included\n\n\nTime complexity: T(n)≤#children⋅T(#childrenn​)+O(1)=O(n)\n\nProblem 3 §\ndef PalindromeDecomposition\n \n\t# memo[n][n] is a 2-d array whose value is all &quot;N&quot;\n\t\n\t# in increasing order of gap\n\tfor gap=1..n\n\t\tfor i=0..n-gap-1\n\t\t\tj = i + gap\n\t\t\t\n\t\t\t# base case\n\t\t\tif i==j\n\t\t\t\tmemo[i][j] = Y\n\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t# if edges are same, check the inner\n\t\t\tif A[i] == A[j]\n\t\t\t\tif memo[i+1][j-1] == Y\n\t\t\t\t\tmemo[i][j] == Y\n\t\t\t\telse\n\t\t\t\t\tmemo[i][j] == N\n\t\t\t\t\t\n\t\t\t# if edges are diff, not palindrome\n\t\t\telse A[i] != A[j]\n\t\t\t\tmemo[i][j] == N\n\t\n\t# memo_count[n][n] is a 2-d array whose value is all 0.\n\t\n\t# in increasing order of gap\n\tfor gap=1..n\n\t\tfor i=0..n-gap-1\n\t\t\tj = i + gap\n\t\t\t\n\t\t\t# if the whole length i-j is palindrome, 1\n\t\t\tif memo[i][j] == Y\n\t\t\t\tmemo_count[i][j] = 1\n\t\t\t\t\n\t\t\t# if the whole length isn&#039;t palindromic...\n\t\t\tminPcount = +infty\n\t\t\tfor k=0..j-i:\n\t\t\t\t# get the minimum palindrome split of all possible splits\n\t\t\t\tmin(minPcount, memo[i][i+k-1] + memo[i+k][j])\n\t\t\tmemo_count[i][j] = minPcount\n\t\n\treturn memo_count[0][n]\n \n\n\nPart 1:\n\nBase case: a length-1 string is a palindrome\nIH1: substring A[i+1][j−1] is a palindrome\n\nIS1: then if A[i]=A[j], the string A[i][j] is a palindrome\n\n\nIH2: substring A[i+1][j−1] is not a palindrome\n\nIS2: then the string A[i][j] cannot be a palindrome\n\n\n\n\n\nPart 2:\n\nBase case: If the string A[i][j] is a palindrome, then it is the minimum palindromic split of count 1\nIH: palindromic substring count has been computed correctly from all proper substrings of A[i][j]\nIS: the minimum palindromic substring if A[i][j] is the minimum of the sums A[i][i+k−1]+A[i+k][j] for k∈[i,j]\n\n\n\nThus proved\n\n\nPart 1 span: O(n2) as filling a 2-d array with O(1) calculation per subarray\n\n\nPart 2 span: O(n3) as filling a 2-d array with O(n) calculation per subarray\n\n\n⇒ Total time complexity: O(n3)\n\n"},"CS330-HW05":{"title":"CS330 HW05","links":[],"tags":["Courses"],"content":"Problem 1 §\nV = [] # all vertices\nE = [] # all edges\n# A[1..m] holds the preference list\n \ndef graphCreation():\n\t# for every friend\n\tfor k = 1..m:\n\t\tprevNode = None\n\t\tfor i = 1..len(A[k]):\n\t\t\tnode = A[k][i]\n\t\t\tif node not in V:\n\t\t\t\tV.append(node)\n\t\t\t# if this is the first node\n\t\t\tif prevNode != None:\n\t\t\t\t# add the edge\n\t\t\t\tE.append((node, prevNode))\n\t\t\tprevNode = node\n \npost = [] # post-time\nvisited = [] # add visited verticies\nclock = 0\n \n# in-class algorithm\ndef DFS(u):\n\tvisited[u] = True\n\tclock += 1\n\tfor (u -&gt; v) in E:\n\t\tif visited[v] == False:\n\t\t\tDFS(v)\n\tclock += 1\n\tpost = (u, clock)\n\t\ndef AllDFS(V, E):\n\tfor v in V:\n\t\tif visited[v] == False:\n\t\t\tDFS(v)\n \n# in-class proof\ndef TopologicalSort():\n\ttopological = Mergesort(post) # reverse merge sort of post-time\n\t# will be sorted by clock time:\n\t# e.g. [(v1, 0), (v4, 3), (v2, 7), ...]\n\treturn [v for v, time in topological]\n\t\t\t\t\nHigh-Level Algorithm §\nThis problem resembles the computation of a Topological Sorting on a Directed Acyclic Graph (DAG) that is created from the preference lists, which may contain contradictions.\n\nWe create a graph where each vertex represents a restaurant and each directed edge from vertex i to j signifies that restaurant i is preferred over restaurant j by some friend. We let n be the number of vertices (restaurants), m the number of edges (preferences), and N the total length of all preference lists.\nWe perform a Depth-First Search (DFS) on this graph to detect a cycle. If there is a cycle (contradictions in the preference lists), we report that no such ordering of restaurants exists.\nIf there is no cycle, we carry out a Topological Sort on this graph which gives the required ordering of restaurants.\n\nProof of Correctness §\n\nLet’s represent the preference list of each friend as a directed path in the graph. The direction from i to j (i→j) represents that restaurant i is preferred over restaurant j. Therefore, if there is an ordering that satisfies all friends’ preferences, the directed graph has to be acyclic.\nWe perform DFS on the graph, and if we find a back edge (which forms a cycle), it means the preferences are contradictory, and no solution exists. So, our DFS serves as a mechanism to validate if such an ordering can exist.\nIf DFS completes without detecting a cycle, we then apply Topological Sorting on this graph. The use of DFS ensures that all vertices are explored and included in the final ordering.\n\nRunning Time §\n\nCreating the graph takes O(N) time as we are examining each preference and creating an edge for it.\nRunning a DFS on the constructed graph takes O(n+m) time, where n is the number of vertices, and m is the number of edges, visited exactly once.\nThe Topological sorting takes O(nlogn) time as uses mergesort via post-time\n⇒ Total running time of the algorithm is O(N)+O(n+m)+O(nlogn)=O(nlogn+m+N).\n\nProblem 2 §\nV = [] # all verticies\nE = [] # all edges, undirected graph\nfor c in Constraints:\n\t# unpack the constraint into the two verticies of a graph\n\tu, v = c\n \n\t# add both verticies\n\tif v not in V:\n\t\tV.append(v)\n\tif u not in V:\n\t\tV.append(u)\n \n\t# add the edge if it doesn&#039;t exist\n\t# note that this is an undirected graph\n\tif both (u, v) and (v, u) not in E:\n\t\tE.append((u, v))\n \n \n \nvisited = []\nteam = [-1] * len(V) # create a team array initialized with -1\n# there is team 0 and team 1\nbipartite = True # assume true until disproven\n \ndef partitionDFS(u):\n\tvisited.append(u)\n\tfor all neighbours v of u:\n\t\t# if same team, not bipartite\n\t\tif v in visited and team[v] == team[u]:\n\t\t\t\tbipartite = False\n\t\t\t\treturn\n\t\t# if different team and not in visitied\n\t\telif v not in visited:\n\t\t\t# assign v the opposite team as u\n\t\t\tteam[v] = 1 - team[u]\n\t\t\tpartitionDFS(v)\n \ndef AllDFS():\n\tfor v in V:\n\t\tif v not in visited:\n\t\t\tteam[v] = 0 # start coloring from team 0\n\t\t\tpartitionDFS(v)\n \nIdea §\nThis problem can be seen as a variant of Bipartite Graph Checking problem, where every edge represents the constraint that two players cannot be in the same team.\n\nWe construct a graph where each vertex corresponds to a player and each edge corresponds to the constraint between players. Let n be the number of vertices (players) and m be the number of edges (constraints).\nWe then perform a Depth-First Search (DFS) on this graph aiming to 2-color the graph where color should alternate between vertices. If during this process we encounter a vertex that has already been visited and has the same color as the current vertex, it means players with existing constraints are in the same team, thus, no valid partition can exist.\nIf the DFS completes without detecting such a situation, the created 2-color partitions are the required team allocations.\n\nProof of Correctness §\nThe underlying premise for this solution relies on the properties of Bipartite Graphs.\n\nIf it’s possible to divide players into two distinct groups such that no two players from the same team have constraints between them, the graph is bipartite.\nDuring the 2-color DFS check, if we come across an already visited and colored node that has the same color as the current node, we conclude that the graph isn’t bipartite - thus, such a partition doesn’t exist. If DFS completes without such contradictions, it means the graph is bipartite and we’ve found a valid partition.\n\nRunning Time §\n\nConstructing the graph takes O(m) time as every constraint creates an edge in the graph.\nRunning DFS across all vertices and edges of our graph takes O(n+m) time.\n⇒ Total running time of the algorithm: O(m)+O(n+m)=O(n+m).\n"},"CS330-HW07":{"title":"CS330 HW07","links":[],"tags":["Courses"],"content":"Problem 1 §\nAlgorithm §\n# each item is in format (time, start/end, event_index)\n# order priosity: smaller time -&gt; start -&gt;\nQ = sort(L, R)\nEvents = []\n\nwhile Q has elements:\n\n\tif Q.next is a start time\n\t\tt &lt;- Q.dequeue\n\n\t\t# if there is a tie in start times\n\t\twhile Q.next.start_time == t.start_time:\n\t\t\tt &lt;- max_end_time(t, Q.dequeue)\n\t\t\t\n\t\t# t is now the earliest starting time with maximum duration\n\t\n\telse Q.next is an end time\n\n\t\t# get latest end time\n\t\twhile Q.next is an end time\n\t\t\tt &lt;- Q.dequeue\n\t\t\t\n\t\t# t is now the latest end time\n\t\t\n\tadd t&#039;s event to Events\n\t\n\tend if\nend while\n\nComplexity §\n\nSorting: O(nlogn)\nLoop: O(n)\n⇒ Total: O(nlogn)\n\nCorrectness §\n\nBase case\n\nLet Y∗=(y1​,…) the optimal solution\nAssume Y∗’s y1​ is not the earliest start time……(1)\nAlgorithm chooses earliest start time x1​.\n\n[x1​.start,y1​.start] is in total cover, but not in Y∗’s cover.\nThis means Y∗ cannot be optimal. This is a contradiciton\nThus Assumption (1) must be wrong\n\n\n\n\nInductive Step\n\nLet Y∗=(x1​…xk−1​,yk​,…) is the optimal solution\nY∗’s yk​ is the optimal next choice after xk−1​\nCase 1: next item in priority queue is an end time\n\nAlgorithm chooses xk​ where…\n\nxk​.start&lt;xk−1​.end\nxk​.end&lt;yk+1​.start\namong such points, max(xk​.end)\n\n\nAssume yk​ is an event that doesn’t satisfy the maximum endtime requirement……(2)\n\nThen, [yk​.end,xk​.end] is in total cover, but not in Y∗’s cover. This is a contradiction.\nThus assumption (2) must be wrong\n\n\n\n\nCase 2: next item in priority queue is a start time\n\nAlgorithm chooses xk​ with the earliest starting time and longest duration\nThis is at least as optimal as yk​, because…\n\nxk​.start≤yk​.start\nxk​.end≤yk​.end\nThus the cover of xk​ is as large as yk​\n\n\nThus xk​ is as optimal as yk​\n\n\n\n\nQED\n\nProblem 2 §\n// Idea: Reverse the order of weights in the graph,\n// then run minimum spanning tree.\n\n// First, reverse the weight ordering of the graph\n\n// negate weights...\nfor e in E\n\te&#039; &lt;- e\n\te&#039;.weight &lt;- -e.weight\n\tadd e&#039; to E&#039;\n\nlet m be the edge with minimum weight in E&#039;\n\n// then monotonically increase so all weights are positive \n// (to use MST algorithm)\nfor e in E&#039;\n\te&#039;.weight += m.weight + 1\n\n// Then, run MST algorithm (Prim or Kruskal)\nV_mst, E_mst &lt;- MST(V, E&#039;)\n\n// the minimum spanning tree in (V, E&#039;) is\n// same as the maximum spanning tree in (V, E)\n\n// Return the edges that are not in the MST.\nreturn E \\ E_mst\nCorrectness §\n\nThe problem is equivalent to getting a tree whose sum of edge weights is maximal.\n\nThis is because the remaining graph must be acyclic (a tree) and…\n…because the sum of removed weights is minimal thus the sum of remaining weights must be maximal.\n\n\nWe can convert the original graph in question to a weight order reversed graph:\n\nSuppose E={e1​,e2​,…,em​} where ei​ has weight wi​\nThen, E′={e1′​,…em′​} where ei′​ has weight wm−i+1​\n\n\nThen, we can leverage the correctness of the MST algorithm (prim’s or kruskal’s) in order to get the minimum spanning tree of this new weight-order-reversed-graph.\nThe edges in this MST is the maximum spanning tree in the original graph\nTherefore, removing these edges we are left with the subset F which is minimal\n\nComplexity §\n\nnegating weights: O(m)\nmonotonically raising weights positive: O(m)\nMST algorith: O(mlogn)\nTotal: O(mlogn)\n"},"CS330-HW08":{"title":"CS330 HW08","links":[],"tags":["Courses"],"content":"Problem 1 §\nAs outlined in This Ed Discussion I assume I will have available the whole flow function at my disposal.\n// Find the min-cut edges\n// let f[e] the max-flow capacity by the flow function\n// let e.cap be the capacity of edge e\n\nRESIDUAL(V, E, f):\n    let r_E = [] // for residual edges\n    for edge e (u-&gt;v) in E:\n        forward_res = e.cap - f[e] // forward residual capacity\n        if forward_res &gt; 0:\n            add edge e (u-&gt;v) with e.cap = forward_res to r_E\n        if f[e] &gt; 0:\n            add edge e&#039; (v-&gt;u) with e&#039;.cap = f[e] to r_E // Only add reverse edge if flow &gt; 0\n    return r_E\n\n// Run DFS to find reachable vertices in residual graph\nreachable = DFS(s, r_E)\nunreachable = V - reachable\n\n// Find min-cut edges\nmin_cut_E = []\nfor a in reachable:\n    for b in unreachable:\n        if edge e (a-&gt;b) in E:\n            add e to min_cut_E\n\nINCREMENT(e):\n    if e not in min_cut_E:\n        f[e] = f[e] + 1\n    else:\n        r_E = RESIDUAL(V, E, f)\n        path = BFS(s, t, r_E) // augmenting path in res graph\n        if path exists:\n            augment_flow(path, 1) // flow augmentation from class\n            // Augment flow along path by 1\n    return f\n\nDECREMENT(e):\n    if f[e] &gt; 0 and e in min_cut_E:\n        f[e] = f[e] - 1\n    else:\n        r_E = RESIDUAL(V, E, f)\n        path = BFS(s, t, r_E) // augmenting path in res graph\n        if path exists:\n            augment_flow(path, -1) \n            // Decrease flow along path by 1\n    return f\n\nCorrectness §\n\nThis leverages the correctness of the max-flow and min-cut algorithm\n\n\nIncrement function\n\nIf the increment is in not in a min-cut the graph is still bottlenecked at the min-cut, and the maximum flow cannot increase. The flow function is the same\nIf the increement is in the min-cut, there may be more flow to be had. T\n\nhere is need only to construct and update using the residual network once because the increment is by 1, which means any path found by BFS on the residual graph is guaranteed to increase the max-flow.\nAlternatively, if the BFS doesn’t find any path in the residual network, this means that the graph has a different min-cut due to the capacity change. But the flow function wouldn’t change because there would be nothing to update (again, leveraging the correctness of the max-flow augmentation step.)\n\n\n\n\nDecrement function\n\nIf the decrement is in a min-cut the graph’s bottleneck is reduced. The max flow will be reduced by 1\nIf the decrement is not in a min-cut the graph’s bottleneck is not reduced, but there may be a change in flow. Use the flow augmentation step (symmetric to the increment in-min-cut situation) to update the flow function\n\n\n\nComplexity §\n\nResidual calculation: O(V+E) but assume V&lt;E thus O(E)\nUsing BFS shortest augmenting path: O(E)\nTotal: O(E)\n\nProblem 2 §\nN = Set of visitors, where n in N is the set of demographics\n// e.g. N[0] = {male, 40-50}\n\nM = Set of advertisers and their preferences and number of ads\n// e.g. M[0] = {male, female}, M[0].showCount = 3\n\n// Construct graph G = (V,E) such that\nfor n in N:\n\tadd n to V\nfor m in M:\n\tadd m to V\n\n// for every advertiser...\nfor m in M:\n\t// check if each visitor has any acceptable demographic\n\tfor n in N:\n\t\tfor dem in M:\n\t\t\tif n contains dem:\n\t\t\t\tadd (n-&gt;m,1) to E // directed edge n-&gt;m weight 1\n\n// Add start and end vertex for max-flow\n\nadd s, t to V\nfor n in N:\n\tadd (s-&gt;n, 1)\nfor m in M:\n\t// by allowing bigger capacity here, required # of visitors\n\t// accessed by the advertisers\n\tadd (m-&gt;t, m.showCount)\n\n// Calculate the max-flow with algorithm from class\nMaxFlow(G)\n\nfor every vertex m such that (m-&gt;t) in E:\n\tif edge (m-&gt;t) is at capacity:\n\t\tcontinue\n\telse // not at capacity; advertiser isn&#039;t satisfied\n\t\treturn False\nreturn true\n\nCorrectness §\n\nIdea is to transform the problem into the matching edges problem described in class, which is a variant of the max-flow problem\n\n\nVisitor is guarateed to be shown only one ad, because s→n∈N has capacity 1, meaning that for any edge n→m∈M only one of them can be used in max-flow (recall all edges n→m for any n∈N,m∈M has capacity 1)\nAdvertiser is guaranteeed to be shown their ad only to the right demographic group, due to the edge matching conditions in line 13-19.\nIf all edges m→t where m∈M is at capacity, it means that the ads have the shown as much as the advertiser requested (showCount). This is because all edges n→m for any n∈N,m∈M has capacity 1, and thus max-flow must use showCount number of n→m edges for max-flow.\n\nThus, if the max-flow algorithm ends up using the full capacity of these edges, the conditions of the problem are feasible\nElse, the advertisers requested showCount has not been satisfied and the conditions are not feasible.\n\n\n\nComplexity §\n\nlet ∣N∣=n,∣M∣=m. Note that both max(∣M[i]∣),max(∣N[i]∣)=O(k) where k is the number of demographic groups\nGraph Construction: line 8-11 O(n+m), line 14-19 O(n⋅m⋅max(∣M[i]∣)⋅max(∣N[i]∣))=O(nmk2)\nMax-flow using BFS shortest augmenting path: O((n+m)(nm+n+m))\n\nn+m+2 is the number of vertices\nnm is the number of edges between n∈N,m∈M\nn+m is the number of edges between s→n, m→t\n⇒ Total O(nm(n+m))\n\n\nChecking m→t is at max capacity: O(m)\nTotal: O(nm(n+m+k2))\n"},"CS330-HW10":{"title":"CS330 HW10","links":[],"tags":["Courses"],"content":"Problem 1 §\nAlgorithm is one that assigns every one element to the set with the smaller sum.\n(a) §\n\nWe first establish that the cost of the algorithm ALG=∑x∈S​x where S is the bigger one of A,B.\n\nlet X={x1​…xn​} where xi​ is the i-th processed element by ALG.\n\n\nALG=∑x∈S​x−xj​+xj​ where xj​ is the last item added to set S\n\n! Looking at the first term, we can establish ∑x∈S​x−xj​≤21​(∑i&lt;j​xi​)……(1)\n\ni.e. Sum in S before xj​≤Average of total before xj​\nWe know this because Sum in S before xj​ is the smaller sum of A,B, and thus the average is bigger than the minimum of the two.\n\n\nThen we also establish ∑i&lt;j​xi​≤∑i&lt;j​xi​+∑i&gt;j​xi​=∑x∈X​x−xj​\n\ni.e. Sum before xi​ ≤ sum before xj​ and sum after xj​ = total sum excluding xj​\n! This implies 21​∑i&lt;j​xi​≤21​(∑x∈X​x−xj​)…….(2)\n\n\n&amp; (1) and (2) together shows ∑x∈S​x−xj​≤21​(∑x∈X​x−xj​)=21​∑x−21​xj​……(3)\n\n\nWe thus show ALG≤21​∑x−21​xj​+xj​=21​∑x+21​xj​……(4)\n\nWe know from the lecture (makespan, lemma 1 and 2) that 21​∑x≤OPT and xj​≤OPT\n\nfirst, because optimal solution cannot be smaller than the total / 2\nsecond, because optimal solution cannot be smaller than any element\n\n\n\n\nThus we establish ALG≤OPT+21​OPT=23​OPT\n\n\n\n\nThus proven\n\n(b) §\n\nWe first investigate the trivial cases when X has respectively 1,2 and 3 elements\n\nCase n=1\n\nOPT=x1​, ALG=x1​ thus α=1\n\n\nCase n=2\n\nOPT=max[x1​,x2​], ALG=max[x1​,x2​] thus α=1\n\n\n\n\nWe then investigate the case when n≥3\n\nAs we also know that x1​&gt;x2​&gt;x3​&gt;⋯&gt;xn​ in the order of assignment by ALG, we can state both that:\n\nix1​+⋯+xi​​≥xi​ thus x1​+⋯+xi​≥ixi​……(1)\nxj​≥n−jxi+1​+⋯+xn​​ thus xj​≥xi+1​+⋯+xn​……(2)\n(1)−(2) yields ∑k=1…i​xk​−xj​≥\n\n\n\n\n\nProblem 2 §\nCode §\nimport csv\nfrom typing import List, Tuple\nimport time\nfrom math import radians, cos, sin, asin, sqrt\nimport random\n \n# Function to load data from CSV file\ndef load_data(riders_filepath) -&gt; List[Tuple[float, float]]:\n    passengers = []\n    with open(riders_filepath, &#039;r&#039;) as file:\n        csv_reader = csv.reader(file)\n        next(csv_reader, None)  # Skip the header\n        for row in csv_reader:\n            sourceLat = float(row[1])\n            sourceLon = float(row[2])\n            passengers.append((sourceLat, sourceLon))\n    return passengers\n \n# Assuming the Haversine function is replaced by a simple Euclidean distance for this simulation\ndef euclidean_distance(point1, point2):\n    return sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n \n# 2-approximation algorithm for k-center clustering using Euclidean distance\ndef k_center_clustering_euclidean(passengers, k, n):\n \n    # sample n elements from passengers\n    passengers = random.sample(passengers, n)\n \n    start_time = time.time()\n \n    # Choose an arbitrary point as the first center\n    hubs = [random.choice(passengers)]\n    for _ in range(1, k):\n        # Find the next hub as the point farthest from any existing hub\n        next_hub = max(passengers, key=lambda p: min(euclidean_distance(p, hub) for hub in hubs))\n        hubs.append(next_hub)\n \n    end_time = time.time()\n    \n    # Assign each point to the closest hub and calculate the cost\n    assignments = {hub: [] for hub in hubs}\n    for passenger in passengers:\n        closest_hub = min(hubs, key=lambda hub: euclidean_distance(passenger, hub))\n        assignments[closest_hub].append(passenger)\n \n    \n    # The cost is the maximum distance of any point to its assigned hub\n    cost = max(euclidean_distance(passenger, closest_hub)\n               for closest_hub, passengers in assignments.items() for passenger in passengers)\n    \n    return hubs, assignments, cost, end_time - start_time\n \n# Example passengers data (replacing the real CSV loading)\npassengers = load_data(&quot;./passengers.csv&quot;)\nprint(len(passengers))\n \n# Run the algorithm for different values of k and record the results\nprint(&quot;K&quot;)\nresults = {}\nfor k in range(50,200,10):\n    hubs, assignments, cost, runtime = k_center_clustering_euclidean(passengers, k, 100)\n    print(k, &quot;,&quot;, runtime)\n \nprint(&quot;N&quot;)\nfor n in range(1000, 5000, 100):\n    hubs, assignments, cost, runtime = k_center_clustering_euclidean(passengers, 10, n)\n    print(n, &quot;,&quot;, runtime)\n \nResults §\n\nProblem 3 §\nimport csv\nfrom typing import List, Tuple\nimport time\nfrom math import sqrt\nimport random\nimport numpy as np\n \n# Function to load data from CSV file\ndef load_data(riders_filepath) -&gt; List[Tuple[float, float]]:\n    passengers = []\n    with open(riders_filepath, &#039;r&#039;) as file:\n        csv_reader = csv.reader(file)\n        next(csv_reader, None)  # Skip the header\n        for row in csv_reader:\n            sourceLat = float(row[1])\n            sourceLon = float(row[2])\n            passengers.append((sourceLat, sourceLon))\n    return passengers\n \n# Euclidean distance function\ndef euclidean_distance(point1, point2):\n    return sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n \n# Function to initialize centroids randomly\ndef initialize_random(passengers, k):\n    return random.sample(passengers, k)\n \n# Function to initialize centroids using k-means++\ndef initialize_kmeans_plusplus(passengers, k):\n    centroids = [random.choice(passengers)]\n    # probabilistic initialization\n    for _ in range(1, k):\n        distances = [min(euclidean_distance(p, c) for c in centroids) for p in passengers]\n        probabilities = [d ** 2 for d in distances]\n        total = sum(probabilities)\n        probabilities = [p / total for p in probabilities]\n        next_centroid = random.choices(passengers, weights=probabilities, k=1)[0]\n        centroids.append(next_centroid)\n    return centroids\n \n# Assign passengers to the nearest centroid\ndef assign_passengers(passengers, centroids):\n    assignments = {}\n    for passenger in passengers:\n        closest_centroid = min(centroids, key=lambda centroid: euclidean_distance(passenger, centroid))\n        if closest_centroid in assignments:\n            assignments[closest_centroid].append(passenger)\n        else:\n            assignments[closest_centroid] = [passenger]\n    return assignments\n \n# Update centroids based on the mean of assigned points\ndef update_centroids(assignments):\n    centroids = []\n    for points in assignments.values():\n        centroids.append(tuple(np.mean(points, axis=0)))\n    return centroids\n \n# Calculate the quality of solution\ndef calculate_quality(assignments):\n    total_distance = 0\n    for centroid, points in assignments.items():\n        total_distance += sum(euclidean_distance(centroid, p) ** 2 for p in points)\n    return total_distance / len(assignments)\n \n# k-means algorithm\ndef k_means(passengers, k, initialize_func):\n    # Initialize centroids\n    centroids = initialize_func(passengers, k)\n    assignments = {}\n    for _ in range(20):  # fixed number of iterations for simplicity\n        # Assign passengers to the nearest centroid\n        assignments = assign_passengers(passengers, centroids)\n        # Update centroids\n        centroids = update_centroids(assignments)\n    return centroids, assignments\n \n# Run the k-means algorithm with different initializations\ndef run_k_means(passengers, k_values):\n    results = {k: {&#039;random&#039;: int, &#039;k-means++&#039;: int} for k in k_values}\n    for k in k_values:\n        for _ in range(3):  # Repeat the experiment 3 times\n            temp_results = {init: {&#039;runtime&#039;: 0, &#039;quality&#039;: 0} for init in (&#039;random&#039;, &#039;k-means++&#039;)}\n            for init, func in [(&#039;random&#039;, initialize_random), (&#039;k-means++&#039;, initialize_kmeans_plusplus)]:\n                start_time = time.time()\n                centroids, assignments = k_means(passengers, k, func)\n                runtime = time.time() - start_time\n                quality = calculate_quality(assignments)\n                temp_results[init][&quot;runtime&quot;] += runtime\n                temp_results[init][&quot;quality&quot;] += quality\n        # average the results\n        results[k][&#039;random&#039;] = (temp_results[&#039;random&#039;][&#039;runtime&#039;] / 3, temp_results[&#039;random&#039;][&#039;quality&#039;] / 3)\n        results[k][&#039;k-means++&#039;] = (temp_results[&#039;k-means++&#039;][&#039;runtime&#039;] / 3, temp_results[&#039;k-means++&#039;][&#039;quality&#039;] / 3)\n    return results\n \n# Example passengers data (replacing the real CSV loading)\n# Here we need to load the data from the file provided by the user\n# passengers = load_data(&quot;/path/to/passengers.csv&quot;)\n \npassengers = load_data(&quot;./passengers.csv&quot;)\nprint(len(passengers))\n \n# Define k values to test\nk_values = [2, 4, 6, 8]\n \n# Run the k-means algorithms and print the results\nresults = run_k_means(passengers, k_values)\nprint(results)\n \nResults §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of ClustersInitializationRuntimeQuality2random0.098172.265082k-means++0.097902.265084random0.156750.771634k-means++0.159210.721646random0.205300.395366k-means++0.215130.350478random0.265860.233318k-means++0.289570.20059"},"Cache":{"title":"Cache","links":["LRU-algorithm"],"tags":["Computing/Computer-Architecture"],"content":"# of Frames​=Block SizeCache Size​=(# of sets)×(# of ways)=(2# of index bits)×(assoc.)​\n\nMost popular caching method: LRU algorithm\n"},"Capital-(Marxism)":{"title":"Capital (Marxism)","links":[],"tags":["Humanities/Marxism"],"content":"Defining Capital §\nCapital is both\n\nstored up labor (as labor produces capital goods), and\npower to command labor:\n\n\nThe capitalist possesses this power not on account of his personal or human properties but in so far as he is an owner of capital. His power is the purchasing power of his capital, which nothing can withstand.\n\nConsider Capital profit and labor wage as fundamentally different products:\n\nprofits are in proportion to the amount of capital, but\nwage is determined by the amount of required subsistence:\n\n\nFirstly, the profits of capital are regulated altogether by the value of the stock employed, […] Furthermore, in many large factories the whole labour of this kind is committed to some principal clerk, whose wages never bear any regular proportion to the capital of which he oversees the management.\n\n\n[Laborer] becomes an appendage of the machine, and it is only the most simple, most monotonous, and most easily acquired knack, that is required of him. Hence, the cost of production of a workman is restricted, almost entirely, to the means of subsistence that he requires for maintenance,\n"},"Cardinality":{"title":"Cardinality","links":["Zermelo-Frankle-Set-Theory-with-Axiom-of-Choice-(ZFC)","Limits-of-Math-and-Computing"],"tags":["Math"],"content":"Referencing How Infinity Works (And How It Breaks Math) - YouTube\ndef. ℶ0​ is the cardinality of natural numbers\ndef. ℶ1​ is the cardinality of real numbers\n\nℶn+1​=2ℶn​ i.e. ℶn+1​ is the cardinality of all the subsets of ℶn​\n\nContinuum Hypothesis §\nhyp. There is no set whose cardinality is between that of integers and real numbers.\n\nExpressed using above notation: ℶ1​=?ℵ1​\nThis hypothesis is proved to be unprovable within ZFC, according to Limits of Math and Computing\n"},"Cartels-and-Collusion":{"title":"Cartels and Collusion","links":["Prisoner's-Dillemma"],"tags":["Economics/Game-Theory","Economics/Micro-Economics"],"content":"Cartels and the Natural Incentive to Defect §\ndef. Cartels are when firms collude to act as a single monopolistic firm.\nHowever, cartels cooperating is not a stable set of strategies, because it has a tendency to unravel, as it is a Prisoner’s Dillemma situation.\nWe can analyze this for two firms in the following two ways:\n\n\n\nUsing the profit for the hypothetical monopoly firm (graph a)\n\nTwo firms each produce 21​xM, and thus together produce xM at pM.\nIf I secretely produces one more unit of good:\n…both will lose red area together [=half each]\n…I will gain the profit of selling the additional units [blue area]\n\n→ Thus I am incentivized to defect and produce more.\n\n\nUsing residual demand for one of the firms\n\nAssume the other firm is diligently cooperating to produce their 21​xM.\nI will face a residual demand curve [=Dr] which is shifted left 0.5xM\nmy best option is to produce at MRr=MC which is 0.75xM, bigger than the promised 0.5xM\n\n→ Thus I am incentivized to defect and produce more\n\n\n\n\n                  \n                  Prisoner’s Dillemma situation:※\n                  \n                \n\n\nAside on Prisoner’s Dilemma §\n\n\n                  \n                  Quote \n                  \n                \ndef. Prisoner’s Dillemma (PD) is any game where the following payoff structure holds…\n…where T&gt;R&gt;P&gt;S:\n\n\nFinitely Iterated PD §\nIn a finitely iterated PD, the Subgame Perfect Nash Equilibrium is to defect every game.\n\nSolving from the last game:\n\nLast Game: Knowing there will be no more games [= same as one-shot game], your dominant strategy is defection.\n2nd to last: Knowing that next game both will defect, your dominant strategy is defection.\n…\nThe whole game unravels into a defection.\n\nInfinitely Iterated PD §\nGame design with:\n\nInfinitely many games\nprobability that one will meet [γ]\ndiscount parameter [PV=rFV​]\n\nIn this case there are multiple Nash Equilibirum Strategies\n\nALL C is a BR to ALL C → (ALL C, ALL C) is an NE\nTRIGGER is a BR to TRIGGER → (TRIGGER, TRIGGER) is an NE\nTrigger Strategy:= strategy where the opponent’s current action triggers my future behavior. e.g. “I will coop. until opp. defects, and defect always after that.”\nTFT is a BR to TFT → (TFT, TFT) is an NE\n\n\n\n                  \n                  These strategies are subgame perfect. Let’s prove that for (TRIGGER, TRIGGER): \n                  \n                \n\n\nIf we’ve always cooperated before, this subgame is same as original game\n→ (TRIGGER, TRIGGER) is an NE\nIf there was non-cooperation before, this subgame has NE:\n→ (ALL D, ALL D) is an NE\n\n→ All subgames have NE (TRIGGER, TRIGGER).\n∴ (TRIGGER, TRIGGER) is a SPNE"},"Cash-Sweep":{"title":"Cash Sweep","links":[],"tags":["Economics/Finance"],"content":"when money is automatically moved into a bank account based on a certain threshold. The money is then put into a higher interest-earning account such as a high interest saving account, money market mutual funds, or short-term certificates or can be used to payoff debt."},"Central-Limit-Theorem":{"title":"Central Limit Theorem","links":[],"tags":["Math/Probability"],"content":"\n\n                  \n                  Intuition: Selecting from a box of 1 to 10, \n                  \n                \n\nAs the sample size (n) increases, observe the following:\n\nμ is constant\nVar is increasing\nThe distribution apporoaches a bell curve\n\n\nthm. Law of Averages (=Law of Large Numbers) let X1​,…,Xn​ be random variables that are i.i.d. If E(X)=μ, and Xˉn​=nX1​+⋯+Xn​​ then for any small value of ϵ:\nn→∞lim​[P(∣X−μ∣&lt;ϵ)]=1\n\nthm. Central Limit Theorem (for averages) let X1​,…,Xn​ be random variables that are i.i.d. If E(X)=μ, SD(X)=σ and Xˉn​=n∑Xi​​ then for a big value of n:\nXˉn​∼Normal(μ,nσ2​)\n∵ for Xˉn​ where n is just a constant\n\nMean: E(Xˉn​)=E(nX1​+⋯+Xn​​)=n1​⋅n⋅E(Xi​)=μ\nVariance: Var(Xˉn​)=Var(nX1​+⋯+Xn​​)=n21​⋅n⋅Var(Xi​)=nσ2​\nStd. Dev: SD(Xˉn​)=Var(Xˉn​)​=n​σ​\n\nthm. Central Limit Theorem (for sums) let X1​,…,Xn​ be random variables that are i.i.d. If E(X)=μ, SD(X)=σ and Sn​=X1​+⋯+Xn​ then for a big n:\nSˉn​∼Normal(nμ,nσ2)\n∵ for Sn​ where n is just a constant\n\nMean: E(Sn​)=E(X1​+⋯+Xn​)=n⋅E(Xi​)=nμ\nVariance: Var(Sn​)=Var(X1​+⋯+Xn​)=n⋅Var(Xi​)=nσ2\nStd. Dev: SD(Sn​)=Var(Sn​)​=n​⋅σ\n\n\n\n                  \n                  Abstract \n                  \n                \nTo Summarize, for a big enough value of n, and Xˉn​ the average and Sn​ the sum:\n\nP(a&lt;Xˉn​&lt;b)≃∫ab​Normal[μ,n2σ​]=∫σ/n​a−μ​σ/n​a−μ​​Normal[0,1]=Φ(σ/n​b−μ​)−Φ(σ/n​a−μ​)P(a&lt;Sn​&lt;b)≃∫ab​Normal[nμ,nσ2]=∫n​σa−nμ​n​σb−nμ​​Normal[0,1]=Φ(n​σb−nμ​)−Φ(n​σa−nμ​)\nrmk. Binomal Approximtion using Normal Distribution. let the following:\n\nX∼Binom[n,p]\nIndicator functions s.t. X=I1​+⋯+In​ where Ii​ defines the i-th event is successful.\nNote that I1​,…,In​ are all i.i.d.\nthen…\nE(Ii​)=p\nVar(Ii​)=E(Ii2​)−[E(Ii​)]2=E(Ii​)−p2=p−p2\n(∵∀I=Indicator, E(I2)=E(I))\nFor a large enough n, X∼Normal[np,np(1−p)]\n(∵I1​,…,In​ are i.i.d., see the additional rule for expectated value and variance)\n\nLindenberg CLT §\nthm. Lindenberg Central Limit Theorem. Given random variables X1​…Xn​, with each E(Xk​)=μk​, Var(Xk​)=σk2​and the following conditions:\n\nThey are independent (No need to be identically distributed)\nlimn→∞​∑σk2​1​E((Xk​−μk​)21∣Xk​−μk​&gt;ϵ∑σk2​​) i.e. variance is not too big\n⇒ Then\n\n∑k=1n​σk2​​∑k=1n​(Xk​−μk​)​⟶n→∞d​N(0,1)"},"Centralized-Power":{"title":"Centralized Power","links":[],"tags":["Philosophy/Political-Philosophy"],"content":""},"Change-of-Variable-(Probability)":{"title":"Change of Variable (Probability)","links":[],"tags":["Math/Probability"],"content":"thm. Linear Change of Variable. let X s.t. P(X=x)=fX​(x). let Y=aX+b. Then…:\nfY​(y)=∣a∣1​fX​(ay−b​)\nthm. Change of Variable (bijective function). let X s.t. P(X=x)=fX​(x). let Y=g(X) where g(x) is an inversible function (=bijective). Then…:\nfY​(y)=∣dy/dx∣fX​[g−1(y)]​\n\nThe bottom is a x-axis reflective version of fX​\nThe left is a 90 degrees counterclockwise rotation of fY​\nThe graph is of Y=g(X) where g(x)=x​\n\nthm. Change of Variable (injective function). let:\n\nX,Y,fX​,fY​,Y=g(X)\nDomain(X) is partitioned [x1​,x2​),[x2​,x3​),… s.t. g(x) is bijective in each interval.\n\nthen fY​(y) can be defined on interval [xi​,xi+1​)…:\nfY​(y)=x:y=g(x)∑​∣dy/dx∣fX​[g−1(y)]​ , when y∈g([xi​,xi+1​))"},"Chebyshev's-Inequality":{"title":"Chebyshev's Inequality","links":[],"tags":["Math/Statistics"],"content":"\nthm. Chebyshev’s Inequality. let X s.t. E[X]=μ, Var[X]=σ2 where σ is not infinite. Then:\n\n∀ϵ&gt;0,P(∣X−μ∣&gt;ϵ)≤ϵσ2​\n\nIntuitively, it means that “only few” data points are “far away” from the mean for any “reasonable” distribution:\n\n“only few”: “≤ϵσ2​”\n“far away”: “∣X−μ∣&gt;ϵ”\n“reasonable”: σ is not infinite.\n\n\n"},"Chi-Squared":{"title":"Chi-Squared","links":["Gamma-Distribution"],"tags":["Math/Common-Distributions"],"content":"χn2​∼i=1∑n​Zi2​\nalso related to the Gamma Distribution:\n\nΓ(n,2)∼χn2​\n"},"Chompsky-Heirarchy":{"title":"Chompsky Heirarchy","links":["Formal-Grammar","Formal-Languages","Turing-Machine","Unrestricted-Grammar","Recursively-Enumerable-Languages","Linear-Bound-Automata","Context-Sensitive-Grammar","Recursive-Languages","Pushdown-Automata","Context-Free-Grammar","Context-Free-Language","Finite-Automata","Regular-Expressions","Regular-Grammar"],"tags":["Computing/Formal-Languages"],"content":"The Chompsky Heirarchy is the levels of capabilities of automata=grammar=languages.\nCapabilities of Automata §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType of AutomataMemoryCan…Can’t..Finite AutomataNonerecognize integersrecgz. arith. expr.Push-down AutomataStackrecgz. arith. expr.compute arith. expr.Turing MachineInfinitecompute arith. expr.determine halting probl.\nThe Heirarchy §\n\nTuring Machine == Unrestricted Grammar == Recursively Enumerable Languages\nLinear Bound Automata == Context Sensitive Grammar == Recursive Languages\nPushdown Automata == Context-Free Grammar == Context-Free Language\nFinite Automata == Regular Expressions == Regular Grammar\n\n"},"Chompsky-Normal-Form":{"title":"Chompsky Normal Form","links":[],"tags":["Computing/Formal-Languages"],"content":""},"Circular-Flow-of-Income":{"title":"Circular Flow of Income","links":[],"tags":["Economics/Macro-Economics"],"content":""},"Class-Traitors":{"title":"Class Traitors","links":[],"tags":["Humanities/Marxism"],"content":""},"Cobb-Douglas-Utility":{"title":"Cobb-Douglas Utility","links":["Uncompensated-Demand-curve","Utility-Function","Marginal-Willingness-to-Pay","Expenditure-Function"],"tags":["Economics"],"content":"def. Two-goods Cobb-Douglas Utility function:\nu(x1​,x2​)=x1α​x21−α​\nlet the income:\nI(p1​,p2​)=p1​x1​+p2​x2​\nUtility Maximization §\nmaxx1​,x2​​u=x1α​x21−α​  such that  I=p1​x1​+p2​x2​\nOrdinary Demand: §\n⎩⎨⎧​x1​(p1​,p2​,I)=p1​αI​x2​(p1​,p2​,I)=p2​(1−α)I​​\nIndirect Utility: §\nv(p1​,p2​,I)​:=u(x1​(p1​,p2​,I),x2​(p1​,p2​,I))=p1α​ p21−α​(1−α)1−α αα​⋅I​​\nExpenditure Minimization §\nminx1​,x2​​I=p1​x2​+p2​x2​  such that  u=x1α​x21−α​\nCompensated Demand: §\n⎩⎨⎧​h1​(p1​,p2​,uˉ)=(p2​p1​​α1−α​)αuˉh2​(p1​,p2​,uˉ)=(p1​p2​​1−αα​)1−αuˉ​\nExpenditure Function: §\nE(p1​,p2​,uˉ)​:=p1​h1​+p2​h2​=(1−α)1−α ααp1α​ p21−α​​⋅uˉ​​"},"Combination-(Probability)":{"title":"Combination (Probability)","links":[],"tags":["Math/Probability"],"content":"Notation §\ndef. Number of Choosing k from n items:\n(xn​)=⎩⎨⎧​(n−k)!⋅k!n!​0​n≥xelse​\nNOTATION. N⋅(N−1)⋅(N−2)⋯(N−k)=Nk−1​"},"Combinatorial-Auction":{"title":"Combinatorial Auction","links":[],"tags":["Economics/Game-Theory"],"content":"Motivation.\ndef. Combinatorial Auction.\n\n1…n bidders\nM={1…m} items\nΩ={(S1​,…,Sn​)∣is partition of M} ← “outcomes”\nEach bidder i has valuation vi​ for every Si​∈2M; i.e. every possible allocation they can get\n\nthm. VCG Mechanism Auction. The following mechanism if DSIC and Welfare Maximizing.\nCompute the optimum allocation S=(S1∗​,…,Sn∗​). Then for an agent i, let\n\nV∗:=∑j=in​vj​(Sj∗​) Optimal Social Welfare\nV−i∗​:=maxi=j​∑j=1n​vj​(Sj,−i​)\n\nwhere sj−1​ is the optimal allocation to j when i is simply removed from auction\nThus v−i∗​ is the optimal social welfare without i\n\n\nThus V∗≥V−i∗​\n\nbecause adding more bidders can only increase social welfare\n\n\n\nClaim. The following allocation rule is DSIC and welfare-maximizing:\nAllocate by max∑j=1n​vj​(Sj​)\nCharge price to i:\npi​= re-run auction without i V−i∗​​​−auction’s welfare − i’s welfare(V∗−vi​(Si∗​))​​\nIntuition. This is similar to charging i the externality of joining the auction;\nProof.\nFirst, 0≤pi​&lt;vi​(Si​), i.e. we don’t charge negative prices, or a price higher than the bid:\n\n without i V−i∗​​​&gt; excluding i V∗−vi​(Si∗​)​​ because , thus 0≤pi​\npi​=vi​(Si∗​)+&lt;0V−i∗​−V∗​​ because V∗&gt;V−i∗​, i.e. adding more bidders cannot decrease welfare.\n\nSecond, fixing other’s bids b−i​, i will be incentivized to tell the truth of bi​. Consider the situation where the auctioneer took these bids and allocated S^=(S1​^​,…,Sn​^​).\npi​​=V−i∗​−(V∗−vi​(Si∗​)):=maxS​j=i∑​bj​(Sj​^​)−j=i∑​bj​(Sj∗​)​​\nNow, the goal of bidder i is to maximize their utility:\nμi​(b)​=vi​(Si​^​)−pi​(b)=vi​(Si​^​)−​maxS​j=i∑​bj​(Sj​^​)−j=i∑​bj​(Sj∗​)​= i can maximize vi​(Si​^​)+j=i∑​bj​(Sj∗​)​​− i cannot control maxS​j=i∑​bj​(Sj​^​)​​​​\nThus, the problem of i maximizing their utility μi​ is equivlant, by design of the pricing mechanism, as maximizing vi​(Si​^​)+∑j=i​bj​(Sj∗​), and this is equivalent to social welfare of the original auction.\nmaxbi​​μi​(bi​,b−i​​)⟺maxS​i=0∑n​bj​(Sj​)\nThus i is incentivized to tell the truth. ■"},"Combined-Income-&-Substitution-Effects":{"title":"Combined Income & Substitution Effects","links":["Income-Effect-(IE)","Substitution-Effect-(SE)"],"tags":["Economics/Micro-Economics"],"content":"Income Effect (IE)\nSubstitution Effect (SE)\nCombined Income/Substitution Effects §\nIn cases where price of one good changes, income and substitution effects happen together. Imagine when gasoline prices rise:\n\n\nGraph (a) shows the change in the budget line due to the increase in price of gasoline.\nGraph (b) shows the compensated budget line to maintain the level of utility.\nGraph (c) shows the ultimate, uncompensated bundle C.\n\nThe process for both effect is as follows\n\nMove along the same utility curve from bundle A → B; substitution effect\nMove to the new budget line from bundle B → C; income effect\n→ this determines if good is normal (homo-thetic or not), quasi-linear, inferior; see below)\n\nIncome and Substitution Effects on Types of Goods §\nThe above case was when gasoline was a normal good. When gasoline is inferior the move due to income effect (B → C) is to the right. The degree to which this occurs coincidentally determines if the good is regular inferior, or (the strange case of) a Giffen good.\n\nObserve that for both graphs B → C is a rightward move, because lowering income (blue → red) increased consumption, meaning this is an inferior good. If C has more of gasoline than A, it is the strange Giffen good.\nBringing it Together §\nThe final is combining all of these movements together. In the following graph, the consumer consumes only shirts and pants. Then they’re given a coupon that discounts the price of shirts (blue → red). Using the compensated budget we determine the substitution effect (b, A → B). Then, depending on whether shirts/pants are normal, inferior (or quasilinear) we can determine where the final income effect (B → C) will lie.\n\nMathematically… §\nAssume:\n\nUtility curve uA​=u(x1​,x2​)\nBudget line I=p1​x1​+p2​x2​\nYour bundle A(x1A​,x2A​),\n\nThen occurs a change in price p1​→p1′​, causing a change in the budget line from I→I′. Then, consider how much you should compensate your income, to stay in the same indifference curve with utility uA​. That is, what is the minimum income that I can spend to reach a certain indifference curve?\nmin p1′​x1​+p2​x2​ s.t.uA​=u(x1​,x2​)\nThus you find the bundle B(x1B​,x2B​), with the size of the substitution effect x1A​→x1B​\nSince this compensated budget is a hypothetical scenario, we will need to move back to the original budget.\nmax u(x1​,x2​) s.t.I′=p1′​x1​+p2​x2​\nto find bundle C(x1C​,x2C​)."},"Commodities":{"title":"Commodities","links":["Futures"],"tags":["Economics/Finance"],"content":"\nOften traded at the Chicago Mercantile Exchange (CME)\nTraded in Futures\n"},"Common-Graph-Problems":{"title":"Common Graph Problems","links":["Traveling-Salesperson-Problem","Minimal-Spanning-Tree-Problem","Common-Graph-Problems"],"tags":["Computing/Algorithms"],"content":"See also:\n\nTraveling Salesperson Problem\nMinimal Spanning Tree Problem\n\nQ. Clique Problem. Given a graph G=(V,E) what is the maximum set of vertices C such that all vertices v in C are fully connected, i.e. for every u,v∈C, there exists edge (u,v)∈E\n\n\nNP-complete problem\n\nQ. Independent Set Problem. (=Stable set =anti-clique) Given a graph G=(V,E) what is the maximum set of vertices C such that no edges connect any two vertices in this set? \n\nComplement with vertex cover.\n\nC is an maximal independent set ⇔ V−C is a minimal vertex cover\nReduction both ways is trivial\nthm. 3-SAT ≤p​ Independent Set\n\n\nConstruction: each clause to a fully connected tri-vertex component, and connect the variable and its negations between tri-vertex components. \n∃x1​…xn​ that satisfies Q where Q has k clauses ⇔ ∃ Independent Set of size k.\n\n\n\nQ. Vertex Cover. Given G=(V,E) find the minimal subset C⊆V such that it covers all edges in the graph.\n\n\ne.g. graph that has a vertex cover comprising 2 vertices (bottom), but none with fewer.\nNP-complete problem\nC is a minimal vertex cover ⇔ V−C is a maximal Independent Set\n\nRed is vertex cover, and white is independent set: \n\n\n\nalg. Approximate Vertex Cover.\n\nChoose any edge e that connects vertices u,v\nremove edge e from graph, as well as any edges that connected to u and v\n\nu,v are added to the vertex cover set\n\n\nRepeat until no edges remain\n\n\nIs a 2-approximation\n\nalg. Approximate Greedy Vertex Cover.\n\nChoose vertex with maximum degree\nAdd this as part of vertex cover. Remove edges connected to this vertex\nRepeat until no edges remain\n\n\nIs not optimal.\n\nQ. Triangle Cover. Given G=(V,E) find the minial subset of vertices that it covers at least one vertex per triangle, for every triangle in graph.\n\nTriangle Cover is NP-complete (reduce from vertex cover)\n\nQ. Dominating Set. Given G=(V,E) find the minial subset S of vertices such that, every vertex in G is either in S or is a neighbor of S\nQ. Critical Vertices. Given connected graph G=(V,E) find all vertices that when removed will disconnect the graph."},"Communism":{"title":"Communism","links":["Private-Property"],"tags":["Humanities/Marxism"],"content":"“Crude” Communism §\n\nThe first positive abolition of private property - crude com­ munism - is therefore only a manifestation of the viieness of private property trying to establish itself as the positive com­ munity.\n\n\nPhysical, immediate possession is the only purpose of life and existence as far as this communism is concerned; the category of worker is not abolished but extended to all men […] The crude communist is merely the culmination of this envy and desire to level down on the basis of a preconceived minimum. It has a definite, limited measure.\n\n\n(2) Communism (a) still of a political nature, democratic or despotic; (b) with the abolition of the state, but still essentially incomplete and influenced by private property, i.e. by the estrange­ ment of man.\n\nActual Communism §\n\nCommunism is the positive supersession of Private Property as human self-estrangement, and hence the true appropriation of the human essence through and for man\n\n\nit is the.genuine resolution of the conflict between man and nature, and between man and man, the true resolution of the conflict between existence and being, between objectification and self-affirmation, between freedom and necessity, between individual and species. It is the solution of the riddle of history and knows itself to be the solution.\n\nand thus\n\nIf we characterize communism itself - which because of its character as negation of the negation, as appropriation of the human essence which is mediated with itself through the negation of private property\n"},"Comparable-Company-(Comps)-Analysis":{"title":"Comparable Company (Comps) Analysis","links":["Income-Statement","EBITDA-Multiple","Price-to-Earnings-Ratio","Balance-Sheet","Free-Cashflow"],"tags":["Economics/Finance"],"content":"Comps Analysis:= comparing companies with multiples.\n\n\n                  \n                  More growth potential = Better.\n                  \n                \n\nBut make sure that the companies being compared are similar in size/industry/geographic regions.\nMultiples Analysis §\n:= using the ratios of accounting line items (like EBITDA, income, etc.) to evaluate the potential growth of a firm.\n\nRatios are calculated from estimations of future balance sheet line items.\nWe use rate of return to compare investments, not price. r=P1​P2​−P1​​. You can’t compare price.\n\nThe following are commonly used multiples:\n\nEBITDA Multiple\nPrice to Earnings Ratio\n\nAccounting Sheets §\n\nBalance Sheet\nIncome Statement\nFree Cashflow\n"},"Compensating-and-Equivalent-Variation":{"title":"Compensating and Equivalent Variation","links":["Utility-Function"],"tags":["Economics/Micro-Economics"],"content":"Q. Coupon Exchange Problem. Josh and Andrew have identical utility functions u(x1​,x2​) where x1​ is milkshakes and x2​ is toys.\n\nTheir incomes are the same\nJosh has a 50% off coupon for x1​.\n⇒ Can Josh and Andrew work out a trade where Andrew buys the coupon?\nFigure out\nLeast Josh would take for the coupon using the expenditure function\nMost Andrew would pay for the coupon (also using the expenditure function)\n⇒ Trade occurs when (least Josh would sell for) &lt; (most Andrew would pay for)\n\n\n\n                  \n                  Idea \n                  \n                \nCV and EV - YouTube\nOh no! the price of good x1​ increased!\n\n🥺 She’s now at a lower Indifference Curve. How much should we give as compensation to get her back on the original indifference curve? ⇒ Compensating Variation\n😈 She’s at a lower indifference curve now. What if instead of having the price of good x1​ increase, we just take money away from her directly? ⇒ Equivalent Variation\n\n\ndef. Compensating Variation (CV). When a price of a good changes, the Compensating variation is the change in expenditure (±) required to maintain the utility.\ndef. Equivalent Variation (EV). An individual may trade a change in income (±) for a change in prices.\n\n\n\nCompensating Variation (CV) is how much money an individual would have to get (or have taken away) to be indifferent to the change in prices"},"Computational-Tractability":{"title":"Computational Tractability","links":["CS-330-Advanced-Algorithms","Chompsky-Heirarchy","Turing-Machine","Independent-Set-Problem","Common-Graph-Problems","Subset-Sum","Traveling-Salesperson-Problem","Hamiltonian-Path/Cycle"],"tags":["Computing/Algorithms","Computing/Formal-Languages"],"content":"Definitions §\nthm. Satisfiability Reducibility. All problems can be reduced to boolean satisfiability problems.\n\nOptimization problems: choose some k, then ask “is there a solution ≤k?, ≥k?” =&gt; Then do a binary search on [0,k] or [k,upper bound]\n\ndef. Computationally tractable problems. There exists a polynomial time algorithm O(nk) for the solution. The set of all computationally tractable problems is denoted P.\n\nMost problems in CS 330 Advanced Algorithms\n\nthm. Polynomial Reduction. Problem π1​:X1​↦{0,1} that is an element of P can be reduced to problem π2​:X2​↦{0,1} problem which is also in P, if there exists a function f:X1​↦X2​ such that:\n\nπ1​(x1​)=1⟺π2​(f(x1​))=1\nf(x) takes polynomial time to complete\n\nf(x) can be a many-to-one function\n\n\n\n\nThis is denoted π1​≤p​π2​\nVisually: \nImplications of when π1​≤p​π2​\n\nπ2​ is at least as hard as π1​.\nP-time:\n\nIf π2​ is a P-time problem and\n! If the input size to π2​ which is ∣f(x)∣ is bounded by ∣x∣c\nThen π1​ is also a P-time problem\n\n\nNP-hardness:\n\nIf π1​ is NP hard → π2​ is also NP-hard\n\n\n\n\n&amp; Techniques for reduction of π1​≤p​π2​\n\nProcess:\n\n\nIf π1​,π2​ is a min/maximization problem, convert it to a decision problem\nIf construction is needed, construct from π1​ to π2​: domain G→G′\n\n! The construction may not suppose you know the answer π1​.\nIf it’s a graph, it should apply to any graph\nIf it’s a set, it should apply to any set\n\n\nIf there is something like “exists solution of size k” in π1​, this k will probability be present in π2​ as well\n\n\n\n\nProve that if ∃ answer to π1​ in domain G, that implies ∃ answer to π2​ in domain G′.\nProve that if ∃ answer to π2​ in domain G′, that implies ∃ answer to π1​ in domain G.\nProven!\n\n\n\nComplexity Classes §\nComplexity classes are an extension of the Chompsky Heirarchy.\n\n\n\nP (Polynomial Time)\nNP (Nondeterministic Polynomial Time) The certificate exists that can be verified in polynomial time. A non-deterministic Turing Machine can solve it in polynomial time because it has many paths.\nco-NP\n\nproblem π∈NP⟹πc∈co-NP:luc_check_circle:\nproblem π∈NP⟺π∈Co-NP is an open problem\nSee Whats the difference between NP and co-NP - Stack Overflow\n\n\nNP-complete: Problems where any NP problem can be reduced into in polynomial time.\n\nIf there is any NP-complete problem that can be shown to be solved in polynomial time, then P=NP.\nthm. Cook-Levin Theorem. CNF-SAT is NP-complete.\n&amp; To prove a decision problem π is NP-complete\n\nShow π∈NP i.e. certificate can be checked in poly-time\nShow π0​≤p​π where π0​ is known NPC problem by…\n\nfrom solution I0​ of π0​, construct solution I of π\nshow that I0​ is solution for π0​ if and only if I is solution for π (both ways needed to show that non-solutions are non-solutions)\n\n\n\n\n“Completeness” can apply to any complexity class. It means that it is the “hardest” problem in that class.\n\n\nNP-hard: Problems that are at least as hard as all NP (and NP-complete) problems\n\nEquivalently, Problems that are as hard or harder than all NP problems\n\n\n\nList of Problems and Reductions §\nNP-Complete Problems §\n\nCNF-SAT (Cooke-Levin Theorem)\n3-SAT\nIndependent Set Problem\nCommon Graph Problems Problem\nSubset Sum Problem\nCommon Graph Problems\nTraveling Salesperson Problem\n\nCycle\n\n\n\nKarp’s 21 NP-complete problems - Wikiwand\nCategory:NP-complete problems - Wikipedia\n(DevonThink) NP-complete Reductions"},"Computer-Architecture":{"title":"Computer Architecture","links":["Instructions-(Computer-Science)","Instruction-Set"],"tags":["Computing/Computer-Architecture"],"content":"Term Definitions §\n\nInstruction: a single operation a processor can perform\nInstruction Set\nArchitecture: An abstract specification of a computer’s hardware—an interface—in order to be able to write software for it. (a contract between HW and SW to interact)\nMicro-architecture: the actual implementation of the architecture design\nComputer: machine that follows simple instructions deterministically\n\n\n\n                  \n                  technology of the time, programming language design, operating system design, the applications to write, and the history of the architecture design.\n                  \n                \n\n\nOSes and languages may also be influenced by the design of the architecture\n\nThe Five Component of a Computer are: Processor(CPU) / Control / Datapath / Memory / IO.\nMoore’s Law determines the trend of\n\nC Programming Language §\nUnix invented by: Ken Thompson\nC language by: Dennis Ritche\n(Think of it as: Newton, to create a new branch of science—physics—, invented calculus to make it possible.)\n\n\n                  \n                  The contraints of technology of the time shaped the design of C. \n                  \n                \n\nLimitations of: compiler size / code size / performance / portability\n→ became a portable assembly language\nDidn’t consider: security / robustness / maintainability / legacy code"},"Conditional-Distribution":{"title":"Conditional Distribution","links":["Joint-Distributions"],"tags":["Math/Probability"],"content":"def. Conditional Distribution. let X,Y be jointly distributed. Then the conditional probability distribution of Y given a specific value of X is the conditional probability distribution.\n\nX,Y are discrete:\n\npmfY∣X=x​(y):=P(Y=y∣X=x)=P(X=x)P(Y=y∧X=x)​\n\n\nX,Y are continuous:\n\n\nfY∣X=x​=fX​(x)fX,Y​(x,y)​\n\nif X⊥Y:\n\nfY∣X=x​(y)=fY​(y)\n\n\n                  \n                  Visually: \n                  \n                \n\n…the red line height divided by the orange line is the p.d.f conditional distribution fX∣Y=y​(x).\n…the orange line is the value of the marginal distribution fY​ at Y=y.\n\nthm. Continuous Multiplication Rule. let X,Y be jointly distributed. Then:\nfX,Y​(x,y)=fX∣Y=y​(x)⋅fY​(y)\nthm. Rule of Average Conditional Probability. let X,Y, Then:\nfX​=∫−∞∞​fX∣Y=y​⋅fY​dyP(X=x)=∫P(X=x∣Y=y)⋅fY​(y)dy"},"Conditional-Probability":{"title":"Conditional Probability","links":["Conditional-Distribution"],"tags":["Math/Probability"],"content":"See also: Conditional Distribution\ndef. Conditional Probability. The probability of event A given that event B has occurred is:\nP(A∣B):=P(B)P(A∩B)​\n\nIf Ω’s elements are all equally likely: P(A∣B)=#B#(A∩B)​\nMultiplication Rule: P(A∩B)=P(A∣B)⋅P(B)=P(B∣A)⋅P(A)\nCompliment Rule: P(A∣B)=1−P(AC∣B)\n\nthm. Chained Conditional Probability. For three events A,B,C:\nP(ABC)=P(A)⋅P(A)P(AB)​⋅P(AB)P(ABC)​=P(A)⋅P(B∣A)⋅P(C∣AB)\ndef. Bayes’ Theorem. For events A, B:\nP(B∣A)=P(A)P(A∣B)⋅P(B)​⇔P(B∣A)P(A)=P(A∣B)P(B)\nVisualization.\n"},"Confidence-Intervals-and-Hypothesis-Testing-in-OLS-Linear-Regression":{"title":"Confidence Intervals and Hypothesis Testing in OLS Linear Regression","links":["Confidence-Intervals","Hypothesis-Testing","Central-Limit-Theorem","Student's-t-test","Student's-T-Distribution"],"tags":["Math/Statistics"],"content":"See also: Confidence Intervals and Hypothesis Testing\nMotivation. Assume we have our estimators for our sample size N using OLS, β0​,β1​^​^​. Now, assuming we have the true population data (impossible in real life) and take 100 samples of size N from the whole population, we get 100 different tuple of estimators (β0​^​,β1​^​). If we plot these on a graph, we get an approximate bell curve. This is due to the Central Limit Theorem. Knowing this fact, we can deduce if there is a correlation between X and Y.\n\nRemark. N≥30 is the minimum required for CLT. N≥100 is a conservative requirement for CLT to apply.\nRemark. We will only look at β1​^​ since it is the more important parameter.\nHypothesis Testing §\ndef. The Null hypothesis in regression is H0​:β1​=0, i.e. there is no correlation.\ndef. Regression T-test. See Student’s t-test. A T-test is a test for rejecting the null hypothesis. let the T-statistic T=Var(β1​^​)​β1​^​−β1Null​​. Then\n{H0​H1​​if ∣T∣&gt;Kotherwise​\n\nThe cutoff value K is determined by how powerful (=α) you want the test to be. This is determined by the Student’s T-Distribution.\n\nThis is because T=d​tN−1​\n\n\nThis is Student’s t-test but with only one random variable.\nNormally, we set the cutoff K=2, i.e. two standard deviations away. This is around an α=0.05 test.\n\nTable of common T-Statistic values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue of TSignificanceSidedness∥T∥&gt;1.95P(β1​=0)&lt;0.05Two-side∥T∥&gt;2.58P(β1​=0)&lt;0.01Two-side\nA Large-sample critical value is simply using the idea that as N→∞ the T→dN→∞​N i.e. the T statistic becomes a normal distribution. Thus the cutoff values are using the standard normal table.\nIntuition. \nConfidence Interval §\nIntuition.\nBias:\nstandard error of regression: mean squared of residuals; also the estimator for the error\nStandard error of…\n\nResiduals → standard error of regression\nbe\nVaraince of β1​^​ is also the variance of CLT limit with the same sample size\n\nRemark. Substantive significance does not imply statistical significance, nor does vice versa. The two are irrelevant."},"Confidence-Intervals":{"title":"Confidence Intervals","links":["Fisher-Information"],"tags":["Math/Statistics"],"content":"Simple Definition §\ndef. Confidence Interval of the parameter θ∈[f(X,ψ),g(X,ψ)] in the probability distribution of a random variable X∼pdf(θ,ψ) is defined as the following:\nC.I.=γ:=P[f(X,ψ)&lt;θ&lt;g(X,ψ)]\n\nwhere functions f(X),g(X) are able to be derived from the random variable X.\n\nThe most common use of confidence interval problems is with CLT, where a parameter of a binomial distribution is estimated. e.g.:\nlet X∼Binom(n,p) where p is the parameter to be estimated.\nFormal Definition §\ndef. Confidence Interval. For Xi​ distributed with an unknown parameter θ, a γ-level confidence interval:\n\n…is an interval in which the probability of θ0​ being in the interval is γ [=1−α-level CI]\n\nthink of γ big is good, α small is good.\n\n\n…is formalized as the following where L,U is a statistic of X:\n\n​P(L&lt;θ&lt;U)=γ⟹2L+U​±2U−L​⟹Confidence Interval=[L,U]​​\ndef. Observed Information. For X1​,…,Xn​∼iidf(x1​,…,xn​;θ), observed information is:\nJ(θ)=−l′′(Xi​;θ)\nAnd for X1​,…,Xn​∼iidf(x;θ)\nJn​(θ)=i=1∑n​−l′′(Xi​;θ)\nRemark. This is Fisher Information, but gathered on data (Xi​)\nthm. (approximating a confidence interval using Fisher Information) In general, if θ^MLE​ can be found and the log-likelihood is twice-differentiable, an approximate CI of level γ=1−α can be approximated by:\nθ^MLE​±Jn​(θ^)​z1−2α​​​\nthm Delta Method. Let unknown parameter θ of r.v. X1​,…,Xn​’s distribution. If Fisher’s approximation [=asymptotically efficient and asymptotically unbiased] conditions are satisfied, the following holds for all g which g’(θ)=0:\nn​(g^​MLE​−g0​)d⟶​n→∞​N(0,[g′(θ)]2⋅σ2)\nand due to Fisher’s Approximation (asymptotic normality):\nn​(g^​MLE​−g0​)d⟶​n→∞​N(0,I(θ0​)g′(θ0​)2​)\nExample. To construct a standard normal distribution’s confidence interval of level γ=1−α:\nP(z2α​​&lt;Z&lt;z1−2α​​)=1−α"},"Consistency":{"title":"Consistency","links":["Chebyshev's-Inequality"],"tags":["Math/Statistics"],"content":"def. Consisteny. An estimator θ^(X1​,…,Xn​) is consistent for parameter θ if:\n∀ϵ&gt;0,  θ^  p⟶​n→∞​  θi.e.n→∞lim​P(∣θ^−θ∣&lt;ϵ)=1n→∞lim​P(∣θ^−θ∣&gt;ϵ)=0\n![[スクリーンショット 2023-02-21 15.40.01.png|center|250]]\ndef. Asymptotic Unbiasedness. An estimator θ^ is asymptotically unbiased if:\nE[θ^n​]⟶n→∞​ θ\n\n\n                  \n                  You can use the following Chebyshev’s Inequality for determining Consistency. \n                  \n                \n\nTransclude of Chebyshev&#039;s-Inequality"},"Constrained-Optimization":{"title":"Constrained Optimization","links":["Budget-Lines","Monotonic-Transformation","Utility-Function","Utility-Maximization"],"tags":["Economics/Micro-Economics","Math/Calculus"],"content":"Lagrangian Method §\nAlg. Lagrangian Optimization.\nLet:\n\nf(x) the target function to optimize\ng(x)=c is the constraint function\nλ is the Lagrange multiplier.\n\n\n(when optimizing for budget constraint) Do a Monotonic Transformation on the Utility Function to make the function easier to manipulate\nThe Lagrangian function is constructed to find the maximum or minimum of a target function subject to constraints:\n\nThe Lagrangian: L(x,λ)=f(x)−λ(g(x)−c)\nλ is an unknown constant\n\n\nThe first-order necessary conditions (also known as KKT conditions) are found by taking the derivative of the Lagrangian with respect to all variables and the Lagrange multipliers, and setting them equal to zero:\n\nFor all i, ∂xi​∂L​=0\n∂λ∂L​=0\n\n\nFeasibility condition:\n\nIs it in the feasible region: g(x)=c?\n\n\nSolve for x,λ ← this is the optimal point\n\n\n\n                  \n                  Example \n                  \n                \nWorked Example\n![[Pasted image 20230905153050.png|Worked Example|625]]\nUtility Maximization\n"},"Consumer-Sentiment-Index":{"title":"Consumer Sentiment Index","links":[],"tags":["Economics"],"content":"University of Michigan performs a monthly telephone survey about how the households feel about the economic climate."},"Consumer-Surplus":{"title":"Consumer Surplus","links":["Marginal-Willingness-to-Pay"],"tags":["Economics/Micro-Economics"],"content":"Consumer Surplus §\nMarginal Willingness to Pay Curve leads naturally to the concept of consumer surplus. Follow the following line of logic:\n\nThe area under the MWTP curve is the Total Willingness to Pay (TWTP)—the amount the consumer was willing to pay to get q amount of goods.\nSince they didn’t have to actually pay that amount (they only paid p×q), they’re better off.\nThe quantitative amount that they’re better off is the Consumer Surplus\n\n\nDeadweight Loss due to Taxation, Analyzed with MWTP Curves §\n\nConsier a distortionary tax on housing prices, per square feet. The blue line shows the price p per sqft of housing. The red line shows the post-tax budget line, and A is the optimal point.\n\n→ What if instead the government took away a lump sum instead of a distortionary tax? The amount taken away is L and the green line shows the new budget. In this hypothetical, ideal case, the optimal is B.\n→ In this case, the distortionary tax collected for bundle A is vertial distance T. As DWL:=L−T, the vertical distance shows the Deadweight Loss for the taxation. (The lump sum was better.)\n\nAlternatively, consider graphing the MWTP for utility uA, using points A,B.\n\n→ With the distortionary tax—optimal bundle: A, CS=a. Govn’t revenue is T=h×(p+t)=b\n→ With the lump sum tax—optimal bundle: B, CS=a+b+c\n\nHow come the consumer has more surplus as B in the bottom graph? Aren’t they indifferent in MWTP?\n\n→ The consumer instead lost the collected lump sum tax, L; ∴L=(a+b+c)−(a)=b+c\n→ ∴ DWL=L−T=(b+c)−(b)=c\n"},"Contagion-(Finance)":{"title":"Contagion (Finance)","links":["leverage"],"tags":["Economics/Finance"],"content":":= a propogation of bear markets, due to the fact that banks lend and borrow from each other\n→ The higher the leverage, the greater the degree of contagion in markets"},"Context-Sensitive-Grammar":{"title":"Context Sensitive Grammar","links":[],"tags":["Computing/Formal-Languages"],"content":""},"Context-Free-Grammar":{"title":"Context-Free Grammar","links":["Pushdown-Automata"],"tags":["Computing/Formal-Languages"],"content":"Section: Pushdown Automata\ndef. a Context-Free Grammar (CFG) is a grammar whose production rules consist of:\nA→x\n…where A is a variable, and x is a string of variables and terminals.\n\n\n                  \n                  CFGs can check for syntactically correct programs \n                  \n                \n\n\nSimple Grammer: a CFG where any pair of (var,term) appear in no more than one rule.\nAmbiguous Grammar: there exists a string in the language that has more than one derivation.\n\ndef. a Parse Tree [=Derivation Tree] shows the derivation steps of a single string from a start symbol according to the rules of a grammar.\n\n\nThe leaves of the tree read right-to-left is the yield of the tree—in this case, aaϵb is the yield.\nThe yield should not contain the λs. (obviously)\n\ndef. Leftmost Derivation. Replace the leftmost variable in every step.\ndef. Rightmost Derivation. Replace the rightmost variable in every step.\n\nthm. Determinability. If the CFG doesn’t contain rules of the form A→λ or A→B, then we can determine for all w if it is in the language of the CFG.\nthm. a Context-Free Grammar is equivalent to an NPDA.\nNPDA → CFG §\nalg. Given NPDA, you can define an equivalent CFG in the following steps:\n\nFor any transitions that pop/push more than one variable onto the stack:\n\nUse a new state to split out the transition, so that one transition only push/pops one variable onto the stack\n\n\nThen, for each transition arc:\n\nIf the transition arc is a pop arc that pops A off the stack, i.e.:\nfrom state i to j is in the form t,A;λ,\nthen construct a production rule for the grammar as:\n(qi​,A,qj​)→t\nIf the transition arc is a push arc that pushes A onto the stack i.e.:\nfrom state i to j is in the form t,γ,Aγ\nthen construct a production rule for the grammar as:\n(qi​,γ,qk​)→t(qj​Aql​)(ql​γqk​), where qk​,ql​ is every possible combination of states in the PDA(!)\n\n\nFinally, of all the rules generated, if any variables on the right side of the arc don’t appear on the left side, that rule is useless.\n\nCFG → NPDA §\nalg. Given a CFG, you can build an LL-NPDA in the following steps. This is a top-down design:\n\nThere are 3 total states in the NPDA: 0, 1, 2\nMove from state 0 to state 1 by adding the start variable S onto the stack\nThe middle state has loops for every rule of the CFG. Pop off the LHS, add the RHS to stack while processing any beginning terminals\nMove from state 1 to final state 2 in a λ-transition\n\n\nalg. Given CFG, you can build an LR-NPDA in the following steps. This is a bottom-up design:\n\nThere are 3 total states in the NPDA: 0, 1, 2\nState 0 has loops for:\n\n…every rule of the grammar. Each rule will pop the RHS and push the LHS of the rule\n…every terminal of the grammar. Each terminal will be pushed onto the stack while reading that terminal.\n\n\n\n\n\nLL Parsing §\ndef. LL(k) Parsing. Left-to-Right, Left-most derivation Parser with k lookaheads.\nalg. Given CFG G, LL(k) parsing occurs in the following steps:\n\nCreate the First(),Follow() sets for each of the each of the variables in the grammar.\n\nFirst() is the set of terminals (including λ) that can lead the string derived.\n\nCreated by checking the variable on the left side of the production: A→aX.\nCommon sense will be enough.\nDon’t forget the check variables that go to λ i.e. disappear. λ is only included in the FIRST set if the whole variable can disappear to λ.\n\n\nFollows() is created by checking the variable on the right side of the production: A→aX.\n\nThe start variable always has the $\\\n\n\n\n\n\n      - Use the first sets to make life simpler. $\\lambda$ is always removed.\n2. Create the $LL(1)$ parse table.\n   1. For each variable, check the rules.\n   2. There must be an entry for **every element of the first set.** check which rule makes the most sense [= FIRST set of the RHS of the rule is the lookahead symbol]\n   3. If FIRST set includes a $\\lambda$ (and the variable can goto) $\\lambda$, ⇒ There must be a $\\lambda$ entry for **every element of the follows set…**\n   4. …however, if the FIRST set includes a $\\lambda$ **but** the variable **cannot** goto $\\lambda$, **find another rule that makes sense to put there.**\n3. Use the table to do the necessary steps for parsing the string.\n\n![Untitled](Untitled%203%201.png)\n\n## LR Parsing\n\ndef. $LR(k)$ Parsing. **L**eft-to-Right, **R**ight-most **Reverse** derivation parser with $k$ lookaheads.\n\nalg. Given CFG $G$, $LR(k)$ parsing occurs in the following steps:\n\n1. Set up the grammar so that it’s ready to parse:\n   1. Change the start symbol to $S&#039;\\rightarrow S$. This is rule #0.\n   2. Number the rules $1$ through $n$.\n2. Calculate the FIRST and FOLLOW sets of variables.\n3. Construct a $LR$ parsing DFA in the following steps:\n   1. The start state has **rule number 0 and rules for $S$**. Place markers ($\\_$) in the beginning.\n   2. For each rule, “process” one _symbol_ at a time.\n      Processing means that to transition to another state…\n      …with arc labeled *symbol\n      …*to create a new state with the marker jumped over that _symbol_.\n   3. You have a new state with one rule that has the jumped marker. Now, add to the state **the closure of that rule.** The closure of a rule is calculated as such:\n      1. If the marker is in front of a variable, the derivation rules for that variables is included\n      2. if the market is in front of a terminal, don’t do anything\n4. Construct a $LR$ Parsing table from the DFA to use during parsing:\n   1. Do the following for every state of the DFA [=every row of the parse table]:\n      1. For every transition arc, put an entry in the table, for that **state number** and **arc label:**\n      2. If it’s just a transition arc, put a **“shift to state #” or sN**\n      3. If it’s a final state, only the entries for the elemetns of FOLLOW should be populated;\n         put a **“reduce by rule number #” or rN** and the rule number is the rule of the state (since it’s a final state, there should be only one rule)\n      4. If it’s a final state but its rule is $S’\\rightarrow S$, it’s special; only populate the “$” entry.\n         put a **“accept” or acc** in that place.\n      5. If the state contains a $**\\lambda$-rule,** then always put a **“reduce by rule #” or rN,** where the rule number is that lambda rule ← this should be in the **“$” column\\*\\*\n\n![Untitled](Untitled%204.png)"},"Context-Free-Language":{"title":"Context-Free Language","links":["Pushdown-Automata"],"tags":["Computing/Formal-Languages"],"content":"Section: Pushdown Automata\nClosure Properties §\nContext-Free Languages are closed under:\n\nUnion\nConcat\nStar\nRegular Intersection (←think: you have states &amp; one stack to combine. It makes sense)\n\nPumping Lemma §\nthm. Pumping Lemma for CFL.\n\n\n\n                  \n                  v,y. Divide it up into different cases and derive contradictions in all of them.\n                  \n                \n\n\nTransforming Context-Free Languages for Easy Parsing §\nYou can do the following modifications on CFLs:\n\nAdd Lambda\nRemove λ-productions. ← Every CFG with λ productions can be made into one w/o λ-productions.\nSubstitution from A→wBv and B→y1​∣y2​∣..∣yn​ into A→wy1​v∣wy2​v∣…∣wyn​v\nRemoving Left-Recursion [e.g. A→Ax] ← Left-recursion is bad, because top-down parsers will get into an infinite loop\n\nUse the above modifications to produce CFLs that do not have useless productions.\nalg. Removing useless productions in CFLs. These steps must go in order!\n\nIdentify useless variables\nUseless variables are ones that: 1. cannot be reached by the start symbol 2. cannot derive a terminal\nRemove useless productions\ni.e. remove all productions that contain a useless variable\nRemove all λ-rules\n\ncreate a set of variables that can ultimately derive a λ\nfor every production that uses those variables on the RHS, derive the λ now.\n\n\nRemove unit productions\n\ndraw the variable dependency graph (1 level)\nFor every production that uses unit production on the RHS, consolidate all into one; i.e. derive it now.\n\n\n\nNormal Forms §\n\n\ndef. Chompsky Normal Form\nEither: one terminal, or two variables. Easy to convert any grammar into CNF.\n\n\nA→aA→BC\n\n\ndef. Geribach Normal Form\nAll rules are the form:\n\n\nV→σV∗\n\nAll λ-free CFGs can be expressed in GNF.\n"},"Continual-Procession-of-Technology":{"title":"Continual Procession of Technology","links":["Moore’s-law"],"tags":["Computing"],"content":"Moore’s law"},"Control-Problem":{"title":"Control Problem","links":[],"tags":["Computing"],"content":""},"Cornout-Quantity-Competition":{"title":"Cornout Quantity Competition","links":[],"tags":["Economics/Micro-Economics","Economics/Game-Theory"],"content":"Firms will choose quantity as the strategic variable.\n\n\nDetermining Firm 2’s Best Response Curve:\n\nWhen firm 1’s quantity = 0, firm 2’s best response is to produce xM (=monopoly quantity)\nWhen firm 1’s quantity &gt; 0, firm 2’s best response derived by the reduction in demand in graph (c):\n\nD (=full market demand) is shifted left by x1​ (=firm 1’s produced quantity) to Dr; Firm 2’s BR is to optimize at MC=MR\n→ This shows that when Firm 1 produces 2xM, D will shift left so much that MR = MC at x1​=0.\n\n\n\n\nFirm 1’s BRC is symmetrical, as shown in (b)\n\n→ Firms produce xC (= cournot quantity)\n→ 21​xM&lt;xC&lt;xB\n\n\n\n\n\n                  \n                  Info \n                  \n                \n\nCornout Price pC converges to competitive prices p=MC as the number of firms increase. (Take my word for this; it’s proven mathematically but not shown above)\nSequential Move: Stackleberg Competition §\n\nStackleberg Leader is Firm 1, Stackleberg Follower is Firm 2\n\n\nFirm 1 considers Firm 2’s expected BRC before making a move:\nProfit maximize knowing firm 2’s BRC:\n\nGraph (a) is the expected Firm 2’s BRC\nWhen Firm 1’s quantity &gt;x∗ then firm 2 produces nothing\n→ Firm 2 faces the full market demand ⇒D=Dr\nWhen Firm 1’s quantity ∈[0,xM] then firm 2 faces the residual demand Dr which is calculated by:\n\nat x1​=xM,\n\n…then BR: x2​=21​xM and Σx=23​xM\n…thus Dr=D−21​xM\n…equivalently Dr(xM)=D(23​xM)\n\n\nat x1​=0,\n\n…then BR: x2​=xM and Σx=xM\n…thus Dr=D−xM=0\n…equivalently Dr(x1​[=0])=D(xM)\n\n\n\n\n\nSequential Move: Entry Deterrence §\nGame Structure §\n\nFirst move is Firm 2’s entry decision\nSecond move is either Cournot Quantity, Stackleberg Quantity competition (Bertrand isn’t considered)\nIf firm 2 enters it must pay a Fixed Cost (FC) which is an economic cost to them (=matters in profit calculation).\n\n\n\nFirm 2 chooses entry;\n\n…if it enters it must pay FC, and two players cornout compete;\n…if it doesn’t its profit is zero, and Firm 2 is a monopoly\n\n\nFirm 2 chooses entry;\n\n…if it enters it must pay FC, and two players stackleberg compete with firm 1 as stackleberg leader;\n…if it doesn’t its profit is zero, and Firm 2 is a monopoly\n\n\nFirm 1 chooses quantity first;\n\n…Firm 2 enters, paying FC, and Firm 2 gets stackleberg follower profit\n…Firm 2 doens’t enter, and Firm 1 gets profit from its original quantity\n\n\n\n⇒ The game structure of (c) allows for strateigic entry deterrance by firm 1:\nEntry Deterence §\n\nFirm 1 may deter entry to maximize profit by setting a certain quantity\nFC is an exogenous variable\n→ firm 2’s entry decision will depend on how big FC is\n→ thus, firm 1’s entry deterrence will depend on how big FC is\n\n\n(a) is the profit against a choice of production quantity for a monopoly.\n→ Then, see (b):\n\nIf FC&gt;FC: Second firm will not enter no matter\n…when FC=πM\nIf FC​&lt;FC&lt;FC: First firm will deter entry by increasing production\n…when FC​=(x s.t. π(x)=πSL)\n…i.e. Firm will deter entry until π drops so much that it’s better to compete (and get Stackelberg profit)\nIf FC&lt;FC​: normal stackleberg competition\n…where firm 1 is Stackelberg Leader (SL) and get πSL\n…and firm 2 is Stackelberg Follower (SF) and get πSF\n"},"Correlation":{"title":"Correlation","links":["Covariance"],"tags":["Math/Statistics"],"content":"def. Correlation (Correlation Coefficient). Correlation is Covariance, but standardized (unitless)\nρXY​​=Corr(X,Y)=Cov(σX​X−μX​​,σY​Y−μY​​)=σX​σY​1​Cov(X−μX​,Y−μY​)=σX​σY​Cov(X,Y)​​\nthm. Correlation Identities.\n\n−1≤ρ≤1\nρXY​=1⇔∃a,b∈R∧a&gt;0∣Y=aX+b\nρXY​=−1⇔∃a,b∈R∧a&lt;0∣Y=aX+b\nρaX,bY​=Corr(X,Y) i.e. linearly invariant\n\nFor R.V. X,Y, the following means the Same Thing §\n\nX,Y are uncorrelated\nCov(X,Y)=0\nVar(X+Y)=Var(X)+Var(Y)\nE(X⋅Y)=E(X)⋅E(Y)\n\nIndependence [X⊥Y] implies all of the above, but any of the above does not imply independence."},"Cost-Function":{"title":"Cost Function","links":[],"tags":["Economics/Micro-Economics"],"content":"\n\n                  \n                  wl+rk. Needs derivation by optimization.\n                  \n                \n\nC:(w,r,xˉ)↦C\n\nProperties\n\nHD1 in input prices\nIncreasing in input prices\nIncreasing in output quantity\n\n\n"},"Cost-Minimization":{"title":"Cost Minimization","links":["Profit-Maximization","Constrained-Optimization"],"tags":["Economics/Micro-Economics"],"content":"See also Profit Maximization\nLong Run Cost Minimization §\nminl,k​ C=wl+rk such that x=f(x)\n\nLagrangian Optimization\nOptimal when Isoquant is tangent to the isocost\n\nHigher isoquant is not always better! it just means you’re producing more\n\n\n\n\n\n                  \n                  Warning \n                  \n                \nBeware when MC is monotonically decreasing. This means that the firm will produce zero or infinity, which is means it is a special case which you need to analyze separately.\n\n"},"Covariance-&-Correlation":{"title":"Covariance & Correlation","links":[],"tags":["Math/Probability"],"content":""},"Covariance":{"title":"Covariance","links":["Variance"],"tags":["Math/Statistics"],"content":"def. Covariance. Covariance measures the joint variability of two R.V.s; let X,Y.\n\nWhen X,Y show similar behavior, Cov(X,Y)&gt;0\nWhen X,Y show opposite behavior, Cov(X,Y)&lt;0\n\nCov(X,Y):​=E((X−μX​)(Y−μY​))=E(X⋅Y)−E(X)⋅E(Y)​\n\nWhen X⊥Y, then Cov(X,Y)=0; but Cov(X,Y) does not imply X⊥Y\nCovariance is a generalization of Variance: Var(X)=E((X−μX​)2)=Cov(X,X)\n\nthm. Relationship between Covariance and Variance. let X,Y. then:\nVar(X+Y)​=Var(X)+Var(Y)−2⋅E((X−μX​)(Y−μY​))=Var(X)+Var(Y)−2Cov(X,Y)​​\n\nWhen X⊥Y then Var(X,Y)=Var(X)+Var(Y)\n\nthm. Bilinearity of Covariance.\n\nCov(aX,aY)=ab⋅Cov(X,Y)\nCov(X,Y+Z)=Cov(X,Y)+Cov(X,Z)\n\nthm. Summed Variance. let X1​,…,Xn​. Then\nVar(i∑​Xi​)=i∑​Var(Xi​)+2∀j,k s.t.j&lt;k∑​Cov(Xj​,Xk​)\nE.G. second summation has (23​) terms:\nVar(X1​+X2​+X3​)​=Var(X1​)+Var(X2​)+Var(X3​)+2⋅Cov(X1​,X2​)+2⋅Cov(X2​,X3​)+2⋅Cov(X1​,X3​)​"},"Cramer-Rao-Lower-Bound-(CRLB)":{"title":"Cramer-Rao Lower Bound (CRLB)","links":[],"tags":["Math/Statistics"],"content":"thm. Cramer-Rao lower bound. Given estimator θ^,\nIf:  E[θ^]=g(θ)⇒Var(θ^)≥I(θ)g′(θ)​\nIf estimator θ^ is unbiased:\nVar(θ^)≥I(θ)1​\n⇒ CRLB is a statement about the bound on the best possible precision [=efficiency] we can get.\n→ If an estimator reaches the CRLB, it is deemed most efficient."},"Credit":{"title":"Credit","links":["Grice's-Maxims"],"tags":["Economics/Finance"],"content":"Is what modern society is built upon.\n\nYou eat food grown, cooked and served by a stranger\nYou lend the bank your money, and expect them to return it to you in the future\nYou live in big apartments with hundreds of total strangers\nYour communication doesn’t break down because you assume the Cooperative Principle\n"},"Critical-Point":{"title":"Critical Point","links":[],"tags":["Math/Calculus"],"content":"\n\n                  \n                  Critical Point \n                  \n                \nA critical point on function f(x) is all xi​ where dxd​f(xi​)=0\n"},"Critical-Vertex":{"title":"Critical Vertex","links":["Graph"],"tags":["Computing/Algorithms"],"content":"Graph"},"Cross-Price-Demand-Curve":{"title":"Cross-Price Demand Curve","links":[],"tags":["Economics/Micro-Economics"],"content":"Relates price of one good p1​ to the quantity demanded of another good x2​"},"CulAnth-203-Marxism":{"title":"CulAnth 203 Marxism","links":["Historical-Materialism","Capital-(Marxism)","Proletatriat-(Marxism)","Communism","Private-Property","Sensuous-Activity","Alienation","Species-being","Artificial-Needs","Base-&-Superstructure","Valorization,-Surplus-Value","Worker-vs-Machine","Commodities","Use-value,-Exchange-value"],"tags":["Courses"],"content":"\nHistorical Materialism\nCapital (Marxism) and Proletatriat (Marxism)\nCommunism, Private Property, Sensuous Activity\nAlienation and Species-being\nArtificial Needs\nBase &amp; Superstructure\nValorization, Surplus Value\nWorker vs Machine\nCommodities - Use value, Exchange value\n"},"Curvature-(Math)":{"title":"Curvature (Math)","links":[],"tags":["Math/Calculus"],"content":""},"Data-is-Everything":{"title":"Data is Everything","links":["(Article)-Magic-Ink---Information-Software-and-the-Graphical-Interface","The-Personal-Computer","Functional-Programming","Turing-Machine"],"tags":["Computing"],"content":"Observe:\n\n(Article) Magic Ink - Information Software and the Graphical Interface\n\n⇒ UI Design is information design (not interactivity).\n\n\nEverything is a File (in The Personal Computer)\nFunctional Programming treats programs as manipulation of data.\nTuring Machine take input and produce output on tape.\n\nAn “effective method” [=algorithm] is just procedures on data\n\n\n\nThus data is what drives every computable process.\n\n\n                  \n                  Abstract \n                  \n                \nEverything is Data. Data is Everything.\n"},"Data-driven-or-Truth-driven":{"title":"Data-driven or Truth-driven","links":["Limits-of-Math-and-Computing","Turing-Machine","Phenomenology","Data-is-Everything","Perspective-Projections-Model","Analytic-Philosophy"],"tags":["Mental-Models","Math","Computing"],"content":"Data-Algorithm structure in the top-down approach; Truth-Proof in the bottom-up approach.\n\nThey merge at Godel’s Incompleteness Theorem\n\nRelation between Gödel’s incompleteness theorem, the halting problem and universal Turing machines - Computer Science Stack Exchange\nLimits of Math and Computing.\nThink: the UTM takes a binary encoding of a TM and can simulate that TM. This is algorithm that is taken as input data to another algorithm.\n\n\nAxioms are fundamental bits of data. Using proofs we derive more data [=conjectures, theorems].\n\nData-Algorithm §\nPhenomenology and to a greater extent whole of Continental Philosophy. Data is Everything. Perspective Projections Model\nTruth-Proof §\nBertrand Russell, G. E. Moore and all of Analytic Philosophy\nAlso see (DevonThink) How to safely think in systems. | Irrational Exuberance"},"Database-Design-Theory":{"title":"Database Design Theory","links":["Relational-Algebra"],"tags":["Computing/Data-Science"],"content":"How to reduce a Relational Model to make it more compact.\nArmstrong’s Axioms §\n\nReflexivity: Y⊆X⟹X→Y\nAugmentation: X→Y⟹XZ→YZ\nTransitivity: X→Y and Y→Z⟹X→Z\n\nSecondary Rules §\n\nDecomposition: X→YZ⟹X→Y,X→Z\nComposition: A→B,X→Y⟹AX→BY\nUnion: X→Y,X→Z⟹X→YZ\nPseudo-transitivity: X→Y,YA→Z⟹XA→Z\nIdentity: X→X\nExtensivity: X→Y⟹X→XY\n\nFunctional Dependency §\ndef. Functional Dependency. If for all tuples in a relation that has the same values for attribute X and attribute Y, then Y is functionally dependent on X.\n∃f:X↦Y⟹X→Y ...read &quot;Y Depends on X&quot;\ne.g. an address relation with street, city, state, zip\n\nzip → (city, state)\n\nZip code determines city and state\n\n\n(zip, state) → zip\n\nThis is a Trivial dependency:= RLHS⊇RHS\n\n\nzip → (state,zip)\n\nThis is non-trivial, but not complete,\nComplete Non-trivial dependency:= LHS∩RHS=∅\n\n\n\nalg. BCNF decomposition procedure. Given a set of dependencies F and relation R, BCNF decomposition determines a minimally redundant (=row-redundant) Relation.\n\nChoose (non-determinisitcally) a dependency in F, let X→Y which is non-trivial (=X is not a superkey)\nDecompose using X into two relations R1​,R2​\n\nR1​ has attributes X∪Y\nR2​ has attributes X∪(attr(R)−Y)\n\n\nRecurse procedure for R1​,R2​\n\n\nIt’s a non-deterministic process (=there can be multiple ways to decompose a relation).\n\ndef. Closure. Given dependencies F, the closure Z∗ of set of attributes Z is all attributes determined by Z through those dependencies\n\nFinding a Key Using Functional Dependencies §\nalg. Given set of functional dependencies F on relation R, find the keys of the schema\n\nFor each dependencies LHS→RHS, compute the closure of LHS such that LHS→LHS∗.\n\nIf LHS∗ doesn’t contain all the attributes of R, then augment LHS such that it reaches all the attributes.\n\ni.e., let Remain=attr(R)−LHS∗\nthen, it must be LHS∪Remain→attr(R)\n\n\nThen, LHS∪Remain is a superkey of R. let this be S.\nCan you reduce this superkey S?\n\nTry taking out one attribute at a time from S to create S′\nIs S′ reducible?\n→ Repeat until we reach something unreducible.\n⇒ The final S′ that is unreducible is a key.\n\n\n\n\nRepeat for all dependencies in F.\n\nMulti-value Dependency §\ndef. Multi-value Dependency (MVD). Y is a multi-value dependency of X if…\n\nfor all tuples with the same values of X…\n…if we swapped all the Y values of them…\n…those entries also exist in the database. then:\n\nX↠Y\nIn colloquial terms, we can say that for every determined value of X, all tuples with values of Y “associated” with that X must exist too.\n\ndef. Trivial MVD. Given relation R(A,B,C):\n\nA,B↠C: Obvious. LHS∪RHS=attrs(R)\nA,B↠A: Obvious. LHS⊇RHS\n\ndef. 4th Normal Form (4NF). if every MVD in relation R is X↠Y such that X is always a superkey, then relation R is in the 4th Normal Form. Example\nMultivalue Dependency Rules §\n\nComplementation: X↠Y⟹X↠attrs(R)−X−Y\nAugmentation: X↠Y,B⊆A⟹XA↠YB\nTransitivity: X↠Y,Y↠Z⟹X↠Z−Y\n\nAutomatically means X↠Y,Y↠Z⟹X↠Z\n\n\nReplication: X→Y⟹X↠Y\nCoalescence: If 𝑋 ↠ 𝑌 and 𝑍 ⊆ 𝑌 and there is some 𝑊 disjoint from 𝑌 such that 𝑊 → 𝑍, then 𝑋 → 𝑍\n\nChase §\nProving theorems about chase. Example: \nalg. Tuple Generation. To enforce X↠Y on table R\n\nFor a determined X=xi​\n\nEnumerate all availble Y=y1​,y2​,…,y?​\nEnumerate all available\n\nlet Q:=attr(R)−X−Y\nEnumerate all availalbe Q=q1​,…,q?​\n\n\nAre all combinations Y×Q available in the table?\n\nIf not, add missing ones\n\n\n\n\nFor another determined X=x2​, repeat\nRepeat for all X=x1​…x?​\n\nalg. Chase. To prove X↠Y…\n\nInitialization: add two tuples (x,y1​,…),(x,y2​,…) to the initial table\nFor each MVD (order doesn’t matter):\n\nTuple Generation: see above\n\n\nFor each FD (order doesn’t matter):\n\nYou may infer equalities, e.g. e1​=e2​\n\n\nDoes X↠Y fully available? (=Y×Remaining Attrs all in table)?\n\nYes → proven\nNo → disproven (counterexample)\n\n\n"},"Database-Indexing":{"title":"Database Indexing","links":[],"tags":["Computing/Data-Science"],"content":"Sequential Index §\n\nCan be dense or sparse\n\nPrimary Index: Key indices are often sparse, because the rows are ordered using the key index\nSecondary Index: Index on other attributes are often dense b/c/ v.v.\n\n\nYou can manually create indexes in SQL:\n{postgresql}CREATE INDEX indexname ON tablename(columnname)\nMulti-column index: Index (A,B) on R(A,B,C) has keys that have A,B concatenated and sorted together.\n\nSee: sql server - what does a B-tree index on more than 1 column look like? - Stack Overflow\nQuote: “With most implementations, the key is simply a longer key that includes all of the key values, with a separator. No magic there;-). In your example the key values could look something like:&quot;123499|John Doe|Conway, NH&quot;,&quot;32144|Bill Gates| Seattle, WA&quot;\n\n\n\nIndex Sequential Access Method (ISAM) §\n\nLookup: \nInsert / Delete: \n\nB+ Trees §\n\nArchitectural Improvement on ISAM\n\nEach block can fan-out a specific amount n\nEach block containes n−1 entries to indicate ranges\nExample: \n\n\nLeaves are sequential → Range queries are possible: \nGuaranteed to be well-balanced (=all nodes are at least half-full)\n\nGuaranteed by recursive node splitting (on insert) and recursive node coalescing or entry stealing (on delete)\nExample: \n\n\nPerformance: access time is O(height)=O(logfanout​#Tuples)\n\n→ You can fit this in memory (4-level B+ Trees are often good enough)\n…recall you also need to node-split of node-coalesce sometimes too\n\n\n"},"Database-Management-System":{"title":"Database Management System","links":["Performance-(Computing)"],"tags":["Computing/Data-Science"],"content":"DBMS must be able to…\n\nStore data persistently.\nHandle query of data by the user\nUpdate data as requested\n\nWhat do databases aim to be?\n\nPhysical Data Independence\n\nHow data is physically stored is abstracted away by the DBMS\n\n\nConcurrency Control\n\nMultiples accesses must be performant and conflict-free\n\n\nRecovery\n\nFailures should be handled and recovered\n\n\nPerformance (Computing)\n\nMust be performant with lots of accesses\n\n\n"},"Debt":{"title":"Debt","links":[],"tags":["Economics/Finance"],"content":"Secured debt:= debt backed by collateral"},"Debter-in-posession":{"title":"Debter in posession","links":[],"tags":["Economics/Finance"],"content":""},"Decentralization":{"title":"Decentralization","links":["Institutional-Design","Centralized-Power","Michel-Foucault","horizontal-organization","Living-With-the-Internet","Spontaneous-Organization","Game-Theory"],"tags":["Economics/Game-Theory","Philosophy/Political-Philosophy","Computing"],"content":"Decentralization is a type of Institutional Design that breaks up central powers into smaller autonomous organizations.\n\nIt distributes Centralized Power into the microphysics of power\nIt allows for horizontal organization\nComputers and the internet allows these structures to naturally form\nSpontaneous Organization\n\nExample\n\nGit (decentralized version control &amp; devops)\nBlockchain (decentralized ledger)\nIPFS &amp; BitTorrent (decentralized file hosting)\nGame Theory Concepts\nResource Regeimes\n\n(DevonThink) 4-3. Ostrom, Collective action and the evolution of social norms\n\n\n"},"Demarcation-Problem":{"title":"Demarcation Problem","links":["Karl-Popper","Analytic-Philosophy"],"tags":["Philosophy/Epistemology"],"content":"Demarcation Problem. Problem in epistemology on how to distinguish science and non-science. Related to:\n\nFalsifiability Criterion\n\nKarl Popper is the modern analytic philosopher to discuss this\n\n\nEmpiricism\n"},"Depth-First-Search":{"title":"Depth First Search","links":["Shortest-Path","Directed-Acyclical-Graph"],"tags":["Computing/Algorithms"],"content":"alg. Depth First Search. DFS, but also record the preand post-times as it is convenient. From Class:\n\nTime Complexity: O(V+E)\nDFS is useful in most graph tasks except Shortest Path (that’s for BFS)\n\n\nthm. Identifying DAGs. If there is no back edge, the graph is a DAG.\nalg. DAG Topological Ordering. Perform DFS with postand pre-time. Check there are no back edges (i.e. no cycle)"},"Derivative-Rules":{"title":"Derivative Rules","links":["Integration-Rules"],"tags":["Math/Calculus"],"content":"\n\n                  \n                  Note \n                  \n                \n\n(DevonThink) IB Math HL Formula Booklet\nIntegration Rules\n\n\nDerivation Rules §\nChain Rule:\ny=g(u)⟹u=f(g(x))⟹dxdy​=dudy​⋅dxdu​\nf(g(x))=f′(g(x))⋅g′(x)\nProduct Rule:\ndxd​f(x)g(x)=f′(x)g(x)+f(x)g′(x)\nFilliping:\ndxdy​=(dydx​)−1\n\nSpecial Functions §\nExponentials §\n\nf(x)=ax then f′(x)=ln(a)⋅ax\nf(x)=ex then f′(x)=ex\nf(x)=ag(x) then f′(x)=ln(a)⋅ag(x)⋅g′(x)\nf(x)=eg(x) then f′(x)=eg(x)⋅g′(x)\n\nTrigonometric Functions §\nFrom (DevonThink) Mnemonic diagram for trigonometric and hyperbolic functions\n\n\nFunctions and cofunctions are on horizontal lines.\nDerivatives of functions on the right have a negative sign; those on the left do not.\nFunctions and reciprocals are on diagonal lines.\n\ne.g. sin(x)=csc(x)1​\n\n\nEach function is the ratio of the next two functions clockwise.\n\ne.g. tan(x)=cos(x)sin(x)​\n\n\nEach function the the product of its two neighbors.\nThe two functions at the top are bounded. The rest are unbounded.\nThe functions that are vertices of a triangle with a Roman numeral inside are related by Pythagorean identities.\n\ne.g. sin(x)2+cos(x)2=1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctionDerivativeIntegralsin(x)cos(x)−cos(x)+Ccos(x)−sin(x)sin(x)+Ctan(x)sec2(x)$-\\lncot(x)−csc2(x)$\\lnsec(x)sec(x)tan(x)$\\lncsc(x)−csc(x)cot(x)$\\ln\n\n\nFunction and cofunctions are on lines that make a 120° angle with the horizontal.\nDerivatives of functions on the right have a negative sign; those on the left do not.\nFunctions and reciprocals are on diagonal lines.\nEach function is the ratio of the next two functions clockwise.\nEach function the the product of its two neighbors.\nThe two functions at the top are bounded. The rest are unbounded.\nThe functions that are vertices of a triangle with a Roman numeral inside are related by Pythagorean identities.\n"},"Derivatives-(Finance)":{"title":"Derivatives (Finance)","links":["Options-(Finance)","Futures","Forwards","Swaps"],"tags":["Economics/Finance"],"content":"Derivatives are tradable contracts.\nProperties of Derivatives §\nWhere is it traded?\n\nOver-the-Counter (OTC): the two parties arrage the terms by themselves. Harder to price.\nExchange-traded: everybody goes to the central exchange to trade. Higher liquidity.\n\nDo you need to execute?\n\nNon-contingent: No need to execute. Options (Finance)\nContingent: Must execute. Futures, Forwards, Swaps, etc.\n"},"Desire-as-the-Object-of-Desire":{"title":"Desire as the Object of Desire","links":["(Philosopher)-Alexandre-Kojeve","(Book)-Phenomenology-of-Spirit","Jealousy"],"tags":["Philosophy"],"content":"…is (Philosopher) Alexandre Kojeve’s interpretation of (Book) Phenomenology of Spirit.\n\nIt is what differentiates humans from others\nEssences of Jealousy and complex love.\n"},"Development-Documentation-for-Writing-Helper":{"title":"Development Documentation for Writing-Helper","links":[],"tags":["Computing","Documentation"],"content":"PDF.js usage\nHow to Build a React PDF Viewer with PDF.js | PSPDFKit\nHow to Build a TypeScript PDF Viewer with PDF.js | PSPDFKit\n\nPDF.js uses workers to parse and render the PDF file. To keep things simple, we’ll avoid bundling the workers and instead copy them to our public folder (named public) using the following:\n\ncp ./node_modules/pdfjs-dist/build/pdf.worker.min.js ./public/\n\n{and then you can do:}\npdfJS.GlobalWorkerOptions.workerSrc =\n\t\t\t\twindow.location.origin + &#039;/pdf.worker.min.js&#039;;\nFeatures §\n\nList of past articles for the user\n\nIncludes what they learned, what they ignored\n\n\nWord order by frequency\n"},"Dialectical-Materialism":{"title":"Dialectical Materialism","links":[],"tags":["Philosophy/Political-Philosophy"],"content":"\nThe law of the unity and conflict of opposites\nThe law of the passage of quantitative changes into qualitative changes\nThe law of the negation of the negation\n"},"Dialectical-Synthesis":{"title":"Dialectical Synthesis","links":["(Philosopher)-G.-W.-F.-Hegel"],"tags":["Philosophy/Epistemology"],"content":"by (Philosopher) G. W. F. Hegel"},"Differential-Equation":{"title":"Differential Equation","links":[],"tags":["Math/Calculus"],"content":""},"Directed-Acyclical-Graph":{"title":"Directed Acyclical Graph","links":["Algorithm-Problem-Tips","Directed-Graph","Depth-First-Search","Shortest-Path"],"tags":["Computing/Data-Structures"],"content":"def. Directed Acyclical Graph (DAG). No shit.\n\nDirected Trees are (obviously) DAGs.\n\nIt also has a topological ordering\n\n\n\nAlgorithm Problem Tips. Detecting if a Directed Graph is Acyclic.\n\nIdea: DFS with tree. If there is no back edge, it is a DAG.\n\nalg. Topological Sort (DFS). DFS with preand post-times. Order by decreasing post-time.\n\nSee Also course\nTopological Sort may not be unique\n\nalg. Shortest Path for DAG.\nIdea: Topological sort of DAG. Then, linearly calculate minimum distance to that.\nslides"},"Directed-Graph":{"title":"Directed Graph","links":["Directed-Acyclical-Graph","Depth-First-Search"],"tags":["Computing/Data-Structures","Computing/Algorithms"],"content":"Terminology\n\nGT: Transpose of directed graph; G with all the directions of edges reversed\nSource vertex: outgoing edges only\nSync vertex: incoming edges only\nStrongly connected directed graph: all edges can reach all other edges\n\nSource Component: only outgoing edges to any vertex of component\nSync Component: only incoming edges to any vertex of component\n\n\nIf directed graph has no cycles, it is a DAG\n\nalg. Kosaraju’s Algorithm for Finding Strongly Connected Components.\n\nIdea: after performing a DFS on graph G, the\n\nlet v be the component with the maximum post-time\nReverse edges of G to get GT.\nlet VC​ be a sync component of GT\n**v must be an element of some sync component VC​\n\n\nSee also: course\n\n\nDFS on G\nDFS on GT but…\n\nwhen you run out of reachable components…\ncut off the DFS tree; that’s a strongly connected component; then\ngo in the next maximum unvisited post-time vertex\n\n\n"},"Disjoint-Set":{"title":"Disjoint Set","links":["Tree","Ackerman-Function"],"tags":["Computing/Data-Structures"],"content":"Data structure containing multiple sets.\n\nStored as array and Tree\nMake set: Initialize singleton (=single element) set for each element\nUnion set\n\nFind owner of both element\nMake tree with one as parent\n\n\nFind owner of elem\n\nPath compression: once you find something, directly attach all nodes of the path to that root\n⇒ with path compression, a sequence of n union-find operations areΘ(n⋅α(n)) where α(n) is the minimum t such that the Ackerman Function A(t,t)≥n"},"Distribution-(Math)":{"title":"Distribution (Math)","links":["Moment-(Probability)","Joint-Distributions","Conditional-Distribution","Uniform-Distribution","Bernouilli-Distribution","Gamma-Distribution","Exponential-Distribution","Normal-Distribution","Student's-T-Distribution","Poisson-Distribution","Binomial-Distribution","Multinomial-Distribution","Hypergeometric-Distribution"],"tags":["Math/Probability"],"content":"def. A distribution gives comprehensive information about an experiment. A distribution can be a table showing the probabilities of all outcomes, or a probability density function.\n\n\n                  \n                  Moment Generating Function\n                  \n                \n\ndef. let Ω be an outcome space. All functions P(x) satisfying the following criteria are probability distributions.\nCriteria:\n\nP(∅)=0 and P(Ω)=1\nFor all A⊆Ω, 0≤P(A)≤1\nIf A and B are disjoint then P(A∪B)=P(A)+P(B)\n\nThe 3rd condition can also be generalized for any distributions:\n\nIf A1​,…,An​ are pairwise disjoint, then P(A1​∪⋯∪An​)=P(A1​)+…+P(An​).\n\nRemark. The distribution P(A)=#Ω#A​ for countable, discrete outcome spaces follow the above axiom.\nFor a random variable X∼f(h) where h is the height of some population, the probability that a&lt;X&lt;b is the shaded area:\n\nCalculated by: P(a&lt;X&lt;b)=∫ab​f(h)dh\nProbability Mass Function §\ndef. Probability Mass Function. For a discrete random variable X, the probability mass function is the function that gies the probability of all values of X.\npmf(x)=P(X=x)\nthm. Addition Rule for Random Variables. For a discrete random variable X, the following is true:\nP(a≤X≤b)=k=a∑b​P(X=k)=k=a∑b​pmf(k)\nProbability Density Function §\ndef. Probility Density Function. fX​(x) is a probility density function of random variable X iff:\n\n∀x,fX​(x)≥0\n∀x,∫−∞∞​fX​(x)dx=1\nP(a≤X≤b)=∫ab​fX​(x)dx\n\ndef. Cumilitave Density Function. FX​(t) is the cumulative density function of random variable X:\nFX​(t):=∫−∞t​fX​(x)dx≡P(X≤t)\nif and only if:\n\n∀t,FX​(t)≥0\nFX​(t) is never decreasing over its domain\nlimt→∞​FX​(t)=1\n\n\n\n                  \n                  Info \n                  \n                \n\nRelationship between fX​(x) and FX​(x) is a derivate and antiderivative.\n\n\n&gt;fX​(x)derivativeanti-derivative​FX​(x)&gt;\n\nNote that when you get FX​(x) you will get a integration constant C. You can get rid of this by using the property limt→∞​FX​(t)=1.\n\n\nJoint Distributions\nConditional Distribution\n\nList of Common Distributions §\n\nUniform Distribution\nBernouilli Distribution\nGamma Distribution\nExponential Distribution\nNormal Distribution\nStudent’s T-Distribution\nPoisson Distribution\nBinomial Distribution\nMultinomial Distribution\nHypergeometric Distribution\n"},"Dividend-Discount-Model":{"title":"Dividend Discount Model","links":["Equity","Interest-Rate"],"tags":["Economics/Finance"],"content":"Dividend at the end of i-th period: Di​=(1+kg​)iD0​\n\nAssumption in this model is that dividend will grow at rate g per year\n\nPresent value of Equity\n\nr is the required rate\nSn​ is the stock price when you sell it (end of period n)\nR:=1+kr​1+kg​​\n\nS0​S∞​​=(1+kr​)D1​​+(1+kr​)2D2​​+⋯+yn​Sn​​=D1​(1+kr​)+⋯=i=1∑∞​(1+kr​)i(1+kg​)iD0​​=D0​i=1∑∞​Ri=D0​⋅1−RR​=D0​kr​−kg​1+kg​​=kr​−kg​D1​​​selling after period nholding stock foreverwhen r&gt;g⟹R&lt;1 (geometric sum) DDM​​"},"Document-Type-Definition":{"title":"Document Type Definition","links":["Extensible-Markup-Language"],"tags":["Computing/Data-Science"],"content":"A schema definition language for XML\nDeclaration\n\nElement &lt;&gt;\n\nElement Declaration §\n&lt;!ELEMENT book (title, author*, publisher?, year?, section*)&gt;\n\nthe element book must contain the following child elements, in order:\n\none title\nzero or more authors\nzero or one publisher\nzero or one year\nzero or more sections\n\n\n\n&lt;!ELEMENT p (#PCDATA | p | ul | dl | table | h1 | h2 | h3)* &gt;\n\nelement p may contain:\n\n{dtd}#PCDATA pure text only (no child elements)\np elements\nul elements\netc.\n* → and zero or more repetitions of them.\n\n\n\nAttribute Declaration §\n&lt;!ATTLIST img\n   src    CDATA          #REQUIRED\n   id     ID             #IMPLIED\n   sort   CDATA          #FIXED &quot;true&quot;\n   print  (yes | no)     &quot;yes&quot;\n&gt;\n\nType is…\n\nID: is the unique id of that element.\n\n! ID is globally unique, regardless of attribute name, element name, etc.\nSee xml - DTD - uniqueness of ID attributes - Stack Overflow\n\n\nIDREF: reference of some id\nCDATA: any string\n(val1|…): can only be either of these values.\n\n\nValue is…\n\n#REQUIRED: necessary\n#IMPLIED: optional\n#FIXED: constant\n⇒ “Final column &quot;default&quot;: default value of that attribute\n\n\n"},"Dominant-Strategy-Equilibrium":{"title":"Dominant Strategy Equilibrium","links":[],"tags":["Economics/Game-Theory"],"content":"Strategy A is dominant over strategy B iff all payoffs regardless of other’s actions.\n\nSometimes no strategy might be dominant\n"},"Dynamic-Programming":{"title":"Dynamic Programming","links":["Knapsack-Problem","Path-Alignment","Matrix-Chain-Multiplication","Longest-Palindromic-Substring","tree","Vertex-Cover-and-Minimum-Independent-Set"],"tags":["Computing/Algorithms"],"content":"\n\n                  \n                  The name &quot;Dynamic Programming&quot; is a misnomer. \n                  \n                \n\n\n\n                  \n                  Motto of Dynamic Programming \n                  \n                \n\n\nDynamic Programming (DP) is a algorithm design paradigm that includes the following features:\n\nAlgorithms are recursive\nThere are overlapping recursive subproblems\nMemoization can be used to reduce re-solving overlapping subproblems\nYou can also write an iterative solution which can be easier to analyze\n\n\n\n                  \n                  Think of DP as a &quot;filling in a table&quot; problem \n                  \n                \n\nTo devise a DP solution to a problem, you must\n\nUnderstand which subproblems overlap\n\nRecursion trees help in identifying overlap\n\n\nUnderstand the subproblem dependencies (which subproblems to solve first)\n\nHow to break ties (e.g. minimum or maximum of two dependencies)\n\n\nChoose which order to iterate\n\nIterate from i=0..n,j=0..n\n\nKnapsack Problem, Path Alignment\n\n\nIterate for increasing gap=j−i from gap=0..n\n\nMatrix Chain Multiplication, Longest Palindromic Substring\n\n\nIterate on all notes of a tree, where the data is stored in the tree node itself.\n\nVertex Cover and Minimum Independent Set\n\n\n\n\n"},"EBITDA-Multiple":{"title":"EBITDA Multiple","links":[],"tags":["Economics/Finance"],"content":"def. Enterprise value. The amout of money you have to pay to buy off the company, including its financial oblications (net debt)\nEnterprise Value = Equity Value + Net Debt\nwhere Net Debt = SR,LR debt + min.inst.pref.stock−cash\n\nNet Debt &gt; 0 when firm has more debt\nNet Debt &lt; 0 when firm has more cash\n\ndef. EBITDA Multiple.\nEBITDA Multiple=EBITDAEV​=ProfitValue​\n⇒ Think: For two firms…\n\n…if the market values one firm higher [=EV is higher] than another firm,\n…even if their profit is relatively smaller [=relative EBITDA], it implies that:\n…the market thinks the firm has growth potential.\n\nThe EBITDA Multiple is a good measure of how good the operations are of a company because…\n\nEBITDA doesn’t deduct financing costs.\nEBITDA can compare companies in similar industries about how good their operations are.\n"},"ECON-205-Intermediate-Microeconomics-II":{"title":"ECON 205 Intermediate Microeconomics II","links":["Monotonic-Transformation","Constrained-Optimization","Rationality-(Economics)","Utility-Function","Budget-Lines","Utility-Maximization","Utility-Maximization-with-Endowments","Uncompensated-Demand-curve","Indirect-Utility-Function","Expenditure-Minimization","Marginal-Willingness-to-Pay","Expenditure-Function","Map-of-Microeconomic-Optimization","Profit-Maximization","Input-Demand-and-Output-Supply","Profit-Function","Cost-Minimization","Monopoly","Microeconomic-Market-Equilibrium","Edgeworth-Box","Pareto-Efficiency","Compensating-and-Equivalent-Variation","Game-Theory","Dominant-Strategy-Equilibrium","Equilibria-in-Game-Theory","Oligopoly","Bertrand-Price-Competition","Cornout-Quantity-Competition"],"tags":["Courses"],"content":"Background Knowledge §\n\nMonotonic Transformation\nConstrained Optimization\nRational Taste\n\nConsumer Choice Theory §\n\nUtility Function\nBudget Constraints\nUtility Maximization\n\nUtility Maximization with Endowments\nOrdinary Demand\nIndirect Utility Function\n\n\nExpenditure Minimization\n\nHicksian Demand Curve\nExpenditure Function\n\n\n&amp; See: Map of Microeconomic Optimization\n\nTheory of the Firm §\n\nProfit Maximization\n\nInput Demand and Output Supply\nProfit Function\n\n\nCost Minimization\n\nConstrained Constrainted Input Demand\n\n\nMonopoly\n\nEquilibrium Theory §\n\nMicroeconomic Market Equilibrium\nTrade between individuals\n\nEdgeworth Box\nPareto Efficiency\nCompensating and Equivalent Variation\n\n\n\nGame Theory §\n\nGame Theory\n\nDominant Strategy Equilibrium\nEquilibria in Game Theory\nSubgame Perfect Nash Equilibirum\nRepeated Game\n\n\nOligopoly Games\n\nBertrand Price Competition\nCornout Quantity Competition\n\n\n"},"Econ-201-Intermediate-Microeconomics-I":{"title":"Econ 201 Intermediate Microeconomics I","links":["Budget-Lines","Utility-Function","Constrained-Optimization","Income-Effect-(IE)","Substitution-Effect-(SE)","Types-of-Demand-Curves-(MicroEcon)","Production-Function","Profit-Maximization","Price-Controls","International-Trade","Game-Theory","Monopoly","Bertrand-Price-Competition","Monopolistic-Competition","Cartels-and-Collusion","Types-of-Goods-(Economics)"],"tags":["Courses"],"content":"Consumer Theory §\n\nBudget Lines\nIndifference Curve\nConstrained Optimization\nIncome Effect (IE)\nSubstitution Effect (SE)\nTypes of Demand Curves (MicroEcon)\n\nTheory of the Firm §\n\nProduction Function\nProfit Maximization\n\nMarket Distortion §\n\nPrice Controls\nInternational Trade\n\nMarket Power §\n\nGame Theory\nMonopoly\nBertrand Price Competition\nMonopolistic Competition\nCartels and Collusion\nPublic Good\n"},"Econ-204-Econometrics":{"title":"Econ 204 Econometrics","links":["Econometrics","Bivariate-Ordinary-Least-Squares-Regression","Confidence-Intervals-and-Hypothesis-Testing-in-OLS-Linear-Regression","Omitted-Variables","Multivariate-Ordinary-Least-Squares-Regression","Homework-5"],"tags":["Courses"],"content":"\n\n                  \n                  Quote \n                  \n                \nCorrelation doesn’t imply Causation\n\n\nEconometrics\nBivariate Ordinary Least Squares Regression\n\nConfidence Intervals and Hypothesis Testing in OLS Linear Regression\nOmitted Variables\n\n\nMultivariate Ordinary Least Squares Regression\n\nHomework 5"},"Econ-210-Macroeconomics":{"title":"Econ 210 Macroeconomics","links":["Macroeconomics","Growth-Rate-Calculations","Gross-Domestic-Product","Agents-of-the-Macro-Economy","Macroeconomic-Market-Equilibrium","Laffer-Curve","Beveridge-Curve","Homogenous-Function"],"tags":["Courses"],"content":"\nMacroeconomics\nGrowth Rate Calculations\nGDP\nAgents of the Macro Economy\nMacroeconomic Market Equilibrium\nLaffer Curve\nBeveridge Curve\nHomogenous Degrees\n"},"Econ-361-Distributive-Justice":{"title":"Econ 361 Distributive Justice","links":["Robert-Axelrod","Robert-Frank","Milton-Friedman","Elinor-Ostrom","Michael-Munger","Joseph-Stiglitz","Thomas-Picketty","Amartya-Sen","John-Stuart-Mill","John-Rawls","Robert-Nozick"],"tags":["Courses"],"content":"\n\n                  \n                  Remember, there are still people starving as we speak. \n                  \n                \n\nApplying Game Theory §\n\nRobert Axelrod - The Evolution of Cooperation\nRobert Frank - Passions within Reason\nJ.L. Mackie - Ending Foot Binding and Infibulation: A Conventional Account\nFriedrich Hayek - The Social Uses of Information\nMichael Rosenberg - Pure Theory of Labor Market Monopsony\nMilton Friedman - Externality\n\nMarket Failures §\n\nJonathan Anomaly - Public Goods and Government Action\nJean Hampton - Free Rider Problems in the Production of Collective Goods\nElinor Ostrom - Collective Action and the Evolution of Social Norms\nRobert Frank - Darwinian Economy, Chapters 1-3\n\nMeasuring Amount of Market Failure §\n\nMichael Munger and Mario Villarreal-Diaz - The Road to Crony Capitalism, Independent Review\nJoseph Stiglitz - The Price of Inequality, Chapter 2: Rent Seeking and Chapter 3\nThomas Picketty\n\nRobert D. Kirkby - Summary of Picketty\nPicketty Explained Blog - Summary of Capital in the Twenty-First Century\n\n\n\nSolutions to Market Failure §\n\nAmartya Sen - Equality of What?\nJohn Stuart Mill - Utilitarianism, Chapter 1, Chapter 2 (pp. 4-11), Chapters 3, 4\nJohn Rawls - A Theory of Justice\nRobert Nozick - Anarchy, State, and Utopia\nDerek Parfit - Equality and Priority\n"},"Econ-371-Finance":{"title":"Econ 371 Finance","links":["Present-Value-Calculations","Bonds-(Finance)","Equity","Comparable-Company-(Comps)-Analysis","Portfolio-Theory","CAPM-Model","Futures","Short-selling","Trade-Through","Interest-Rate","Derivatives-(Finance)","Contagion-(Finance)"],"tags":["Courses"],"content":"\nPresent Value Calculations\nBonds (Finance)\nEquity\nComparable Company (Comps) Analysis\nPortfolio Theory\nCAPM Model\nFutures\nGlossary\n\nShort-selling\nTrade-Through\nInterest Rate\nDerivatives (Finance)\nContagion (Finance)\n\n\n"},"Econometrics":{"title":"Econometrics","links":["Correlation"],"tags":["Economics"],"content":"\nEstablishing causality\n\ndoes “policy/behavior/program → outcome”?\nX→Y?\n\n\nChallenges\n\nIs it random noise?\n\nRemoving outliers\n\n\nAre there exogenous factors?\n\n\n\nX is endogenous if X correlates with ϵ (error)—non-causal, lurking variable\n\n\n\n\nX is exogenous if X doesn’t correlation with ϵ—likely causal\n\n\n\n\n\n\nRegression Analysis: Yi​=β0​+β1​Xi​+ϵi​\n\nβ0​: intercept. Less interesting\nβ1​: slope. More Interesting\nX: independent variable\nY: dependeng variable\n\n\n! be careful…\n\nCorrelation = Causation\nA higher slope doesn’t mean higher correlation; a higher Correlation coefficient (ρ=σX​σY​σXY​​) does mean higher correlation.\nStatistical Significance = Real-life Effects\n\n\n"},"Economic-Democracy":{"title":"Economic Democracy","links":["(Book)-The-Case-for-Economic-Democracy","(Economist)-Andrew-Cumbers"],"tags":["Economics","Philosophy/Political-Philosophy","index"],"content":"\n(Book) The Case for Economic Democracy\n(Economist) Andrew Cumbers\n(DevonThink) Economic Democracy and Labor Productivity\n"},"Economics-Course-Requirements-(4-or-3-left)":{"title":"Economics Course Requirements (4 or 3 left)","links":["ECON-205-Intermediate-Microeconomics-II"],"tags":["Courses"],"content":"B.S. in Economics, Finance Concentration\nFrequently Asked Questions: New Econometrics Sequence\nCourses\nMath Requirement :luc_check_circle: §\nCompletion of a higher-level math course (MATH 212 or higher) demonstrates proficiency in lower-level math courses; therefore, lower-level requirements may be waived for students who have successfully completed higher-level math courses.\n\nMATH 111L Laboratory Calculus I OR MATH 105L Laboratory Calculus and Functions I AND MATH 106L Laboratory Calculus Functions II\nMATH 122 Introductory Calculus II OR MATH 112L Laboratory Calculus II OR MATH 122L Introductory Calculus II with Applications\nMATH 202 Multivariable Calculus for Economics or MATH 212 Multivariable Calculus OR MATH 222 Advanced Multivariable Calculus OR any higher-level math course with MATH 212 as prerequisite.\n\nCore Economics Courses :luc_check_circle: §\n\nECON 101D Economics Principles\n\nSKIPPED, Grades transferred from AP\n\n\nEconometrics (see FAQs for more information about this new sequence)\n\nSKIPPED: ECON 104D Statistical Foundations of Econometrics and Data Science\nECON 204D Econometrics Data Science (must be taken before senior year)\nIll be taking STAT 230 → STAT 432 → Econ 204D\n\n\nMicroeconomics\n\nECON 201D Intermediate Microeconomics I\n==ECON 205 Intermediate Microeconomics IID== Intermediate Microeconomics II [2]\n\n\nMacroeconomics\n\nECON 210D Intermediate Macroeconomics\n\n\n\nElectives (2 or 3 left) §\nNo Concentration (1 left) §\n\nFive Economics General Electives at the 300or 400-level, of which one must be a 300level and one must be a 400-level\n\nECON 361 PPE Distributive Justice\nECON 372 Asset &amp; Risk\nECON 673 Mathematical Finance\nECON 565 Algorithmic Game Theory\nECON 386 PPE Capstone?\n(ECON 500-549 may only be counted toward the major with approval from the director of undergraduate studies))\n\n\n\nGeneral Restrictions §\n\nA maximum of two economics transfer credits will be accepted toward the major. This applied to courses taken in the United States and to study abroad courses. One exception if the London School of Economics full-year (fall and spring) program, from which a maximum of four courses may be counted toward the major.\n\nEffective for courses taken after the Spring 2018 semester, we will no longer accept transfer credits for the following courses: ECON 204, ECON 205, ECON 208, and ECON 210. Courses that are part of “Duke In …” programs count as Duke courses and not transfer courses (please note that a few “Duke In …” programs are hybrids in which some courses count as Duke courses but students may also take transfer courses at the foreign institution). Also, inter-institutional courses are not considered transfer courses, nor are pre-matriculation credits. If you have questions about whether a course taken away from Duke would be considered a transfer course, please consult the director of undergraduate studies or associate director of undergraduate studies before taking the course.\n\n\nDukeHub enforces prerequisites for many Economics courses.\n1 Students with credit for both AP Macro and AP Micro (4 or higher) may receive credit for ECON 101. To receive credit for ECON 101 using an international standardized exam, please visit the Trinity College policy for qualifying scores.\n2 Prerequisites are enforced for ECON 205D. They include ECON 201, and either MATH 202 or MATH 212 or MATH 222.\n3 Students are limited to counting a maximum of TWO Fintech courses toward the finance concentration requirements.\n"},"Edgeworth-Box":{"title":"Edgeworth Box","links":["Pareto-Efficiency","Uncompensated-Demand-curve","Budget-Lines"],"tags":["Economics/Game-Theory"],"content":"\nContract Curve is the set of pareto efficient points in the edgeworth box.\n\nExample. The black curve in (b) is the contract curve. \nThe gradient of the two curves must be equal in every point of the contract curve.\nThe Core is the set of Pareto efficient allocations that make both parties better off than their endowments. (The highlighted section inside the green is the core.) \n\n\nEquilibrium is when the exchange rate of goods (i.e. the price ratio) is set in such a way that trade occurs and there is market clearance (i.e. Qdemand​=Qsupply​).\n\nIt must be pareto efficient.\nExample of a Disequilibirum. Here. price is set to 1 orange=1 banana. Blue player will move E1​→A, and red E2​→B. But those two points do not meet.\nExample of Equilibirum. Here, price is set to 1.5 orange=1 banana. Blue moves E→C, and Pink moves E→C and therefore demand for bananas is equal to supply of bananas, and demand for oranges are equal to supply of oranges.\nSolving for the equilibirum:\n\nGet the Ordinary Demand for good x1​ from both parties.\n\nin this case, the Budget Constraint is I=p1​e1​+p2​e2​\n\n\nSet the equilibirum condition Qdemand​=Qsupply​\n\nx1A​+x1B​=e1A​+e1B​\nDemand by A+Demand by B=Endowment of A+Endowment of B\n\n\nSolving this for the price ratio p2​p1​​ will yield the relative price (=price that will lead to a trade that results in an equilibirum).\n\n\n\n\n"},"Efficiency-(Statistics)":{"title":"Efficiency (Statistics)","links":["Cramer-Rao-Lower-Bound-(CRLB)"],"tags":["Math/Statistics"],"content":"Cramer-Rao Lower Bound (CRLB)"},"Efficient-Market-Hypothesis":{"title":"Efficient Market Hypothesis","links":[],"tags":["Economics/Finance"],"content":""},"Egotistic-Altruism":{"title":"Egotistic Altruism","links":["Be-Nice","Self-Before-Others"],"tags":["Economics/Game-Theory","Sociability"],"content":"\n\n                  \n                  Find a selfish reason to be mindful of others. \n                  \n                \n\n\nDon’t trust yourself to be nice just out of your own benefit; it should also benefit you.\nPersonal Priority. You can only take care of others when you have taken care of yourself\n"},"Elasticity-of-Substitution":{"title":"Elasticity of Substitution","links":["Rationality-(Economics)","Marginal-Rate-of-Substitution-(MRS)","Utility-Function","Cobb-Douglas-Utility"],"tags":["Economics/Micro-Economics"],"content":"\n\n                  \n                  tastes.)\n                  \n                \n\ndef. Elasticity of Substitution. The EoS of good x1​ of x2​ asks “how much % x2​ are you willing to give up to get one more % of x1​“?\nσ2,1​:=% change in x2% change in x1​=∣%ΔMRS1,2​%Δx1​x2​​​∣≡dln∣MRS1,2​∣dlnx1​x2​​​\n⇒ MRS is the Marginal Rate of Substitution (MRS) of good\nHigher elasticity of substitution (EoS) means the following things:\n\nUtility Function has less curvature\nx1​ and x2​ are better substitutes\n\nTaste and elasticity\n\nHomothetic tastes when MRS depends on x1​x2​​ alone:\n\ni.e. MRS(tx1​,tx2​)=tkMRS(x1​,x2​)\n\n\n\nElasticity &amp; Indifference Curve Shapes §\n\nCobb-Douglas EOS §\n\nu(x1​,x2​)=x1α​x21−α​\nMRS=−1−αα​⋅x1​x2​​\n\n⇒ Then we observe:\n​x1​x2​​=MRS1,2​α1−α​⟹dln∣MRS∣d​lnx2​x1​​=dln∣MRS∣d​(lnMRS+ln1−αα​)⟹dln∣MRS∣dlnx2​x1​​​=1 =σ2,1​​​\n⇒ We also can see that it is homogenous and thus homothetic:"},"Elinor-Ostrom":{"title":"Elinor Ostrom","links":[],"tags":["People"],"content":""},"Emergent-Phenomena":{"title":"Emergent Phenomena","links":[],"tags":["Philosophy"],"content":"\n\n                  \n                  E pluribus unum\n                  \n                \n"},"Entity-Relationship-Model":{"title":"Entity-Relationship Model","links":["Relational-Algebra"],"tags":["Computing/Data-Science"],"content":"\n\n                  \n                  Abstract \n                  \n                \nWe’re trying to model real-world data subject to real-world constraints. The entity relationship model is a simple method to do so.\nDesign goals:\n\nRedundancy is bad\nAttributes cannot be lists!\nTrade-off between capturing more constrainsts vs. simplicity\n\n→ Common Sense &gt; capturing all constraints\n\n\nDon’t introduce nonexistent constraints\n\n\nComponents of an E/R Model\n\nEntity—with attributes\n\nKey attributes are underlined\n⇒ This is NOT like database keys, in the case that weak entities require both its keys and its supporting entity’s keys to identify it uniquely.\n\n\nRelationship—with attributes\n\nRelationship attributes are not the same as entity attributes: they cannot be duplicated.\ne.g. StopsAt cannot have multiple times for each Train-Station pair: \n\n\n\nComplex Relationships §\n(DevonThink) 3. E/R Design\nMultiplicity §\n\nMany-to-Many: Most types of relations \nMany-to-One\n\nhalf-circle arrow: exactly one \nNormal arrow: one or zero\n\n\nOne-to-One\n\nRoles in a Relationship §\n\nif you’re taking two items from the same entity, the roles need to be clarified. \n\nN-ary Relationships §\n\none-to-one (=binary) relationships is not the norm.\nyou can break it up into many relationships…\n\n…but it’s hard and may require new relations, entities, etc.\n\n\ne.g. a isMemberOf relationship can have multiple members, but only (zero or) one initiator for a group—member pair\n\n\nWeak Entity &amp; Supporting Relationships (=Heirarchical relationships) §\n\nmore data is needed to uniquely identify the thing\ne.g. a room number in a building needs the building identification to uniquely identify it\n\n\nISA Relationship (Subclassing) §\n\nliterally “A is-a B” relationship\nInherits the attributes (with the key), relationships\n\n\nTranslating into Relational Model §\n(DevonThink) 4. E/R Translation\n\nEntity Set → Table. Train,LocalTrain, etc.\n\nAttribute → Column of Table. number, engineer\nKey attribute → Key number\n\n\nRelationship → Table of keys (on both entities) and attributes if necessary.\nWeak Entity Set → Table, but including all the keys up the hierarchy\n\nExpressTrainStopsAtStation includes keys of ExpressTrains\nYou need the keys of all supporting entities to uniquely identify it\n\n\nIsA Relationship → Table, but we can choose how to translate it:\n\nEntity-in-all-superclass-and-specific-class\n\nScattered List\n\n\nEntity-only-in-specific-class\n\nScattered List\n\n\nAll-entities-in-one-table\n\nGoogle BigTable’s approach\nNULL when a subclass’s attribute doesn’t apply\nResults in a sparse table (we have good ways to store this these days!)\n\n\n\n\n\n\n\n                  \n                  Full example\n                  \n                \n\n"},"Equilibria-in-Game-Theory":{"title":"Equilibria in Game Theory","links":["Strategy","Linear-Programming","Integer-Linear-Programming","Signaling-Game"],"tags":["Economics/Game-Theory"],"content":"Intuition. Hierarchy of Games in game theory. \nPure Strategy §\ndef. Pure Strategy Nash Equilibrium is a set of strategies (one for each player) which is the best response strategy of each other’s move; i.e. you can’t deviate without destabilizing the equilibrium.\n∀i∈Players​, si​=argmins∈Si​​c1​(s,s−i​​)\nwhere:\n\ns−i​: strategies of all other players (excluding i-th player)\ns: strategy for i-th player\nS: strategy set for i-th player\nci​(s): cost function for i-th player\n\nHow to Find PNE §\n\nIn Simultaenous Games: Use the Corner method\nIn Sequential Games:\n\nMake the Decision Tree into a Payoff Matrix, and use the Corner method\nJust test out every combination of Strategy\n\n\n\nalg. Corner Method to find NE. Finding the Nash Equilibrium in a payoff matrix.\n\nFor each player:\nFor each other player’s move; highlight the line of the best response\n→ When both the left and top lines are highlighted that is NE.\n\nExample. Driving left or Right:\n\n\nSubgame Perfect Nash Equilibrium (for Pure Strategy) §\ndef. A Non-credible threat is when the follower in a sequential move game says: “If I can’t win, I’ll take you down no matter how much it costs to me.”\n\nIn the above game, (R,R) is an example of a non-credible threat because when Player 1 chooses L, Player 2’s best response is L—but Player 2 threatens to go R. This is because they want the best possible outcome for them, R,(R,R)\n\ndef. Subgame Perfect Nash Equilibria (SPNE) are NE where the follower will only choose strategies that are best for them, and can’t threaten the leader beforehand with non-credible threats.\nalg. Backwards Induction. To find the SPNE of a game, use Backwards Induction:\n\nDetermine the last player’s best strategy\nThe second-last player knows what the last player will do. Then determine what the second-last player will do.\nContinue solving until the first player.\n\n\nExample. Caterpillar game unraveling\nSPNE={(d,d),(d,d)}, payoff is (1,0) which is a lot worse off that global optimal.\n\nMixed Strategy §\ndef. Mixed Strategy Nash Equilibrium\n\nEach player i chooses a distribution σi​ over strategy set Si​ ← σi​ is public knowledge\nAt runtime, each player chooses strategy si​∼σi​ independent of other players\n(s1​,…,sn​) is a Mixed Nash Equilibrium if:\n\n∀i​∀si​∈Si​,  Expected cost after switchingEs−i​∼σ−i​​[Ci​(si′​,s−i​​)]​​≥Expected cost of potential equal. strat.Es−i​∼σ−i​​[Ci​(si​,s−i​​)]​​\nalg. Computation of MNE using Linear Programming.\n\nThe payoff matrix denotes what the column player (=minimizing player) gives to row player (=maximizing player)\n\n\nRow player thinks that column player will minimize exchange (stackleberg solution)\nColumn player thinks that row player will maximize exchange (stackleberg solution)\nStrategy tuple that satisfies both conditions is an MNE.\n\nExample. In the following game:\n\nRow player will aim to:\nmaxx1​,x2​​column’s expected strategy min( column goes left3x1​−2x2​​​, column goes right −x1​+x2​​​)​​​ Given column’s expected strategy, the best response ​\nThis is also known as the Stackleberg solution. Letting z=min(3x1​−2x2​,−x1​+x2​) we have the following linear program:\nzz10​≤3x1​−2x2​≤−x1​+x2​=x1​+x2​≤x1​,x2​​​\nEquivalently, column player will aim to miny1​,y2​​max(3y1​−y2​,−2y1​+y2​). Letting w=max(3y1​−y2​,−2y1​+y2​) We have the following linear program:\nww10​≥3y1​−y2​≥−2y1​+y2​=y1​+y2​≤y1​,y2​​​\nThe above two linear programs are dual problems to each other. The solution to both of these programs are:\n{x1​=73​,x2​=74​y1​=72​,y2​=75​​\nAnd this is also the mixed strategy nash equilibirum to the game.\n■\nCorrelated Nash Equilibrium (CNE) §\ndef. Correlated Nash Equilibrium\n\n3rd party computes joint distribution of all parties S1​×S2​×⋯×Sn​, to joint distribution σ ← this joint distribution is public knowledge.\nAt runtime, 3rd party draws strategy vector s=(s1​,s2​,…,sn​)∼σ\n3rd party tells player i to play si​\nPlayer i computes the posterior distribution of what other players will do: σ−i​\n(s1​,…,sn​) is a Correlated Nash Equilibrium iff:\n\n∀i​∀si​∈Si​,  Expected cost after switchingEs−i​∼σ−i​​[Ci​(si′​,s−i​​)]​​≥Expected cost of potential equal. strat.Es−i​∼σ−i​​[Ci​(si​,s−i​​)]​​\nalg. CNE Computation Using Integer Linear Programming:\n\nExample of Correlated Nash Equilibirum: Traffic Lights game:\n\nImperfect Information §\nBaysian Nash Equilibrium §\nIn a\n\n\nmixed strategy (players each have probability of action)\n\n\none-shot game\ndef. Baysian Nash Equilibirum is a set of strategies that are mutual best responses given a certain probability of other’s actions\n\n\nFor a BNE to exist, each person’s strategy must assign probabilities such that the opponent is neutral to each of their options. (if not, it means there is a dominant strategy for the opponent)\n\n\ndef. Perfect Baysian Equilibirum. In a mixed strategy, finitely repeated game, with each player having a belief a Perfect Baysian Equilibirum is one that\n\nthe player’s beliefs are consistent with strategy (rational)\nis a BNE for each subgame\n→ Signaling Game is a good example\n"},"Equity":{"title":"Equity","links":["Dividend-Discount-Model","Comparable-Company-(Comps)-Analysis"],"tags":["Economics/Finance"],"content":"Terminology §\n\nEquity. Equity is a partial ownership of an asset (mostly companies)\n\nEquity in a mortgage\n\n\n\nEquity=Net Market Value−Unpaid Balance\n- Equity in a company.\n\t- **Equity index**\n\t\t- $\\coloneqq$ a weighted average of equities\n\t\t- e.g. _S&amp;P 500, Dow Jones Ind. Avg., Nasdaq, Rusells 2000_ ← different indicies with different weights\n\t\t- ownership rights (like voting in decisions) are relinquished\n\t- [[Private Equity Firms]]\n\t- Earnings Season\n\t\t- Firms post earnings every quarter. Earnings $\\equiv$ Income $\\equiv$ Profit.\n- You buy an equity because you expect\n\t1. It pays regular dividends\n\t2. You expect its price to go up ← S&amp;P has always beaten any other asset class\n\n\nDividend Discount Model. Pricing an equity.\nMinority Interest (=non-controlling interest):= The interests of a non-controlling stockholder.\n\nThis interest is not interest rate. It’s about having a stake.\n\n\nFully Dilluted Shares:= total sum of outstanding shares.\nCommon Stock: may not pay dividend; but if it does, you expect it to grow\nPreferred Stock: No voting rights, but during bankruptcy you can liquidate first.\nComparable Company (Comps) Analysis. Which equity to buy?\nDividend Yield\n\nDividend Yield=Share priceDividend per share​"},"Estimator":{"title":"Estimator","links":["Bias-(Statistics)","Variance","Efficiency-(Statistics)","Consistency","Mean-Squared-Error","Likelihood-(Statistics)"],"tags":["Math/Statistics"],"content":"def. Statistic. let X1​,…,Xn​ observable random variables [= data of an experiment]. then statistic T is:\nT=δ(X1​,...,Xn​)\n\nδ:{X1​,…,Xn}→R i.e. is a real-valued function\nδ cannot contain unknown variables\n\ndef. Estimator [= point estimate] is a statistic used to estimate the parameter of the model we think the data is showing. Note the following notation convention:\nθ^=δ(X1​,...,Xn​)\n\nAssume X as an r.v. of an experiment, whose model includes parameter θ.\nTo estimate ground truth parameter θ, we can use an estimator r.v. θ^(X)\nA specific estimate for a particular observed value x1​ is denoted θ^(x1​)\nAn estimator has to be a function of known variables &amp; data only.\nVar(θ^)=E[(θ^−Eθ^)2], NOT E[(θ^−θ)2] ← This is MSE\n\nHow Good is Your Estimator? §\n\nAccuracy is higher. Increased as Bias (Statistics) is decreased\nPrecision is higher. Increased as Variance V(θ^) is decreased\nEfficiency (Statistics) is higher. If estimators θ^1​,θ^2​ have the same accuracy, but V(θ^1​)&lt;V[θ^2​] then the former is more efficient than the latter.\nConsistency.\nMean Squared Error is lower.\nLikelihood (Statistics) is higher.\n\n→ In general, making sure to reduce bias of estimators is important. Note that:\n\nIf you can write down what the bias is mathematically [= characterize the bias], then you can make a new estimator that doesn’t have the bias.\nBias usually decreases as the data points increase\n\n\n\n                  \n                  Example \n                  \n                \nlet X1​,…,Xn​∼iidN(μ,σ2) and\nlet estimator θ^:=∑in​ai​Xi​ where\n\na1​,…,an​ are weights that sum to 1. [= weighted average]\nθ^ is estimating μ. σ is known.\n\n\n\n\nHow accurate is θ^? [=what is the bias?]\n\n\n\n&gt;&gt;E[θ^]&gt;&gt;&gt;&gt;B(θ^)​=E[∑ai​Xi​]=∑E[ai​Xi​]=∑ai​E[Xi​]=μ=0&gt;​&gt;&gt;&gt;\n\n\n\nHow precise is θ^? What are the best ai​,…,an​?\n\n\n\n&gt;&gt;V(θ^)&gt;&gt;&gt;&gt;&gt;​=V[∑ai​Xi​]=V[a1​X1​]+⋯V[an​Xn​]=a12​V[X1​]+⋯an2​V[Xn​]=a12​σ2+⋯an2​σ2=σ2∑ai2​&gt;&gt;​&gt;&gt;&gt;\n\n→ Thus V[θ^] is minimized when ai​=n1​.\n"},"Euclid's-Algorithm-for-Greatest-Common-Denominator-(GCD)":{"title":"Euclid's Algorithm for Greatest Common Denominator (GCD)","links":[],"tags":["Computing/Algorithms","Math"],"content":"Algorithm §\nEuclidean algorithm - Wikiwand\n\nThe Euclidean algorithm is based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number.\nFor example, 21 is the GCD of 252 and 105 (as 252 = 21 × 12 and 105 = 21 × 5), and the same number 21 is also the GCD of 105 and 252 − 105 = 147. Since this replacement reduces the larger of the two numbers, repeating this process gives successively smaller pairs of numbers until the two numbers become equal. When that occurs, they are the GCD of the original two numbers.\n\ndef gcd(a, b):\n\twhile b != 0:\n\t\ta, b = b, a % b\n\t\t# guaranteed a &gt; b, \n\t\t# because remainder is smaller than divisor\n\t\t# needs to be one-line because the new a and b musn&#039;t\n\t\t# interfere for the old a and b\n\treturn a\n\nIn this implementation, the algorithm iteratively calculates the remainder when dividing the larger number a by the smaller number b. It then updates a to be b and b to be the remainder. This process continues until b becomes zero, indicating that a is the GCD of the original two numbers.\n\n\nThe implementation has a time complexity of O(log(min(a, b))) as it iteratively reduces the numbers by taking their remainders.\n\nTime Complexity §\nStack Overflow\n\nSo the number of iterations is linear in the number of input digits. For numbers that fit into cpu registers, it’s reasonable to model the iterations as taking constant time and pretend that the total running time of the gcd is linear.\nOf course, if you’re dealing with big integers, you must account for the fact that the modulus operations within each iteration don’t have a constant cost. Roughly speaking, the total asymptotic runtime is going to be n^2 times a polylogarithmic factor. Something like n^2 lg(n) 2^O(log* n). The polylogarithmic factor can be avoided by instead using a binary gcd.\n"},"Everything-is-a-File":{"title":"Everything is a File","links":[],"tags":["Computing"],"content":"Everything is a File (Wikipedia)\n\nEverything is a file is an idea that Unix, and its derivatives, handle input/output to and from resources such as documents, hard-drives, modems, keyboards, printers and even some inter-process and network communications as simple streams of bytes exposed through the filesystem name space. […] The advantage of this approach is that the same set of tools, utilities and APIs can be used on a wide range of resources and a number of file types.\n"},"Expected-Value":{"title":"Expected Value","links":[],"tags":["Math/Statistics"],"content":"Expected Value §\ndef. Expected Value. For random variable X with countably many outcomes, its expected value E(X) is defined as:\nE(X)E(X)​:=∀x∈range(X)∑​x⋅P(X):=∫−∞∞​x⋅fX​(x)dx​for discretefor continuous​​\nProperties. The following identities hold for expected values, with constant k, random variables X,Y.\n\nE(k)=k\nE(X+Y)=E(X)+E(Y) Linearity\nE(k⋅X)=k⋅E(X)\nE(X⋅Y)=∑∀z​z⋅P(X⋅Y=z)\n\nIf X⊥Y then E(X⋅Y)⇒E(X)⋅E(Y) (reverse does not hold)\n\n\nlet g be a function over range(X). Then, E(g(x))=∑∀x​g(x)⋅P(X=x) (Law of Unconscious Statistician)\n\n! E(g(x))=g(E(x))\nE(Xk)=∑∀x​xk⋅P(X=x)\n\n\n\nthm. Tail Sum Formula. when X is a non-negative discrete random variable:\nE(X)=i=0∑∞​P(X≥i)\nRemark. The Tail Sum formula is useful when the random variable is defined as the minimum or maximum of a certain set of events (e.g. minimum of multiple dice rolls, etc.)\n\n\n                  \n                  Expectation Manipulation from class:\n                  \n                \n\n\nConditional Expected Value §\nE(g(X,Y))=∬R2​g(x,y)fX,Y​(x,y)dA\ndef. Conditional Expectation. let X,Y be jointly distributed. Then the conditional expected value is defined…\n\n…over an event: E(X∣A)=∑∀A​x⋅P(X=x∣A)\n…over an event on a random variable E(X∣Y=y)=∫−∞∞​x⋅fX∣Y=y​(x) dx\n…over a random variable: E(X∣Y)=∑∀x∀y​x⋅P(X=x∣Y=y)\n\n! While expectation conditioned on an event is a value, an expectation conditioned over a random variable is another random variable\nwith more rigour: E(X∣Y):=E(X∣σ(Y))\nIntuition. Think of it as “given all information by Y, what’s the new random variable?”\n\n\n\nProperties.\n\nE(aX+bY∣A)=a⋅E(X∣A)+b⋅E(Y∣A) linearity\nE(X)=∑∀i​E(X∣Ai​)⋅P(Ai​) where A1​,…,An​ is a partition of Ω.weighted summation\nIf X is F-measurable, E(X∣F)=X\nIf X is independent of H then E(X∣H)=E(X) Taking out independent factors. \nif X is F-measurable, E(XY∣F)=XE(Y∣F) Taking out what’s known\n\ne.g. E(Xt+0.2​Xt​∣F)=Xt​E(Xt+0.2​)\n\n\nTower Property: if X is a random variable, and F⊂G then: E(E(X∣G)∣F)=E(X∣F)\n\n&amp; Think of it as “high-res camera” then “low-res camera”; the final picture is low-res.\n\n\n\nthm. Conditional Joint Expectation. X,Y and R∈[Range(X)×Range(Y)]. Then:\nE(X∣x,y∈R)where fX,Y∣x,y∈R​(x,y)​=∬R​x⋅fX,Y∣x,y∈R​(x,y) dA=P(x,y∈R)fX,Y​(x,y)​​​\nthm. Calculating Expected Value from Conditional Expected Value. (Identity 2 above) let X,Y be jointly distributed. Then the expected value of X is calculated:\nE(X)E(X)​=∀y∑​E(X∣Y=y)⋅P(Y=y)=E(E(X∣Y))=∫−∞∞​E(X∣Y=y)⋅ fY​(y)⋅dy=E(E(X∣Y))​​\n\nUseful for computing E(X) when X depends on Y.\nWorks regardless of whether X,Y are random or discrete, and when mixed.\n"},"Expenditure-Function":{"title":"Expenditure Function","links":["Marginal-Willingness-to-Pay","Budget-Lines","Homogenous-Function","Shepard's-Lemma"],"tags":["Economics/Micro-Economics"],"content":"def. Expenditure Function. (≈Income Function) Relates prices and a certain utility to the required income to achieve that level of utility.\nE:(p1​,p2​,uˉ)↦I\nDerivation. Substitute Hicksian Demand functions into the Budget Constraint.\nProperties.\n\nHD1 in prices (E(tp1,tp2​,uˉ)=tE(p1​,p2​,uˉ))\nIncreasing in prices (∂p1​∂E​,∂p2​∂E​≥0)\n\nequal when you’re not buying that good.\nHigher prices require more income\n\n\nIncreasing in utility (∂u∂E​&gt;0)\n\nMore utility requires more expenditure.\n\n\nUse Shepard’s Lemma in order to get back to Hicksian Demand Curve\n"},"Expenditure-Minimization":{"title":"Expenditure Minimization","links":["Monotonic-Transformation","Utility-Maximization"],"tags":["Economics/Micro-Economics"],"content":"min I=p1​x1​+p2​x2​ such that uˉ=u(x1​,x2​)\n\nYou can do a Monotonic Transformation\nUse lagrangian\ncorner solutions, etc.\nSee Utility Maximization for the reverse casew\n"},"Exponential-Distribution":{"title":"Exponential Distribution","links":[],"tags":["Math/Common-Distributions"],"content":"def. Exponential Distribution. X has exponential distribution with intensity λ:\nX∼Exp(λ)fX​(x)={λe−λx0​x&gt;0else​FX​(t)=1−e−λtE(X)=λ1​           SD(X)=λ1​​\n\nThe exponential distribution is useful when modeling wait time X in a call center, with “business” of λ (e.g. “on average λ calls per hour”).\nBe careful that λ is the number of events in timespan, not the average time it takes between events.\n"},"Exponential-Family":{"title":"Exponential Family","links":["Normal-Distribution","Exponential-Distribution","Chi-Squared","Bernouilli-Distribution","Poisson-Distribution","Binomial-Distribution","Multinomial-Distribution","Hypergeometric-Distribution"],"tags":["Math/Probability"],"content":"def. Expoential Family of Distributions is the distributions whose pdfs are in the following form:\nfX​(x∣θ)=h(x)⋅exp[η(θ)T(x)−A(η)]\nIncludes:\n\nNormal Distribution\nExponential Distribution\nChi-Squared\nBeta\nDiriochelt\nBernouilli Distribution\nPoisson Distribution\n\nAs well as:\n\nBinomial Distribution (with fixed number of trials)\nMultinomial Distribution (with fixed number of trials),\nNegative binomial (with fixed number of failures)\nGeometric (Not Hypergeometric!)\n"},"Exponential-Function":{"title":"Exponential Function","links":[],"tags":["Math"],"content":"By definition… (Taylor Approximation)\nex=x→∞lim​(1+x1​)x=e\nOr by definition, the family of functions that satisfy:\nf(x+y)=f(x)⋅f(y)\n(See Euler’s formula with introductory group theory - YouTube)\nAnother Definition:\nex=n=0∑∞​n!xn​=1+x+2x2​+…"},"Extensible-Markup-Language":{"title":"Extensible Markup Language","links":["Screenshot-2023-10-24-at-18.21.58.png","XPath-and-XQuery"],"tags":["Computing/Data-Science"],"content":"\nSemi-structured data (well-structured)\n\nDocument format (well-formed)\n\n\nSelf-describing\n\n{xml}&lt;book author=&quot;C. Darwin&quot;&gt;The Origin of Species&lt;/book&gt;\n\nTag: {xml}&lt;book&gt;,&lt;/book&gt;\nAttribute: {xml}author=&quot;C. Darwin&quot;\n\nID: id is a special attribute that is unique\n\n\nNamespace: definition of your schema\n\n&lt;myNS:book xmlns:myNS=&quot;http://.../mySchema&quot;&gt; \n\t&lt;myNS:title&gt;...&lt;/myCitationStyle:title&gt;\n\t&lt;myNS:author&gt;...&lt;/myCitationStyle:author&gt;...\n&lt;/book&gt;\n\nElements: The Origin of Species\n{xml}&lt;![CDATA[Tags: &lt;book&gt;,…]]&gt; means character data (escape not required)\nTree Representation\nUse XPath and XQuery to query it\n"},"Fast-Exponentiation":{"title":"Fast Exponentiation","links":[],"tags":["Computing/Algorithms"],"content":"Observation:\nxn={x(x2)(n−1)/2,(x2)n/2,​\\mboxifn\\mboxisodd\\mboxifn\\mboxiseven​\nThus we can achieve O(logn) multiplications."},"Federal-Communication-Commission":{"title":"Federal Communication Commission","links":[],"tags":["Computing"],"content":"A U.S. Government agency that regulart"},"Federal-Funds-Rate":{"title":"Federal Funds Rate","links":["Interest-Rate","Federal-Reserve","Monetary-Policy"],"tags":["Economics/Finance"],"content":"The minimum Interest Rate declared by the Federal Reserve.\n\nThis is what we mean when we normally say ”The Fed is increasing instrest rates” as Monetary Policy\n"},"Federal-Reserve":{"title":"Federal Reserve","links":["Federal-Funds-Rate","inflation","Securities-and-Exchange-Commission","Too-Big-To-Fail","Monetary-Policy"],"tags":["Economics"],"content":"The United States’ central bank independent of the government (or the Treasury) that has the following roles:\n\n\nSetting Interest Rates: The Federal Reserve has a significant influence over the interest rates in the U.S. economy. It can adjust the Federal Funds Rate to control inflation and stabilize the economy.\n\n\nManaging the Money Supply: The Fed controls the amount of money circulating in the economy through measures such as open market operations, reserve requirements for banks, and setting the discount rate [&gt;=Federal Funds Rate].\n\n\nRegulating Financial Markets: It supervises and regulates banks to ensure they are safe and sound. It also monitors their impact on the financial system. (This is separate from the SEC)\n\n\nActing as a Lender of Last Resort: In times of financial distress or crisis, the Federal Reserve provides funds to financially strained banks to prevent bankruptcy and protect the economy. (This causes problems of Too Big To Fail)\n\n\nMaintaining Financial Stability: The Fed works to maintain the stability of the financial system and contain systemic risk that may arise in financial markets.\n\n\nProviding Banking Services to Depository Institutions: These services include supplying the economy with fiat money (U.S. dollar), managing those finances, and processing payments.\n\n\nConducting National Monetary Policy: The Fed works towards achieving maximum employment, stable prices, and moderate long-term interest rates in the U.S. economy.\n\n\nPreventing Banking Panics: The Fed was initially created to prevent widespread panics in the banking sector. Today, it continues to act as a stabilizing force in the financial system.\n\n\nPromoting a Safe and Effective Payment System: The Federal reserve system also ensures the reliability of payment methods including checks, cash, and electronic transactions.\n\n\nKeep in mind that these roles and responsibilities may evolve over time, and additional duties may be undertaken depending on economic and financial conditions."},"Federal-Trade-Commission":{"title":"Federal Trade Commission","links":["Monopoly"],"tags":["Economics"],"content":"Consumer protection agency in the United States.\n\nMost famously, it protects against mergers &amp; acquisitions that lead to Monopoly\n"},"Fetishization":{"title":"Fetishization","links":[],"tags":["Humanities/Marxism"],"content":"Fetishization in general refers to giving meaning to something that does not inherently have meaning.\n\nFetishization of social cues: Finding meaning in social cues is a type of fetish. The actions, expressions or words have less meaning than what we feel they do.\n\nFetishization, Social Nature §\nWe see, then, that everything our analysis of the value of commodities previously told us is repeated by the linen itself, as soon as it enters into association with another commodity, the coat. Only it reveals its thoughts in a language with which it alone is familiar, the language of commodities.\nIt is thus that this value first shows itself as being, in reality, a congealed quantity of undifferentiated human labour."},"Finite-Automata":{"title":"Finite Automata","links":["Regular-Expressions"],"tags":["Computing/Formal-Languages"],"content":"Regular Expressions\nDefinitions §\ndef. Automata is an abstract model of a computer.\ndef. Regular Language is a language that can be expressed by a FSM\ndef. Trap state is a state in which any symbol input leads to the same state.\ndef. Closure of qi​ is simply the set of states reachable from qi​ with only λ.\nDeterministic Finite Automata (DFA) §\n[= Finite State Machine]\nDFA=(Q,Σ,δ,q0​,F)\n\nQ is the set of all states\nΣ is the set of all symbols\nδ:Q×Σ→Q is a function mapping from current state to the next state\n\nδ∗(qi​,λ)=qi​ [= empty strings lead to itself]\nδ∗(q,wa)=δ∗(δ(q,w),a) where a is a single symbol [= processes only one per tick]\n\n\nq0​ is the start state (entry point)\nF is the set of final states\n\n\n\n                  \n                  a,b.\n                  \n                \n\ndef. Language. A string is accepted by a DFA when:\n\nAfter processing the string, the DFA is in a final state\nThe string is in the language\n\nThe set of all aceepted strings by a DFA is the language of the DFA:\nL(M)={w∈Σ∗∣δ∗(q0​,w)∈F}\ni.e. all the strings which, after processing it thru δ∗, it lands on a final state\nNon-deterministic Finite Automata (NFA) §\ndef. Non-deterministic Finite Automata can have multiple edges with the same labels; i.e.\nδ:Q×Σ∪λ→2Q\ni.e. from the current state, you can go to multiple states.\n\n\n                  \n                  Example Non-deterministic FSM \n                  \n                \n\n\ncorr. “there exists a walk between qi​,qj​ whose labels concatenate to w” is equivalent to:\nqj​∈δ∗(qi​,w)\nthm. All NFA can be convered into a DFA which:\nDFA={ QΣδq0​FD​}​=2Q=QD​×Σ=QD​={Q∈QD​∣∃qi​∈Q where qi​∈FN​}​\nProving Regularity after Applying Properties §\ne.g. let L be a regular language. For all strings in L replace one a with b, and let this new language L′. Is this a regular language?\npf. let Mbe a DFA for L.\n\nMake a copy M1​,M2​ and enclose it in a new machine, M′\nFor all a arcs in M1​ write a b arc to the corresponding destination state in M2​.\nThe start state for M1​ is M′ start state.\n\n\nNow, let\n\nw=uav, and w′=ubv where w,u,v∈Σ∗\nδ1∗​(q0​,u)=qi​, δ′(qi​,b)=qj​, δ2∗​(qj​,v)∈F as we outlined above\n\n\nIf w∈L\nthen\n\n→ proofs often involve duplicating the machine in some way.\nDFA Minimization §\nExample:\n\n\n"},"Fisher-Information":{"title":"Fisher Information","links":["Maximum-Likelihood-Estimator","Expected-Value","Likelihood-(Statistics)"],"tags":["Math/Statistics"],"content":"Fisher information helps us find better estimators.\n\nCramer-Rao Lower Bound shows what the best estimators can do with their precision\nReaching the CRLB means it’s finite-sample efficient\nThe Maximum Likelihood Estimator is also a very good estimator.\n\ndef. Fisher Information is the amount of information we have about the unknown parameter. It’s the Expected Value of score.\n\nGiven fX​(x;θ), if X has a high peak we may assume that X carries a lot of information about θ.\nIf X is spread out a lot, we may assume that X carries little information. Thus:\n\nI(θ)​:=Var[s]=E[s2]−E[s]2=E[s2]​​\n\n(2) → (3) as we know that E[s2∣θgt​]=0\n\nthm. Addition of fisher information. if X1​,…,Xn​∼iidfXi​​(x1​,…,xn​∣θ), then:\nIn​=n⋅I\nthm. Score and Fisher Information. if fX​(θ;x) is twice differentiable wrt θ, and under certain regularity conditions:\nI(θ)=−E[∂θ2∂2​lnfX​(X;θ)]\n\nKnowing this we also know that E[s’]=−E[s2].\n"},"Focal-Point":{"title":"Focal Point","links":[],"tags":["Economics/Game-Theory"],"content":"When you ask two strangers to meet up at New York on Jan 1st., without giving them information, and without letting them coordinate, they will likely meet up in Times Square, at Midnight. This is a focal point.\nA Focal Point is a 결집점 in a game with no coordination; a conspicuous point, either by nature or culture, that we can identify."},"Foreign-Direct-Investment":{"title":"Foreign Direct Investment","links":[],"tags":["Economics/Finance"],"content":"When companies directly invest into factories, workforce (=capital) in the country"},"Formal-Grammar":{"title":"Formal Grammar","links":[],"tags":["Computing/Formal-Languages"],"content":"Grammar is formally defined as a tuple:\nG=(V,T,S,P)\n\nV are variable symbols to be used in the language (they can’t be in a string, because they’re placeholders)\nT are terminal symbols to be used in the language.\nS is the start variable\nP is the production rules\n\n\n\n                  \n                  Notation for production rules: \n                  \n                \n\n\nw⇒z : w derives z\nw∗​z : w derives z in zero or more steps\nw+​z : w derives z in one or more steps\n"},"Formal-Languages":{"title":"Formal Languages","links":["Formal-Grammar"],"tags":["Computing/Formal-Languages"],"content":"Notations are borrowed from [[Set Theory]]\ndef. Σ is the set of all symbols\ndef. A string is a finite set of symbols\ndef. A language is a set of strings\nString Manipulation §\n\nλ is the empty string\nconcat(w,v)=w∘v=wv\n\n…naturally, wn=w⋯w\n\n\nreverse(w)=wR\nlen(w)=∣w∣\n\n\nWe can also define languages as containing strings. Some common ones:\n\nΣ∗ ← set of strings, which are concatenated symbols 0 or more times\nΣ+← set of strings, which are concatenated symbols 1 or more times\n\n\n\n                  \n                  \\Sigma=\\{a,b\\}. Then:\n                  \n                \n\n\nΣ∗={λ,a,b,aa,ab,ba,bb,...}\nΣ+={a,b,aa,ab,ba,bb,...}\n\nYou can also use set operations on languages\n\nL1​∘L2​={xy∣x∈L1​,y∈L2​}\n\n…naturally, L1​∘L1​=L12​\n\n\nL1​ˉ​=Σ∗−L1​\n\n\nA language given a grammar is defined as a set of terminal-only strings that are derivable from the starting strings. More formally:\nL(G)={w∈T∗∣S∗​w}"},"Forwards":{"title":"Forwards","links":["Derivatives-(Finance)","No-Arbitrage"],"tags":["Economics/Finance"],"content":"Forwards are a contract that promises to buy/sell an underlying asset at a certain strike price.\nExample. A farmer wants to sell their wheat, and a mill wants to buy some wheat. The current price of wheat is \\20$\n\nThe farmer is afraid of the price of wheat going down\nThe mill is afraid of the price of wheat going down\nTherefore, they draft up a contract: one year from now, they will transact the wheat at a price, \\22$ determined right now. One year from now, they will transact. This is a forward contract.\nUnderlying asset: wheat\nt: time of contract\nT: time of execution\nForward price (=Strike price): F_{T}(t)=K=\\22$\nSpot price: S(0)=\\20$\nContract size: how many units of wheat?\n! no money/assets changes hands now. Thus the value of a portfolio with a forwards contract is zero on the day they entered.\nNow, assume you’re neither a farmer nor a mill, and you just want to bet on the price of wheat.\nShort position: think wheat price will increase. At execution, you will buy wheat from the spot market at price S(T) and give it to the counterparty at price K according to the futures contract\n\nPayoff: K−S(T)\n\n\nLong position: think wheat price will decrease. At execution, you will get price the wheat for price K and then sell at the spot market for S(T)\n\nPayoff: S(T)−K\n\n\n\nthm. (The fair price of a forwards) Under assumption of No-Arbitrage, the fair strike price of a futures contract entered at time t and executed at future date T is:\nK=FT​(t)=S(0)e(r−q)(T−t)\n\nr is the risk-free rate\nq is the dividend rate. In many cases q=0. It only applies to underlying stocks.\nOne may prove this by contradiction, by assuming it doesn’t hold, and constructing an arbitrage portfolio:\n\nthm. (Value of ongoing forward contract) For a futures contract entered at t=0, executed at T, the value of this contract at intermediate time t is:\nFT​(t)=(FT​(t)−FT​(0))e−r(T−t)\nIntuition.\n\n! Value of a forward contract is not same as the faire price.\nFT​(t) is strike price of a hypothetical contract from t to T\nFT​(0) is strike price of a hypothetical contract from 0 to T\nThe difference of these two prices, discounted at risk-free rate.\n"},"Free-Cashflow":{"title":"Free Cashflow","links":["Income-Statement"],"tags":["Economics/Finance"],"content":"def. Cashflow Statement. A table showing the firm’s cash in and out.\n\nD&amp;A addback: you’ve already payed for the factory (un-smearing the cost)\nCapEx &amp; Salvage: money you’re paying for the capital [=capital expenditure], and cash from selling the capital\nChange in Working Capital: Recieveables &amp; Payables\n\nNet Income=Revenue−(COGS, SG&amp;A)​EBITDA​−(D&amp;A+Interest)−Taxes\nFree Cash Flow=Net Income−CapEx+D&amp;A addback±Net Recievables\ne.g. The process of valuing a firm using Free Cash Flows:\n\n\nCalculate Net Income (excl. interest, tax, etc.) from Income Statement\nCalculate Free Cashflow (w. CapEx &amp; Salvage)\nDiscount future cash flows to get Net Present Value = Firm Value\nDiscount parameter is usually obtained from investments with similar risk profiles, or the WACC formula.\n\n\n\n                  \n                  Internal Rate of Return (IRR). It’s another type of interest rate.\n                  \n                \n\n\nToo much cash isn’t a good thing, because it means they’re not getting high return on capital.\n"},"Fund-(Finance)":{"title":"Fund (Finance)","links":[],"tags":["Economics/Finance"],"content":"Performance of Funds §\nTypes of funds that are actively managed [= ∃ fund manager] includes Hedge funds and Mutual funds. Comparison:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHedge FundsMutual FundsRegulations on what to investLinientStrictRegulation on who can joinStrict (only rich people)Linient (anybody)Risk ToleranceHighLowShort-selling?CanCannot (regulation)"},"Future-Value-Calculations":{"title":"Future Value Calculations","links":["Value-of-Money","Present-Value-Calculations","Interest-Rate","Banker's-Rule"],"tags":["Economics/Finance"],"content":"How to calculate the future value of some cash.\n\nA dollar now is worth more than a dollar tomorrow.\nCan be denoted in basis points as well.\nReverse of Present Value Calculations\n\nDefinitions §\n\nk: periods per year\nτ: duration (in years)\nn: number of compounding periods.\n\nn=τ⋅k\n\n\nF0​: Present value = Principal = Amount lent/borrowed at time t0​\n\nFτ​: Future Value = Amount at time t0​+τ\n\n\nr: annual interest\nR: Interest Rate\n\nAlso:\n\nBanker’s Rule is a common way to simplify math calculations.\nDiscount Rate=1+τ⋅r1​.\n\ndef. Simple Interest\n⇒ Only principal is invested at the end of each year\nFτ​=(1+τ⋅r)F0​\ndef. Compound Interest\nDivide into k periods per year, and compound for n periods:\nFn periods​=(1+kr​)nF0​\nDivide into k periods per year, and compound for τ years (equivalent formula):\nFτ years​=(1+kr​)τkF0​\n⇒ Conceptual Demonstration of Compounding Interest:\n\ndef. Continuous Compounding\nCompound for τ years (equivalent formula):\nF(x)=eτ⋅r⋅F0​\n⇒ You can get this formula from compound interest and setting k→∞\ndef. Fractional Compounding\nFx​=(1+kr​)xF0​ where x∈R+\n⇒ You can do things like ”5.35 months of daily compounding.”\n\n\n                  \n                  Note \n                  \n                \nFor a fixed τ, Fk​(τ) is monotonic for k. ⇒ Proof in slides. (Using binomial expansion)\n"},"Futures":{"title":"Futures","links":["Forwards","tags/Short","tags/Long"],"tags":["Economics/Finance","Short","Long"],"content":"Futures contracts are same as Forwards but is exchange-traded. The pricing formula is exactly the same, but there is less counterparty risk because the exchange verifies the counterparty.\nA future is a contract that says “A will sell B a certain amount of resource R for $X at date N in the future.” This is because A wants to hedge against depreciation of R, and B appreciation of R.\n\n\nReduces risk for both parties\n\nFarmers have long position on wheat [=worry that wheat price↑]\nGeneral Mills have short position on wheat [=worry that wheat price↓]\n\n\n#Total Sellers=#Total Buyers i.e.#Short =#Long positions (you can’t subdivide a contract)\nUsed for liquid [=high trade volume] assets, usually highly demanded commondities like gold, oil.\nFutures Price Formula.\n\nFt​=S⋅exp[(r+q)⋅t]\n\n\nS is the spot [current] price of the commodity\n\n\nr is the riskless rate, continuous compounding at timespan (%)\n\n\nq is the carry rate, continuous compounding at timespan (%)\n\n\nt is the time to maturity (timespan)\n\n\nFutures Price &gt; Spot price (=Contango)\n\n…unless Backwardation: Futures Price &lt; Spot Price. ← This is abnormal.\n\n\n\nFutures Price converges to Spot price as it closes into maturity\n\n\n\n\n\n                  \n                  Futures Exchanges are large corporations themselves which manage these contracts. \n                  \n                \n…examples: Chicago Mechantile Exchange (CME), Tokyo Commodities Exchange (TOCOM), etc.\n…Transactors have escrow accounts to not incur fees and ensure safety of assets\n…Ensutre parties aren’t bankrupt and can carry out the exchange\n\nRolling the Contract. You don’t want to take delivery of the contract, but the contract is expiring soon. What you do? ⇒ You roll over the contract, i.e. sell your current contract and buy another one that matures later."},"GSF-386-Politics-of-Sexuality":{"title":"GSF 386 Politics of Sexuality","links":["Prospectus"],"tags":["Courses"],"content":"Prospectus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnitDateTopicReadings and ActivitiesDueUnit I: Theoretical FrameworksJanuary 12Syllabus Overview and Introductions-January 19DefinitionsSiobhan Somerville, “Queer”;E. Patrick Johnson, “Quare” studies;Kathy Peiss, “‘Charity Girls’ and City Pleasures”; Listen: Baldwin reads Baldwin - Joey Part 1&amp;2 from Giovanni’s RoomJanuary 26LawStraight StateFebruary 2InventionsSelections of Not Gay; Brandon Ambrosino, “The Invention of Heterosexuality”; Michel Foucault, The History of Sexuality, Volume 1; Essay 1February 9QuareSelections from Black Queer Studies: A Critical AnthologyUnit II: Politics and Poetics of RememberingFebruary 16SpaceAriana Vigil, “Heterosexualization and the State”; Audrey Yue and Helen Hok-Sze Leung, “Notes towards the queer Asian City”; Rae Garringer, “Fabulachians”; Becki L. Ross, “Sex and (Evacuation from) the City”; Prospectus for Final ResearchFebruary 23MemoryRachel Gelfand, “Between Archives”; Horacio N. Roque Ramírez, “Sharing Queer Authorities”; Elizabeth Lapovsky Kennedy, “Telling Tales”March 1KinshipKids on the Street; Essay 2March 8PoliticsSrinivasan, The Right to Sex–Part 1Spring BreakUnit III: Futures/PastsMarch 22PerformanceStagestruck; Feminist Theory Workshop–3/22-23!March 29DesireAmia Srinivasan, The Right to Sex–Part 2April 5HistoryAbundance;Essay 3April 12Project WeekIn class: Stay on Board: The Leo Baker Story; Rough DraftApril 19Reflections &amp; Sharing Final ProjectsFinal Research Papers"},"Game-Theory":{"title":"Game Theory","links":["Rationality-(Economics)","Institutional-Design","Philosophy,-Political-Science,-Economics","Random-Variable","Equilibria-in-Game-Theory","Oligopoly","Cartels-and-Collusion","Bertrand-Price-Competition","Cornout-Quantity-Competition","Battle-of-the-Sexes","Prisoner's-Dillemma","Signaling-Game","Rock-Paper-Scissors","Traffic-Routing"],"tags":["Economics/Game-Theory","Economics/Micro-Economics"],"content":"A theory of interaction between rational agents.\n\nInstitutional Design is the field in public policy &amp; PPE that makes sure game theory &amp; agent incentives are taken into account\nIt’s part of economics because it’s about rational agents interacting\n\nGame §\ndef. Game. Assuming n players:\n\nSi​: Strategy space of player i\ns=(s1​,s2​,…,sn​): which strategy combination happened.\nc1​(s),c2​(s),…,cn​(s): associated cost of s happening for each player\n\nTypes of Games §\n\nTiming:\n\nStatic (=Simultaneous)\nDynamic (=Sequential Move)\n\n\nStrategy Formulation:\n\nPure Strategy: deterministic mapping from information set to action set \nMixed Strategy: probabilistic mapping that depends on a Random Variable\n\n\nInformation availability:\n\nComplete Information\nIncomplete Information\n\n\nRepetition:\n\nOne-off,\nFinite-Repetition,\nInfinite-Repetition\n\n\nPayoff structure:\n\nZero-sum: each strategy tuple sums to zero\nPositive/Negative Sum\n\n\n\nEquilibria Types §\n→ See Equilibria in Game Theory\n\nStatic, Pure, One-off→ Nash Equilibrium\nDynamic, Pure → Subgame Perfect Nash Equilibirum\nStatic, Incomplete, (Pure or Mixed) → Baysian Nash Equilbilibrium (BNE)\nDynamic, Incomplete, (Pure or Mixed) → Subgame Perfect Baysian Nash Equilibirum (PBE)\n\nGames Modeled by Game Theory §\n\n\n                  \n                  (DevonThink) Game Theory List of Games for many types of simultaneous game&#039;s payoff matricies.\n                  \n                \n\n\nOligopoly Games\n\nCartels and Collusion\nBertrand Price Competition\nCornout Quantity Competition\n\n\nBattle of the Sexes\nPrisoner’s Dillemma\n\nPublic Good Game\n\n\nSignaling Game\nStag Hunt\nAssurance Game\nChicken Game\nRock Paper Scissors\nTraffic Lights\nTraffic Routing\n\nNotation §\n\nStrategy Tuple of player A is denoted SA=(S1A​,S2A​)\nPayoff to player A given strategy SA by A, SB by player B, etc.\n⇒ is denoted πA(SA,SB…)\n\nAlternatively, Cost is given as: CA​(SA,…)\n\n\nNash Equilibria are Tuples: NE=(S1A​,S2B​)\n\nIn a simultaneous game, all player’s strategy should be specified\nIn a sequential game, the second player (=follower)’s strategy should include the response for all of the first player (=leader)’s moves.\n\nNotation: SPE=(S1A​;S1B​,S2B​)\n\n\n\n\n\nBranches of Game Theory §\n\nExperimental game theory\nEvolutionary game theory—using game theory to explain strategies that affect natural selection\nApplied game theory\n"},"Gamma-Distribution":{"title":"Gamma Distribution","links":["Central-Limit-Theorem"],"tags":["Math/Common-Distributions"],"content":"def. Gamma Distribution. In a Poisson Point Process with intensity λ, let Wi​ be the “wait times” between the i−1 -th and i-th event. let X be the total “wait time” for r events; i.e. X=W1​+⋯+Wr​. Then X is distribued over Gamma:\nX∼Γ(r,λ)fX​(x)={λe−λx(r−1)!(λx)r−1​1​x&gt;0else​FX​(t)=1−k=0∑r−1​e−λtk!(λt)k​E(X)=λr​               SD(X)=λr​​​\n\n\n                  \n                  Tip \n                  \n                \n\nGamma Distribution also follows the Central Limit Theorem. Thus limr→∞​X∼Normal"},"Gender-is-a-performance":{"title":"Gender is a performance","links":["Pride"],"tags":["Philosophy/Queer-Theory"],"content":"Pride"},"Generalized-Likelihood-Ratio-Test":{"title":"Generalized Likelihood Ratio Test","links":[],"tags":["Math/Statistics"],"content":"thm. Generalized Likelihood Ratio Testing (GLT). Comparing Hypotheses:\n\n\nH0​∈Θ0​\n\n\nH1​∈Θ1​\nThe Generalized Likelhood Ratio is:\n\n\nΛ~=supθ∈Θ0​​L(θ)supθ∈Θ0​∪Θ1​​L(θ)​=usuallyL(θ^MLE​)L(θ0​)​\nThe Generalized Likelihood Ratio Test (GLT) is:\nδ:{H0​H1​​else if Λ~&gt;c​\n\n\n                  \n                  \\tilde \\Lambda is often very hard to manipulate. Use Wilk’s phenomenon in order to approximate the cutoff regions.\n                  \n                \n\n\n\nthm. Wilk’s Phenomenon. For Hypotheses\n\n\nH0​∈Θ0​ ← r0​ dimentions\n\n\nH1​∈Θ1​ ← r1​ dimensions\ni.e. H0​⊂H1​ then:\n\n\n\n\n2lnΛd⟶n→∞​​χr1​−r0​2​"},"Gini-Coefficient":{"title":"Gini Coefficient","links":[],"tags":["Economics/Macro-Economics","index"],"content":"measures inequality."},"Git-for-Every-Minimal-Incremental-Feature":{"title":"Git for Every Minimal Incremental Feature","links":["Atomic"],"tags":["Computing"],"content":"Atomic git commits. Commit every time you implement the smallest chunked chage. You should have many commits in a single day."},"Global-Industry-Classification-Standard":{"title":"Global Industry Classification Standard","links":["Commodities","Real-Estate"],"tags":["Economics"],"content":"classified industries into:\n\nEnergy\nMaterials\nIndustrials\nConsumer Discresionary\nConsumer Staples\nHealthcare\nFinancials\nInformation Technology\nCommunication Services\nUtilities\nReal Estate\n"},"Graph":{"title":"Graph","links":["Tree","Directed-Graph","Directed-Acyclical-Graph","Screenshot-2023-10-25-at-21.15.21.png","Screenshot-2023-10-25-at-21.14.48.png"],"tags":["Math","Computing"],"content":"def. Graph. A Graph is defined as:\n\nOrdered pair (V,E) where\n\nV is the set of all vertices\nE={x,y∣x,y∈V and x=y}\n\n\n\nTypes of Graphs §\n\nVariations on:\n\nDirected or Undirected\nConnected or Not Connected\nCyclical or Acyclical\n\n\nCommon types:\n\nTree is a connected undirected acyclical graph\n\nForest is a set of trees\n\n\nDirected Graph\n\nDirected Acyclical Graph\n\n\n\n\n\nProperties of Graphs §\n\nDegree of a vertex v∈V: number of edges that connect to v\nLoop: an edge that connects a vertex to itself\nCycle: a path that starts and ends with the same vertex\n\nRepresentation of Graphs in Memory §\n\nAdjacency Matrix\n\n2D table of all nodes: Store a 1 if the edge between two nodes exists, 0 otherwise. Example\n\n\nAdjacency List\n\nArray of all vertices, which are also linked list that list all reachable neighbors. Example\n\n\n\n\nComplexities for the two types of representations: \n\nPaths §\n\nv1​→v2​→⋯→vk​\nA Simple Path is one that does not repeat verticies\n\nSpecial Variants §\n\nA Metric weighted graph is where the edge weights satisfy the triangle inequality; i.e. the vertices lie on a surface, and the edges are Euler distances of the verteces\n"},"Greedy-Algorithm":{"title":"Greedy Algorithm","links":["Proof-by-contradiction","Interval-Scheduling","Proof-by-Contradiction","Scheduling-Problem","Minimal-Spanning-Tree-Problem"],"tags":["Computing/Algorithms"],"content":"\n\n                  \n                  Be Skeptical of Greedy Algorithms \n                  \n                \n\n\n\nOften for optimization problems (minimize/maximize)\nHard to argue for correctness\n\nUse Proof by contradiction, or the exchange argument\n\n\nWorks well for approximating optimal solutions\n\nWhen the correct algorithm is intractable (O(2n)), then often the greedy solution that is O(nk) is useful (=tractable)\n\n\nGradient descent is a greedy optimization algorithm.\n\nLinear optimization algorithms are “analytical” and correct, but take lots of time.\nGradient descent algorithms (all of ML) is greedy, but not the global optimum solution. They’re “good enough” and “tractable”\n\n\n\nProof of Correctness §\n\nFeasibility: there is an algorithm that gives a solution that obeys the constraints of the problem\nOptimality: the algorithm’s solution is the best possible. Use either:\n\nProof by Contradiction\n\nLet solution by greedy algorithm solution A.\nAssume there is a more optimal solution O∗\nThen derive a contradiction\n\n\nExchange Argument\n\nLet solution by greedy algorithm solution A\nAssume there is a optimal solution O∗\nBy exchanging individual elements which don’t reduce optimality, slowly show A is at least as good as O∗\n\n\nStaying Ahead\n\nAt every stage of the greedy algorithm, show that it is at least as good as the optimal solution\n\n\n\n\n\nExamples\n\nScheduling Problem\nMinimal Spanning Tree Problem\n"},"Greibach-Normal-Form":{"title":"Greibach Normal Form","links":[],"tags":["Computing/Formal-Languages"],"content":""},"Gross-Domestic-Product":{"title":"Gross Domestic Product","links":[],"tags":["Economics/Macro-Economics","index"],"content":"GDP can be calculated by the following three methods:\nGDP §\nDefined equivalently in 3 ways:\nGDP=Y​=f(l,k)=Il​+Ik​+π+tax=C+I+Gdeficit​+NE​​\n\n(1) uses the production function f(l,k) against the whole economy\n(2) uses the Income method\n(3) uses the Output method. Generally, C&gt;G&gt;I&gt;NE.\n\nConsumption is stable in recessions because you need to eat; Investment suffers\nGovernment spending composes around 1/3 in US, 1/2 is EU\nNote that NE−Export−Imports and thus Net Exports can be negative.\n\n\n\n📎 Observee (2) and (3) are same because the macroeconomic circular flow is a closed loop.\n\n\nHistorically, the LR trend of GDP has been growing exponentially since the industrial revolution…\n\n…GDP per capita [←a better measure of well-being] is growing too…\n…Consumption per capita [←an even better measure of well-being] is growing too!\nThis growth seems to be coming from growth in labor income, while capital income is stagnant.\n\n\nShort-run fluctuations in GDP are due to the business cycle.\nSome economic activity is not in GDP:\n\nInformal market (big in some countries; e.g. 1/3 in Brazil)\nHome production; i.e. homemaking\n\n\n\nCalculation Methods §\n\n\nOutput [= production] method\nY​=VA1​+VA2​+⋯=(P1​)+(P2​−P1)+⋯​\n\n\nExpenditure method:\nY=C+I+G+NX\n→ C is only for final sales, not for intermediate goods. Final sales are always to the household.\n\n\nIncome method\nY=IK​+IN​+π+t\n→ Note that firms pass on their taxes to the consumer (HH), or corporate taxes are paid by entrepreneurs who are at the end of the day HHs.\n\n\nGNI [= GNP] §\n\nGNI method\nYD=IKD​+IND​+π+t\n…t if both incomes are pre-tax incomes\nGNP method\nYD=C+I+G+NX+NI\n…where NI is the net domestic income (inflow - outflow)\n\nDeflator &amp; CPI §\n\n\nDeflator is the reduction of nominal GDP in years after the base year due to inflation.\n\n\nDeflator=YRealYNominal​⋅100\n…thus the real gdp for year i is calculated YiR​=Deflatori​YiN​​⋅100\n\nDeflator is 100 in the base year\nDeflator ∝ inflation rate\n\n\n\nCPI is another index to reduce the nominal GDP. Assume the current year is i, base year b\n\n\nCPIcurrent​=∑pbaseqbase∑pcurrentqbase​⋅100=Yof basein current dollars​Yof basein current dollars​​⋅100​\n…thus the real gdp for year i is calculated YiReal​=CPIi​YiNominal​​⋅100"},"Ground-Truth":{"title":"Ground Truth","links":[],"tags":["Math/Statistics"],"content":"the actual true value, or the current idealized value of a r.v."},"Growth-Rate-Calculations":{"title":"Growth Rate Calculations","links":["Malthusian-Growth","Gross-Domestic-Product","Industrial-Revolution"],"tags":["Economics/Macro-Economics"],"content":"\nGrowth of GDP is exponential since the Industrial Revolution\nGrowth Rate is always quoted as the exponential growth rate eg\nGrowth rate itself is mostly constant\n\nYt​=Yt−1​⋅egeg≈Yt−1​Yt​​≈Yt−n−1​Yt−n​​\n\nPredicting growth into the future: Yt−n​⋅en⋅g=Yt​\nAdding growth rates: gx​=gy​+gz​\ne.g. Ynominal=Price Level⋅Yreal\n⇒ gYnominal=gprice level+gYreal\nDividing growth rates: ga​=gb​−gc​\ne.g. GDP per Capita=populationY​\n⇒ gGDP per Cap.=gY−gpopulation\nfunction of growth rate:\nlet A=f(B) then ga​=∂B∂A​⋅f(B)B​⋅gb​\ne.g.2. Y=KαN1−α ⇒ elasticity gY=αgα​+(1−α)gN​\n"},"Hacking-Flash-Apps":{"title":"Hacking Flash Apps","links":[],"tags":["Computing"],"content":"https://github.com/jindrapetrik/jpexs-decompiler is a flash decompiler and ide. Edit flash source &amp; view assets, etc.\nhttps://github.com/ruffle-rs/ruffle is a emulator for flash written in rust. It doesn’t support all of the API yet. If you look at the FAQ there’s a link to a webarchive’s archive of adobe’s flash debugger tools. It’s in google drive.\nSwiftchan is an archive of .swf files (mainly flash games)."},"Hash-Table":{"title":"Hash Table","links":[],"tags":["Computing/Data-Structures"],"content":"Hash table - Wikiwand\n\n\n                  \n                  Tip \n                  \n                \nThe nice thing about hash tables is that most algorithms are O(1) unless hash collisions occur.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithmAverageWorst caseSpaceΘ(n)O(n)SearchΘ(1)O(n)InsertΘ(1)O(n)DeleteΘ(1)O(n)"},"Hashing-Algorithms":{"title":"Hashing Algorithms","links":[],"tags":["Computing/Algorithms"],"content":"def. Hash Function. A function that maps data to a hash table.\n\nData is denoted S⊂U (=Universe)\n\nUniverse can be continuous or discrete\n\n\nHash table T has m slots, thus T=[m]\n\n[m]:={0,…,m−1}, [m]+:={1,…,m−1}\nAccess time is O(1)\n\n\nh(x) is the hash function that takes in data point x\n\nUniform Hash Function §\ndef. Uniformity. A hash function is uniform iff:\n∀i∈[m], P[h(x)=i]=m1​\ne.g. Modular Hash Function.\nh(x)=x mod m\n\nIf x is random, P[h(x)=i]=m1​\n\nUniversal Hash Function §\nalg. Universal Hashing. A hash function is universal iff:\n∀x=y P[h(x)=h(y)]≤m1​\n\nInitialization: Choose a random h from family H\nUse that h(x) for all future hashing needs for that dataset\n⇒ probability of collision is m1​\n\ne.g. Universal Modular Hash Function.\ndef. Linear Congruence Hashing (integer key)\n\nChoose a very large prime number p (bigger than the number of things you need to hash=∣U∣)\nConstruct a hash table of size m\nConstruct a family of hash functions H={ha,b​(x)=(ax+b mod m) mod m}∣a,b&lt;p\n⇒ H is a universal family\n\ndef. Multiply-Shift Binary Hashing (integer key) (SotA)\n\nCollision §\ndef. Collision Probability.\nAlternative Techniques §\nDouble Hashing §\n\n\nConstruction: S→hT→hi​Ti​\nTi​: the secondary hash table\nSi​: set that denotes all the elements hashed to Ti​. Depends on what S, the data, actually is\n\nE[Si​]≈mn​\nFor some slack, we usually make the secondary hash table ∣Ti​∣=O(∣Si​∣2)\n\n\n\nBloom Filters §\n\nFrom a family of hash functions H choose k different functions h1​…hk​\nInitialize boolean array T which has size m\n\n\nInsert(x)\n\ncalculate h1​(x),…hk​(x) and store them into T[h1​(x)]…T[hk​(x)].\nIf there’s alreay a 1 in the table, keep it that way\n\n\nSearch(x)\n\ncalculate h1​(x),…hk​(x)\nIf all of T[h1​(x)]…T[hk​(x)] returns 1, it’s highly likely that it is present.\n\n\n(false positive rate=) Probability that search(x) returns True, even if x∈/S: ≈(1−e−kn/m)k……(1)\n\n\n\n\n(1)……n is not really known. m is limited by memory. So choose k as big as possible\n\nk=ln2⋅nm​ is good ⇒ false positive search(x) is ≈0​.\n\n\nProbability that bit B[j]=0 is (1−m1​)k∣S∣\n"},"Hedonism":{"title":"Hedonism","links":["Drugs-Catalogue","Desire","Functional-Daily-Life"],"tags":["Philosophy"],"content":"\nDrugs Catalogue\nDesire\n\nHedonism is not bad if you can live a normal life.\n\n\n                  \n                  If it&#039;s not life-ruining drugs… \n                  \n                \n"},"Hedonistic-Utilitarianism":{"title":"Hedonistic Utilitarianism","links":[],"tags":["Philosophy/Ethics"],"content":"Morality is Simply a Discussion of Suffering §\nAll of ethics and morality is a discussion of pain and suffering, and who has it."},"Historical-Materialism":{"title":"Historical Materialism","links":["Proletatriat-(Marxism)"],"tags":["Humanities/Marxism"],"content":"Productive Forces, RoP, Historical Progression §\n\n[Capitalism] has pitilessly torn asunder the motley feudal ties that bound man to his “natural superiors”, and has left remaining no other nexus between man and man than naked selfinterest, than callous “cash payment”. […] It has resolved personal worth into exchange value, and in place of the numberless indefeasible chartered freedoms, has set up that single, unconscionable freedom—Free Trade.\n\n\nIt has converted the physician, the lawyer, the priest, the poet, the man of science, into its paid wage labourers.\n\n\nProductive forces are the steady march of technology which drive economic growth through “good ideas” and efficiency.\nRelations of production are the social institutions that facilitate (or hinder) production—the context within which production occurs\n→ Think: patent systems (transforming ideas into property), protection of proptery, chattle slavery\n\n\nDirection of History §\n\nOur epoch, the epoch of the bourgeoisie, possesses, however, this distinct feature: it has simplified class antagonisms. Society as a whole is more and more splitting up into two great hostile camps, into two great classes directly facing each other—Bourgeoisie and Proletatriat (Marxism).\n\nFeudalistic society i.e. guilds + birth-determined rank → Capitalist Society i.e. private property, wage-labor\n→ Productive forces has outgrown capitalism—”specture haunting Europe”\n⇒ Constant change in ideas, innovation, striving for growth, and societal thought, idealology changed to serve its mode of production\n\nAll fixed, fast-frozen relations, with their train of ancient and venerable prejudices and opinions, are swept away, all newformed ones become antiquated before they can ossify. All that is solid melts into air, all that is holy is profaned, and man is at last compelled to face with sober senses his real conditions of life, and his relations with his kind.\n\n⇒ BZ RoP destroys national boundaries, cosmopolitan nature of production as it brings together global supply of goods into global consumer demand (ultimately, the “epidemic of overproduction”)\n\nThe bourgeoisie has through its exploitation of the world market given a cosmopolitan character to production and consumption in every country.\n\n\nIt compels all nations, on pain of extinction, to adopt the bourgeois mode of production; it compels them to introduce what it calls civilisation into their midst, i.e., to become bourgeois themselves. In one word, it creates a world after its own image.\n\n⇒ Ultimately consuming the whole world economy into its RoP\n\nCommunism is the necessary form and the dynamic principle of the immediate future, but communism is not as such the goal of human develop­ meant - the form of human society.9\n\nand communism is thus inevitable:\n\nenough. In order to supersede private property as it actually exists, real communist activity is necessary. History will give rise to such activity, and the movement which we already know in thought to be a self-superseding move­ meant will in reality undergo a very difficult and protracted process.\n\nbecause the brotherhood of man is real\n\nThe brotherhood of man is not a hollow phrase, it is a reality, and the nobility of man shines forth upon us from their work-worn figures.\n\n\n\n[…] in one word, the feudal relations of property became no longer compatible with the already developed productive forces; they became so many fetters. They had to be burst asunder; they were burst asunder.\n\n\na society that has conjured up such gigantic means of production and of exchange, is like the sorcerer who is no longer able to control the powers of the nether world whom he has called up by his spells.\n\n⇒ When PF outgrows RoP, naturally PF wins; i.e. communism is inevitable. Feudalism to Capitalism, Capitalism to Communism\nCurrently we observe this as the epidemic of overproduction:\n\n[…] the epidemic of over-production. Society suddenly finds itself put back into a state of momentary barbarism […] And how does the bourgeoisie get over these crises? On the one hand by enforced destruction of a mass of productive forces; on the other, by the conquest of new markets, and by the more thorough exploitation of the old ones.\n"},"Homogenous-Function":{"title":"Homogenous Function","links":["Monotonic-Transformation"],"tags":["Economics","Math"],"content":"def. Homogenous production function\nf(xz,xK,xN)=xλf(z,K,N)\n\n\nz: Productivity\n\n\nK: Capital\n\n\nN: Labor\ni.e. if the inputs are multiplied by x, the output is multiplied by xλ.\n⇒ We say: ”f is a homogenous function of degree λ”\n\n\nThis implies we have increasing returns to scale\n\n\nMonotonic Transformation of a homogenous function is also homogenous.\n\n"},"Hotelling's-Lemma":{"title":"Hotelling's Lemma","links":[],"tags":["Economics/Micro-Economics"],"content":"∂p∂π(w,r,p)​∂w∂π(w,r,p)​∂r∂π(w,r,p)​​=x(w,r,p)=−l(w,r,p)=−k(w,r,p)​output supplylabor demandcapital demand​​"},"Huffman-Text-Compression-Algorithm":{"title":"Huffman Text Compression Algorithm","links":[],"tags":["Computing/Algorithms"],"content":"The Huffman algorithm is the most efficient single-text compression for text.\nHow Computers Compress Text: Huffman Coding and Huffman Trees - YouTube\n\nProven to be most efficient for single-character compression\nUses a complete binary tree to store data\nIs a optimization problem.\n\nExample of Huffman Tree.\n\nNumber indicates summed frequency\n0: left, 1: right\n\n0010 encodes n\n111 encodes (blank)\n\n\n⇒ The more frequent the letter, the shorter the encoding is\nNo encoding is a prefix of any other tree (each letter is encoded by different number of bits)\n\n"},"Human-Development-Index":{"title":"Human Development Index","links":["United-Nations"],"tags":["Economics","index"],"content":"Published by the United Nations"},"Humanism":{"title":"Humanism","links":[],"tags":["Humanities"],"content":"\nHumanism is a philosophical stance that emphasizes the individual and social potential, and agency of human beings, whom it considers the starting point for serious moral and philosophical inquiry.\nThe meaning of the term “humanism” has changed according to successive intellectual movements that have identified with it. During the Italian Renaissance, ancient works inspired Italian scholars, giving rise to the Renaissance humanism movement. During the Age of Enlightenment, humanistic values were re-enforced by advances in science and technology, giving confidence to humans in their exploration of the world. By the early 20th century, organizations dedicated to humanism flourished in Europe and the United States, and have since expanded worldwide. In the early 21st century, the term generally denotes a focus on human well-being and advocates for human freedom, autonomy, and progress. It views humanity as responsible for the promotion and development of individuals, espouses the equal and inherent dignity of all human beings, and emphasizes a concern for humans in relation to the world.\nStarting in the 20th century, humanist movements are typically non-religious and aligned with secularism. Most frequently, humanism refers to a non-theistic view centered on human agency, and a reliance on science and reason rather than revelation from a supernatural source to understand the world. Humanists tend to advocate for human rights, free speech, progressive policies, and democracy. People with a humanist worldview maintain religion is not a precondition of morality, and object to excessive religious entanglement with education and the state.\nContemporary humanist organizations work under the umbrella of Humanists International. Well-known humanist associations are Humanists UK and the American Humanist Association.\nWikipedia\n\nThe principle of inherent life value\n직업에 귀천은 없다"},"Hypergeometric-Distribution":{"title":"Hypergeometric Distribution","links":[],"tags":["Math/Common-Distributions"],"content":"def. Hypergeometric Distribution. Describes probabilities with successive draws without replacement. In a population of N with K members containing the feature, among n trials the probability of drawing k with the feature is:\nXP(X=k)​∼Hypergeometric[N,K,n]=(kn​)Nn​Kk​(N−K)n−k​​=(nN​)(kK​)(n−kN−K​)​​\ndef. Geometric Distribution. let X s.t. Range(X)=N, where X is the number of trials until the first success, and success probability is p. X is Geometrically p-Distributed:\nX∼Geom(p)P(X=k)=p(1−p)k−1\n\nwhere X is the number of total trials and p is the probability of an event—normally, geometric distributions are used to model the number of successful events before a failure.\ne.g. the number of basketball free throws until a failure\nE(X)=p1​\nSD(X)=p1−p​​\n\n\n\n                  \n                  Info \n                  \n                \n\nY=X−1 is also a geometric distribution; the number of failures before a success.\n\n\n&gt;Y∼Geom(p)&gt;P(Y=k)=p(1−p)k&gt;\ndef. Negative Binomial Distribution. let X describe the number of successes with probability p before r failures. X is a Negative Binomal Distribution where:\nX∼NegBinom(r,p)P(X=k)=(kk+r−1​)(1−p)rpk\n\nE(X)=1−ppr​\nSD(X)=1−ppr​​\n"},"Hypothesis-Testing":{"title":"Hypothesis Testing","links":["Estimator","Likelihood-Ratio-Test","Generalized-Likelihood-Ratio-Test","Student's-t-test","Wilcoxon-Signed-Rank-Test","Wilcoxon-Rank-Sum-Test","Permutation-Test"],"tags":["Math/Statistics"],"content":"def. A Hypothesis test is a criteria to determine between two statements about an unknown parameter of a distribution:\nδ:{H0​H1​​if criteriaif alternative​\nwhere:\n\nH0​ is the Null hypothesis\nH1​ is the Alternative hypothesis\n\ndef. Type I and II Errors, as well as size and power are defined as follows:\n\nType I Error is the probability of a false positive:\nP(Assume H1​ ∣ H0​ is true) → the Size of the test = Level of the test\nType II Error is the probability of a false negative:\nP(Assume H0​ ∣ H1​ is true) → the Power of the test\n\n⇒ Constructing a γ=1−α -level test is to construct one that has a true negative rate of γ [= a false positive rate of α]\nP-Values §\ndef. let X1​,…,Xn​∼iidF(). then the p-value is the minimum α [= false positive rate = size] at which you would adopt H1​.\n\nCommon Hypothesis Tests §\n\nLikelihood Ratio Test\nGeneralized Likelihood Ratio Test\n\nMultiple Hypothesis Testing §\nMotivation. Assume 20 sets of sample data. Then the false positive rate of one sample:\nP(at least one significant result ∣ all is null)=1−P(all is null ∣ all is null)=1−(1−0.05)20≈0.64\nWhat Is the Bonferroni Correction?\nA good explaination of the bonferroni correction.\n⇒ This is too high to be acceptable. This is p-hacking. Thus:\ndef. Bonferroni Correction. In m-tests on a single dataset X1​,…,Xn​, level must be changed to mα​ in order to make a reasonable test.\nMore Types of Tests §\n\nStudent’s t-test\nWilcoxon Signed Rank Test\nWilcoxon Rank Sum Test\nPermutation Test\n"},"Income-Effect-(IE)":{"title":"Income Effect (IE)","links":[],"tags":["Economics/Micro-Economics"],"content":"Assume you consume only pasta and steak. As exogenous income rises, you consume more steak, and less pasta. In this case, steak is a normal good, while pasta is an inferior good.\ndef. The Income effect is a change in consumption only due to income\nAbsolute changes in consumption define normal/quasi-linear/inferior goods\n\n\n                  \n                  ↑ — Normal Good\n                  \n                \nIncome↑…consumption same — Quasi-linear Good\nIncome↑…consumption↓ — Inferior Good\n\n\nOn the other hand, relative changes in consumption define luxury/homothetic/necessary goods.\n+% Income &gt; +% consumption — **Luxury Good\n+%** Income = +% consumption — **Homothetic Good\n+%** Income &lt; +% consumption — **Necessary Good**\n\n\nIncome Elasticity of Demand §\n"},"Income-Statement":{"title":"Income Statement","links":[],"tags":["Economics/Finance"],"content":"Example Income Statement:\n\na Income Statement is a table showing the firm’s income [≈ profit]\n\nCoGS: variable cost\nSG&amp;A: admin cost\nEBITDA = Profits before financial costs\nDepreciation &amp; Amortization: Capital depreciation\nEBIT = Profits after capital depreciation\n"},"Independence-(Math)":{"title":"Independence (Math)","links":["Stat-230"],"tags":["Math/Probability"],"content":"Independence of Events §\nthm. For partition B1​,…,Bn​ of Ω, for all A⊆Ω:\nP(A)=P(AB1​)+P(AB2​)+⋯+P(ABn​)=P(A∣B1​)⋅P(B1​)+⋯+P(A∣Bn​)⋅P(Bn​)\n\ni.e. P(A) is the weighted average of conditional probabilities P(A∣Bi​) with weights P(Bi​).\n\ndef. Independence of Two Events. Two events A, B are independent if\nP(A∣B)=P(A∣BC)⇔P(A∣B)=P(A)\nthm. Necessary and sufficient for independence of two events:\nA,B are independent⇔P(AB)=P(A)⋅P(B)⇔P(AB)=P(B)⋅P(A∣B)\n\n\n                  \n                  Warning \n                  \n                \n\nEvents are independent not when they are related, but instead whether one event influences the probabilities of another.\ndef. Independence of Three Events. Three events A, B, C are independent if both:\n\nThey are pairwise independent\nP(ABC)=P(A)⋅P(B)⋅P(C)\n\nthm. Necessary and sufficient for independence of two events; if both:\n\nP(AB)=P(A)⋅P(B)\nP(C∣AB)=P(C∣ACB)=P(C∣ABC)=P(C∣ACBC) ← all of them have to be equal.\n\nthm. Multiplication Rule for Three Independent Events.\nP(ABC)=P(A)⋅P(B)⋅P(C)\nIndependence of Random Variable §\ndef. Independence. Two Stat 230s X,Y are independent IFF for all pairs of (x,y)\n∀x,y​P(X=x,Y=y)=P(X=x)⋅P(Y=y)⇕∀x,y​P(X=x∣Y=y)=P(X=x)\n\ni.e. the same as it is for events.\n\ndef. For n random variables X1​,…,Xn​ are mutually Independent IFF for all (k1​,…,kn​):\n∀x,y​P(X1​=k1​,...Xn​=kn​)=P(X1​=k1​)⋅⋯⋅P(Xn​=kn​)"},"Indirect-Utility-Function":{"title":"Indirect Utility Function","links":["utility","Constrained-Optimization","Uncompensated-Demand-curve","Homogenous-Function","Roy's-Identity"],"tags":["Economics/Micro-Economics"],"content":"The indirect utility function relates (Prices, Income) directly to utility, assuming the person is utility-maximizing\nUtility Function: Indirect Utility Function: ​u:(x1​,x2​)↦utilityv:(p1​,p2​,I)↦utility​​\nDerivation Process:\n\nUse Constrained Optimization to derive Demand Functions x1​(p1​,p2​,I), x1​(p1​,p2​,I)\nSubstitute these demand functions into the original utility function u(x1​,x2​) to get the indirect utility function.\n\nProperties\n\nHD0 in (Prices,Income) → Check after derivation!\n\nIn other words, inflation in prices and income doesn’t change utility\n\n\nDecreasing in prices (∂p1​∂v​,∂p2​∂v​&lt;0), Increasing in income (∂I∂v​&gt;0)\n\n→ Check after derivation!\n\n\nUse Roy’s Identity to get back to the Marshallian Demand.\n"},"Industrial-Revolution":{"title":"Industrial Revolution","links":[],"tags":["Humanities"],"content":""},"Inflation-vs.-Recession":{"title":"Inflation vs. Recession","links":["Inflation"],"tags":["Economics/Macro-Economics"],"content":"Paul A. Volcker was the fed chair who brought down high Inflation at the cost of a recession in the 1980’s.\n⇒ Generally, central banks will try to bring down inflation first, at the cost of a recession."},"Inflation":{"title":"Inflation","links":[],"tags":["Economics/Macro-Economics","index"],"content":"Inflation:= change in the price of the basket of goods [= a representative sample of GDP]\nI.R.=Pt−1​Pt​−Pt−1​​\n\n\nInterest rate ∝ inflation rate [← explained later]\n\n\nHistorically, price of capital equipment (relative to consumption goods) have gone down\n…but the price of housing have gone up\n→ thus must know inflation of what?\n\nTotal Inflation: CPI measured using the representative basket of goods\nCore Inflation: represents the long run trend in the price level.\n\nIn measuring long run inflation, transitory price changes should be excluded.\n⇒ Exclude items frequently subject to volatile prices, like food and energy\ni.e. the change in Core CPI\n\n\n\n\n\nInflation data is often shown as Year-over-Year (YoY) which means you can’t compound\n\n"},"Initial-Public-Offering":{"title":"Initial Public Offering","links":[],"tags":["Economics/Finance"],"content":""},"Inner-Product":{"title":"Inner Product","links":[],"tags":["Math/Linear-Algebra"],"content":"\nReal and Complex numbers R,C: Arithmetic Muliplication\nReal space Rn: Dot Product\n→ You generalize up to Hilbert Spaces\n\nNOT a\n\nMatrix Product\n"},"Input-Demand-and-Output-Supply":{"title":"Input Demand and Output Supply","links":["Uncompensated-Demand-curve","Profit-Function","Utility-Maximization","Cost-Minimization","Input-Demand-and-Output-Supply","Profit-Maximization-for-Perfect-Competition","Production-Function","production-function"],"tags":["Economics/Micro-Economics"],"content":"Long Run Input Demand §\ndef. Ordinary Input Demand. Ordinary Demand for Inputs (=labor l, capital k)\n\nTo derive: Profit Function\nProperties:\n\nHD0 in (w,r,p)\nDecreasing in own-price ←regardless of anything! (Unlike Utility Maximization)\n\n\n\ndef. Conditional Input Demand. Demand of input (=labor l, capital k) in order to produce a certain level out output xˉ\n\nHOWTO get: Cost Minimization\nProperties\n\nHD0 in input prices w,r\nDecreasing in own-price\n\n\n\nShort Run Input Demand §\nShort run (own-price) labor demand: lkˉ​(w,p∣kˉ)\n\nkˉ is a parameter. Set it as the long-run input demand k(w,r,p).\nIn the short run, a change in w or p will only operate within lkˉ​ with no change in kˉ possible.\nIn the long run, we simply calculate the long-run input demand l(w,r,p).\nRelationship between long-run input demand, visually: \n\nLong Run Output Supply §\ndef. Ordinary Supply. Relates the prices of inputs and output with the quantity of output produced\nx:(w,r,p)↦x\nProperties\n\nHD0 in prices w,r,p\nincreasing in output price x\ndecreasing in input price w,r,\n\n(HowTo) Derive Supply Function §\nMethod 1:\n\nGet Input Demand and Output Supply from Profit Maximization\nSubstitute into the Production Function x=f(l(w,r,p),k(w,r,p))\nSimplify to get x(w,r,p).\nMethod 2:\nGet cost function from Cost Minimization\n\nShort Run Output Supply §\nShort run (own-price) output supply: xkˉ​(w,p∣kˉ)\n\nkˉ is a parameter. Set it to the long-run input demand k(w,r,p)\nUse the production function x=f(l,k) but with kˉ fixed.\nIn the short run, a change in w or p will only operate within xkˉ​ with no change in kˉ possible.\nIn the long run, we simply calculate the long-run output demand l(w,r,p).\n\n"},"Institutional-Design":{"title":"Institutional Design","links":["horizontal-organization","Centralized-Power","Microphysics-of-Power","Utility","Game-Theory"],"tags":["Economics/Game-Theory","Philosophy/Political-Philosophy"],"content":"\nHorizontal vs Vertical organization structure\nCentralized vs Decentralized (Microphysics of-) Microphysics of Power\nIncentives &amp; Payoff structures in Game Theory\n"},"Instruction-Set":{"title":"Instruction Set","links":[],"tags":["Computing/Computer-Architecture"],"content":"Instruction Set: a set of instructions—the “vocabulary”—that a process of certain architecture understands (x86, MIPS, etc.)\nTypes of Instruction Set: ARMv7, ARMv8 (64-bit), MIPS, x86 (32, 64-bit). The concept of the instruction set supports the stored-program concept (idea that instructions and data are both stored in main memory, with a memory heirarchy.)"},"Instructions-(Computer-Science)":{"title":"Instructions (Computer Science)","links":["Instruction-Set","Branching-(Computer-Science)"],"tags":["Computing/Computer-Architecture"],"content":"2.1. Introduction §\nInstruction Set language of the hardware. Vocabulary understood by a given architecture.\n2.2. Operations of the Computer Hardware §\n\nRegister: Places to store variables\n\nAn instruction in MIPS looks like this:\nadd $s1, $s2, $s3 which means\ns1 ← s2 + s3\nNotice that one instruction has three operands. In MIPS, there are 32 registers and 230memory addresses which can be used as operands.\n\nSimplicity favors regularity. It’s easier to design hardware for a fixed number of operands and registers.\n\n2.3. Oper_and_s Of the Computer Hardware §\n\nWord: size of a register (32-bits in a 32-bit architecture)\ni.e. 1 word = 4 bytes = 32 bits &gt; Smaller is Faster. Having only 32 registers is simple; having 3 operands is simple. More registers will lead to slower clock cycles. &gt;\nAlignment Restriction: data in memory must be aligned to multiples of 4.\ni.e. since each memory address refers to one byte, valid memory addresses are 0x04, 0x08, … (each are 1 word in size).\nEndian-ness: big endian refers the leftmost 0x04 referring to 0x04~0x07 and so on. little endian refers to the rightmost 0x04 referring to 0x01~0x04 and so on. Picture: big▶️little\n\nData Transfer Instruction: instruction to move data between RAM and registers. Supply the memory address stored in a register\n// Load Word Example\nlw $t1, 8($s1)\n// access *address* stored in $s1 in RAM\n// move down 8 bytes (offset)\n// and store that in register $t1\n \n// Store Word Example\nsw $t1, 8($s1)\n// the same thing, but storing $t1 into memory addr $s1 offset by *2 words*\nNote that the offset is in bytes, not words. → Thus to access arr[1] you should offset 4($s1). Offset addressing is natural for arrays and structs (from C).\nImmediate Operations: instructions to add a constant to a register value. Immediate operations are very fast, since they don’t need to load data from memory.\naddi $s1, $s2, 4 // &quot;add immediate&quot;\n// store in s1 the sum of value in s2 and 4\n// s1 = s2 + 4;\n2.4. Signed and Unsigned Numbers §\n0000 0000 0000 0000 0000 0000 0000 0000 // 32 bit number = 1 word\n// most significant bit            // least significant bit\nTwo’s Compliment Representation: think of it like this:\n−231−1⋅⋅⋅−1,0,1⋅⋅⋅231\n1000 0000 ... 0000 = -2^31 - 1\n...\n1111 1111 ... 1111 = -1\n// this is where it resets...\n0000 0000 ... 0000 = 0\n0000 0000 ... 0001 = 1\n...\n0111 1111 ... 1111 = 2^31\n// and overflow back to 1000 0000 ... 0000 = -2^31 - 1\nThis representation is the default since 1965.\nAdvantages of Two’s Compliment Representation:\n\nBinary to Decimal: most significant bit is -2^32 and the rest is normal binary\nNegation: invert bits and add one—this works both ways!\nSign Extension(=Precision Extension) (16→32 bits, etc.): extend the sign bit leftward (most significant bit)\ni.e. positive number: extend the zeros; negative number: extend the one.\n\n2.5. Representing Instructions in the Computer §\nRegister Representation:\n#8  ~ #15: $t0 ~ $t7\n#16 ~ #23: $s0 ~ $s7\nInstruction in Machine Code: instructions can be R-type or I-type. Each have different sized-fields (but is 32 bits in total).\n// R type instruction example\nadd $t1, $s1, $s2\n// divided into fields:\nop   | rs  | rt  | rd  |shamt| funct | // field names\n0    | 17  | 18  | 8   |  0  | 32    | // machine code in decimal\n6b   | 5b  |  5b |  5b |  5b | 6b    | // bit size of each field\nEach part of the instruction is called a field. List of Field Names:\n\nop: opcode, the operation\nrs: register source\nrt: register source 2\nrd: register destination (result of the operation)\nshamt: shift amount\nfunct: function code, select a specific variant of the opcode.\n\nLine label. loop in the following instruction is a line label. It’s just there for humans; like comments in code.\nloop: lw [...]\n\t\t\tadd [...]\n\t\t\tj loop\n\n2.6. Logical Operations §\n| Operation | command | example | explaination | comments |\n| ----------- | --------- | ----------------- | ------------------------------ | ----------------------------------------------------------------------------------------- | ------------------------------------------------ |\n| Shift Left | sll | sll t2,s0, 4 | s0to left 4 bits, store in t2 | equivalent to multiply by 2^n |\n| Shift Right | srl | srl t2,s0, 4 | s0to right 4 bits, store in t2 | equivalent to divide by 2^n |\n| AND | and, andi | and t0,t1, $t2 | t0 = t1 &amp; t2 | andi has a constant in place of a second register. |\n| OR | or, ori | or t0,t1, $t2 | t0 = t1 | t2 | orihas a constant in place of a second register. |\n| NOT | nor | nor t0,t1, $t2 | | NOR(A, B) = NOT (A OR B) thus if one operator is 0, then it becomes a simple NOT command. |\n2.7. Instructions for Making Decisions §\n\nBasic block: a block of assembly code without branches\n\nTwo types of jump operations: conditional branches and unconditional jump.\nConditional Branches:\nbeq $t0, $t1, L1 // branch if equal\n// if t0 == t1, goto label L1\nbne $t0, $t1, L1 // branch not equal\n// if t0 != t1, goto label L1\nUnconditional Jump:\nj Exit // go to label Exit\nLess Than (branch on less than is not included since it takes clock cycles)\n/* SIGNED */\nslt $t0, $s1, $s2 // set on less than\n// if s1 &lt; s2, t0 = 1 ; else t0 = 0\nslti $t0, $s1, 10 // set on less than immediate\n// second argument is a constant\n \n/* UNSIGNED */\nsltu $t0, $s1, $s2 // slt, but compare as if two integers are unsigned\nsltiu $t0, $s1, $s2 // slti, but compare as if two integers are unsigned\n\n\n                  \n                  Treating signed numbers as unsigned is useful in checking 0 \\leq x \\lt y. &gt; sltui $t0, $s1, 10 ← if s1 is negative, treating it as an unsigned number the 2^{31}th place has a 1 which makes it a *very big signed number—*and thus t0 is set to 0. If s1is positive then its value is same signed or unsigned; thus s1 &lt; 10 is evaluated normally.\n                  \n                \n\n2.8. Supporing Procedures in Computer Hardware §\nProcedure: =function call in assembly.\n\nLeaf Procedure. A procedure that doesn’t call any other procedures. (think: the end of a branch)\n\nTo execute a procedure you need to:\n\nPut parameters in a place expected by the procedure\nTransfer control to the procedure\nPerform task\nPut result value in expected place (by the caller)\nreturn control to caller\n\nRegisters (usually) used for procedure calling:\n\na0 ~ a3: pass arguments\nv0 ~ v1: return values\nra: return address (return here after procedure is done)\nProgram Counter (PC): holds the memory address of the instruction currently executing\n\nInstruction specialized for procedure calling:\njal Procedure // jump and link\n// an unconditional jump + saves current calling address to $ra\n// used when transferring control to the procedure\njr $ra // jump register\n// an unconditional jump but instead of a Label, jump to the address in register $ra\n// used when returing control to the caller from a procedure\n\nSpilling Registers into the Stack\n\nStack. When a0 ~ a3 isn’t enough for a procedure, s0 ~ s7 should be saved into a memory portion call thed stack. (t0 ~ t9 is not saved)\n\n$sp points to the end of the stack (where spilled data should be stored next)\nplacing and removing data is called push / pop\nBy convention stack grows from higher memory address to lower memory address, e.g. from 0x08 to 0x04, and by one word each.\n\n\nProcedure Frame (=Activation Record). Space on stack reserved for storing the procedure’s local varaibles and arrays that don’t fit into registers. The frame is located below the saved registers at the little end of the stack.\n\n$fp (frame pointer) points to the little end of the procedure frame. This is useful because stack pointer can change during a procedure.\n\n\n\nCalling a leaf procedure:\n\n\nadjust stack pointer by number of registers that requires saving:\naddi $sp, $sp, -12 // stack grows down\nsw $s1, 8($sp) // push first variable\nsw $s2, 4($sp)\nsw $s3, 0($sp)\n\n\nexecute procedure body\n\n\nsave return value to registers v0 ~ v1\n\n\nrestore register values to registers\nlw $s3, 0($sp) // pop last variable\nlw $s2, 4($sp)\nlw $s1, 8($sp)\naddi $sp, $sp, 12 // stack deletes up\n\n\nData preserved/not preserved across procedure calls:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreservedNot Preserveds0 ~ s7t0 ~ t9sp stack pointera0 ~ a3 argument registerra return addressv0 ~ v1 return value registerstack above stack pointer (bigger end)stack below stack pointer (littler end)\n(Not storing the temporary registers t0~t9 reduces memory store/load.)\n\nAllocating Data on the Heap\nStructure of the executable, from big end to little end:\n\nThe Stack grows down (higher address → lower address). sp → 7fff fffc\nThe Heap grows up (lower addr → higher addr).\nText Segment (instructions)\nReserved\n\n2.9. Communicating with People §\nASCII Table on p.106. Notice that:\n\none ascii charater is 1 byte (=8 bits)\n0b0 is NULL\nupper and lower letters differ by 32=25\n\nTo store strings, either:\n\nfirst position in string stores length\naccompanying variable holds string length or\nthe last position marks the end of the string\n\nSinced people use ASCII a lot, and ASCII is 1 byte, there are data transfer instructions for bytes:\nlb $t0, 0($sp) // load byte (and sign extend)\nsb $t0, 4($sp) // store byte and sign extend\nlbu $t0, 0($sp) // load byte unsigned\nsbu $t0, 4($sp) // store byte unsigned\n\nUnicode is used by Java and other modern languages. Notice that:\n\nUTF-16 is default, using 16-bits\nUTF-8 is ASCII-compatible, and is variable-length\nUTF-32 uses 32 bits\n\nMIPS also accomodates UTF-16 by providing data transfer instructions for half-words (16-bits):\nlh $t0, 0($sp) // load halfword and sign extend\nsh $t0, 4($sp) // store halfword and sign extend\nlhu $t0, 0($sp) // load halfword unsigned\nshu $t0, 4($sp) // store halfword unsigned\n2.10. MIPS Addressing for 32-bit Immediates and Addresses §\nWhen you need to load a 32-bit constant into register, you need two instructions (a new one, lui):\n/* loading 0000 0000 0011 1101 0000 1001 0000 0000 */\nlui $s0, 61 // load upper immediate;\n// 61   = 0000 0000 0011 1101 (upper half of constant)\nori $s0, $s0, 2304 // logical OR immediate;\n// 2304 = 0000 1001 0000 0000 (lower half of constant)\n// s0 has the full value.\nAddressing in Jumps and Branches\njump instruction:\nj 10000\n|  2  |             10000                 |\n|  6b |              26b                  |   // can use 26 bits for addressing\n \nbranch instruction:\nbne $s0, $s1, Exit\n|  5   |  16   |  17   |        Exit      |\n| 6b   |  5b   |  5b   |        16b       |   // can only use 16 bits for addressing"},"Integer-Multiplication":{"title":"Integer Multiplication","links":[],"tags":["Computing/Algorithms"],"content":"alg. Long Multiplication. (grade school) algorithm\n\nComplexity: Time O(n2).\n\nalg. Karatsuba Multiplication. Recursive algorithm to compute\n\nIdea: To get x×y…\n\nLet a,b,c,d be digits of x=abˉ,y=cdˉ, and observe:\n\nx=10n/2a+b\ny=10n/2c+d\n\n\nx×y=10nac+10n/2(ad+bc)+bd\n\nRecurse Compute a×c …(1)\nRecurse Compute b×d …(2)\nTo compute the middle term, instead of computing a×d,b×c we do:\n\nRecursively compute (a+b)(c+d)=ac+ad+bc+bd …(3)\n(3) subtract (1) and (2) to get middle term ad+bc\n\n\n\n\n\n\nSee Karatsuba Multiplication in 13 min - YouTube\n\n\nComplexity\n\nTime: O(nlog2​3≈n1.58)\nSpan: T∞​(n)=O(n) (by parallelzing all 3 recursive calls)\n\n\n\nalg. Lattice Multiplication. Developed for longer integer hand-calculation."},"Integration-Rules":{"title":"Integration Rules","links":[],"tags":["Math/Calculus"],"content":""},"Interest-Rate":{"title":"Interest Rate","links":["Present-Value-Calculations","Federal-Funds-Rate"],"tags":["Economics/Finance"],"content":"Rates in General §\ndef. Rate is a simple abstraction of interest rate so that we can calculate things easily.\nR​=F(0)F(τ)−F(0)​=τ⋅r=(1+kr​)τk−1​ (Definition) (...for simple interest) (...for compound interest)​​\ne.g. when banks say they have an APR (Annual Percentage Rate) of 10.99%, their actual rate [=Annual Percentage Yield] is:\nR=(1+36510.99%​)365−1≈11.61%\ndef. Required Rate of Return (RRR) (=Discount Rate).\n\nRRR of asset: Minimum amount of profit that the investor will accept\nRRR in general: Average rate of similar-risk investments in the market\n\ndef. Internal Rate of Return (IRR).\n\nAnnual rate of growth of an investment.\nSolution of the equation NPV(rIRR​)=set0\n\nSee NPV calculations\n\n\n\nInterest Rate §\nEquivalently:\n\nDiscount rate\nPrice of Money\n\nTypes of interest rates:\n\nYield: interest rate on bonds\nMortgage Prime rates\nLIBOR (inter-bank interest rates)\nFederal Funds Rate\n"},"International-Trade":{"title":"International Trade","links":["Balance-of-Payments"],"tags":["Economics/Macro-Economics"],"content":"Balance of Payments"},"Jean-Baudrillard":{"title":"Jean Baudrillard","links":["Postmodernism","Living-in-HyperReality"],"tags":["People","Philosophy"],"content":"The postmodern condition (Postmodernism)\ndef. Simulation\n\nbtw. illusion problem!= simulation problem\nThere is no way out of the simulation\npostmodern age: no longer a distinction between illusion and reality ⇒ this situation is a simulation\n“stranded in a world without referents”\nThe desert of the real (everything scary about postmodernism!)\n“tribute to individuals which is property and function of the simulation”\n“the analyst of simulation, therefore, is subject to the very rule he or she analyzes”\nSimulation attempts to\n\nhypermarket\ndef. Referent\n\na signpost into reality\n\ndef. Simulacra\ndef. Hyperreality. See Living in HyperReality"},"John-Rawls":{"title":"John Rawls","links":[],"tags":["People","Economics/Game-Theory"],"content":"\nJustice as Fairness\nMaximin Principle\nBasic Rights\n"},"John-Stewart-Mill":{"title":"John Stewart Mill","links":["utilitarianism"],"tags":["People"],"content":"utilitarianism"},"John-Stuart-Mill":{"title":"John Stuart Mill","links":[],"tags":["Economics","Philosophy","People"],"content":""},"Joint-Distributions":{"title":"Joint Distributions","links":["Marginal-Distribution","Expected-Value"],"tags":["Math/Probability"],"content":"Discrete Joint Distribution §\ndef. Joint Distributions of two discrete random variables X,Y encode the probabilities for every pair of (x,y) for (X,Y). Following is an example of a joint distribution where X is the result of rolling a first dice and Y the result of a second roll.\n\nREMARK. Joint Distributions are distributions too, which means it has to follow all the rules of distributions (e.g. ∑=1)\nContinuous Joint Distribution §\ndef. Joint Probability Density. Let X,Y be two independent continous random variables. Then the joint probability density function is defined as the derivative of the cumulative density function:\nfX,Y​(x,y):=∂x∂y∂2FX,Y​(x,y)​\nfX,Y​(x,y)ΔxΔy=P((x,y)∈R)∴fX,Y​(x,y)=Δx,Δy→0lim​ΔxΔyP(x,y)∈R​\nAnd thus the following holds:\n\nP((x,y)∈R)=∬R​f(x,y) dA where R is an event\n∬R2​fdA=1\nX⊥Y⇔fX,Y​=fX​⋅fY​\n\n\nThe blue volume in the picture is the probability of the event X and Y are in R.\n\nSee Joint Marginal Distribution\nSee Joint Expected Value\n\nFull Visual Example §\n\n\nBlue is the probability density function, fX,Y​(x,y)={π1​0​(x−1)2+y≤1else​\nRed is the marginal probability density of Y,\n\nf∗Y(y)={∫∗x=1−1−y2​1+1+y2​f_X,Y(x,y)dx0​y∈[−1,1]else​\nMinimum and Maximum Joint Dist §\nthm. Let X1​,…Xn​ be i.i.d.; let P=min(X1​,…,Xn​),Q=max(X1​,…,Xn​). Then:\nfP,Q​(p,q)={n(n−1)⋅fXi​​(p)⋅fXi​​(q)⋅[∫pq​fXi​​(x)dx]n−20​y&lt;zelse​\n\nExamples of Joint Distributions §\n\n\n                  \n                  Tip \n                  \n                \n\nRecall that joint distributions are also distributions [=encapsulate fully the information of an experiement].\nUniform Joint §\nthm. If X,Y are both uniformly distributed over [x1​,x2​],[y1​,y2​]≡Ω, then…\n\nheight of the distribution is ∣Ω∣1​ (where ∣Ω∣ denotes the area of the outcome space.)\nP((x,y)∈R)=∣Ω∣∣R∣​\n\n\nNormal Joint (Linear Combination) §\nthm. Linear Combination of Normal Distributions. If X1​∼N(μ1​,σ1​) and X2​∼N(μ2​,σ2​) then:\n∀a,b∈R,aX1​+bX2​∼N(aμ1​+bμ2​,a2σ12​+b2σ22​)\nNormal Joint (Product) §\nthm. if X∼N(μ1​,σ2) and Y∼N(μ2​,σ2) (i.e. std. dev. is the same) then\n\nfX,Y​=fX​⋅fY​\nVolume of sector θ from (μ1​,μ2​) is 2πθ​\n\n\nRayleigh Distribution §\n[=Squared &amp; Rooted Joint Normal]\ndef. Rayleigh Distribution. let X,Y∼Normal(0,σ) and W=X2+Y2​, then:\nWfR​(r)FR​(r)E(X)=σ2π​​   ​∼Rayleigh(σ)R∼RL(σ)=σ2r​e2⋅σ2−r2​ for r&gt;0=1−e2⋅σ2−r2​ for r&gt;0      SD(X)=σ24−π​​​\n\nWhere σ is the “scaling factor” (standard dev. must be same for X,Y)\nIf σ=1 then W is a Standard Rayleigh distribution:\n\nthm. Standardizing Rayleigh Distributions. If W∼RL(σ):\nσW​∼RL(1)"},"Joseph-Stiglitz":{"title":"Joseph Stiglitz","links":[],"tags":["Economics/Game-Theory","Economics/Finance","People"],"content":"Occupy Wall Street Movement"},"K-Clustering-Problem":{"title":"K-Clustering Problem","links":[],"tags":["Computing/Algorithms"],"content":"Notation §\n\np: a point\nϕ:p→c: which center is assigned to point p\nc1​…ck​: centers\n\nk: number of centers we want\n\n\n\nK-Max §\nQ. K-Max. K-Clustering with cost as maximum distance from center\nalg. K-Max approximation.\n\nChoose any point as one center\nChoose the point most distant from any chosen center points\nRepeat until you have any many centers as you wish\n\n\nTight 2-approximation\n\nUse triangle inequality\n\n\n\nK-Means §\nQ. K-Means. K-Clustering with cost total squared distance (equiv. to cost mean squared distance):\nC=∑∣∣p−ϕ(p)∣∣2\nalg. Lloyd’s Approximation Algorithm. (=K-Means Approximation)\n\nInitialize k centers arbitrarily\nDivide into clusters\nRecompute new centers as the mean point of each cluster\nRepeat as many times as you want\nAnalysis\n\n\nGuaranteed that every iteration will only decrease cost (=will converge)\nConverged centers are not guaranteed to be global optimum\n\nalg. K-Means++ Approximation Algorithm.\n\nInitialize k centers by…\n\nChoose any first center c1​\nChoose randomly another point as the next center, but the probabiliy of choosing point p is proportional to distance between ci​ and p\nRepeat until we have all centers c1​…ck​\n\n\nRun Llyod’s algorithm\nRe-initialize, re-run Llyod for T trials\nAnalysis\n\n\nE(ALG[T=1])=O(5lnk+10)OPT\nALG[T=O(logϵ1​)]=O(lnk)OPT with probability 1−ϵ\n\ni.e. after T=O(logϵ1​) trials…\nthe best trial run will very likely have cost O(lnk)OPT\n\n\n"},"Karl-Marx":{"title":"Karl Marx","links":[],"tags":["People","Philosophy/Political-Philosophy","Philosophy","Economics"],"content":""},"Knapsack-Problem":{"title":"Knapsack Problem","links":["utility","Subset-Sum"],"tags":["Computing/Algorithms"],"content":"def. Knapsack Problem. You have items 0…n−1 with costs c[0…n−1] and utility v[0…n−1] with a budget B. What is the maximum utility you can achieve? Course Description\nIdea: Simple iteration DP\n\nthm. Knapsack is NP-complete\n\nIdea: Reduce from Partition Problem.\n"},"Knuth–Morris–Pratt-Substring-Matching":{"title":"Knuth–Morris–Pratt Substring Matching","links":[],"tags":["Computing/Algorithms"],"content":"You can match text of length m with pattern of length n in O(m+n). (Brute force matching takes O(mn)).\n\nLeetcode Problem: Leetcode 28. Find the Index of the First Occurrence in a String\nExplanation Video: Knuth–Morris–Pratt(KMP) Pattern Matching\n\nGiven two strings needle and haystack, return the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack.\n1. Build Prefix-suffix Table §\ne.g.1:\npattern: A B A B C  \ntable:   0 0 1 2 0\nmeaning: e.g.  |--there is a prefix of length 2\n\ne.g.2:\nindex:   0 1 2 3 4 5 6 7 8 \npattern: a a b a a b a a a\ntable:   0 1 0 1 2 3 4 5 2\nmeaning e.g.         |--there is a prefix of length 4\n\n2. Compare Text with Pattern §\ntext:    a b x a b c a b c a b y\n---------------**********^---------\n\t\t\t\t\t\t |- y != c, search from index 2 \npattern: a b c a b y\nindex    0 1 2 3 4 5\ntable:   0 0 0 1 2 0\n\nTime to build table: O(n)\nTime to search text: O(m)\nTotal Time: O(m+n)"},"LRU-algorithm":{"title":"LRU algorithm","links":["Cache"],"tags":["Logistics","Computing/Algorithms"],"content":"Least Recently Used (LRU) is a cache replacement algorithm\n\nIn computing, cache replacement policies (also frequently called cache replacement algorithms or cache algorithms) are optimizing instructions, or algorithms, that a computer program or a hardware-maintained structure can utilize in order to manage a cache of information stored on the computer. Caching improves performance by keeping recent or often-used data items in memory locations that are faster or computationally cheaper to access than normal memory stores. When the cache is full, the algorithm must choose which items to discard to make room for the new ones.\nWikipedia\n"},"Laffer-Curve":{"title":"Laffer Curve","links":[],"tags":["Economics/Macro-Economics"],"content":"\nGovernment Revenue:\nG=wNSt=w(h−l)t\nObserve:\n∂t∂G​=w(h−l)−tw∂t∂l​\n\nIn economics, the Laffer Curve illustrates a theoretical relationship between rates of taxation and the resulting levels of the government’s tax revenue. The\nThe shape of the curve is a function of taxable income elasticity—i.e., taxable income changes in response to changes in the rate of taxation. As popularized by supply-side economist Arthur Laffer, the curve is typically represented as a graph that starts at 0% tax with zero revenue, rises to a maximum rate of revenue at an intermediate rate of taxation, and then falls again to zero revenue at a 100% tax rate. However, the shape of the curve is uncertain and disputed among economists.One implication of the Laffer curve is that increasing tax rates beyond a certain point is counter-productive for raising further tax revenue. Particularly in the United States, conservatives have used the Laffer curve to argue that lower taxes may increase tax revenue. However, the hypothetical maximum revenue point of the Laffer curve for any given market cannot be observed directly and can only be estimated—such estimates are often controversial. According to The New Palgrave Dictionary of Economics, estimates of revenue-maximizing income tax rates have varied widely, with a mid-range of around 70%.The Laffer curve was popularized in the United States with policymakers following an afternoon meeting with Ford Administration officials Dick Cheney and Donald Rumsfeld in 1974, in which Arthur Laffer reportedly sketched the curve on a napkin to illustrate his argument. The term “Laffer curve” was coined by Jude Wanniski, who was also present at the meeting. The basic concept was not new; Laffer himself notes antecedents in the writings of the 14th-century social philosopher Ibn Khaldun and others.\nWikipedia\n"},"Leverage":{"title":"Leverage","links":[],"tags":["Economics/Finance"],"content":""},"Lifetime-(Programming-Language)":{"title":"Lifetime (Programming Language)","links":[],"tags":["Computing"],"content":"Rust has generic lifetime annotations.\nValidating References with Lifetimes - The Rust Programming Language\nfn main() {\n    let r;                // ---------+-- &#039;a\n                          //          |\n    {                     //          |\n        let x = 5;        // -+-- &#039;b  |\n        r = &amp;x;           //  |       |\n    }                     // -+       |\n                          //          |\n    println!(&quot;r: {}&quot;, r); //          |\n}                         // ---------+\n…this returns a compiler error, because variable x does not live long enough, even though r is a valid value."},"Likelihood-(Statistics)":{"title":"Likelihood (Statistics)","links":["Estimator"],"tags":["Math/Statistics"],"content":"Likelihood:\n\ninstead of asking “given parameter θ what is the probability of r.v. X=x”…\n…ask: “given data about X, what is the likelihood of parameter θ being within an interval?”\n\nThis is the best way to evaluate an estimator.\nLikelihood Function §\ndef. The likelihood function for X1​,…,Xn​∼iidfXi​​(x1​,…,xn​∣θ) is the likelihood that given the data, the liklihood of parameter to be that value:\nL(θ)=fX​(θ;x)Ln​(θ)=fn​(θ;x1​,...,xn​)=iidi=1∏n​fXk​univar​(θ;xk​)\nwhere for the liklihood function, the variable is θ and the parameters are x1​,…,xn​.\n\nThe domain of the likelihood function is the parameter space.\n\n\n\n                  \n                  Info \n                  \n                \n\nAlternatively, understanding L to be the (mythical) value of the pdf is another way to think of it.\n확률(Probability) vs 가능도(Likelihood)\ndef. the Log liklihood is simply the natural log of the likelihood function. It exists because it’s just easy to manipulate.\nl(θ)=log[L(θ)]=log[fXi​​(θ;x)]ln​(θ)=log[Ln​(θ)]=log[i=1∏n​fXk​univar​(θ;xk​)]\nScore §\ndef. the Score is the derivative of the log likelihood. It measures how close the estimator θ^ is to the actual value of θ.\ns(θ)=∂θ∂logL(θ)​\n\nScore is best when 0, and the absolute value measures how far away θ^ is from actual θ. Signed for direction.\nE[s∣θgt​]=0 ← under regularity conditions. Obviously, if we know real θ, then the score is perfect.\n"},"Likelihood-Ratio-Test":{"title":"Likelihood Ratio Test","links":["Likelihood-(Statistics)"],"tags":["Math/Statistics"],"content":"def. let X1​,…,Xn​∼f(;θ). To compare hypotheses:\n\n\nH0​:θ=θ0​\n\n\nH1​:θ=θ1​\nWe define the Likelihood (Statistics) Ratio Λ as:\n\n\nΛ(X1​,...,Xn​)=L(θ0​;X1​,...,Xn​)L(θ1​;X1​,...,Xn​)​\nThen devise the Likelihood Ratio Test (LRT) for cutoff c:\nδ:{H0​H1​​else if Λ&gt;c​\nThe Significance of the LRT is in that it is the most powerful test:\nthm. Neymann—Pearson Lemma. Given level α [=false positive rate = type I error rate], then the LRT is the Uniformly Most Powerful (UMP) test.\nThere is also another significant result for certain likelihood ratio. But first define:\ndef. Monotone Likelihood Ratio (MLR) is when the likelihood ratio Λ is never decreasing for some variable x.\nThen we can determine the UMP of such Λ:\nthm. Karlin—Rubin Theorem. In a distribution with Λ that is MLR for region T(x) [statistic] for a sampling X1​,…,Xn​ in this distribution the UMP is:\nδ:{H0​H1​​else if T&gt;c​"},"Limit-Laws":{"title":"Limit Laws","links":[],"tags":["Math/Calculus"],"content":"L’Hôpital’s Rule §\nx→clim​g(x)f(x)​=x→clim​g′(x)f′(x)​\nInapplicable Scenarios\nSure, here are the limit laws in markdown table format:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLawsDescriptionConstant Lawlimx→a​c=cIdentity Lawlimx→a​x=aSum/Difference Lawlimx→a​[f(x)±g(x)]=limx→a​f(x)±limx→a​g(x)Product Lawlimx→a​[f(x)⋅g(x)]=limx→a​f(x)⋅limx→a​g(x)Quotient Lawlimx→a​g(x)f(x)​=limx→a​g(x)limx→a​f(x)​ (if limx→a​g(x)=0)Exponentiation Lawlimx→a​[f(x)]n=[limx→a​f(x)]nComposition Lawlimx→a​[f(g(x))]=f(limx→a​g(x))Squeeze TheoremIf f(x)≤g(x)≤h(x) for all x in some interval containing a, and limx→a​f(x)=L=limx→a​h(x), then limx→a​g(x)=L.Intermediate Value TheoremIf f(x) is a continuous function on the closed interval [a,b] and C is any number between f(a) and f(b), then there exists at least one number c in the interval (a,b) such that f(c)=C."},"Limits-of-Math-and-Computing":{"title":"Limits of Math and Computing","links":["Algorithm","Turing-Machine","(Book)-Godel-Escher-Bach"],"tags":["Philosophy/Analytic","Math","Computing/Formal-Languages"],"content":"\n\n                  \n                  Church-Turing Thesis \n                  \n                \nA function can be calculated by an Effective Method if and only if it is computable by a Turing Machine.\n\ni.e.\n\nAlgorithms and Turing Machines are the same thing.\nTuring Machines are the most power type of possible automation.\n\n\n\n                  \n                  Warning \n                  \n                \nIt cannot be proven until we know exactly what an ”Effective Method” is. Can humans be described by an effective method? Nobody knows.\n\nSee (Book) Godel Escher Bach for a deep dive."},"Linear-Algebra":{"title":"Linear Algebra","links":["main-diagonal","Matrix-Chain-Multiplication"],"tags":["Math/Linear-Algebra"],"content":"Vector Algebra\n\naTa=∣a∣2 is a scalar. aaT is an n×n matrix.\n∣a∣=a12​+⋯+an2​​\n\nMatrix can be\n\n\nSymmetric\n\nPositive-definite\nPositive-semi-definitie\nNegative-definite\nNegative-semi-definite\n\n\n\nDiagonal: entries outside the main diagonal are all zero\n\nDiagonalizable: A is diagonalizable iff exists invertible matrix P where PAP−1 is a diagonal matrix\n\nInverse of a diagonalizable matrix is also diagonalizable\nOrthogonally diagonalizable: A is orthogonally diagonizable iff exists orthogonal matrix P where PAPT=PAP−1=I\n\n\n\n\n\nInvertible vs Singular\n\nInvertible: There exists an inverse = determinant is non-zero ∣A∣=0\n\nThe inverse of a symmetric matrix is also symmetric\n\n\nSingular: There does not exist an inverse\nAA−1=I\n\n\n\nOrthogonal:A is orthogonal iff A−1=AT\n\n\nRank:= dimension of the vector space generated by its columns\n\n\nIdentity Matrix I\n\n\nDeterminant: scalar value that determines if the matrix has a determinant\n\n\nMatrix Algebra Identities\n\nNon-commutative\n\nCommutative only with scalars\n\n\nDistributive (w.r.t. matrix addition)\nAssociative\n\nComputational complexity depends on which you multipliy first: Matrix Chain Multiplication\n\n\nTranspose-distribution (AbC)T=CTbAT\nInverse-distribution (AB)−1=B−1A−1\nTranspose-Inverse (AT)−1=(A−1)T\n"},"Linear-Programming":{"title":"Linear Programming","links":[],"tags":["Computing/Algorithms"],"content":"Integer Linear Programming §\nExample of a {0,1} ILP problem.\n\n\n{0,1} ILP is NP-complete\nGeneral ILP is also NP-complete\n"},"List-of-Elasticities-and-Rates-of-Substitution":{"title":"List of Elasticities and Rates of Substitution","links":["Price-Elasticity-of-Demand","Elasticity-of-Substitution","Technical-Rate-of-Substitution"],"tags":["Economics"],"content":"\nElasticity of Demand\nElasticity of Substitution\nTechnical Rate of Substitution\n"},"Living-With-the-Internet":{"title":"Living With the Internet","links":["The-Personal-Computer","Attention-is-Currency","Highly-Sensitive-Person-(HSP)","Stream-of-Content","CGP-Grey","Negative-Emotions-are-Not-Helpful","Nudging"],"tags":["Computing/Internet"],"content":"\n\n                  \n                  Abstract \n                  \n                \n\nThe internet is a world you can access with the terminal of your personal computer.\nThe internet was, is, and always will be uncharted territory.\n\n\nPrinciples §\n\n\n                  \n                  Digital Minimalism \n                  \n                \n\n\nCritical Information Consumption\n\nTo navigate the dangers of the web, you need critical thinking – but also critical ignoring\n\n\nAttention is Currency\n\nModern society, wrote Simon, faces a challenge: to learn to “allocate attention efficiently among the overabundance of sources that might consume it.”\n\nAttention is the currency of the modern internet businesses.\nCorporate internet tries to entice you with your attention. (there is a distinction between corporate internet and grassroots internet.)\n\n\nBeware Overstimulation. Give attention to what matters only.\n\nUse the Stream of Content—pick out only what you find nutritious, and ignore the rest.\n\n\nCGP Grey: Thinking about Attention by CGP Grey\n\n\nRefraining from Outrage\n\nCGP Grey: Though Germs by CGP Grey\nNegative Emotions are Not Helpful.\n\n\nSolution: Nudging\n\nBuild in friction to reduce the chances of you giving into attention-sucking content or outrage content\n\n\n"},"Living-in-HyperReality":{"title":"Living in HyperReality","links":["Jean-Baudrillard","Emergent-Phenomena","Constructed-World","Society's-Scripts","Ask-vs.-Guess-Culture","Social-Media","Living-With-the-Internet","The-Personal-Computer","Tools-and-Structures-Define-Your-Capacity","Pride","Gender-is-a-performance"],"tags":["Philosophy/Epistemology"],"content":"We no longer live in a world that is understandable through the natural way of things. (Jean Baudrillard)\n\nThe Emergent Phenomena our societies have constructed, from Gender Roles to Ask vs. Guess Culture, Social Media is far removed from our nature as human beings or our nature of what it means to be a social creature.\n⇒ It’s past the tipping point of being connected to the “real”, thus Baudrillard’s Hyperreality\nThe naturalism argument no longer holds\n“It’s the way it’s always been” no longer holds ground\n\nInstead of invoking hyperreality as something to be afraid of, embrace it.\n\nLiving With the Internet\nThe Personal Computer\nTools and Structures Define Your Capacity\nPride, and Gender is a performance\n"},"Log-Rules":{"title":"Log Rules","links":[],"tags":["Math"],"content":"\nProduct Rule:\n\nlogb​(xy)=logb​x+logb​y\n\n\nQuotient Rule:\n\nlogb​(x/y)=logb​x−logb​y\n\n\nPower Rule:\n\nlogb​xn=n⋅logb​x\n\n\nChange of Base Rule:\n\nlogb​a=logc​blogc​a​\n\n\nIdentity Rule:\n\nlogb​b=1 and logb​1=0\n\n\n"},"Lognormal-Distribution":{"title":"Lognormal Distribution","links":["Normal-Distribution"],"tags":["Math/Common-Distributions"],"content":"def. Lognormal Distribution. A random variable X is a lognormal random variable iff\n\nX=ln(N(μ,σ2)) i.e. log of the Normal Distribution random variable\nPDF:\n\nfX​(x)=xσ2π​1​exp[−2σ2(lnx−μ)2​]\n\nE(X)=eμ+2σ2​\nVar(X)=(eσ2−1)e2μ+σ2\nCDF:\n\nFX​(x)=Φ(σlnx−μ​)\n- Where $\\Phi$ is the Standard [[Normal Distribution]] CDF.\n"},"Longest-Common-Sequence":{"title":"Longest Common Sequence","links":[],"tags":["Computing/Algorithms"],"content":""},"Longest-Palindrome-Algorithm":{"title":"Longest Palindrome Algorithm","links":[],"tags":["Computing/Algorithms"],"content":""},"Longest-Palindromic-Substring":{"title":"Longest Palindromic Substring","links":[],"tags":["Computing/Algorithms"],"content":"def. Longest Palindromic Substring.\ndef. Smallest Palindromic Decomposition. Homework Link"},"Machine-Learning":{"title":"Machine Learning","links":[],"tags":["Computing"],"content":"Machine Learning §\nOverview §\nTopics in:\nNN, CNN, RNN, LSTM, Reinforcement, GAN\nTools and libraries:\npyplotlib, scipy, numpy, tensorflow\nAsk HN: Full-on machine learning for 2020, what are the best resources? | Hacker News\nSince the “How Do I Learn AI/ML” question pops up on Hacker News once a month (m… | Hacker News\n\n\n                  \n                  Honestly, skip all of the courses. Pick a problem to solve, start googling for common models that are used to solve the problem, then go on github, find code that solves that problem or a similar one. Download the code and start working with it, change it, experiment. All of the theory and such is mostly worthless, its too much to learn from scratch and you will probably use very little of it. There is so much ml code on github to learn from, its really the best way. When you encounter a concept you need to understand, google the concept and learn the background info. This will give you a highly applied and intuitive understanding of solving ml problems, but you will have large gaps. Which is fine, unless you are going in for job interviews. \n                  \n                \n\nReferences §\nPapers with Code - The latest in Machine Learning\nTowards Data Science\nMachine Learning Glossary | Google Developers\nLinks &amp; Tutorials §\nForwardpropagation - ML Glossary documentation\nhttps://www.youtube.com/watch?v=NfnWJUyUJYU&amp;list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC\nCS231n Convolutional Neural Networks for Visual Recognition\nMathematics for Machine Learning\nMathematics for Machine Learning\nStanford CS321n\nCS231n Convolutional Neural Networks for Visual Recognition\nGenerative Adversarial Networks §\nUnderstanding Generative Adversarial Networks (GANs)"},"Macroeconomic-Market-Equilibrium":{"title":"Macroeconomic Market Equilibrium","links":[],"tags":["Economics/Macro-Economics"],"content":"General Equilibrium §\n\nMarket Equilibrium is a point that is both optimal and feasible:\n\nOptimality: HHs, firms, govn’t are maximally happy; π, u maximized, government plan is in place\nFeasibility: Budget/Resource Constraints are in action\n\nFeasibility §\n\nLabor Market: NS=ND\nGoods Market: YS=C+G ←“Resource Constraint”\nHH income: C+T=wN+π ←”Consumer Budget Constraint”\nGovernment G=T ←”Government Budget Constraint”\n\n⇒ If the two of three budget constraints are satisfied, the other one is automatically satisfied\n\nDeriving the Equilibrium §\n1. Production Possibility Frontier §\n\n\nPPF indicates current level of technology, levels of govn’t spending\nGradient of the PPF is the Marginal Rate of Transformation (MRT) of the economy; i.e. opportunity cost of transforming between consumption and leisure. This is equivalent to wage.\nGovernment speding is indicated by a downward shift of the PPF\n\n2. Household Optimization §\n\n\nConsider: even if households do not work, they will get π+Net Transfers amount of income\n\n1+2: Market Equilibrium §\n\nAt Point A∗(C∗,L∗):\n\nFirms produce maximally, HHs maximize utility\nIs feasible [=inside PPF, inside HH budget constraint]\nMRS=MRT=−w\n\n\n\n                  \n                  Examples of Disequilibrium \n                  \n                \n→ The left case: MRS=MRT but is infeasible ∵ household indifference curve is outside\n→ The right case: feasible, but household utility is not maximized\n\n\nChanges in Equilibrium §\n\nChanges in Government Spending [= G], normally due to war, or infrastructure projects\n\n\n\n\n                  \n                  Government can crowd out private consumption, because it reduces HH income (income effect): \n                  \n                \n\nTaxes↑ ⇒ HH income ↓ ⇒ YD↓ ⇒ NS↓ ⇒ Y likely increases ∵ Y↑=C↓+G⇑"},"Macroeconomics":{"title":"Macroeconomics","links":["Unemployment","Inflation","Circular-Flow-of-Income","Taxation"],"tags":["Economics/Macro-Economics"],"content":"Macroeconomics studies Aggregate Pheonomena of: GDP / Employment / Investment / Inflation &amp; Money / LR growth / Investment, Govn’t Expenditure, Exports.\n\nBegan formally from the 20th century after the Great Depression; before that it ways just microeconomics. (Keynes &amp; Hayaek)\nStatic vs Dynamic\n\nStatic: comparing at the same point in time\nDynamic: comparing at different points in time\ne.g. Comparative statics, like an exogenous increase in demand causing QS​ to increase\n\n\nEconomic Time in macro covers usually the following timeframes:\n\nShort Run (SR): several years\nMedium Run (MR): 1~2 years\nLong Run (LR): Decades\n\n\nReal vs Nominal Variables.\n\nReal: real things as units (cars, hours, gallons), or base-year currency\nNominal: values in currency at a certain year\n\n\n\nUnemployment\nInflation\nCircular Flow of Income\nTaxation"},"Majority-Voting-Algorithm":{"title":"Majority Voting Algorithm","links":["CS330-HW03"],"tags":["Computing/Algorithms"],"content":"From CS330 HW03\nProblem. Call an array good if more than half of its elements are the same. The domain from which the elements are taken is not necessarily ordered like integers so one cannot make comparisons like “Is A[i] &gt; A[j]?” or sort the array, but one can check whether two elements are the same in O(1) time.\nIdea: a majority element must be majority in either of the two halves of an array.\nAlgorithm:\n# array to check\nA[1..n]\n \nfunction Count(l, r, elem)\n\t# base case\n\tif l = r\n\t\treturn 1\n \n\t# divide array into two, and count there\n\tm = ceiling((l+r)*0.5)\n\tl_count = PCount(l, m, elem)\n\tr_count = PCount(m+1, r, elem)\n \n\t# return the sum of left and right counts\n\treturn l_count + r_count\n \nfunction MajorityElem(l,r)\n\t# base case\n\tif l = r\n\t\treturn A[l] \n \n\t# check left, right half majority\n\tm = ceiling((l+r)*0.5)\n\tleft_majority = MajorityElem(l, m)\n\tright_majority = MajorityElem(m+1, r)\n \n\t## check consensus\n \n\t# check no consensus first; if one is null return the non-null\n\tif left_majority == NULL and right_majority != NULL:\n\t\treturn right_majority\n\tif left_majority != NULL and right_majority == NULL:\n\t\treturn left_majority\n \n\t# there cannot be a both no consensus case\n \n\t# consensus agrees\n\tif left_majority == right_majority:\n\t\treturn left_majority\n\t\n\t# consensus disagrees\n\telse if left_majority != right_majority:\n \n\t\t# count the majority\n\t\tleft_count = Count(l, r, left_majority)\n\t\tright_count = Count(l, r, right_majority)\n\t\t\n\t\t# if tied, no consensus\n\t\tif left_count == right_count\n\t\t\treturn NULL\n\t\t\n\t\t# winner exists, return the winner\n\t\treturn left_count &gt; right_count ? left_majority : right_majority\n\t\t\n\nCount function: T(n)≤2T(2n​)+O(1)=O(n)\nMajorityElement function: T(n)≤2T(2n​)+2⋅O(n)\n\nFirst term: recursive calls\nSecond term: calls Count\nSolution: T(n)=O(nlogn) (same recurrence relation as mergesort)\n\n\nCorrectness (Brief argument):\n\nBase case: in n=1 the element is majority element\nIH: Assume MajorityElement(k/2) is correct.\nIS:\n\nIf one of them has no majority but the other ones does, the one with majority must be the true majority because of an element exists in more than half of the total array it must be that in one of the halves it will be the majority.\nIf both halves have a majority, then the one that has more counts in the total array wins\nThere cannot be a both halves no-majority case as explained in (1)\n\n\n\n\n"},"Malthusian-Growth":{"title":"Malthusian Growth","links":[],"tags":["Economics/Macro-Economics"],"content":""},"Map-of-Microeconomic-Optimization":{"title":"Map of Microeconomic Optimization","links":["Utility-Maximization","Uncompensated-Demand-curve","Indirect-Utility-Function","Roy's-Identity","Expenditure-Minimization","Marginal-Willingness-to-Pay","Expenditure-Function","Shepard's-Lemma","Profit-Maximization","Input-Demand-and-Output-Supply","HowTo---Profit-Maximization-for-Perfect-Competition-for-Perfect-Competition","Hotelling's-Lemma","Cost-Minimization","Cost-Function"],"tags":["Economics/Micro-Economics"],"content":"Utility Maximization (Household) §\n\n\nUtility Maximization\n\nUncompensated Demand Function (p1​,p2​,I)↦x1​\nIndirect Utility Function (p1​,p2​,I)↦u\nRoy’s Identity\n\n\nExpenditure Minimization\n\nCompensated Demand Function (p1​,p2​,u)↦x1​\nExpenditure Function (p1​,p2​,u)↦I\nShepard’s Lemma\n\n\nInvert between Indirect Utility Function and Expenditure Function by solving for I or u depending on what you want.\n\nProfit Maximization (Firm) §\n\n\nProfit Maximization maxl,k​ π=px−wl−rk  such that x=f(l,k)\n\nOrdinary Input Demand and Output Supply (w,r,p)↦l,k\nOutput Input Demand and Output Supply (w,r,p)↦x\nProfit Function (w,r,p)↦π\n\nHotelling’s Lemma back to input demand\n\n\n\n\nCost Minimization minl,k​ c=wl+rk such that x=f(l,k)\n\nConditional Input Demand and Output Supply (w,r,xˉ)↦l,k\nConditional Cost Function (w,r,xˉ)↦C\n\nShepard’s Lemma back to conditional input demand\n\n\n\n\nmaxx​px−C(w,r,xˉ) for cost function to output supply\nsubstitute Input Demand and Output Supply into Conditional Input Demand to get Ordinary Input Demand and Output Supply\n"},"Marginal-Distribution":{"title":"Marginal Distribution","links":["Joint-Distributions"],"tags":["Math/Probability"],"content":"Discrete Marginal Distribution §\ndef. Marginal Distributions are used when you only have the joint distribution available, and you want to get the distribution of one of the variables. They’re called that because you can write them in the margins.\n\nJoint Marginal Distribution §\ndef. Marginal Distribution of Continuous Joint Distributions. Let X,Y and given fX,Y​. Then PDF of X is:\nfX​(xˉ)=∫y=−∞∞​fX,Y​(xˉ,y)dy\n…and vice versa for fY​.\n\nThe red area is the marginal probability density at x."},"Marginal-Rate-of-Substitution-(MRS)":{"title":"Marginal Rate of Substitution (MRS)","links":["Utility-Function","Rationality-(Economics)"],"tags":["Economics/Micro-Economics"],"content":"def. Marginal Rate of Substitution. MRS1,2​ asks “How much of x2​ can you give me for one unit of x1​?”\n\nMRS1,2​ means MRS of good x1​ for good x2​, i.e. “one unit of x1​ for how many units of x2​?&quot;&quot;\n\nMRS1,2​:=−dx1​dx2​​=−∂x2​∂u​∂x1​∂u​​\n\n\n                  \n                  x_{1},x_{2} changes spots in the nominator/denominator in the final formula.\n                  \n                \n\n\n\nThe tangent of Utility Function at a certain point is the MRS at that specific point (more accurately, its absolute value of the tangent)\nConvexity Assumption causes MRS to decrease (=Diminishing MRS) rightward (the more of good x1​ we consume, the more we would like x2).\nWhen two people have different MRS, then they can trade for both of their utility.\n\ne.g. MRSA​=−3, MRSB​=−21​\n⇒ they can trade at any rate where MRSA​&lt;rate&lt;MRSB​ to both of their benefit.\n\n\n\nDeriving the MRS Formula §\nWe’re looking for a place on the scalar field provided by u(x1​,x2​) where the gradient is zero.\n⇒ Thus we need to find a point where the following is true:\n∇u(x1​,x2​)=δx1​δu​dx1​+δx2​δu​dx2​=0\nBy the definition of MRS it’s the gradient of a indifference curve, i.e. the gradient of a level curve in the field:\nMRS=−dx1​dx2​​=−∂u/∂x2​∂u/∂x1​​\nChanges in MRS §\n\nMRS can change due to exogenous factors; in this case the whole indifference curve will change accordingly too.\ne.g. After 9.11., air travel is perceived by consumers to be less safe.\n\n\n\nObserve that air travel becoming less safe, causes the MRS to decrease, as one is willing to trade less money for flying by air.\nYou can think of this from a different perspective by instead of thinking of air safety as an exogenous factor, actually quantifying air safety as a measure as well, and graphing it together.\nGraphing air safety as the third axis shows that we can in fact visualize the decrease in MRS of air miles and $‘s of other consumption.\n\n"},"Marginal-Willingness-to-Pay":{"title":"Marginal Willingness to Pay","links":["CGP-grey","Consumer-Surplus","Expenditure-Minimization","Utility-Function","HD"],"tags":["Economics/Micro-Economics"],"content":"\n\n                  \n                  Abstract \n                  \n                \nBasically, “In a vacuum, how much do you want it?”\n\nThink CGP grey’s “I’d pay anything for Apple to make X.”\nUsed to calculate Consumer Surplus\nDoesn’t really show up in actual reality. A theoretical construct to help calculate some things\n\n\nMarginal Willingness to Pay (MWTP) Curve is derived from a single Indifference curve’s MRS along different consumption quantities—when the y-axis is measured in $ of other goods.\nDerivation. Expenditure Minimization of the Utility Function\nProperties.\n\nDecreasing in own price\nHD0 in prices (h1​(tp1​,tp2​,uˉ)=h1​(p1​,p2​,uˉ))\n\nIn other words, if the ratio of prices are the same, your behavior won’t change (but you may need more income).\n\n\n\n⇒ The area under the MWTP curve is closely related to Consumer Surplus.\n→ Think of it as the amount a consumer is willing to pay per unit of good—when they want it more, they’ll obviously willing to pay less for even more of it, so the MWTP slopes down.\n\nThis is sometimes called the Compensated Demand Curve, since you can think of it in terms of:\n\nA change in price (blue to red)\nCompensating your budget by a certain amount so you’re not worse off than before (red to green)\nGraphing the optimal points along quantity consumed and price\n\nMWTP curves can be same or different to own-price demand curves (OPDC). This is because\n\n\nMWTP accounts only for the substitution effect, while OPDC accounts for both the substitution effect and the income effect\nMWTP is along one indifference curve—a fixed utility—, while OPDC doesn’t care about utility.\n\n"},"Market-Beta":{"title":"Market Beta","links":["CAPM-Model"],"tags":["Economics/Finance"],"content":"Market Beta (β)\n\nVariables\n\nSecurity i with (Ri,​,σi​,μi​)\nMarket (RM​,σM​,μM​)\n⇒ Market Beta measures the degree to which the security price correlates with the market price\n\nMarket beta is a constant, derived from past data\n\n\nMarket’s beta is 1.\n\n\nDefinition: βi​:=σM2​Cov(Ri​,RM​)​=ρi,M​⋅σM​σi​​\nβi​=μM​−μf​μi​−μf​​ or equivalently μi​−μf​=β(μM​−μf​) ← ”CAPM Model”\n\nUnder assumptions V is positive definite, μ​,e are linearly independent and 0≤μf​&lt;max(μ​)\nβ&lt;0: security return is opposite of market\nβ=0: security is uncorrelated to market\n0&lt;β&lt;1: security return moves same as market, but fluctuates less (defensive)\n1&lt;β: security return moves same as market, but fluctuates more (aggressive)\n\n\n\n\n\n                  \n                  Security Market Line \n                  \n                \nPlotting μi​−μf​=β(μM​−μf​) with return vs market beta:\n\n"},"Market-Power":{"title":"Market Power","links":["Monopolistic-Competition"],"tags":["Economics/Micro-Economics"],"content":"Monopolistic Competition"},"Markov-Inequality":{"title":"Markov Inequality","links":[],"tags":["Math/Statistics"],"content":"thm. Markov Inequality. For a non-negative random variable X, the following hols for all positive constant a:\nP(X≥a)≤aE(X)​"},"Martha-Nussbaum":{"title":"Martha Nussbaum","links":["cosmopolitan"],"tags":["People"],"content":"\nAuthor of Cultivating Humanity, book about what a cosmopolitan university education system would look like\nAuthor of article (DevonThink) Patriotism and Cosmopolitanism - Boston Review\n"},"Math-581-Mathematical-Finance":{"title":"Math 581 Mathematical Finance","links":["Value-of-Money","Present-Value-Calculations","Future-Value-Calculations","Interest-Rate","Banker's-Rule","Annuity","Perpetuity","Bond-Price","Portfolio-Theory","Dividend-Discount-Model","CAPM-Model","Market-Beta","Measuring-Security-Performance","Binomial-Tree-Model-of-Security-Pricing","Stochastic-Process","Stochastic-Calculus","Options-(Finance)","No-Arbitrage","Black-Scholes-Merton-Derivative-Pricing-Formula","581-Exam-Topics"],"tags":["Courses"],"content":"\nTime Value of Money\n\nPresent Value Calculations\nFuture Value Calculations\nInterest Rate\nBanker’s Rule\n\n\nAmortizing Securities\n\nAnnuity\nPerpetuity\nBond Price\n\n\nPortfolio Theory\n\nDividend Discount Model\nCAPM Model\n\nMarket Beta\n\n\nMeasuring Security Performance\n\n\nBinomial Tree Model of Security Pricing\n\nLog Normal Model of Security Pricing\n\n\nStochastic Process &amp; Stochastic Calculus\nOptions (Finance)\n\nNo-Arbitrage\nBlack-Scholes-Merton Derivative Pricing Formula\n\n\n581 Exam Topics\n"},"Math-582-Financial-Derivatives":{"title":"Math 582 Financial Derivatives","links":["Probability","Expected-Value","Binomial-Tree-Model-of-Security-Pricing","Stochastic-Process","Stochastic-Calculus","Quadratic-Variation","Forwards","Futures","No-Arbitrage","Options-(Finance)"],"tags":["Courses"],"content":"\n\n                  \n                  Young man, in mathematics you don&#039;t understand things. You just get used to them.&quot; \n                  \n                \n\n\nProbability\n\nConditional Expected Value\n\n\nBinomial Tree Model of Security Pricing\nStochastic Process\n\nStochastic Calculus\nQuadratic Variation\n\n\nForwards\n\nFutures\n\n\nNo-Arbitrage (=Law of One Price)\n\n\n\nPayoff: the gross outcome of an investment or trade. The amount you earned from that trade, regardless of commissions, extraneous costs, etc.\nProfits: the gross outcome of an investment or trade, including commissions, extraneous cost\n\n→ the distinction happens in Options (Finance). Payoffs don’t consider the option premium, while the profits do.\n\n\nReturn: short for “rate of return”. The percentage of profit (not payoff!) per original investment. Quoted in percentage (%) or log-returns.\n"},"Mathematical-Induction":{"title":"Mathematical Induction","links":[],"tags":["Math"],"content":"Weak Induction [=Normal Induction] §\n\nBase case\n\nP(1) is true \n\nInductive Hypothesis (IH)\n\nAssume P(k) is true\n\nInductive Step\n\nP(k) is true ⇒P(k+1) is true\nStrong Induction §\n\nBase cases\n\nP(1)…P(n) is true\n\nInductive Hypothesis (IH)\n\nAssume P(k) is true\n\nInductive Step\n\nP(k) is true ⇒P(k+1) is true"},"Mathematical-Proof":{"title":"Mathematical Proof","links":["Mathematical-Induction","Proof-by-contradiction"],"tags":["Math"],"content":"Types of proofs:\n\nMathematical Induction\nProof by contradiction\nContrapositive Proof: prove p⟹q by proving ¬q⟹¬p\n"},"Matrix-Chain-Multiplication":{"title":"Matrix Chain Multiplication","links":["Dynamic-Programming"],"tags":["Computing/Algorithms"],"content":"Matrix Chain Multiplication. Given matricies A0​,…,An−1​ with dimensions (m0​×m1​),(m1​×m2​),…,(mn−1​×mn​), which order should we multiply the matricies in order to minimize the number of scalar multiplications? Description from course\nIdea:\n\nSearch every subproblem but with Dynamic Programming.\nfor every chain of matricies, get the split with minimum multiplication required.\nIterate for increasing gap (=chain length)\n\n\n"},"Matrix-Multiplication":{"title":"Matrix Multiplication","links":["Parallel-Algorithms","Inner-Product","tags/processors"],"tags":["Computing/Algorithms","processors"],"content":"Traditional (Textbook) algorithm is O(n3)\nWe have gotten it down to ≈O(n2.37)\nalg. PInnerProduct. Use PSum from Parallel Algorithmss. Calculates Inner Product of two vectors\n\nSpan T∞​(n)=O(logn)\n#processors p=O(lognn​)\nWork: W∞​(n)=O(n)\n\nalg. Matrix Multiplication.\n\nT(n)=O(n3)\n\ndef MatMult(A, B)\n\tfor i=1 to n\n\t\tfor j=1 to n\n \n\t\t\t# calculate inner product\n\t\t\tc_ij = 0\n\t\t\tfor k=1 to n\n\t\t\t\tc_ij += a_ik * b_jk\n\t\t\tend for\n\t\tend for\n\tend for\nalg. Parallel Matrix Multiplication.\ndef PMatMult(A, B)\n\tparallel for i=1 to n\n\t\tparallel for j=1 to n\n\t\t\tc_ij = 0\n\t\t\tfor k=1 to n\n\t\t\t\tc_ij += a_ik * b_jk\n\t\t\tend for\n\t\tend for\n\tend for\nalg. Parallel Recursive Matrix Multiplication. A Divide-and-Conquer algorithm.\nIdea:\n\nC=(C11​C21​​C12​C22​​),A=(A11​A21​​A12​A22​​),B=(B11​B21​​B12​B22​​),\n(C11​C21​​C12​C22​​)=(A11​A21​​A12​A22​​)(B11​B21​​B12​B22​​)=(A11​B11​+A12​B21​A21​B11​+A22​B21​​A11​B12​+A12​B22​A21​B12​+A22​B22​​)\ndef PRMatMult(A, B, C)\n \n\t# base case\n\tif n=1\n\t\treturn a * b\n\t\t\n\t# parallel recursion\n\tspawn PRMatMult(A_11, B_11, D_11)\n\t...# 8 parallel threads\n\tspawn PRMatMult(A_22, B_22, E_22)\n\tsync\n \n\t# add matricies D and E together\n\tparallel for i=1 to n\n\t\tparallel for j=1 to n\n\t\t\tc_ij = d_ij + e_ij\n\nSpan T∞​(n)≤T∞​(2n​)+O(1)=O(logn)\nWork W∞​(n)≤8⋅W∞​(2n​)+O(n2)=O(n3)\n"},"Maxima-of-Point-Set-Algorithm":{"title":"Maxima of Point Set Algorithm","links":[],"tags":["Computing/Algorithms"],"content":"Maxima of a point set - Wikiwand\nProblem. Let P=(x1,y1),(x2,y2),…,(xn,yn) be a set of n points in the two-dimensional Euclidean plane. A point (xi,yi) dominates another point (xj,yj) if xi&gt;xj and yi≥yj or xi≥xj and yi&gt;yj (that is, if it is up and to the right). A point is maximal if it is not dominated by any other point.\nIdea:\n# A[(int, int)] is the structure of the array\n \nfunction FindMaximal()\n\tmax_y = -Infinity\n\t\n\t\n\t# Sort the points in decreasing order by x-coordinate\n\tReverseMergeSort(A[])\n\tmax_y = A[0]\n\tresult = [A[0]]\n\t\n\t# traverse in reverse x direction (right to left)\n\tfor p in A\n\t\t# if y is bigger than anything seen before it&#039;s maximal\n\t\tif p.y &gt; max_y then\n\t\t\tresult.append(p)\n\t\t\tmax_y = p.y\n\t\t\t\n\treturn result\n\nReverseMergeSort O(nlogn)\nFindMaximal T(n)=O(n)+O(nlogn)=O(nlogn)\nCorrectness (Brief argument):\n\nThe rightmost element must be the maximal element\nIf the current element has a higher y value than any other element on the right side of it, it must be a maximum\nIf the current element has a lower y value than another point on the right of it, it is dominated by that point and thus cannot be a maximum\n\n\n"},"Maximum-Flow-Problem":{"title":"Maximum Flow Problem","links":["Directed-Graph","Depth-First-Search","Shortest-Path"],"tags":["Computing/Algorithms"],"content":"Q. Maximum Flow Problem. Given a Directed Graph whose edges are labeled with flow capacity, what is the maximum flow that can be pushed from vertex s to t?\nQ. Minimum Cut Problem. Given a Directed Graph whose edges are labeled with flow capacity, and a source and target vertex s,t, how do you partition the graph into A,Aˉ such that the flow capacity from A→Aˉ is minimized?\n⇒ The two questions give the same answer. i.e…\n\nthe flow rate between A→Aˉ is the maximum flow rate of the graph\nFor the edges cut by the min-cut…\n\nall forward edges operate at full capacity. (The sum of forward edges is the maximum flow capacity of the whole graph.)\nall backward edges operate at zero capacity\n\n\n\nalg. Ford-Fulkerson Flow Maximization. (FFF-Max) Worked Example\n\nIdea: Choose some suboptimal flow, then increamentally find a better solution.\n\n\nChoose some suboptimal flow through the graph.\nConstruct a residual network.\n\nFor edges who have residual capacity, keep those edges but labeled with that residual capacity.\nFor every edge, construct a reversed edge with the used capacity (this is useful later)\n\n\nIf there is still a path from s→t in the residual network, then more flow can be pushed.\n\nThis path is called the ”flow augmenting path”\nThe minimum capacity of any edge in the augmenting path is the ”bottleneck” of the path.\n! Choice of the augmenting path is important. Discussed below.\nRewire the original graph so that…\n\nforward edges have their flow decreased by the bottleneck amount\nbackward edges—decrease flow of the forward edge represented by that backward edge by that bottleneck in the original graph\n\n\n\n\nRepeat calculation until there is no path s→t in the residual graph.\n\n\nalg. Minimum Cut. How to remove the minimum number of edges such that there is no flow from source s to sync t?\n\nRun the Flow Maximization algorithm\nConstruct a residual graph of the final max-flow graph.\nFrom the source vertex, perform a search (DFS/BFS) and mark all reachable edges.\n⇒ The edges between reachable and unreachable verticies is the min-cut edges.\n\n\n\nValue of a cut is the sum of forward edges (=edges in the direction reachable → unreachable)\n\nIn case of a min-cut one needs to minimize this value\n\n\nValue of Min-cut v is equal to the maximum flow f thru the graph\n\n(⇒) Given a min cut s→t and its value v, maximum flow thru s→t is f≥v because backward edges (15 is graph above) is\n\n\nA min-cut may not be unique\n\nTo find all of them, residual → run reachability → traverse cut edges to the nodes (may be multiple!)→ run reachability\n\n\n\nalg. Choice of Augmenting Path. There are three common options:\n\n! Assumptions:\n\nAll capacities are integers\nf∗ is the ideal flow rate\n\n\n\n\nNaive Method\n\nFinding any path using BFS takes O(E) time (fully connected graph)\nIteration count: O(f∗)\nTotal: O(Ef∗)\n\n\nAugmenting path with largest bottleneck.\n\nCan use modified Shortest Path algorithm with O(ElogV) time complexity\nIteration count: O(Elogf∗)\nTotal: O(E2logf∗)\n\n\nBFS shortest augmenting path (Edmond-Karp)\n\nTakes O(E)\nIteration count: O(VE)\nTotal: O(VE2)\n\n\nState of the art: O(VElogV), and even O(E+ϵ)\n\nDiscussion §\nFormalization §\n\nthe answer is given as a flow function f:E↦R&gt;0.\n\ni.e. “assign each edge a flow value”\n\n\nall vertices except the source and target vertices s,t must have net zero divergence (=net zero inflow/outflow)\n\ni.e. “the water can’t disappear or appear randomly in any vertex.”\n\n\n\nApplied Problems §\n\n\n                  \n                  k things with a separate set of l things, suspect flow/cut problem.\n                  \n                \n\nQ. Edge Matching Problem. (=Maximum Cardinality Matching) Given an undirected, partitioned graph, return the maximum number of edges that don’t share a vertex\n\nIdea: reduce to a max-flow problem. \nalgorithm:\n\nFor a graph partitioned A,B, make all edges A→B directed.\nConstruct start and end node s,t, and construct s→a∈A, t←b∈B\nCalculate the max-flow from s→t\nThe edges that are used in the max-flow problem is the solution.\n\n\nComplexity: O(VE)\n\nQ. Advertiser and Viewer Demographic Matching Problem\nQ."},"Maximum-Likelihood-Estimator":{"title":"Maximum Likelihood Estimator","links":["Likelihood-(Statistics)"],"tags":["Math/Statistics"],"content":"def. The Maximum Likelihood (Statistics) Estimator is an estimator θ^ that maximizes L.\n\nIt also works for log likelihood, because the natural log is a monotonic function:\n\nθ^MLE​​=θmax​ Ln​(θ;x1​,...,xn​)=θmax​ lnLn​(θ;x1​,...,xn​)​\nUnder certain regularity conditions, we can find the MLE by finding stationary points in the log likelihood. These are called the likelihood equation:\n∂θ∂lnLn​(θ)​=set to0\nTo consider whether this stationary point is the maximum (as opposed to a miminum) either:\n\ntake the second derivative…\n…or find out via other means\n\n\n\n                  \n                  Properties of MLEs: \n                  \n                \n\n\nMLEs are always a function of a sufficient statistic.\nMLEs are not necessarily unbiased.\nMLEs may not reach the CRLB in variance.\n\nthm. Functional Equivariance of MLE: Given parameter θ and let α=g(θ). Then:\ng(θ^MLE​)=g invertibleα^MLE​\n⇒ the estimator for any function over the parameter can then be found easily.\nthm. Asymptotic Normality of MLE. [=Fisher’s Approximation]\nlet data generated by a univariate single parameter distribution X1​,…,Xn​∼iidf(x1​,…,xn​,θ).\nlet also that θ^MLE​ is found by the likelihood equation ∂θ∂s​=0. Then both are equivalently true:\nθ^MLE​⟶n→∞​N(θ0​,I(θ0​)1​)\nn​(θ^MLE​−θ0​)⟶n→∞​N(0,I(θ0​)1​)"},"Mean-Squared-Error":{"title":"Mean Squared Error","links":[],"tags":["Math/Statistics"],"content":"def. Loss function is a function that encapsulates the “closeness” of the estimate and ground truth. For example a loss function may measured the geometric closeness by:\nL(θ,e)=(θ−a)2\ndef. Mean Squared Error (MSE) is the arithmetic mean of the loss function defined:\nMSE(θ^)​=EX​[(θ^−θ)2]=∫x=−∞∞​(θ^−θ)2fX∣θ=θ^​(x)​\n\n\n                  \n                  Info \n                  \n                \n\nThe following shows why MSE is useful; it is just a sum of variance and bias squared.\n\n\n&gt;EX​[(θ^−θ)2]=Var[θ^]+Bias[θ^]2&gt;"},"Measuring-Security-Performance":{"title":"Measuring Security Performance","links":["Risk-(Finance)","Measuring-Security-Performance","Market-Beta"],"tags":["Economics/Finance"],"content":"Measurment Metrics. The following are used commonly to measure performance of funds. It does not imply, however, that they are meaningful, useful, or correct. Most of them compare risk to return\n\nMeasuring Security PerformanceMeasuring Security Performanceio]]\nJenson’s Alpha. Measures performance against CAPM’s Predictions.\n\nα=(ri​−rf​)−β(rm​−rf​)\nSharpe Ratio §\ndef. Sharpe Ratio. The sharpe ratio for portfolio with return Rp​\nSP​=σp​E[Rp​−μf​]​=σp​μp​−μf​​​​\n\nUsed to compare securities given their risk &amp; returns\nMeasures the “risk-normalized return”\nHigher means a better risk/return profile\n\nTreinor’s Ration §\nMeasures against Systematic (=market) Risk.\nTM=βE[Excess return]​\nReminder: Market Beta is about how correlated the asset is to the market."},"Memory-Access-Control":{"title":"Memory Access Control","links":[],"tags":["Computing"],"content":"\nRead\n\nConcurrent Read\nExclusive Read\n\n\nWrite\n\nConcurrent Write\nExclusive Write\n\n\n"},"Michel-Foucault":{"title":"Michel Foucault","links":["Postmodernism","Centralized-Power","Checks-and-Balances","Dialectical-Synthesis","Social-Institution"],"tags":["Philosophy","People"],"content":"Notes from What Power Is — Michel Foucault\nBasics\n\n20C thinker\nPostmodernism\n\nMisconceptions of Power\n\nHierarchical power is only a part\n\n→ instead, power is an oceanic force of nature\n\n\nTheoretical vs Empirical\n\nFocault talks both about theoretical and empirical power\nEmperical: sexuality, prison systems, etc. (historical crystallizations)\nTheoretical: generalized, abstract conception, a “theory of power”\n\n\nPrior political analyses of power: misconception\n\ne.g. 1984, Leviathan, Marx’s class relations\ncharacterized as the struggle between powerful vs not powerful\nmost power here is negative power (punishes)\noriginates probably from the western legalistic tradition\n→ but majority of power is not this (misrepresentation)\n\n\n\nImmanent “microphysics” of power\n\ninvisible, but real and measurable\npower is a quality of the social world (not metaphysical, natural force like Nietzche)\nintentional &amp; non-subjective (e.g. party DJ putting on “Killing me softly”)\n\nintentional: you are technically free.\nnon-subjective: people will find you uncool\n\n\n\nResistance\n\nResistance is not exterior to power, but is intrinsic\nrevolutionaries → if there is no resistance, there is no power\nrelations of power is only allowed in a free society\n\n“free” in that you’re technically free to resist\n\n\n\nFull Definition of Power\n\n“Power must be understood in the first instance as the multiplicity of force relation imminent in the sphere in which they operate”\ne.g. dressing for school\n\nlegal, etc. (traditional)\nschool clique\n“what’s cool”\n\n\n⇒ They all operate simultaneously\n\nRelations of Power\n\nLocal force relations act on each other (even in the absence of the individual to act on)\n\ne.g. war. Discipline &amp; punish: perpetual battle\n\n\nstruggle, strengthen, weaken. etc\nThey can also bond &amp; combine\n\ne.g. fashion + peers, school + parents kinda match\n\n\n\nInstitutional Crystallization (social)\n\n“tactics” of power\nlocal micro-physics of power → coalesce to create school &amp; state, bigger power\nnobody individually can predict the effects of micro-power\ne.g. legal, agreement, new forms of government\n"},"Microeconomic-Market-Equilibrium":{"title":"Microeconomic Market Equilibrium","links":["Profit-Function","Utility-Maximization"],"tags":["Economics/Micro-Economics"],"content":"Perfect Competition §\nConditions:\n\n\nZero profit condition: π=px−c(x)=0\n\n\nPricing at marginal cost:\n\n! only when MC is constant…\np=MC\n\n\n\nTheory of the Firm but with…\n\nFixed costs, i.e. π=px−wl−rk−FC\nFirms will enter and exit\n\n⇒ supply price and quantity is at π=0\n\n\n→ market price and individual firm’s quantity supplied are determined by this formula.\nThis is equivalent to the minimum average cost condition minx​AC(x).\n\n\n\nUtility Maximization but…\n\nConsider only one good, x1​\nThere are more than one consumers. x1market demand​=x1​×# of consumers\n\n\n\nMarket equilibrium: \n\nPrice is set at p∗ because it is fully determined by the firm.\n\nThe quantity supplied changes by firms entering and exiting (not by individual firms ramping up or cutting down on production!)\n\n\nQuantity demanded (=aggregate quantity supplied) of x∗ is fully determined by the consumer.\n\n\n\nShort Run §"},"Microeconomics":{"title":"Microeconomics","links":["Philosophy,-Political-Science,-Economics","Decentralization"],"tags":["Economics/Micro-Economics"],"content":"\nCourse process.\nEconomics as a Science §\nEconomics is a science as it creates hypotheses and conducts experiments if they are correct.\n\nSince you can’t often create tests IRL, econometrics deals with interpreting real-world data as a substitute for an experiment\nExperimental economics is a small discipline where you actually conduct experiments\n\n(This is not as scientific as it seems. See Philosophy, Political Science, Economics)\nBuilding Blocks of Microeconomics §\nAssume: People are rational in their pursuit of perceived self-interests. Such people are called economic agents.\n→ …thus people respond to incentives\n→ …then as a consequence of multiple economic agents interacting, there will emerge social consequences.\nEconomic Modeling §\nModeling in economics is a process of distilling the essence of a real-world phenomenon, in order to make predictions about the real world.\n\n\n                  \n                  Model → Optimization → Equilibrium \n                  \n                \n\n\nModel: form a model that describes an economic phenomena (make assumptions, Ceteris Paribus)\nOptimization: consider how economic agents will interact within such assumptions &amp; models. Use math if necessary.\nEquilibrium: predict what will ultimately happen in such a model? What is the ending point? (make predictions)\n\n(Economically, an equilibrium (=Pareto Equilibrium) is a point where no agent can be better off without making another agent worse off)\n\n\n                  \n                  doesn’t need to have realistic assumptions. It just needs to predict reality really well.\n                  \n                \n\n→ Consider: A good pool player doesn’t need to know kinematics to score well. Similarly an economic model doesn’t need realistic assumptions if it can realistically predict real-world economic phenomena.\nOther Key Points §\n\nThe world isn’t a zero sum game. Adam Smith: think of purchasing a piece of bread at 2.Yougetthebread,andyouarebetteroff.Thebreadmakervaluesthebreadatlessthan2, so they’re better off.\nAttribution Bias. People’s actions are due more to the sum of the incentives acting on them, rather than their inherent quality. Don’t attribute it to their personality.\nDecentralization. The rules of the economic game shapes a bottom-up spontaneous order that shapes the world and rations limited resources.\n"},"Microphysics-of-Power":{"title":"Microphysics of Power","links":["Michel-Foucault"],"tags":["Philosophy/Political-Philosophy"],"content":"Michel Foucault"},"Minimal-Spanning-Tree-Problem":{"title":"Minimal Spanning Tree Problem","links":["Graph","Tree","Priority-Queue","Disjoint-Set","Greedy-Algorithm","Shortest-Path"],"tags":["Computing/Data-Structures","Computing/Algorithms"],"content":"Q. Minimum Spanning Tree. From a connected, undirected, edge-weighted Graph, make a subset graph that:\n\nconnects all vertices together\nminimum possible total edge weight\n⇒ will always be a Tree\n→ there could be multiple spanning tree (MST is not unique)\n\nalg. Prim’s Algorithm\n\nIdea: Gradually build one big tree\n\nChoose random vertex, then add all edges connected to it to a Priority Queue.\nUse the lightest edge in the priority queue and connect to that vertex.\n\n\n\nalg. Kruskal’s Algorithm.\n\nIdea: Build many trees, and gradually combine them.\n\nChoose lightest edge, then create a tree with that (using a Disjoint Set)\nIf the vertices are already part of a small tree…\n\nif they’re different trees, combine the two trees with that edge\nif they’re same trees, don’t connect.\n\n\n\n\nTime complexity: O(ElogV)\n\nlem. Exchange Argument. Correctness proof for both Prim’s and Kruskal’s algorithm.\n\nIdea: If we have MST, choosing the lightest edge to connect them (greedily) is MST.\n\nAssume MST T∗.\nSplit into two components C1​,C2​.\nAmong edges that connect C1​,C2​ you choose the lightest edge e∗.\n⇒ Then, C1​,C2​,e is a minimum spanning tree.\n\n\n\nDiscussion §\n\nPrim’s and Kruska’s algorithms are both Greedy Algorithms\nIf edge weights are distinct, there is a unique minimum spanning tree\nDoesn’t have anything to do with Shortest Path algorithms.\n\nApplications §\nQ. Minimal Edge Removal to Acyclic Graph. Given a connected, undirected, weighted graph G=V,E we will remove edges F such that the remaining graph will have no cycles. What is F with the minimum total weight sum?\n\nCompute maximum spanning tree on G\n\nthis can be done by modifying Prim’s or Kruskal’s algorithm…\n2….or by running minimum spanning tree on a new graph with negated edge weights\n\n\nF is the set of edges not contained by this MST\n"},"Moment-(Probability)":{"title":"Moment (Probability)","links":["Distribution-(Math)"],"tags":["Math/Statistics"],"content":"→ An alternative way to get an estimator quick and dirty.\nMethod of Moments §\nalg. Method of Moments (MOM):\nlet X1​,…,Xn​∼iidf(x;θ1​,…,θp​). Then to get estimators for θ1​,…,θp​:\n\nlet μi​=E[Xi]=gi​(θ1​,…,θn​)\ngather data on Xˉi to get an emperical estimate mi​\n\nObserve that because E[Xi]≈Xˉi, this means that μi​≈mi​\n\nlet function gi​ which maps θ1​,…,θn​↦gi​​mi​ can be inverted\nGet system of equations for as many i’s as necessary\nSolve the system of equations as θ^i​=gi−1​(m1​,…)\n\n\n\n                  \n                  Limitations on MoM: \n                  \n                \n\n\nEstimators may not make sense (negative numbers, complex number, etc.)\nIllegal\n\ndef. Moment Generating Functions\n(alternative specification of pdf. Makes it easy to calculate things.)\nMoment generating functions are defined as such:\nMX​(t)=E[exp(tX)]\n…where MX​ is differentiable k-times around zero to be able to generate the k-th moment.\nYou can build an MGF from a pmf of pdf:\n\nfor pmfs: MX​(t)=∑​etxi​p(xi​)\nfor pdfs: MX​(t)=∫−∞∞​etxf(x)dx\n\nIt can generate the k-th moment like such:\nE(Xk)=∂xk∂k​MX​(0)=MX(k)​(0)\n\n\n                  \n                  MGFs are unique to each distribution. If an RV has the same MGF, they are distributed identically. \n                  \n                \n\nthm. Linear Combination of MGFs. Given r.v. W=X1​,…,Xn​, and Xi​ are iid,:\nMW​(t)=MX1​​⋅MX2​​⋯MXn​​"},"Monetary-Policy":{"title":"Monetary Policy","links":[],"tags":["Economics/Macro-Economics"],"content":""},"MongoDB-Reference":{"title":"MongoDB Reference","links":["Regular-Expressions"],"tags":["Computing/Data-Science"],"content":"All You Need is… §\n{json}db.bib.find({ title:/[dD]atabase/, price:{$lt:50} })\n\nRegex enclosed in /regex/\nthe argument for find is\nSyntax error: NoSQL queries will return none. SQL will return error\nMongoDB (NoSQL!)\n\nSchema: Document⊂Collection⊂Database\n\ndocument is a single json object (with _id as unique identifier in collection)\n\n\n\n\n\nIf You want more… §\n\n{json}mydb.mycollection.find() ← all documents\nSelection in {js}find(…)\n\n{js}find({ title: &quot;databases&quot; }) ← accepts JSON.\nstring pattern matching is in Regular Expressions. {js}title: /[dD]atabase/\nmultiple patterns: and operation by default.\n\n→ but JSON must have unique keys. {price:…, price:…} doesn’t work (silent error)\nalternative: $and: […, …]\n\n\nwhen there arrays: an ∃ operation.\n{js}$elemMatch: { title: /Section/ } ← performs match per array element\n{js}&quot;arr.0&quot;: &quot;match&quot; ← index matching for array\nbuilt-in functions\n\n{$lt:…}\n{$and:[…, …]}\n\n\n\n\nProjection in {js}find(…, { _id: false, attr1: true, …})\nSort by {js}find().sort({ ISBN: 1 })\n\n1 is ascending (0 → ∞, a → z)\n-1 is descending\n\n\n\nMisc Facts §\n\nQuery strings are valid JSON objects\nprint it to console:{js}printjson(db.collections.find().toArray())\n\nEven More: Aggregation Pipeline §\n\nEach step of the pipeline transforms the json object in some way.\nAggregation steps can include:\n\nSelection: {js}$match\nProject: {js}$project\nSort: {js}$sort\n{js}$addFields\n{js}$unwind\n{js}$lookup\n{js}$group\n\n\n\n"},"Monopoly":{"title":"Monopoly","links":["Market-Power","Uncompensated-Demand-curve","Cost-Function","Unconstrained-Maximization","Price-Elasticity-of-Demand","Pareto-Efficiency","Compensating-and-Equivalent-Variation"],"tags":["Economics/Micro-Economics"],"content":"Definitions §\n\nMonopoly is when a single firm has all the Market Power. Monopolies normally have:\n\nHigh barriers to entry\nProduction in elastic portion of demand\nSingle firm with market power\n(usually) economies of scale\n\n\nA Natural Monopoly is when it’s hard for firms to achieve break-even without a monopoly market strucutre\n\nMarket Demand for Monopolies §\n\n\n                  \n                  MR &gt; 0, i.e. where ε &gt; 1\n                  \n                \n\n\nMonopolies face the whole market demand curve.\nMarginal Revenue (MR) curve will slope down with the same intercept and twice the gradient of the Demand curve.\nVisual Profit Maximization when MC=MR (See below for math)\n\nFirm produces xM (=monopoly quantity)\nMarket price settles at pM (=monopoly price)\nFirm makes profit of c+d\nDeadweight loss is e\n\nDWL happens because monopolist increase price more than optimum\n\n\n\n\n\n\nProfit Maximization for Monopolies §\nProfit maximization for monopolies:\nmaxx​p(x)⋅x−C(w,r,x)\n\np(x) is the inverse of the Ordinary Demand of goods\nC(x) is the Cost Function that is derived w.r.t x\n\n! Not simply wl+rk!\n\n\nTo solve, use Unconstrained Maximization\n\nFirst Order Condition: ∂x∂π​=0\n\nThis is the same condition as MR=MC condition.\n! In third-degree price discrimination, MR=MC can only be used if MC is constant.\n\n\nWhen elasticity is constant ⇒ Use the Elasticity condition: p(x)=1+ϵd​1​MC​\n\nElastic: p&gt;MC thus excess profit\nUnit elastic: p=MC thus zero profit\nInelastic: p&lt;MC thus negative profit (monopolist doesn’t produce)\n\n\n\n\n\n\nPrice Discrimination for Monopolies §\nFirst Degree Price Discrimmination §\nEverybody pays exactly how much they’re willing to pay.\n\n\nDemand curve is same as marginal revenue curve\nPareto efficient because there is no deadweight loss; everybody pays exactly as they want to pay, and no extra profit is lost\n\n→ but the benefit goes to the monopolist only\n\n\n\nTwo-part Tariff §\n\n\nConsumers are made to pay amount A to enter the market\nThen charge consumers p=MC\nSame as Equivalent Variation\n\nProcess of enforcing the tariff\n\nConsumers at utility u1​ˉ​ and only buys good x2​\nFirm enters the market and starts selling good x1​ at price p=MC. The consumer gains utility u2​ˉ​\nFirm realizes they could get more profit. They charge amount A to enter the market. Thus the consumer loses amount of difference.\nConsumers are back to u1​, but they’re still buying x1​ because they’re just as happy as when they only bought x2​.\n\nYou can get market ticket price by either\n\nusing Equivalent Variation on the right graph\nusing integration on the left graph\n\nThird Degree Price Discrimination §\n\nIdea: Charge differently based on consumer characteristics (=based on Price Elasticity of Demand)\n⇒ Split consumers into two groups A,B with elasticity ϵDA​,ϵDB​ and…\n\nIf MC is constant → use the elasticity condition\nIf MC is not constant → use normal profit maximization:\nmaxxA​,xB​​π=pA(xA)⋅xA+pB(xB)⋅xB​Total Revenue​−C(xA+xB)\n\n\nYou will eventually get MRA=MC and MRB=MC\n"},"Monotonic-Transformation":{"title":"Monotonic Transformation","links":["Utility-Function"],"tags":["Math","Economics"],"content":"def. Monotonic Transformation. A transformation of a Utility Function is one that preserves the order of preferences of the original function. i.e., f is a monotomic transformation of utility function u(x) iff:\n∀(xA,xB), if u(xA)&gt;u(xB)⟹f∘u(xA)&gt;f∘u(xB)\nSimple monotonic transformations:\n\nAdding or subtracting a constant\nMultiplying or dividing by a positive constant\nFor always positive functions, exponentiation to a constant\nFor always positive functions, logarithm to a constant base\n\nthm. monotonic transformation equivalence. If one utility function is a monotonic transformation of another, you may treat it for all practical purposes as the same utility function."},"Moore’s-law":{"title":"Moore’s law","links":[],"tags":["Computing/Computer-Architecture"],"content":"\n\nMoore’s law is the observation that the number of transistors in an integrated circuit (IC) doubles about every two years. Moore’s law is an observation and projection of a historical trend. Rather than a law of physics, it is an empirical relationship linked to gains from experience in production.\nThe observation is named after Gordon Moore, the co-founder of Fairchild Semiconductor and Intel (and former CEO of the latter), who in 1965 posited a doubling every year in the number of components per integrated circuit, and projected this rate of growth would continue for at least another decade. In 1975, looking forward to the next decade, he revised the forecast to doubling every two years, a compound annual growth rate (CAGR) of 41%. While Moore did not use empirical evidence in forecasting that the historical trend would continue, his prediction held since 1975 and has since become known as a “law”.\nMoore’s prediction has been used in the semiconductor industry to guide long-term planning and to set targets for research and development, thus functioning to some extent as a self-fulfilling prophecy. Advancements in digital electronics, such as the reduction in quality-adjusted microprocessor prices, the increase in memory capacity (RAM and flash), the improvement of sensors, and even the number and size of pixels in digital cameras, are strongly linked to Moore’s law. These ongoing changes in digital electronics have been a driving force of technological and social change, productivity, and economic growth.\nIndustry experts have not reached a consensus on exactly when Moore’s law will cease to apply. Microprocessor architects report that semiconductor advancement has slowed industry-wide since around 2010, slightly below the pace predicted by Moore’s law. In September 2022 Nvidia CEO Jensen Huang considered Moore’s law dead, while Intel CEO Pat Gelsinger was of the opposite view.\nWikipedia\n"},"Moral-Hazard":{"title":"Moral Hazard","links":[],"tags":["Economics/Game-Theory"],"content":"e.g. Insurance. People with insurance tend to take more risks because they know they’re insured."},"Multinomial-Distribution":{"title":"Multinomial Distribution","links":["Binomial-Distribution"],"tags":["Math/Common-Distributions"],"content":"def. Multinomial Distribution. For outcome space Ω, k mutually exclusive events each with probabilities p1​,…,pk​ s.t. ∑i=1k​pi​=1, let random variables Xi​ denote the number of outcomes whose probability is pi​, when n trials are done:\nX1​,...,Xk​∼Multinomial(n,k,p1​,...,pk​)P(X1​=x1​,...,Xk​=xk​)=x1​!,...,xk​!n!​⋅p1x1​​×⋯×pkxk​​\n⇒ Multinomial is a generalization of the Binomial Distribution."},"Multivariate-Ordinary-Least-Squares-Regression":{"title":"Multivariate Ordinary Least Squares Regression","links":["Omitted-Variables","Bivariate-Ordinary-Least-Squares-Regression"],"tags":["Math/Statistics"],"content":"Yi​=β0​+β1​X1i​+β2​X2i​+β3​X3i​+ϵi​\n\n\n                  \n                  Omitted Variables in the regression. Refer to the document in that case.\n                  \n                \n\nVariance of Parameter Estimators §\nIn this multivariate model the variance of β1​^​ is:\nVar(β1​^​)=N⋅Var(X1​)⋅(1−R12​)σ^2​\nwhere\n\nσ^2 is the variance of the regression\nR1​ is the coefficient of determination in the auxiliary regression X1i​=γ0​+γ1​X2i​+γ2​X3i​+ϵi​. It measures multicolinearity\n\n&amp; It measures how much of X1​ can be explained by X2​,X3​\nIn perfect multicolinearity R1​=1 then X2​,X3​ will perfectly determine X1​, thus X1​ is not relevant anymore.\n&amp; Equivalently R2​ is the coefficient of determination from X2i​=γ0​+γ1​X1i​+γ2​X3i​+εi​, etc. etc.\n\n\n1−R12​1​ is known as the variance inflation factor. The higher the multicollinearity, the more inflated is the variance of parameter estimators.\nMulticollinearity is not a problem iff:\n\nit occurs only between control variables\nVar(β1​^​) is small enough; i.e. β1​^​ is statistically significant.\n\n\nN is the sample size\n\nCoefficient of Determination §\nIn multivariate situations (as opposed to bivariate) the coefficient of determination of the whole regression (R2) is more troublesome because\n\nAdding more variables will only increase R2\nThus one is incentivized to increase the number of (possibly irrelevant) independent variables.\nTo mitigate this, we report the adjusted R2 values instead, with a penalty for each additional independent variables used.\n\nHypothesis Testing regarding Coefficients §\nStandardizing Coeffiefficients §\nExample. In the model GDP=β0​+β1​Life Expectancy+β2​Literacy Rate, if we want to compare the effects of life expectancy and literacy, we cannot simply compare the values of β1​,β2​. This is because their units are different, i.e. β1​ is in YearDollars​ and literacy is in Percentage PointDollars​. Thus we need to standardize them:\nβistd​^​=Var(βi​^​)​βi​^​−E(βi​^​)​\nwhich shows “how much Y increase in units of σY​ does one unit σXi​​ increase in X cause?”\nRemark. Only X1​ and X2​ need be standardized to compare β1​^​ and β2​^​’s effects. Y need not be standardized.\nHypothesis Testing about Coefficients §\nLet the model Yi​=β0​+β1​X1,i​+β2​X2,i​+β3​X3,i​+ϵi​. Sometimes we may want to check if β1​^​=?β2​^​, or β1​^​=β2​^​=?0. In these cases we use a hypothesis test. Let Runrestricted2​ be the R2 of this regression. Now, before we do anything we need to…\n\n\n                  \n                  X_{1},X_{2}.\n                  \n                \n\nCase 1: H0​:β1​^​=β2​^​=0. Then the model under null would change to:\nYi​=β0​+β3​X3,i​+ϵi​\nWe run regression on this new model and get Rrestricted2​\nRemark. This is not equivalent to running a t-test on HA​:β1​^​=0∧^β2​=0 because X1​,X2​ may be multicollinear.\nCase 2: H0​:β1​^​=β2​^​. Then the model under null would change to:\nYi​=β0​+β1​(X1,i​+X2,i​)+β3​X3,i​\nWe run regression on this to also get Rrestricted2​.\nWe can observe that in both cases, Runrestricted2​&gt;Rrestricted2​, always, because “restricting” the model will lead only to less (coefficient of-) determination. Now, the bigger this difference is, the more likely that the null is false. We formalize this using the F-test:\ndef. F-Test. For the F-statistic defined as:\nFq,N−k​:=(1−Runres2​)/(N−k)(Runres.2​−Rrestr.2​)/q​\nwhere\n\nq is “how many equal signs in null hypothesis”\nk is the degrees of freedom (=number of coefficients in the _un_restricted model)\nThen:\n\n{H0​H1​​elseif F&gt;K​\nwhere K is the critical value. The critical values are:\n\nK=3.00 in case 1 (H0​:β1​^​=β2​^​=0)\nK=3.84 in case 2 (H0​:β1​^​=β2​^​)\n"},"Murakami-Haruki":{"title":"Murakami Haruki","links":[],"tags":["People"],"content":""},"Nash-Equilibrium":{"title":"Nash Equilibrium","links":[],"tags":["Economics/Game-Theory"],"content":"def. Nash Equilibrium is a set of strategies (one for each player) which is the best response strategy of each other’s move; i.e. you can’t deviate without destabilizing the equilibrium\nalg. Finding the Nash Equilibrium in a payoff matrix.\nCorner Method to find NE in payoff matrix.\n1. For each player:\n2. For each other player’s move; highlight the line of the best response\n→ When both the left and top lines are highlighted that is NE.\nPure Strategy Nash Equilibirum §\ndef. PSNE. s=(s1​,s2​) is PSNE for two players 1,2 iff:\ns1​s1​​=argmins∈S1​​c1​(s,s1​)=argmins∈S2​​c2​(s1​,s)​​\nHow to Find NE §\n\nIn Simultaenous Games: Use the Corner method\nIn Sequential Games: Make the Decision Tree into a Payoff Matrix, and use the Corner method:\n\ne.g. Driving left or Right\n\n\nSubgame Perfect Nash Equilibrium (SPNE) §\ndef. A Non-credible threat is when the follower in a sequential move game says: “If I can’t win, I’ll take you down no matter how much it costs to me.”\n\nIn the above game, (R,R) is an example of a non-credible threat because when Player 1 chooses L, Player 2’s best response is L—but Player 2 threatens to go R. This is because they want the best possible outcome for them, R,(R,R)\n\ndef. Subgame Perfect Nash Equilibria (SPNE) are NE where the follwer will only choose strategies that are best for them, and can’t threaten the leader beforehand with non-credible threats.\nTo find the SPNE of a game, use Backwards Induction:\n\nDetermine the last player’s best strategy\nThe second-last player knows what the last player will do. Then determine what the second-last player will do.\nContinue solving until the first player.\n\n\nSPNE={(d,d),(d,d)}, payoff is (1,0) which is a lot worse off that global optimal."},"Nimf-Anthy-installation":{"title":"Nimf-Anthy installation","links":[],"tags":["Computing"],"content":"Nimf-Anthy Installation §\nHow do I restore the default repositories?\n하모니 공지 - 하모니카 저장소 공개키 서명 문제 해결방법\n하모니카(HamoniKR) 하모니 - apt update시 인증서 문제\nInstallation\nsoftware-properties-gtk # open the gui repsitories \ncurl -sL https://apt.hamonikr.org/setup_hamonikr.jin | sudo -E bash - # add the Hamonikr repos to apt sources\nsudo apt install ca-certificates # update certificate authority files\nsudo apt install update # update apt cache\nsudo add-apt-repository ppa:hodong/nimf # add nimf dev&#039;s repo to apt source\n# and the logout and back in.\nThe hamonikr repo and ppa:hodong/nimf has different versions names. The binaries are probably the same but apt will complain about dependency versions. Installing the hodong/nimf repo will resolve the dependency issue."},"No-Arbitrage":{"title":"No-Arbitrage","links":[],"tags":["Economics/Finance"],"content":"def. Arbitrage Portfolio. Portfolio with value Π(t) at time t is an arbitrage portfolio if:\nΠ(t)=0⟹ price doesn’t go down P(Π(T)≥0)=1​​ and  price guarateed to rise P(Π(T)&gt;0)&gt;0​​\nAlternatively we can define it as:\n if started negative... Π(t)&lt;0​​⟹ ...will be non-negative P(Π(T)≥0)=1​​\ndef. Law of One Price (LOP). Two portfolios with the same future value must have the same value to begin with. if time t&lt;T then:\nΠA​(T)=ΠB​(T)⟹ΠA​(t)=ΠB​(t)\nthm. (Arbitrage is equivalent to LOP) If there is no arbitrage portfolio, then the law of one price holds.\nProof. Contrapositive: If law of one price doesn’t hold, there is an arbitrage portfoilo. Let portfolios A,B such that\nΠA​(T)=ΠB​(T) and ΠA​(t)&lt;ΠB​(t)\nThen construct the following new portfolio that:\n\nLong position on A\nShort position on B\nThen\n\n\nAt time t, value is  Asset ΠA​(t)​​− Cash Debt ΠA​(t)​​− Asset Debt ΠB​(t)​​+ Cash ΠB​(t)​​= Assets ΠA​(t)−ΠB​(t)​​=0\nAt time T, value is  Asset ΠA​(T)​​− Cash Debt ΠA​(t)er(T−t)​​− Asset Debt ΠB​(T)​​+ Cash ΠB​(t)er(T−t)​​= =0 ΠA​(T)−ΠB​(T)​​+ &gt;0 (ΠB​(t)−ΠA​(t))er(T−t)​​&gt;0\n\nThe cash investments are risk free, thus this is determined.\n\n\nTherefore this portfolio is an arbitrage portfolio.■\n\nOne can similarly prove the other way. Thus LOB is equivalent to No Arbitrage condition."},"No-Regret-Dynamics":{"title":"No-Regret Dynamics","links":[],"tags":["Economics/Game-Theory"],"content":"Motivation. Suppose you have to choose which route to take every day driving to work. You devise a complicated strategy, but one day your neighbor and coworker, who takes the same route to work every day, says “I don’t have a strategy, I just take this one route every day.” Wouldn’t it be regretful if it turns out, in total, your route took more time than your co-worker?\ndef. Online Decision Making Game.\n\nPlayer has N actions to choose from, X={1,…,N}\nAt time t…\n\nPlayer constructs a distribution pt over the set of actions X\nAdversary chooses a loss for each action taken, li​∈{0,1}, for every action i∈X where 0 represents no loss and 1 represents a loss. (In our example, think of it as traffic conditions causing a delay.)\nPlayer’s distribution pt is realized into action kt∈X. The player incurs a loss of lktt​.\n\n\nThe player’s goal is to minimize total loss, which we will define shortly.\nWe play for time t=0,…,T. Number of iterations T is predetermined.\n\nWe characterize the loss if we always chose action i (like how our neighbor always takes the same route) for time t=1,…,T as:\nLiT​:=t=1∑T​lit​\nThus we can define also:\n\nLminT​ is the minimum total loss if we could only choose one action every time\nLALGT​:=∑t=1T​lALGt​ is the total loss of player playing strategy of ALG, lALGt=1​,lALGt=2​,….\n\ndef. External Regret. For a player playing strategy ALG, the regret for this strategy is:\nRALG​:=LALGT​−LminT​\ni.e. the difference between total loss of algorithm and the best total loss of one-action-every-time strategy.\nAlgorithms for Online Games §\nalg. Greedy algorithm. This algorithm chooses the action whose one-action-fits-all-time loss is smallest\n\nInitially: x1=1\nAt time t, choose xi​ such that we take the minimum possible LiT​. In other words:\n\nxit​ such that =argmini​LiT​\n\nBreaks ties determinimistically, with the action with lowest index.\n\nMotivation. This algorithm is really bad. Instead, we can try to confuse our adversary by mixing our strategy, i.e. a randomized algorithm.\nalg. Randomized Weighted Majority.\n\nInitially, play i with probability pi1​=N1​ for all i∈X\nAt time t…\n\nwit​={wit−1​(1−η)wit−1​​if incurred loss, i.e. lit−1​=1if incurred loss, i.e. lit−1​=1​\n\nwhere η is the discount factor\n\n\nCalculate this new weight for every strategy i, and then play i with probability\n\n\n\nwhere Wt:=∑i∈X​wit​\n\nThis is a much better algorithm; in fact we can show how small its regret is.\nthm. (Regret Bound of Randomized Weighted Majority) For η≤21​, the loss of RWM algorithm satisfies:\nLRWMT​≤(1+η)LminT​+ηlnN​\nProof. Let Ft denote the fraction of weights that are discounted because they incurred a loss. We show this is equal to the expected loss at timestep t:\nFt​:=Wt∑i;li​=1​wit​​=i;li​=1∑​Wtwit​​=i∈X∑​Wtwit​​lit​=i∈X∑​pit​lit​=E(lt)​​\nNow, we can express Wt+1 using Ft way, by splitting the summation into those that incurred a loss and those that didn’t.\nWt+1​:=i∈X∑t​wit​=(1−η)∑i;li​=1​wit​i;li​=1∑​wit​(1−η)​​Wt−∑i;ili​=1​i;li​=0∑​wit​​​=(1−η)FtWt+(Wt−FtWt)=Wt−ηFtWt=Wt(1−ηFt)​​\nWe can now construct the inequality:\nmaxi​wit+1​(1−η)LminT​LminT​ln(1−η)​≤Wt+1≤Wt+1=W1(1−nF1)×⋯×(1−nFT)=Nt=1∏T​(1−ηFt)=lnN+t=1∑T​ln(1−ηFt)≤lnN+t=1∑T​ηFt=lnN+ηt=1∑T​Ft=lnN+ηLRWMT​​sum is greater than maxmax weight is one with min lossW1=NTaking log on both sides∀x∈R,ln(1−x)≤−x​​\nAnd with some algebra and inequality: ∀z∈R,−ln(1−z)≤z+z2\nLrwmT​≤ηlnN​−ηLminT​ln(1−η)​≤ηlnN​−(1+η)LminT​\n■"},"Noam-Chomsky":{"title":"Noam Chomsky","links":["Chompsky-Heirarchy","Chompsky-Normal-Form"],"tags":["People"],"content":"From the\n\nChompsky Heirarchy\nChompsky Normal Form\n"},"Normal-Distribution":{"title":"Normal Distribution","links":["Normal-Distribution","Approximating-Distributions","Standardizing-a-Random-Variable","Chebyshev's-Inequality"],"tags":["Math/Common-Distributions"],"content":"def. Normal Distribution. A random variable X distributed over a Normal distribution with mean μ and standard deviation σ is denoted:\nXfX​(x)P(a&lt;X&lt;b)​∼Normal(μ,σ)=σ2π​1​⋅e−21​(σx−μ​)2=∫ab​σ2π​1​⋅e−21​(σx−μ​)2dx​\ndef. Cumulative Distribution Function (CDF) of a Normal Distribution\nΦ(z):=∫−∞z​2π​e−t2/2​dt\nObserve:\n\nz→+∞,Φ(z)→1\nz→−∞,Φ(z)→0\nΦ(−z)=1−Φ(z)\n\ndef. Standard Normal Distribution. A standard normal distribution is a normal distribution where μ=0,σ=1\nX∼Normalstd.​(0,1)\n\n\n                  \n                  Tip \n                  \n                \nYou can approximate a bunch of distributions using the Normal.\n\nrmk. Linear Transformation of Normal Distribution. If X∼N[μ,σ2], then\n\nE(aX+b)=aE(X)+b=aμ+b\nVar(aX+b)=a2Var(X)=a2σ2\n\n\n⇒ Thus (aX+b)∼Normal(aμ+b,a2σ2)\n\n\n\nremark. Exponentiating Transformation of Normal Distribution. If X∼N(μ,σ2)\nE[eX]=eμ+2σ2​\n(using Law of Unconscious Statistician)\nrmk. Standardizing the Normal Distribution. Given X∼N(μ,σ2):\n\nY=σX−μ​ has the standard normal distribution\nThe pdf is as follows:\n\nP(a&lt;X&lt;b)=∫ab​σ2π​1​⋅e−21​(σx−μ​)2dx   ⇒   ∫σa−μ​σb−μ​​σ2π​e−x2/2​dz\n\nSee also Standardizing a Random Variable\n\nrmk. Empirical Rule: Rule of thumb for calculating probabilities (integrals) of normal distributions\n\n! Generalized version: Chebyshev’s Inequality\n1 std. dev. away is ≈66%; 2 std.dev. away is ≈0.95%\n\nEstimators §\nlet\n\nX∼N(μ,σ2)\nX1​,…,Xn​∼iidN(μ,σ2)\n⇒ Log likelihood:\n\nlnLn​(μ,σ2∣x1​,...,xn​)=−2n​ln(2π)−2n​ln(σ2)−2σ21​i=1∑n​(xi​−μ)2\nScore §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne R.V.Multiple Datas(μ)=σ2x−μ​sn​(μ)=σ2nμ−∑i=1n​xi​​s(σ)=σ3(x−μ)2​−σ1​sn​(σ)=σ3∑i=1n​(xi​−μ)2​−σn​\nMLEs §\nμ^​=n1​i=1∑n​xi​\nσ^2=n1​i=1∑n​(xi​−xˉ)2\nFisher Information §\n\nUnknown μ, known σ2\n\nI(μ)=σ21​In​(μ)=σ2n​\n\nKnown μ unknown σ2\n\nI(σ2)=2σ21​In​(σ2)=2σ2n​"},"Oligopoly":{"title":"Oligopoly","links":["Bertrand-Price-Competition","Cornout-Quantity-Competition"],"tags":["Economics/Macro-Economics","Economics/Game-Theory"],"content":"Definitions §\n\nBest Response Curve: A function that determines one’s best response value of the strateigic variable, given the opponent’s strategic variable value.\n\nThis is equivalent to the pure strategy function (see definition)\n\n\nDr denotes residual demand for one firm\n\nAssumptions §\n\nAssume that products are identical [=no differentiation]\nAssume there are two firms in the market\nAssume that MC is constant\n\nTypes §\n\nBertrand Price Competition\n\nProduct Differentiation\n\n\nCornout Quantity Competition\n\nSequential (=Stackleberg)\n\n\n\n\nThere are thus 2 ways to fill in the gap between 2-firm oligopoly and perfect compeition:\n\nN-firm Oligopoly as an extension of Cornout’s 2-firm oligopoy analysis\nMonopolistic Competition, where firms strategically set differentiation and price\n"},"Omitted-Variables":{"title":"Omitted Variables","links":[],"tags":["Math/Statistics"],"content":"Motivation. What if there are external variables that are related to Y? We want to characterize how off we might be if we omitted a variable.\nOmitting a Variable from a True 2-var Model §\nAssume the true model should have the following:\nYi​^​=β0​^​+β1​^​X1,i​+β2​^​X2,i​+ϵi​^​\nWe didn’t take into account X2​, and we mistakenly used the following model:\nY^i′​=β0′​^​+β1′​^​X1,i​+ϵi​^​\nThen, our estimators β0′​^​ and β1′​^​ will be different from β0​^​,β1​^​. We can state precisely how off they will be:\nβ1′​^​=β1​^​+β2​^​δ1​^​\nwhere δ1​^​ is from the auxiliary regression\nX2,i​=δ0​^​+δ1​^​X1,i​+τi​\nOmitting a Variable from a 3-var Model §\n\nTrue model: Yi​^​=β0​^​+β1​^​X1,i​+β2​^​X2,i​+β3​^​X3,i​+ϵi​^​\nOmitted model: Yi′​^​=β0′​^​X1,i​+β2′​^​X2,i​+ϵi′​^​\nEstimator relationships:\n\nβ1′​^​=β1​^​+β3​^​δ1​^​\nβ2′​^​=β2​^​+β3​^​δ2​^​\n\n\nAuxiliary regression: X3​=δ0​^​+δ1​^​X1​+δ2​^​X2​\n\nGeneral Form §\nFor true model:\nYi​^​=β0​^​+β1​^​X1,i​+⋯+βn−1​^​Xn−1,i​+βn​^​Xn​\nwhere Xn​ is the omitted variable. Our omitted model:\nYi​^​\nβi′​^​=βi​^​+β^​omit​δi​^​\nwhere\nXomit​=δ0​^​+δ1​^​X2​+⋯+δ^n−1​Xn−1​"},"Optimal-Stopping-Problem":{"title":"Optimal Stopping Problem","links":[],"tags":["Economics/Game-Theory/Games"],"content":"Motivation. Imagine a gambling situation, where there is a sequence of prizes inside boxes. The gambler knows the distribution of these boxes, but is only shown one at a time. They can claim only one box, and once a box is opened the prize must be claimed or trashed. How can the gambler act?\n\ndef. (Optimal stopping problem) let prizes of random variables X1​,X2​…,Xn​ be distributed F1​,F2​,…,Fn​. The gambler only knows the distribution of each of these boxes, and the order in which the boxes are shown is shuffled randomly.\nthm. Prophet Inequality. There is a strategy for the gambler to achieve at least 21​ of the optimal revenue, i.e.:\nE(payoff)≥21​E(maxi=1n​Xi​)\nwhere…\n\npayoff is the payoff to the gambler\nlet X∗:=maxi=1n​Xi​, a random variable. This is what the “Prophet” gets, i.e. a optimal strategy.\nAdditionally, the theorem states that this strategy is a optimal cutoff strategy, which is one that stops if the payoff from the current opened box is larger than predetermined cutoff w.\n\nProof. We know that payoff=base payoff+excess payoff where\n\nbase payoff is w\nexcess payoff is Xj​−w, where Xj​ is the box we stop at\nWe also know that these two are random varibles:\n\nw={w0​if X∗≥welse​\nexcess payoff={E((Xj​−w)+)0​if stopped at Xj​if never stopped​\nNow, the expected payoff is:\nE(payoff)= expected base P(X∗≥w)⋅w​​+ expected excess j=1∑n​P(stopping at Xj​)⋅E((Xj​−w)+)​​​​\nWe know that the probability of stopping at Xj​ (from the first case of excess payoff) is:\nP(stopping at Xj​)​=P(maxi=1j−1​Xi​&lt;w)≥P(maxi=1n​Xi​&lt;w)=P(X∗&lt;w)​...that boxes before j where &lt;w...that all boxes are &lt;wby definition of X∗​​\nThus:\nE(payoff)≥P(X∗≥w)⋅w+ take out since X∗ is not relevant to j P(X∗&lt;w)⋅j=1∑n​E((Xj​−w)+)​​\n(lemma 1)\nOn the other hand, the expected prophet payoff is\nE(X∗)​=E(w+maxj=1n​(Xj​−w))≤w+E(maxj=1n​(Xj​−w)+)≤w+j=1∑n​E((Xj​−w)+)​by definition of (…)+sum greater than max​​\n(lemma 2)\nNoticing lemma 1 and lemma 2 both have term ∑j=1n​E((Xj​−w)+), we can organize for that:\nP(X∗&lt;w)E(payoff)−w⋅P(X∗≥w)​≥j=1∑n​E((Xj​−w)+)≥E(X∗)−w\nSimplifying we get\nE(payoff)≥21​E(X∗)\n■"},"Optimistic-Nihilism":{"title":"Optimistic Nihilism","links":[],"tags":["Philosophy"],"content":""},"Options-(Finance)":{"title":"Options (Finance)","links":["Derivatives-(Finance)","No-Arbitrage","Present-Value-Calculations","Binomial-Tree-Model-of-Security-Pricing","Stochastic-Process","Approximating-Distributions","Normal-Distribution","Black-Scholes-Merton-Derivative-Pricing-Formula"],"tags":["Economics/Finance"],"content":"def. An option is a contract that gives the holder the right to exercise the option (=buy/sell a stock) at the strike price at or before the strike date.\n\nIt is a type of Derivative\nThe person who gives out the option contract is the writer.\nThe person who gets the option contract is the holder.\n\nTypes of Options §\n\nExercise timing\n\nAmerican option: anytime on or before the strike date.\nEuropean option: at the strike date only.\n\n\nExercise type\n\nCall option: you hold the right to sell the stock at the strike price.\nPut option: you hold the right to buy the stock at the strike price.\n\n\n\nDefinitions §\n\nS(t): price of underlying asset, at time t\nC(t): price of a call option, at time t\nP(t): price of a put option, at time t\nK: strike price of a call or put option\n\nSingle-Term Option Pricing §\nCall Option §\nAssume continuous compounding at rate r between time period [t0​,t1​].\nAssume No-Arbitrage.\n\nPayoff: max[S(t1​)−K,0]\n\n0 is when the stock price as decreased, so you don’t decide to sell.\naccording to the law of one price, must equal to =C(t1​)\n\n\nProfit: max[S(t1​)−K,0]−C(t0​)exp[r(t1​−t0​)]\n\nFirst term is the payoff\nSecond term is because you borrowed money at the risk-free rate r to buy the call option.\nWe don’t know what C(t0​) is yet…see below.\n\n\n\nCall-Put Parity §\ni.e. you can always use a stock plus a put option to simulate an equivalent call option, and v.v.\nC(t0​)=S(t0​)+P(t0​)−Ke−r(t1​−t0​)\nBinomial Option Pricing Model §\nQ. What is the fair price (=No-Arbitrage) of a European call option at time t0​?\n\nYou can’t really just use Present Value Calculations because it’s a derivative.\nUsing the same Binomial Tree Model of Security Pricing but in options.\n\n\nFind the purchase of stock and borrow of money that would have the equivalent payoff as the call option (=reproducing portfolio)\nUse that to price the stock\nThen, according to the No-Arbitrage that must be the same as the current price of the call option\nDo this for n periods for a binomial tree\n\nCall Option Details §\nPayoff (=price of call option at time t1​)\nC(t1​)={max[S(t1​)⋅u∗​K,0]max[S(t1​)⋅d∗​K,0]​probability pprobability 1−p​\n\nu,d is the uptick factor and the downtick factor\n\nReproducing Portfolio Details §\n\n\nΔ(t): number of stocks you purchase\n\nwill increase due to dividend payouts: Δ(t1​)=Δ(t0​)eq(t1​−t0​)\nq is the dividend rate\n\n\n\nb: amount you borrow at time t0​\n\n\n⇒ Value of portfolio\n\n\nV(t0​)V(t1​)​=Δ(t0​)⋅S(t0​)−b=Δ(t1​)⋅S(t1​)−ber(t1​−t0​)​​\nCalculating Fair Call Price §\n⇒ Equate V(t1​)=C(t1​) and solve for Δ(t),b to obtain the reproducing portfolio.\nΔ(t0​)b​=(u−d)S(t0​)Cu​(t1​)−Cd​(t1​)​e−q(t1​−t0​)=u−ddCu​(t1​)−uCd​(t1​)​e−r(t1​−t∗0)​​\n\nCalculate V(0):\n\nV(t0​)=e−q(t1​−t0​)(u−d)S(t0​)Cu​(t1​)−Cd​(t1​)​S(t0​)−e−r(t1​−t0​)u−ddCu​(t1​)−uCd​(t1​)​\n\n⇒ Thus V(t0​)=C(t0​) and simplifying:\n\nC(t0​)=e−r(t1​−t0​)[u−de(r−q)(t1​−t0​)−d​Cu​(t1​)+u−du−e(r−q)(t1​−t0​)​Cd​(t1​)]\nGeneralizing into n terms §\n\nWith the No-Arbitrage we have E[C(t1​)]=C(t0​)e(r−q)(t1​−t0​).\nThen define the risk-neutral probability of an uptick such that\n\np∗​:=u−de(r−q)(t1​−t0​)−d​\nthm. Risk-Neutral Binomial Call Pricing Formula\nC(t0​;n)=e−(nh)ri=0∑n​(in​)p∗i​(1−p∗​)n−iCui​dn−i​​(tn​)\nwhere\n\nCuidn−i​(tn​):=max[C(t0​)uidn−i−K,0]\nun​≈eσhn​​\ndn​≈e−σhn​​\nh:=t1​−t0​\nr,q is the risk-free rate and dividend rate\n\nBlack Scholes European Option Pricing Formula (Risk-neutral Derivation) §\nBy taking n→∞ we see\n\nS(t) becomes a Stochastic Process\nBinomial sum can be approximated by a Normal Distribution\n\nthm. Black Scholes Option Pricing Formula (Risk-Neutral)\nC(t)d1​d2​​=e−(T−t)qS(t)N(d1​)−e−(T−t)rKN(d2​)=σT−t​ln(S(t)/K)+[(r−q)+2σ2​](T−t)​=d1​−σT−t​=σT−t​ln(S(t)/K)+[(r−q)−2σ2​](T−t)​​​\nwhere\n\nS(t) are log-normal returns (=geometric brownian motion)\n\nσ is the scale of this motion\n\n\nN(d) is the CDF of the Standard Normal Distribution\n\nBlack Scholes European Option Pricing Formula (GBM and PDE Derivation) §\nSee Black-Scholes-Merton Derivative Pricing Formula"},"Parallel-Algorithms":{"title":"Parallel Algorithms","links":["Memory-Access-Control"],"tags":["Computing/Algorithms"],"content":"Terminology\n\nTp​(n): span; time taken for p-processes with input size n\n\nYou may choose p=∞ i.e. T∞​(n) to analyze if unlimited parallelism was possible.\n\n\nWp​(n): work; total time by all individual process; Wp​(n)≤p⋅Tp​(n)\nspeedup: speedup factor compared to a single-process algorithm\nspawn: start a new process\nsync: wait until all processes are finished\n\nParallel algorithms…\n\nFor Memory Access Control we will concern ourselves with concurrent read &amp; exclusive write model of computation\nare easy to make from recursive algorithms.\n\nalg. PSum. Normal algorithm is O(n). Following is a parallel algorithm:\nfunction psum(l, r)\n\tif l=r\n\t\treturn A[l]\n\tspawn ls = psum(l, m)\n\tspawn rs = psum(m+1, r)\n\tsync\n\treturn ls + rs\n\nSpan: T∞​(n)≤T(2n​)+1=O(logn)\nParallel algorithm recursion tree: \n\ndepth is logn, each takes constant time ⇒ T∞​(n)=O(logn)\n\n\n…if number of processors are limited to p, then: \n\nTp​(n)=O(pn​+logp) where pn​ is the time for subproblems that cannot be parallelized (problem size is pn​) and logp is the parallel algorithm runtime.\nWp​(n)=O(p(pn​+logp))=O(n+plogp)\n\n\n…if number of processros p=lognn​, then:\n\nTp​(n)=O(logn)\nWp​(n)=O(n)\n\n\n"},"Pareto-Efficiency":{"title":"Pareto Efficiency","links":["Utility"],"tags":["Economics"],"content":"Pareto Efficiency is a state where you cannot make one person better off without making another person worse off."},"Path-Alignment":{"title":"Path Alignment","links":[],"tags":["Computing/Algorithms"],"content":"Homework"},"Percieved-value—Real-value":{"title":"Percieved value—Real value","links":[],"tags":["Humanities/Marxism"],"content":"Percieved value—Real Value §\n(Use-value vs Exchange-value)\nDiscrepency decreases in the long run.\nPeople will notice that you brought value to their lives, but it may take time."},"Performance-(Computing)":{"title":"Performance (Computing)","links":[],"tags":["Computing/Computer-Architecture"],"content":"Perfomance measurement: Execution Time and Throuput/Bandwidth. (Decreasing execution time always improves thruput) And intuitively performance is inversly related to execution time: Performance=Execution Time1​.\nMeasuring Performance §\n\nProgram Execution Time (is the sum of…)\n\nCPU Time (e.g. 10 picoseconds)\n\nUser CPU Time\nSystem CPU Time\n\n\nI/O Time\n\n\nClock Cycle (= tick, clocks, cycles) e.g. one tick on this CPU is 250 picoseconds.\nInstruction Count: Number of instructions in a program.\n\n⇒ Clock Per Instruction (CPI) / IPC (Instructions Per Clock)\nThus\nCPU Time (s)=# of cycles×Tick Time (s)# of cycles=# of Instructions×avg. CPI\nAnd:\nCPU Time (s)=Clock Rate (Hz)# of Instructions×CPI​\nThe Three Factors of Performance: Clock Rate / Instruction Count / CPI\n$$\n\\text{Performance}= \\frac{\\text{Seconds}}{\\text{Program}}\\\\=\\frac{\\text{Instructions}}{\\text{Program}}\\times \\frac{\\text{Clock Cycles}}{\\text{Instruction}}\\times \\frac{\\text{Seconds}}{\\text{Cycle}}\\\\\\\\\n=\\text{Instruction Count} \\times \\text{CPI} \\times \\text{Clock Rate}\n\n\n$$\n\nList of Common Misconcpetions Performance §\n\nImproving one aspect doesn’t improve overall performance porportionally\n\nAmdahl’s Law:\nExecution timeafter improvment​=Amount of improvmentExecution timeaffected by improvement​​×Execution timeafter improvment​\n→Some perfomance targets are impossible with partial improvements. See example on p.50."},"Performative-Gender":{"title":"Performative Gender","links":["Judith-Butler","Social-Construct","Microphysics-of-Power"],"tags":["Philosophy/Queer-Theory"],"content":"\nJudith Butler’s Theory of Gender Performativity, Explained - YouTube\nJudith Butler’s Gender Performativity, Part 2: What is “Performativity?” - YouTube\n\n\n\n                  \n                  Definition of Gender \n                  \n                \nGender is the stylized repetition of acts through time.\n\nKey Points §\n\nJudith Butler, Gender Trouble.\n“Performance”!= “Performative”\n\n→ Performative means that gender is not like a wardrobe of “gender” you can choose from every day.\n⇒ Instead, it means gender is constructed into being. It is not something that you can easily choose, due to Microphysics of Power and other things\n\n\nImagine gender as something like “I am a Duke student.” You are not born a duke student, nor is Duke a real physical fact of reality.\n\nYou partially chose, partially was embraced by Duke, the socially constructed organization.\nThen gender is perfectly severed from biology (=disproves biological essentialism).\n\n\n"},"Permutation-Test":{"title":"Permutation Test","links":[],"tags":["Math/Statistics"],"content":""},"Perpetuity":{"title":"Perpetuity","links":["Sequence-Summation"],"tags":["Economics/Finance"],"content":"A∞​=(1+r/k)P​+(1+r/k)2P​+⋯=r/kP​\n(Geometric Sequence Summation)"},"Phenomenology":{"title":"Phenomenology","links":["Celine-Wei","Phenomenology","(Philosopher)-G.-W.-F.-Hegel","(Philosopher)-Edmund-Husserl","(Philosopher)-Martin-Heidegger","Maurice-Merleau-Ponty","Jean-Paul-Satre","Michel-Foucault","Judith-Butler","Jacques-Derrida","Plato","meditation"],"tags":["Philosophy"],"content":"\n\n                  \n                  Celine Wei: It Feels Right.\n                  \n                \n\nNotes from What is Phenomenology? The Philosophy of Husserl and Heidegger - YouTube\nHistory of Phenomenology\n\n18C Kant proposed, and briefly picked up by Hegel\nBut only until (Philosopher) Edmund Husserl did it become mainstream\n\nStudent: (Philosopher) Martin Heidegger\n\n\nGermany → France\n\nMaurice Merleau-Ponty, Jean-Paul Satre\n\n\nModern philosophers: Michel Foucault, Judith Butler, Jacques Derrida\n\nDefining Phenomenology\n\n\nGreek word for “Appears”\n\n\nFocus on the first hand experience\n\n\nexperientialist &gt; rationalist\n\n\ne.g. time\n\nrationalist: measurement of numerical time\nexperientialist: subjective personal time\n\ne.g. the “pretty girl minute”\n\n\n\n\n\ne.g. fear\n\nrationalist: physiological change, observable behavior of beings in fear\nexperientialist: how fear colors perceptions, the conscious experience of fear\n\n\n\nReversal of Plato\n\nMost of philosophy until Husserl has been about following Plato\nRepresentational Theory\n\nPlato’s Cave analogy(representational theory)\nHumans have\nIt’s a tragedy; our senses only allow incomplete access to reality\nPeaked in Decartes’ mind-body dualism\n\n\n\n\n\nHusserl’s ”Transcendental Phenomenology”\n\nObjective study of the subjective\nTheory of “Intentionality” = “about-ness”\n\nCoined by Brentano (teacher)\nconsciousness cannot be isolated; it’s always interacting with its subjects\nstudy of how the object of consciousness interacts with the structure of consciousness\n\n\nwhether the object is a fantasy/reality/dream/memory doesn’t matter\nPhenomenological method\n\nBracketing: setting aside judgements, filters, and gathering the raw experience;\nEidetic reduction (≈imaginary variation): reduce to essence\n\nMess with the attributes of the phenomenon\ne.g. Fear ⇒ attributes: lack of choice, freeze\n\n\nEnd goal of phenomenology: getting to the universal, objective understanding of the concept\n\n\n\n\n\nHeidegger disagrees: ”Existential Phenomenology”\n\nHusserl: trying to make a science of the consciousness\nHeidegger’s ontological twist:\n\nthe goal is instead understanding the nature of being\nexperience and consciousness cannot be separated\nentanglement varies between people\ne.g. fear of you, an animal, or an Aztec warrior is different\n\n\n\n\n\nEastern connection (China, India)\n\ne.g. Meridian system by China, Chakra by India\nLooks stupid from rationalist perspective\n⇒ but it’s the mapping of the first-person experience of the body\ne.g. meditation: observe the experience\n\na form of bracketing (stopping judgement, and just experiencing thoughts and emotions)\nZen buddism\nDaoism may have influenced Husserl directly too\n\n\n\n\n"},"Philosophy,-Political-Science,-Economics":{"title":"Philosophy, Political Science, Economics","links":["Economics/MicroEconomics/Game-Theory"],"tags":["Humanities","Economics/Game-Theory"],"content":"Why? §\nLack of Scientific Rigour in Economics §\nEconomics doesn’t have predicting power (the test of correctness for scientific theories)…\n\ncouldn’t predict recessions\ndoesn’t pursue distributive justice\nbetter for designing economics/social institutions, maintaining social order, or incentivizing individuals in society\n\nThis is because in economics the strategy of people are not considered:\n\n\n                  \n                  parametric agents.\n                  \n                \n→ In reality, individuals act based on how they think others may act; i.e. they are strategic agents.\n\n→ Conventional economic theory is useless as a predictor—it’ll only satisfy one’s curiosity\n→ The correct/better theory for modeling economics is Game Theory—the theory of strategic interactions\n"},"Physical-Data-Organization":{"title":"Physical Data Organization","links":[],"tags":["Computing/Data-Science"],"content":"\nStorage ranges from Fast &amp; Small → Slow &amp; Big \n(DevonThink) Numbers Every Programmer Should Know By Year\n\nIdea: a trip to the next hierarchy is orders of magnitude slower.\n\n\n\nAnatomy of a Hard Drive §\n\nParts of a mechanical hard drive: \n\nAccess Time=Seek+Rotation+Transfer\nAll data is transfered in blocks! (512B~4KB)\nRecords (=Tuples) can be fixed length of dynamic length\n\nBLOB fields: e.g. images. These link out to external locations\n\n\n\nStoring Many Tuples in One Block §\n\nOften many tuples will fit in one block. There are multiple schemes to lay them out.\n\nN-ary Storage Model (NSM) §\n\nData stored from the beginning of the block\nIndex stored at the end of the block\nEvery update/delete operation will reorganize everything! → Use gaps inbetween records (=sparse block)\nHard to cache, because queries will often only access a few columns\n\n\nPartition Attributes Across (PAX) §\n\nCluster columns together\nVariable length columns will have index at the end\nKeep the fields together (=dense block)\n\n\nColumn Stores §\n\nStore the whole table by columns\ne.g. Apache Parquet\n"},"Pipelining":{"title":"Pipelining","links":[],"tags":["Computing/Computer-Architecture"],"content":"\nIn computing, a pipeline, also known as a data pipeline, is a set of data processing elements connected in series, where the output of one element is the input of the next one. The elements of a pipeline are often executed in parallel or in time-sliced fashion. Some amount of buffer storage is often inserted between elements.\nComputer-related pipelines include:\nInstruction pipelines, such as the classic RISC pipeline, which are used in central processing units (CPUs) and other microprocessors to allow overlapping execution of multiple instructions with the same circuitry. The circuitry is usually divided up into stages and each stage processes a specific part of one instruction at a time, passing the partial results to the next stage. Examples of stages are instruction decode, arithmetic/logic and register fetch. They are related to the technologies of superscalar execution, operand forwarding, speculative execution and out-of-order execution.\nGraphics pipelines, found in most graphics processing units (GPUs), which consist of multiple arithmetic units, or complete CPUs, that implement the various stages of common rendering operations (perspective projection, window clipping, color and light calculation, rendering, etc.).\nSoftware pipelines, which consist of a sequence of computing processes (commands, program runs, tasks, threads, procedures, etc.), conceptually executed in parallel, with the output stream of one process being automatically fed as the input stream of the next one. The Unix system call pipe is a classic example of this concept.\nHTTP Pipelining, the technique of issuing multiple HTTP requests through the same TCP connection, without waiting for the previous one to finish before issuing a new one.Some operating systems may provide UNIX-like syntax to string several program runs in a pipeline, but implement the latter as simple serial execution, rather than true pipelining—namely, by waiting for each program to finish before starting the next one.\nWikipedia\n"},"Poisson-Distribution":{"title":"Poisson Distribution","links":["Poisson-Limit-Theorem"],"tags":["Math/Common-Distributions"],"content":"See also: Poisson Limit Theorem\ndef. Poisson Distribution. A random variable X which model the number of events in a fixed interval of time, where each event is rare and there is a large number of events, is well modeled by a Poisson Distribution with the intensity of the event λ:\nX∼Poisson(λ)P(X=k)=e−λk!λk​\n\nE(X)=λ\nSD(X)=λ​\nP(X&lt;n)=∑k=0n−1​e−λk!λk​\n\nEstimators §\nlet X1​,…,Xn​∼Poi(λ)\nlnLn​=∏i=1​xi​!λ∑i=1n​xi​enλ​\nsn​=λ∑i=0n​xi​​−n\nλ^MLE​=n∑i=1n​xi​​=Xˉn​\n\n\n\n\n\n\n\n\n\n\n\n\n\nsinglemultipleI=λ1​In​=λn​"},"Poisson-Limit-Theorem":{"title":"Poisson Limit Theorem","links":["poisson-distribution"],"tags":["Math/Probability"],"content":"Poisson Limit Theorem (Simplified) §\nthm. Poisson Limit Theorem. With a random variable X∼Binom(n,p), as n→∞,p→0, X≈Poi(np), since:\nX=I1​+⋯In​ where each is an indicator of success of the i-th event, and as limn→∞,p→0​ this defines the poisson distribution.\nPoisson Scatter Theorem §\ndef. An experiment process which adheres to the following creteria is a Poisson Scatter process.\n\nNumber of hits are finite\nNo multiple hits on one point\nHits are homogenous and independent (any non-overlapping region’s hit number is independent.)\n\nTHM Poisson Scatter Theorem. In a poisson scatter process:\n\nNumber of hits over area R is a Poisson Random variable\nThe number of hits in each disjoint region is independent of each other (definiiton #3)\nThe rate of hits (λ) is proportional to its area\n\nPoisson Addition Rule §\nthm. If X∼Poi(λX​) and Y∼Poi(λY​) and X⊥Y then:\nX+Y∼Poi(λX​+λY​)\n\n\n                  \n                  Info \n                  \n                \nNote also that the bionmial distribution as something like this too. If X∼Bi(nX​,p) and Y\\sim \\text{Bi}(n_Y,p)\n$$Y\\sim \\text{Bi}(\\lambda_Y) and X⊥Y then X+Y∼Bi(nX​+nY​,p)\n\nPoisson Point Process §\ndef. Memory-less-ness (continuous). X is memoryless iff:\n∀s,t&gt;0,  P(X&gt;s+t∣X≥s)=P(X&gt;t)\n\n\n                  \n                  Read it like this: \n                  \n                \n\nLHS: “probability of waiting t more minues, given that you’ve already waited s minutes.”\nRHS: “probability of just waiting t minutes in total.”\nGeometric and Exponential distributions satisfy this property. See the following.\ndef. Poisson Point Process (PPP). PPP can be described in the following three equivalent definitions in region R with intensity λ (per unit):\n\nGiven a region Ri​, let random variable N(Ri​) be defined as the number of events in the region. If..:\n\n∀i∈R,  N(Ii​)∼Poi(λ⋅∣Ri​∣), where ∣Ri​∣ is the “size” of the region and λ the intensity\nN(Ri​) are all independent of each other\n⇒ …then the event occurs in a PPP\n\n\nA process where the waiting time Wi​ between two sequential events is distributed as an exponential distribution ∀i∈R,  Wi​∼Exp(λ)\nTotal region size X for r events is distributed over gamma X∼Γ(r,1/λ)\n"},"Portfolio-Theory":{"title":"Portfolio Theory","links":["Efficient-Market-Hypothesis","Dividend-Discount-Model","Random-Variable","Risk-(Finance)"],"tags":["Economics/Finance"],"content":"Assume the Efficient Market Hypothesis.\n\n\n                  \n                  Info \n                  \n                \nHow to maximize returns while minimizing risk [=volatility =std. dev.]?\n→ Since risk/return is proportional to each other, you choose one optimization goal.\n\n\nSingle Stock (See Dividend Discount Model)\nPrice of stock i at time t: Si​(t)\nReturn of stock i at time t: Ri​(t)=Si​(t0​)Si​(t)−Si​(t0​)​+Si​(t0​)Di​​\n\nSi​,Di​,Ri​ are all Random Variables\n\n\nExpected Return of stock i: μi​=E[Ri​]\nVolatility (=Risk) of Stock i: σi​=Var[Ri​]​\n\nTwo-stock Portfolios §\n\nCovariance of stocks i,j: σi,j​=Cov[Ri​,Rj​]\nCorrelation of stock i,j: ρi,j​=σi​,⋅σj​σi,j​​ such that −1≤ρ≤1\nWeights w,1−w for (R1​,μ1​,σ1​),(R2​,μ2​,σ2​)\nPortfolio Expected Return: μp​=wμ1​+(1−w)μ2​\nPortfolio Variance:\n\nσp2​​=w2σ12​+(1−w)2σ22​+2w(1−w)ρ⋅σ1​σ2​=(σ12​−2ρσ1​σ2​+σ22​)w2+2(ρσ1​σ2​−σ22​)w+σ22​​function of w​​\n\nMinimum Variance Portfolio:\n\nW1−W​=σ12​−2ρσ1​σ2​+σ22​σ22​−ρσ1​σ2​​=σ12​−2ρσ1​σ2​+σ22​σ12​−ρσ1​σ2​​​​\nN-Stock Portfolio §\nR=​R1​⋮RN​​​  μ​=​μ1​⋮μN​​​  w=​w1​⋮wN​​​  V=​σ1​⋮σN,1​​…⋱…​σ1,N​⋮σN,N​​​\n\nKnown information: σi​,σij​,ρij​,μi​ are all known\nMultiple Stocks (=Portfolio)\n\nPosition of portfolio p at time t (=amount invested): Vp​(t)=∑i=1N​ni​Si​(t)\nPortfolio Return at time t Rp​(t)=wT⋅R\nPortfolio Expected Return at time t: μp​(t)=w⋅μ​\nPortfolio Variance: σp2​=∑i=1N​wi2​σi2​+2∑1≤i&lt;j≤N​wi​wj​ ρ σi​σj​=wTVw\n\n\n\ndef. Feasible Portfolios. Set of tuples (σ=risk,μ=expected return) given we have many assets weighted w.\n\nFp​={(σp​,μp​)∣w∈R} ← Short-selling is allowed\nLower the correlation [= the more negatively correlated] ∝ the better diversification is.\n\nWhen two stocks have perfectly negative correlation ρ=−1, the efficient frontier touches the y-axis (=σp​=0)\n\n\n\nEfficient Frontier §\nminw​ σp​=wTVw​ such that wT1=1, wTμ​=μp​\n\n“Minimize the variance σ such that the sum of weights are 1 and the portfolio return is μ (a constant)”\nKey Assumption: neither the returns μi​ or risks σi​ are identical, and there is no perfect correlation ρi,j​=±1\nMinimum for given return level μp​ at:\n\nw=AC−B2C−Bμp​​V−1e+AC−B2μp​A−B​V−1μ​\n\nEfficient Frontier:\n\n{(σp​,μp​)∣σp2​=AC−B2Aμp2​−2Bμ_p+C​}\n\n…where A,B,C are scalars:\n\nA=eV−1e\nB=μ​TV−1e=eTV−1μ​\nC=μ​TV−1μ​\n\n\n\n\n"},"Positive-Leisure":{"title":"Positive Leisure","links":["(Youtube)-Spaceship-You","CGP-Grey","Positive-Leisure","理(ことわり)","tags/Logistics/Productivity","Slack","감성","성민석","Gratitude","Well-Timed-Breaks","Importance-of-Inputs","Emergent-Phenomena","Hedonism","(Book)-How-to-Be-Miserable---Randy-J-Paterson","Creation-Station"],"tags":["Emotion","Logistics/Productivity","Economics","Logistics"],"content":"Leisure in economics is simply “not working.” Instead, I define positive leisure as:\ndef. Positive Leisure. Positive leisure means two things:\n\nCreating at Leisure. You’re doing “work” that benefits you/others. CGP Grey definition\nLeisurely Pace. Making sure life is not rushed, and there is space.\nUneconomic Leisure. Leisure that is not about consumption (i.e. buying things/spending money).\n\n\n\n                  \n                  Leisure is about making sure you&#039;re life is full of &quot;doing things at your leisure,&quot; or going through life at a &quot;leisurely pace.&quot; \n                  \n                \n\nPositive leisure understands…\n\n…that resting in the modern world means mental, not physical relaxation\n…that leisure is derived from structure\n…that not doing anything may also be draining\n\nPositive leisure implements the following:\n\nDo things at your leisure (=do things as slowly as you want.)\n\nMost things can stand being done later\nSometimes you have to not give a shit\nYou’re not rushing because you need to, you’re rushing because society pressures you to.\nForcibly Slow Down, Set a Very Long Time Slot, etc. Using deliberate slowing down as a strategy for managing stress or overwhelming situations.\n\n\nProductivity#Logistics/Productivity is not against leisure. In fact, productivity is about doing things well and at your leisure.\n\nMove between tasks at a leisurely pace\nWhen you do this, you have more Slack.\nProcrastination happens when you fail to combine productivity and leisure well.\n\n\n감성 cannot exist without emotional peace.\n\nEven in turmoil you can have emotional peace, and time to reflect on those feelings.\n성민석 says 감성 is what makes life worth living. And it is for you too.\n\n\nPositive leisure often leads to Gratitude\n\nIt’s easier to enjoy the things (=physical things &amp; immaterial things) you already have.\n\n\n\nCreating Positive Leisure §\n\nBreathe\nMoving slowly through life\nIf you have too much things to do, either…\n\n…remove the offender\n…increase productivity (get the simple things done fast)\n\n\n\nStructured Relaxation (=Leisurely Relaxation) §\nFind Leisure that either:\n\nRelaxes you fully\n\nSleep, Music, Movie\nRelaxing on a couch doing nothing\n\n\nYou feel good having done it\n\nReading a book or magazine article\nNew content, e.g. on Netflix, subscribed Youtubers, Mastodon, etc.\n\n→ These should be your own curation, from people/sources you’ve chosen\n\n\n\n\n\nFun and Pleasure is derived from Structure\n\nPleasure and pain is derived not from absolutes [=it isn’t a sliding scale from pleasure to pain] but is an Emergent Phenomena from structures in life.\n\nHedonistic Adaptation and the reverse for pain evidences this.\n\n\nStructure is essential, and doesn’t harm pleasure. It more often actually enhances it.\n\nspice and honey is enhanced at duke, as Celine does.\n(Book) How to Be Miserable - Randy J Paterson: taking part in “hedonism” is ineffective because impulses!= what’s good for you. “Those who eschew impulses are better hedonists than you are.”\n\n\nThis is fotunate, because this means that Creation Station is not mutually exclusive from pleasure. To achieve both, aim to maintain separation between locations.\n\nFlow Work §\n\nHappiness Through Achieving the State of ‘Flow’: Seeking happiness in the immersion and engagement of tasks.\n"},"Postructuralism":{"title":"Postructuralism","links":["Decentralization","Michel-Foucault","Jacques-Derrida","Jean-Baudrillard","Noam-Chomsky"],"tags":["Philosophy/Epistemology"],"content":"\nPoststructuralism is Decentralization (of power, of…)\n“Microphysics of power is too strong. We shouldn’t trust social institutions as they are agents of power. Everything is relative and a social construction.”\nMoral relativism\n\nRelated People\n\nMichel Foucault, Jacques Derrida, Jean Baudrillard\nNoam Chomsky hates it. See Debate Noam Chomsky &amp; Michel Foucault - On human nature [Subtitled] - YouTube\n"},"Potential-Game":{"title":"Potential Game","links":["Traffic-Routing"],"tags":["Economics/Game-Theory"],"content":"def. Potential Game. A game is a potential game iff there exists a potential function that satisfies the following, for any player i that changes their strategy from si​→si′​:\nChange in Potentialϕ(si′​,s−i​)−ϕ(si​,s−i​)​​=Change in Costci​(si′​,s−i​​)−ci​(si​,s−i​​)​​\n\ns−i​ denotes the strategies of the remaining players.\n\nthm. Potential Games always has a PNE Intuitively this is because:\n\nWhen a player improves their strategy to reduce their own costs, the potential decreases\nThis repeats until nobody can switch their strategy\nAlso:\n\n\nThis NE is also achievable; let the system run, and it will reach NE (best-response dynamics)\nEvery local minimum of the potential function is a NE\n\nPotential Games §\n\nTraffic Routing\n"},"Power-Consumption-(Computing)":{"title":"Power Consumption (Computing)","links":["Performance-(Computing)"],"tags":["Computing/Computer-Architecture"],"content":"1.7. The Power Wall §\nUntil recently power usage and clock rate have increased together. Recently limits were reached with cooling capacity, and thus processors cannot be designed consume more power. Power consumption is summarized as:\nPower∝21​×Capacitive Load×Voltage2×Frequency\n\nCapacitive load depends on each transistor\nNote the voltage is squared\nFrequncy is the CPU clock rate\n\nMisconceptions §\n\nComputers at low utilization don’t always use little power\n\nPower doesn’t really scale linearly with performance; even computers at idle use lots of power. e.g. Google’s servers use 33% of peak power at 10% utilization.\n\nPerformance helps energy efficiency\n\nDesigning for performance means the task takes less time, and thus total energy consumption decreases.\n\nUse all three performance metrics: frequency, CPI, Instruction count\n\ne.g. MIPS (million instructions per second) is a very bad metric for measuring performance. It’s defined as MIPS=Execution time×106Instruction count​. and is also equal to CPI×106Clock rate​. So MIPS will always not take into consideration at least one of the three performance metric, and thus can be very decieving."},"Present-Value-Calculations":{"title":"Present Value Calculations","links":["Security-(Finance)","Value-of-Money","Future-Value-Calculations","interest-rate","inflation"],"tags":["Economics/Finance"],"content":"In pricing the value of a security, we consider that humans discount future returns.\n\nReason we need to discount value: Value of Money\nThis is the reverse of Future Value Calculations.\n\ndef. Discounted Cash Flows (DCF) are used to price assets.\n\nEach cash flow is discounted by the discount rate. They are summed to get the present value of all the cash flows\nWe always want asset price↑, risk↓\nrate of return ∝ volatility ← knowing how to trade off these two is important\n\nTo calculate the DCF of some future cash flow FV’s current present value PV:\nPV​=(1+kr​)t×kFV​=(1+q)nFV​​\n\nr is the Annual Percentage Rate [= quoted rate]\nk is how many times to compound every year\n\n→ thus r/k is the interest rate per compound period (=q)\n\n\nt is the number of years\n\n→ thus t×m is the number of **total compounding periods (=n)\n\n\n\nYou can calculate the present value of multiple identical cash flows of amount C:\nPV​=(1+r)1C​+(1+r)2C​+⋯+(1+r)nC​=rC​(1−(1+r)−n)​\nWhat is the value of interest rate r?\n\nIn individual investments/loans, r determined usually by looking at similar assets in the market\nr∝ inflation; if inflation is high, money isn’t worth much in the future, so lender demands more interest rate\n\nNet Present Value §\nNPV(r)=PV(r)−Initial Investment"},"Price-Controls":{"title":"Price Controls","links":[],"tags":["Economics/Micro-Economics"],"content":"Four types of price controls exist:\n\nPrice Floors\nPrice Ceilings\nTaxes\nSubsidies\n\n"},"Price-Elasticity-of-Demand":{"title":"Price Elasticity of Demand","links":[],"tags":["Economics/Micro-Economics"],"content":"ϵD​​:=%Δp%Δx​=pΔp​xΔx​​=xp​ΔpΔx​→xp​dpdx​=dlnpdlnx​​​\n\nElasticity can be:\n\n∣ϵD​∣&gt;1…Elastic\n∣ϵD​∣=1…Unit Elastic\n∣ϵD​∣&lt;1…Inelastic\nReminder that elasticity is almost always negative (for a downward-sloping demand curve (=ordinary good))\n\n\nElasticity of demand can vary along a single demand curve\n\nThere is an “elastic portion” and “inelastic portion”\nElasticity determines if a firm should produce more or less to increase revenue. \n\n\n"},"Price-to-Earnings-Ratio":{"title":"Price to Earnings Ratio","links":["EBITDA-Multiple"],"tags":["Economics/Finance"],"content":"def. Price to Earnings Ratio (P/E Ration).\nP/E=Earnings per ShareShare Price​=Net IncomeMCAP​\n⇒ Think: for two firms…\n\n…if the market values the shares higher,\n…even though the earnings are low,\n…the market thinks the firm has growth potential.\n\nHow Good is the P/E Ratio? §\n\nThe P/E ratio contains finance information so it’s a noisier measure compared to the EBITDA Multiple.\nIt’s a better measure for the pure returns you get on the share.\n→ EBITDA is a bigger, general rule of thumb, while P/E could be better for a small invester.\n\n\n\n                  \n                  fundamental analysis.\n                  \n                \n\n\n\n                  \n                  \\frac{\\text{Company Size}}{\\text{Profitability}}, thus measuring the growth of the company.\n                  \n                \n"},"Priority-Queue":{"title":"Priority Queue","links":[],"tags":["Computing/Data-Structures"],"content":"Time Complexity §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperationfind-maxdelete-maxinsertincrease-keyMergeBinary-HeapΘ(1)Θ(logn)O(logn)O(logn)Θ(n)Fibbonacci HeapΘ(1)O(logn)Θ(1)Θ(1)Θ(1)\nTournament Tree §\n\n\nGetting the largest item is O(n) which is same as the naive approach but…\nGetting the next-largest element after having built the tree is O(logn) time\n\nGetting the next-k largest elements is O(klogn) time\n\n\n"},"Prisoner's-Dillemma":{"title":"Prisoner's Dillemma","links":["Game-Theory","Econ-361-Distributive-Justice","Elinor-Ostrom","John-Rawls","Robert-Axelrod","Robert-Frank"],"tags":["Economics/Game-Theory","Economics/Game-Theory/Games"],"content":"Prisoner’s Dillemma is a symmetric game where the dominant strategy for both players is to defect, but collaboration would have yielded greater results. It is one of the most important problems that Game Theory aims to solve, and have applications in many fields. It is also known as the collective action problem or the free-rider problem. It is the cause of the Tragedy of the Commons.\n\nEcon 361 Distributive Justice is a course that dealt with this in-depth, thru the lens of PPE.\nElinor Ostrom\nJohn Rawls\nRobert Axelrod\nRobert Frank\n"},"Private-Equity-Firms":{"title":"Private Equity Firms","links":["Initial-Public-Offering"],"tags":["Economics/Finance"],"content":"Funds that have a lot of money, and use that to buy out a public company to take it private → improve its operations → IPO again to sell it at a higher price (often a big failure)"},"Private-Property":{"title":"Private Property","links":["Proletatriat-(Marxism)"],"tags":["Humanities/Marxism"],"content":"\nthe theory of the Communists may be summed up in the single sentence: Abolition of private property\n\n\nBut does wage-labour create any property for the labourer? Not a bit. It creates capital, i.e., that kind of property which exploits wage-labour, and which cannot increase except upon condition of begetting a new supply of wage-labour for fresh exploitation\n\n\nWhen, therefore, capital is converted into common property, into the property of all members of society, personal property is not thereby transformed into social property. It is only the social character of the property that is changed. It loses its class character.\n\n\nFinally, communism2 is the positive expression of the abolition of private property and at first appears as universal private property.\n"},"Probability":{"title":"Probability","links":["Cardinality"],"tags":["Math/Probability"],"content":"def. An Outcome Space (Ω) is the set of all possible outcomes of an experiment\ndef. An Event (A) is a subset of an outcome space. (die face is even)\nExample. For a 6-sided die:\nΩAA​={1,2,3,4,5,6}={2,4,6} ⊂Ω​​\ndef. Probability for countable sets. When all outcomes in Ω are equally likely, and Ω is a finite set,\nP(A)=#Ω#A​\nwhere #A denotes the number of elements in set A (its cardinality).\nProperties.\n\nP(AC)=1−P(A)\nIf A⊆B then P(A)≤P(B)\nIf A⊆B then P(B∩AC)=P(B)−P(A)\nP(A∪B)=P(A)+P(B)−P(A∩B)\nTrivial cases:\nP(∅)=0\nP(Ω)=1\nA⊆B is equivalent to A→B (i.e. if A then B)\n\nMeasure Theory Definition §\n\n\n                  \n                  Probability is just an abuse of notation \n                  \n                \n\nMotivation. We need a sigma algebra to define a probability, because there’s problematic set theory problems with uncountably infinite sets.\ndef. Sigma Algebra. A sigma-algebra on a set Ω denoted σ(Ω) contains certain subsets of A, such that it follows the following properties\n\n{∅,Ω}⊆σ(Ω)⊆2Ω (biggest and smallest possible sigma-algebras)\nlet A be a subset of Ω. If A∈σ(Ω) then AC∈Ω. (closed under complement)\nlet A,B a subset of Ω. If A,B∈σ(Ω) then A∪B∈σ(Ω) (closed under finite union)\n\ndef. Measure. let X,σ(X). A measure μ is a function from measure space to to real numbers:\nμ:σ(X)→R\nthat satisfies the following properties:\n\nNon-negative: ∀A∈σ(X), μ(A)&gt;0\nCountable Additivity: μ(A∪B)=μ(A)+μ(B)\nμ(∅)=0\n\nIntuition. Consider a measure as a means to measure the “size” of the set (not Cardinality). On a real number line, the set (2,5)≡{x∈R∣2&lt;x&lt;5} has length 3. The set (5,9) has length 4. Thus (2,5)∪(5,9)=(2,9) has length 3+4=7.\ndef. A Measure space is simply a set X and its sigma-algebra Σ together in a tuple:\n(X,Σ)\nProbability Space §\ndef. A Probability measure on X,σ(X) is a special type of measure P:σ(X)→R that satisfies P(Ω)=1\ndef. A Probability Space is like a measurable space, but also together with the probability function:\n(Ω,F,P)\nInterpretations of Probability (Philosophically) §\nMotivation. What is probability? Interpretations of its definitions seem meaningless without real-life experiments.\nTwo main interpretations of probability are:\n\nThe Objectivist Interpretation—relative frequency of occurrence, if the experiment is conducted indefinitely\nThe Subjectivist Interpretation—degree of belief; how much you would bet on an event.\n"},"Production-Function":{"title":"Production Function","links":["Homogenous-Function"],"tags":["Economics/Micro-Economics","Economics/Macro-Economics"],"content":"xx​=f(l)=f(l,k)​Short RunLong Run​​\nTerminology §\ndef. Marginal Product. The additional output in units of good x giving additional input (labor or capital)\nMPl​=dldf​,MPk​=dkdf​\ndef. Marginal Revenue Product. The additional output in units of dollars ($) given one unit of additional input (labor or capital)\ndef. Law of Diminishing Marginal Product. As the input increases over a certain point, the marginal product of the input decreases. This happens for both labour and capital in production functions.\n\nMathematically, There exists some l∗ such that:\n\n∂l∂MPl​​​l∗​&lt;0\nReturns to Scale §\nProduction functions are one of three types:\n\nDecreasing Returns to scale: tf(l,k)&lt;f(tl,tk)\nConstant Returns to scale: tf(l,k)=f(tl,tk)\nIncreasing Returns to scale: tf(l,k)&gt;f(tl,tk)\n\nMicro Production Function §\nShort Run §\ndef. SR Production function. f(l,k) where l denotes labor input, and k denotes capital input. Range is the units of output good.\n\nShort Run (SR) is defined as the timeframe where l can be varied, but k is fixed.\nAlso known as production frontiers or producer choice sets.\nWhile production frontiers can take on any shape, the most common shape is that of (b) (due to Law of diminishing marginal product).\n\n\ndef. SR Marginal Product of Labor (MPl​) The marginal change in output with a unit more of labor.\n\nIn the short-run production function, that is the gradient of the function, i.e. MPl​=dldf​.\n\n\nLong Run §\n\nMacro Production Function §\nHomogenous Function"},"Profit-Function":{"title":"Profit Function","links":["Input-Demand-and-Output-Supply","Hotelling's-Lemma"],"tags":["Economics/Micro-Economics"],"content":"def. Profit Function. Given input and output prices, returns the maximum achievable profit.\nπ(w,r,p)\n(HowTo) Derive Profit Function §\n\nGet Input Demand and Output Supply.\nSubstitute into the profit equation\n\nπ=p⋅x(w,r,p)−w⋅l(w,r,p)−r⋅k(w,r,p)\n\n\n\nProperties §\n\nHD1 in w,r,p\nIncreasing in p\nDecreasing in w,r\nHotelling’s Lemma applies\n"},"Profit-Maximization":{"title":"Profit Maximization","links":["Cost-Minimization"],"tags":["Economics/Micro-Economics"],"content":"See also Cost Minimization\nShort Run (One-input) §\n\\text{max} ~ \\pi=px-wl-r \\bar{k} ~ \\text{such that} ~ x=f(l,\\bar{k})\n$$ ...where $\\bar{k}$ is fixed at the long run optimal point.\nMaximization of profit (reaching maximum iso-profit line) against the [[Production Function]]\n- Uses [[Constrained Optimization]]\n- Optimal is where the production function is tangent to the isoprofit line: $-\\frac{w}{r}=TRS$\n\t- reminder: [[Technical Rate of Substitution|TRS]] is the tangency of the isoprofit line.\n\t- the slope of the isocost line is $-\\frac{w}{r}$\n- Result is the [[Input Demand]]s and Output [[Supply Function]]\n- As in [[Utility Maximization]], you can substitute these results into profit formula $\\pi-px-wl$ to get the [[Profit Function]].\n\nAlternative Characterization\nmax_{x,l}~p\\cdot f(l,\\bar{k})-wl-r\\bar{k}\n- Where $c(w,x)$ is the reformulation of the production function into a function of input $l$, multiplied by $w$ (= $\\text{input quantity}\\times \\text{input price}$ given a certain level of output $x$)\n- Solvable simply by finding where tangent is zero.\n- You will get short run [[Input Demand]]s \n\n## Long Run (Multiple Input)\n\nmax_{x,l,k}~ \\pi=px-wl-rk ~ \\text{such that} ~ x=f(l,k)\n- First Order Condition (FOC) is equivalent to $pMP_{L}=w,pMP_{k}=r$\n\t- i.e. produce when the additional revenue is equal to price of inputs\n\t- i.e. $MR_{L}=MC_{L},MR_{k}=MC_{k}$\n- Gets you [[Input Demand]] functions\n- ! Beware returns to scale: If Increasing Returns/Constant Returns to scale, then only one of the inputs are used. (See below for more details)\n\n## Profit Maximization Problem\n\nThere are two main ways for profit maximization:\n\n- One-step: \n\t- $\\text{max}\\ \\pi=px-wl-rk \\ \\text{ s.t. } x=f(l,k)$ to get profit-maximization condition $p\\cdot \\text{MP}_l=w, \\ \\ p\\cdot MP=r$\n\t- This gets you the input demand functions $l(p,w,r), k(p,w,r)$ and output supply $x(p,w,r)$.\n- Two-step:\n\t  1. $\\text{min} \\ c=wl+rk \\ \\text{ s.t. } \\ x=f(l,k)$ to get cost-minimization condition $-\\frac{w}{r}=\\frac{\\text{MP}_l}{\\text{MP}_k}(=\\text{TRS})$\n\t\t This gets you conditional input demand functions $l(x,w,r), k(x,w,r)$\n\t  1. Solve $p=\\text{MC}^{\\text{LR}}$ by using above conditional input demand functions (recall $\\text{MC}=\\frac{\\delta C}{\\delta x}$)\n     This gets you output supply $x(p,w,r)$\n\n\n### Special Case: Labor and Capital Are Perfect Compliments\nf(x)=min(l^\\alpha,k^\\alpha)\n- When $0&lt;\\alpha&lt;1$ it has decreasing returns to scale\n\t- → simply solve two [[Unconstrained Maximization]] problems:\n\t- $max_{l}\\pi =pl^a-wl-rk$, \n\t- $max_{k}\\pi =pk^a-wl-rk$,\n\t- $l^\\alpha = k^\\alpha$ &lt;- **dont forget!**\n\n### Special Case: Labor and Capital Are Perfect Substitutes\n$$f(l,k)=(l^\\alpha+k^\\alpha)^\\beta\n\nwhen 0&lt;α⋅β&lt;1: decreasing returns to scale\nwhen α≥1: Isoquant is bowed in the wrong direction\n\n→ Use only one of the inputs (corner solution)\n\n\nwhen 0&lt;α&lt;1 but β≥α: Isoquant may or may not be bowed in the wrong direction.\n\n→ May or may not use only one input (corner solution)\n\n\n\nExample plot to demonstrate this\nManipulate[\n Plot3D[(l^alpha + k^alpha)^beta, {l, 0, 5}, {k, 0, 5}, \n  AxesLabel -&gt; {&quot;l&quot;, &quot;k&quot;, &quot;x&quot;}, PlotRange -&gt; All], {alpha, 0.1, \n  5}, {beta, 0.1, 5}]"},"Proletatriat-(Marxism)":{"title":"Proletatriat (Marxism)","links":[],"tags":["Humanities/Marxism"],"content":"Defining the Proletatriat §\nMarx “calling into being” the class of the proles. The process of their organization organically produces the communist party as a political power.\n\nThe proletariat goes through various stages of development. With its birth begins its struggle with the bourgeoisie\n\n\nDisorganized proles\nOrganized by BZ [=companies, etc.]\n\n\nAt this stage, the labourers still form an incoherent mass scattered over the whole country, and broken up by their mutual competition\n\n\nOrganized Unions\nSeparate, permanent unions to fight for wage\n\n\nNow and then the workers are victorious, but only for a time. The real fruit of their battles lies, not in the immediate result, but in the ever expanding union of the workers.\n\n\nNationally organized communist community\n\n\nIt was just this contact that was needed to centralise the numerous local struggles, all of the same character, into one national struggle between classes.\n\n\nUltimately the formation of the communist party\n\n\nThis organisation of the proletarians into a class, and, consequently into a political party\n\nIt is unfortunately for the bz that the proles—the class whose founding was caused by capital—will eventually be the ones to destroy them:\n\nBut not only has the bourgeoisie forged the weapons that bring death to itself; it has also called into existence the men who are to wield those weapons—the modern working class—the proletarians.\n\n\nWhat the bourgeoisie therefore produces, above all, are its own grave-diggers. Its fall and the victory of the proletariat are equally inevitable.\n"},"Proof-Techniques":{"title":"Proof Techniques","links":[],"tags":["Math"],"content":"Algebraic\n\nYou can split summations (even infinite ones)\n∏i​eXi​=e∑i​Xi​\nEquivalently, ln(∏i​Xi​)=∑i​(lnXi​) Pushing log thru product makes it a sum\nTelescoping sums and products\n\nProbability\n\nTower property E(X)=E(E(X∣F))\nLinearity of expectation. You can switch expectation and summation\nDefinition of expectation is an integral (or summation)\n\nCalculus\n\n∫x​∫y​dydx=∫y​∫x​dxdy switch integrals\n\nInequalities\n\nSum greater than max\nMax greater than averages\nAverage greater than min.\nCombined:\n\n\nln(1−x)≤−x always."},"Pushdown-Automata":{"title":"Pushdown Automata","links":[],"tags":["Computing/Formal-Languages"],"content":"def. Pushdown Autotmaton (PDA)\nM=(Q,Σ,Γ,δ,q0​,z,F)\nWhere:\n\nΓ: stack alphabet\nz: stack bottom market\nδ is a transition function where for q,q′∈Q,σ∈Σ,γ∈Γ\n\n(q,σ,γ)↦{(q′,γ∗),...}\n…where the destination set is a finite set.\n…and strings can be accepted by either final state xor an empty stack (equivalent)\n\n\n                  \n                  We’re going to deal mostly with nondeterministic pushdown automata (NPDA), as they are more useful. \n                  \n                \n\nDescribed mechanically:\n\nInput tape is read only once, left to right\nA read head has finite number of states\nA read head has a stack from which the top letter can be read off\n\n\ndef. the current Configuration of a PDA is described as a tuple\n(q,aw,bx)⊢(q′,w,x)\nwhere ⊢ represents a single step of a PDA.\n\n\n                  \n                  final state or an empty stack; these two definitions are equivalent.\n                  \n                \n"},"Python-Common-Operations":{"title":"Python Common Operations","links":[],"tags":["Computing","Computing/Algorithms"],"content":"\nInitialize integer as positive or negative infinity: {python} i: int = float(&#039;-inf&#039;|&#039;inf&#039;)\n\nCollections §\nSets §\n\nSet difference{python} set(list1) - set(list2)\n\nO(n)\n\n\nSet lookup {python}i in {1, 2, 3}\n\n==Constant time O(1) lookup==\n\n\n\nList §\n\nList Sort {python} list2 = sorted(list1)\n\nO(nlogn)\n\n\nDeduplication{python} list1 = list(set(list1))\n\nO(n)\n\n\nCounter {python}Counter(&quot;mississippi&quot;)\n\nreturns {python}{&#039;i&#039;: 4, &#039;s&#039;: 4, &#039;p&#039;: 2, &#039;m&#039;: 1}\nO(n)\n\n\nAccumulate{python} accumulate(list1)\n\nO(n)\n\n\n\nDictionary §\n\nDefault value for dictionary entries{python}dictionary = collections.defaultdict(list|int)\nHelps when you have a list of dicts, or accumulating a count in a dict.\n"},"Quantitative-Problem-Solving-Tips":{"title":"Quantitative Problem Solving Tips","links":[],"tags":["Math","Computing","Meta-Learning"],"content":"\nFind the invariant.\n\nWhat does not change? e.g. radius in a circle\n\n\nFind identities.\n\nCan we keep the problem, but invert our perspective?\n\n\n"},"Random-Variable":{"title":"Random Variable","links":[],"tags":["Math/Probability"],"content":"def. Random Variable A Random Variable X from probability space (Ω,F,P) to measurable space (R,B) is a function:\nX:Ω→R\n\nB is the Borel sigma-algebra on real numbers (=Borel set)\n\nThis is a set of open intervals on R that satisfies σ-algebra properties\n\n\nfor all B∈B, it is true that X−1(B)={ω∈Ω∣X(w)∈B}∈F\n\nYou’re given an open interval (=set) B on real numbers\nGet all ω such that X(ω) is within this range (=pre-image)\nThis set (a subset of Ω) must be in the sigma-algebra of Ω\nThis applies to any range B=(a,b)\n&amp; We can then say ”X is F-measurable” or ”F has enough information to measure X“.\nIntuition. Let there be other sigma-algebras G,H such that G⊂F⊂H, in addition to X being F-measurable. In this case…\n\nX is H-measurable because H has more information than F.\nHowever, X is not G-measurable because G has less information than F.\n\n\n\n\n\ndef. Probability on a Random Variable. Probability function on random variable X is a function PX​:R→[0,1] such that:\nPX​(B):=P(X−1(B))=P({ω∈Ω∣X(ω)∈A})\n\n&amp; i.e., the probability we encounter daily P(X…) is simply a shorthand notation PX​=P∘X−1\nYou put the interval B, and get the probability of all ω in that interval’s pre-image\nthus by abuse of notation we write: PX​(B)≡P(X∈B)≡P(X−1(B))\n\nExample. Let Ω is the trajectory of a coin toss. This is a very big set, and probably also infinite. On the other hand, let X a random variable for\nX={10​if headsif tails​\n\n\n\nThis is much simpler and more useful.\n\n\nThis means X−1(1)={All trajectories that land in heads}.\nIf X−1∈F, then we can try to measure the probability of P(X−1(1))=P({All trajectories that land in heads})\nBecause we know the pre-image of 1 is in F, we know that P(X−1(1)) is defined.\n\nthm. Addition Rule for Random Variables. For a discrete random variable X:\nP(a≤X≤b)=k=a∑b​P(X=k)\nFunctions of Random Variables §\nMotivation. Functions can be made of random variables; for example let Y=∣X−1∣. In order to investigate Y, need a way to derive the probability distribution of Y from X.\nExample. Let Y be a random variable defined by a function of another random variable X; Y=f(X). Then:\nP(Y=y)=P(f(X)=y)=all x s.t. f(x)=y∑​P(X=x)\nthm. Random variables X,Y are equal when:\n\nRange(X)=Range(Y)\n∀k∈Range​P(X=k)=P(Y=k)\n\nIndicator Functions §\ndef. Indicator Functions. For event A⊂Ω, the indicator function IA​ is a random variable (i.e. function) such that:\nIA​:Ω→{0,1}s.t.IA​(ω)={01​ω∈/Aω∈A​\nRemark.\n\nIndicator functions are useful in probability for solving problems, not for being a fundamental mathematical object.\nRemember that indicator functions are also random variables. All the rules for random variables apply, including the identities for expected values.\n\nProperties. let I an indicator function describing an event with probability p, then:\n\nE(I)=p\nVar(I)=p(1−p)\n"},"Rationality-(Economics)":{"title":"Rationality (Economics)","links":["Utility-Function","Monotonic-Transformation","HD","Marginal-Rate-of-Substitution-(MRS)"],"tags":["Economics"],"content":"Microeconomics Assumptions §\ndef. Rational Preference. Consider two basket of goods A,B. A rational preference is one that is all of the following:\n\nComplete: either A≻B,B≻A,or A∼B\n\nComparable: You have to choose either of the above three\n\n\nTransitive: if A≻B and B≻C, then A≻C\nMonotonic: x1​⪰x2​\n\nThe following are optional conditions\n\n\nConvexity: Averages are bette than extremes. (Preference of variety)\n\n\nThe rational preference assumption and the convexity assumption will together be enough to define an Utility Function.\n\n\nTwo tastes are same if they have the same utility function, or the utility functions are Monotonic Transformations of one another.\n\n\nNotation §\n\nA∼B: A is equally preferable with B\nA≻B: A is strictly preferred to B\nA≿B: A is equally or more preferable to B\nA∼B: A is equally preferred to B (indifferent)\n\ndef. Homothetic Tastes. Two equivalent definitions\n\nTastes are homothetic if the utility function (or its Monotonic Transformation) is HD of degree k&gt;0\nTastes are homothetic if the MRS depends only on x1​x2​​\n\n​u(x1​,x2​) is homogenous of degree k⟹u(tx1​,tx2​)=tku(x1​,x2​)​​\n\nQuasilinear Taste §"},"Readers-Writer-Locking":{"title":"Readers-Writer Locking","links":[],"tags":["Computing/Algorithms"],"content":"Solution to concurrency control problem.\n\nThere are two locks: read lock and write lock\nOnly those with a read lock can read, and write lock can write\nThere must always be either\n\nMany reader locks, no writer lock\nOne writer lock, no reader lock\n\n\n"},"Recurrence-Relation":{"title":"Recurrence Relation","links":["most"],"tags":["Computing/Algorithms"],"content":"Table of Common Recurrence Relations §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecurrenceAlgorithmSolutionT(n)=T(2n​)+O(1)binary searchO(logn)T(n)=T(n−1)+O(1)sequential searchO(n)T(n)=2T(2n​)+O(1)tree traversalO(n)T(n)=T(2n​)+O(n)quick selectO(n)T(n)=2T(2n​)+O(n)mergesort, quicksortO(nlogn)T(n)=T(n−1)+O(n)Insertion or selection sortO(n2)\nQuick Bits §\n\nT(n)≤T(a⋅n)+T(b⋅n)+cn where a+b&lt;1 is almost always O(n).\n\nMaster Theorem §\nSee Master theorem (analysis of algorithms) - Wikiwand\n⇒ Can be used to solve most recurrence relations.\nFirst, Identify the recurrence relation in the following form\nT(n)=a⋅T(bn​)+O(nclogkn)\nConsider three casesp\n\nCase 1: logb​a&lt;c\n\n⇒ T(n)=Θ(nc)\n\n\nCase 2: logb​a=c\n\n⇒ T(n)=Θ(nclogk+1n)\n\n\nCase 3: logb​a&gt;c\n\n⇒ T(n)=Θ(nclogkn)\n\n\n\nRecurrence Tree §"},"Recursively-Enumerable-Languages":{"title":"Recursively Enumerable Languages","links":[],"tags":["Computing/Formal-Languages"],"content":"Section: Turing Machines\ndef. Language L is recursively enumerable iff there exists a TM M such that L=L(M).\n→ The class of languages that TMs can represent is a recursively enumerable language. You can think of languages which are “countable.”\ndef. Language L is recursive iff there exists a TM M which, for every string w in L, it halts.\n→ Recursive Languages are languages in which a TM always halts.\n\nDistinction between RE and R languages.\n\n\nlemma. If S is a countable set, then 2S [= the set of all subsets of S] may not be countable.\nthm. There exists languages over alphabet Σ that are not recursively enumerable. (proof using lemma.)\nthm. If L is recursively enumerable, Lˉ may not be RE.\nthm. If L and Lˉ are both RE, then L and Lˉ is Recursive.\nthm. If L is recursive, Lˉ is recursive."},"Reduced-Price-of-Capital-Goods":{"title":"Reduced Price of Capital Goods","links":[],"tags":["Economics/Game-Theory"],"content":"The most significant thing about technological improvement is equality.\nNobody cares if you are or aren’t from a noble family; if you have money it’s fine.\n\nThink—pride and prejudice society was rejecting those who have gained prestige with just money.\nThink also: discussion with mom, when she says “technological improvement is irrelevant; where there are people, there is love and relationship.” Only those who are lucky enough to be born in the modern age has that prestige of emotion, etc. Also think: the hindsight paradox.\n\n(DevonThink) My Ordinary Life: Improvements Since the 1990s · Gwern.net (reader mode)"},"Refactoring-Reduces-Cognitive-Load":{"title":"Refactoring Reduces Cognitive Load","links":["Working-Memory"],"tags":["Computing"],"content":"Refactoring is useful in coding because it reduces the cognitive load to Working Memory."},"Refinancing":{"title":"Refinancing","links":["Federal-Funds-Rate"],"tags":["Economics/Finance"],"content":"e.g. refinancing a mortgage\n→ When the Federal Funds Rate decreases and therefore the general interest rate decreases, your mortgage payments can be payed off by another new loan you take."},"Regular-Expressions":{"title":"Regular Expressions","links":["Finite-Automata","Regular-Languages"],"tags":["Computing/Formal-Languages"],"content":"Section: Finite Automata = Regular Languages\nRegular expressions have three operators:\n+  ∗  ∘  ​UnionZero or MoreConcat​\n\nBegin with ϕ,λ,Σ∈RegEx\nAll above operators on the three, and the alphabet, is a regular expression\n\n\n\n                  \n                  Example \n                  \n                \n→ Odd number of a’s, and then even number of b’s: a(aa)∗(bb)∗\n→ 3 or less a’s and ends in ab: b∗(a+λ)b∗(a+λ)b∗ab\n\nRegEx ≡ DFA §\npf. RegEx ⇒ DFA\n\nFirst define the building blocks\n\n\nRepresentation of ϕ\nRepresentation of {a}\n\nRepresentation of λ\n\n\nThen buid on them recursively:\n\n\npf. DFA ⇒ RegEx\n\nWith a DFA with one final state, convert into a generalized transition graph (GTG) [=edges can be a RegEx]\nIf the GTG has two states:\nUnique RegEx for this DFA: (rii∗​rij​rjj∗​rji​)∗rii∗​rij​rjj∗​\n\nIf GTG has three or more states it is in the form:\n\n\n"},"Regular-Grammar":{"title":"Regular Grammar","links":[],"tags":["Computing/Formal-Languages"],"content":"Section: Finite Automata\ndef. Right Linearity. A grammar G=(V,T,S,P) is right-linear iff all elements in the range of the production rules Range(P) only contain variables on the right side of the result; i.e.:\nS→aP∣aSP→a\n\n\n                  \n                  Observe that a right-linear grammer is same as being able to extend it only on the right. Left-linear grammar is vice versa \n                  \n                \n\ndef. A Regular Grammar is a grammar that is exclusively either right-linear or left-linear.\nRegGrammar ≡ DFA §\npf. Regular Grammar ⇒ DFA\nfor G=(V,T,S,P), for each rule in P, convert a variable into a state; arcs identify the rightside (or leftside) terminal variable\npf. DFA ⇒ Regular Grammar"},"Regular-Languages":{"title":"Regular Languages","links":["Finite-Automata"],"tags":["Computing/Formal-Languages"],"content":"Equivalent to Finite Automata\ndef. Regular Languages are languages that can be described by either\n\nFinite Automata\nRegular Grammar\nRegular Expressions\n\nClosure of Regular Languages §\nRegular languages are closed under the following operations:\n\nUnion, Intersection\nConcatenation, Star\nCompliment\nQuotient\nL1​/L2​={w∣∃x∈L2​ s.t. wx∈L1​} ← left quotient\nL1​\\L2​={w∣∃x∈L2​ s.t. xw∈L1​} ← right quotient\nHomomorphism\n\nHomomorphism is a function h s.t.:\n\n\n\nh:\\Sigma\\mapsto\\Gamma^*\n$$So: $w=a_1a_2…a_n \\Rightarrow$ $h(w)=h(a_1)h(a_2)…h(a_n)$\n\n## Pumping Lemma and Disproving Regularity\n\nlem. **Pumping Lemma.** let regular language $L$, and $w\\in L$ whose length is $m$ or greater. Then $w$ can be decomposed s.t.:\n\n1. $w=x\\circ y \\circ z$\n2. $xy$ has a length no larger than pumping length $m$\n3. $y$ is not an empty string\n\nIn this case, a newly constructed string $w’=x\\circ y^i \\circ z$ must also be in language $L$.\n\nProof by using the pumping lemma should be:\n\n1. Choose **any** single string $w$ in the language and express it in terms of a positive int. $m$\n2. Decompose the string into $x,y,z$, **where** $|xy|\\leq m$\n3. Show that $x\\circ y \\circ z$ is not in $L$, for a **certain** $i=0,1,…$\n   → i.e. pick an i, and show that it doesn’t work if m is positive\n4. Contradiction!\n\n---\n\nIn general, you can disprove regular languages by knowing\n\n- Finite languages are always regular\n- the Pumping Lemma"},"Relational-Algebra":{"title":"Relational Algebra","links":["Turing-Machine","Database-Management-System","Monotonic-Transformation"],"tags":["Computing/Data-Science"],"content":"(DevonThink) 2. Relational Algebra\nMathematical formulation of operations on relational data.\n\nMade by E.F. Codd\nEquivalent to domain-independent relational calculus\n\nRelational calculus is a specialization of first-order logic\n\n\nIsn’t Turing-complete because it lacks recursion.\n\nBut because of this, relational algebra is decidable\n\n\nHigh level and declarative (procedural implementation is left to DBMS)\nEasy to optimize by the DBMS\n\nDefinitions §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDBMS jargonCommon-sense nameRelationTableAttributeColumnDomainData typeTupleRow\n\nA row is defined as a tuple\n\nDuplicate rows are not allowed\nA tuple is considered same if all attributes are same.\n\n\nSchema: the tuple of attribute names (≈class)\nInstance: the instantiation of the schema (≈object)\nKey (see below for formal definition):\n\ne.g. address(street_addr,city,state,zipcode)\n\n⇒ {street_addr,city,state} is a key\n\n..but {street_addr,state} is not a key\n\n\n⇒ {street_addr,zipcode is also a key\n\n\n\n\n\ndef. Superkey. A set of attributes K is a superkey of relation R if there exists a dependency:\n\ndef. Key. A superkey K is a key of relation R if there is no other superkey with smaller number of attribute elements. (=minimally identifies each tuple/row)\nalg. Determining if a superkey is a key.\n\nReduce one element and see if the key is superkey.\nIf a reduction of one element doesn’t yield a smaller superkey, we got it.\nIf it does, recurse.\n\n\nRelational Operators §\nFundamental Operations §\n\nSelection: σp​⋅R\n\n“Give me the entries of relation R that satisfies condition p”\n\n\nProjection: πL​⋅R\n\n“Give me the attribute L for each entry of relation R”\nDuplicates are filtered out\n\n\nCross Product: R×S\n\nSame as cross product of normal sets.\n⇒ Every row of R per every row of S\n\n\nUnion: R∪S\n\nR,S must have identical schema\nAll rows on R,S combined\nDuplicates are removed\n\n\nDifference: R−S\n\nR,S must have identical schema\nRows in R that are not in S\n\n\nRenaming: ρS​⋅R\n\nRename attribute S of relation R\n\n\n\nDerived Operators §\n\nJoin [=Theta Join]: ⋈p​\n\ndefinition: σp​(R×S)\n“Cross Product the relations, and then filter by condition p”\ni.e. “Join the two databases so based on condition”\n\n\nNatural-Join: ⋈\n\ndefinition: πL​(R⋈p​S)\n\nwhere p pairs common attributes\nwhere L is the union of the attribute names\n\n\n“Theta join, but the duplicate attribute is removed”\n\n\nOuter-Join\n\nLeft outer join: R ⟕ S (leave null entries from R)\nRight outer join: R ⟖ S (leave null entries from S)\nFull outer join: R ⟗ S (leave null entries from R,S both)\n\n\nIntersection: R∩S\n\nR,S must have identical schema\n\ndefined as R−(R−S)≡S−(S−R)\n\n\n\n\nSymmetric Difference: (R−S)∪(S−R)\n\nMonotonicity §\n(Vaguely related to: Monotonic Transformation)\ndef. Monotonic Operation. If input adds more rows, does the output also only add more rows? i.e. f is a monotone operator iff:\nR⊆R′⟹f(R)⊆f(R′)\n\nDifference (R−S) is the only non-monotone operator among simple operators\n\nMonotone for R but not for S\n\n\nIf an operation is not monotone, it must include the difference operator.\n\nExpression Tree Notation §\n\n\nSimpler to use and understand\nDesigned both bottom-up or top-down\n\nProperties of Relational Algebra §\n\n\nSelection Properties\n\nIdempotent: σA​(R)=σA​σA​(R)\nCommutative: σA​σB​(R)=σB​σA​(R)\nConjunction: σA∧B​(R)=σA​(σB​(R))=σB​(σA​(R))\nDisjunction: σA∨B​(R)=σA​(R)∪σB​(R)\nBreaking up: σA​(R×P)=σB∧C∧D​(R×P)=σD​(σB​(R)×σC​(P))\nDistribution over Difference: σA​(R∖P)=σA​(R)∖σA​(P)=σA​(R)∖P\nDistribution over Union: σA​(R∪P)=σA​(R)∪σA​(P)\nDistribution over Intersection: σA​(R∩P)=σA​(R)∩σA​(P)=σA​(R)∩P=R∩σA​(P)\n\n\n\nSelection and Projection Commutativity\n\nπa1​,…,an​​(σA​(R))=σA​(πa1​,…,an​​(R)) where fields in A⊆{a1​,…,an​}\n\n\n\nProjection Properties\n\nIdempotent: πa1​,…,an​​(πb1​,…,bm​​(R))=πa1​,…,an​​(R) where {a1​,…,an​}⊆{b1​,…,bm​}\nDistributing on Union: πa1​,…,an​​(R∪P)=πa1​,…,an​​(R)∪πa1​,…,an​​(P)\n! Projection doesn’t distribute along difference:\n\nπA​({⟨A=a,B=b⟩}∖{⟨A=a,B=b′⟩})={⟨A=a⟩}\nπA​({⟨A=a,B=b⟩})∖πA​({⟨A=a,B=b′⟩})=∅\n\n\n! Projectio doesn’t distribute along intersection:\n\nπA​({⟨A=a,B=b⟩}∩{⟨A=a,B=b′⟩})=∅\nπA​({⟨A=a,B=b⟩})∩πA​({⟨A=a,B=b′⟩})={⟨A=a⟩}\n\n\n\n\n\nRename Properties\n\nρa/b​(ρb/c​(R))=ρa/c​(R)\nρa/b​(ρc/d​(R))=ρc/d​(ρa/b​(R))\nDistribution on Difference: ρa/b​(R∖P)=ρa/b​(R)∖ρa/b​(P)\nDistribution on Union: ρa/b​(R∪P)=ρa/b​(R)∪ρa/b​(P)\nDistribution on Intersection: ρa/b​(R∩P)=ρa/b​(R)∩ρa/b​(P)\n\n\n\nProduct and Union\n\nCartesian Product and Union: (A×B)∪(A×C)=A×(B∪C)\n\n\n"},"Revenue-Maximizing-Auctions":{"title":"Revenue-Maximizing Auctions","links":[],"tags":["Economics/Game-Theory"],"content":"Motivation. Sometimes we aim to maximize revenue instead of social welfare.\n\nIn this case, we assume that we know a little bit about the bidder’s private valuations—we know the distributions vi​∼Fi​. (we call this a baysian auction.)\n\nThis is because unless we do this, the revenue maximization is impossible, since it is hard to guarantee DSIC.)\n\n\nThen, in a DSIC auction M=(x(b),p(b)), the revenue maximization goal can be phrased as:\n\nmaxx,p​ E(∀i∑​pi​(v))\nIn order to do to this, we introduce a new concept, and as we will see later it will simplify the above problem.\ndef. Virtual Value. For a bidders’ true valuation vi​ whose CDF is Fi​(vi​), PDF is fi​(vi​), the virtual value is defined as:\nφ(vi​):= want to collect vi​​​− information penalty fi​(vi​)1−Fi​(vi​)​​​\nProperties.\n\nφ(vi​)≤vi​\nφ(vi​) may be negative\n\nIntuition. As annotated above, treat vi​ as the amount of money we would like to collect from bidder i. But this is discounted by a certain factor (=“information penalty”) because we know a bit of information about them—their distribution.\nWe can rephrase the above revenue maximization problem in terms of virtual value as the following:\nthm. (expected virtual welfare is expected revenue)\nE​ revenue ∀i∑​pi​(v)​​​=E​ virtual welfare ∀i∑​φi​(vi​)xi​(v)U​​​\nMotivation. Now the revenue maximization goal is simply to maximize expected virtual welfare. However, we still need to make sure the bidders are truthful, because we need to maximize over v.\n\n⇒ Using Myerson’s lemma, we can create auctions that are truthful. The following are conditions of Myerson’s lemma, applied in the context of virtual value.\nThen, we will give an example of this in a single-item many-bidder auction where these conditions hold.\n\ndef. Regular Distribution. A distribution is regular iff if its virtual value function is monotonic.\ngame. (Baysian single item auction).\n\nAssuming that all bidder’s values follows an i.i.d. regular distribution, thus making this auction DSIC, i.e. bi​=vi​\nCalculate virtual values φ1​,…,φn​ for all bidder’s bids v1​,…,vn​\nGive the item to the bidder with the highest virtual value φ1​, unless the highest bidder’s virtual value is below zero—φ1​&lt;0.\n\nThe the highest bidder’s virtual value is below zero, throw away the item. Auction is over.\n\n\nCharge them max(v2​,φ−1(0))—either the price corresponding to zero virtual value, or the second-price of the second highest virtual value.\n\nCharge payments according to Myerson’s formula.\n\n\nThis auction will maximize revenue.\n\nVisual Interpretation. An example of three different cases where different distributions lead to different virtual value\n\nthm. (General case of revenue maximization.)\n\nAssuming Fi​ is regular for every bidder i…\n\n\nTransform the truthfully reported valuation vi​ into corresponding virtual value φi​(vi​)\nAllocate by maximizing virtual welfare: x(v)=argmaxX​∑∀i​φi​(vi​)⋅xi​\nCharge payments p(v) according to the\n"},"Riemann–Stieltjes-integral":{"title":"Riemann–Stieltjes integral","links":[],"tags":["Math/Calculus"],"content":"def. Riemann-Stieltjes Integral. let functions f(x),g(x):R→R. Then the R-S integral with f(x) as the integrand and g(x) as the integrator is defined:\n∫ab​f(x)dg(x)=n→∞lim​i=0∑n​f(xi​)[g(xi+1​)−g(xi​)]​​\nwhere a=x0​≤⋯≤xn​=b\nVisualization. The value of the integral is the shadow projection of the purple fence (=from x−g plane to the intersecting line of f(x) and g(x)). The lower the slope of g(x), the denser the shadow is. See: Riemann–Stieltjes integral - Wikiwand\n"},"Risk-(Finance)":{"title":"Risk (Finance)","links":["Credit","Variance","Bond-Price"],"tags":["Economics/Finance"],"content":"def. Risk is related to the uncertainty in payment back when giving a loan.\n\nExample of a riskless investment is investing into the bank upto the federal deposit insurance (~$100K)\nTypes of risk include:\n\nCredit Risk (=counterparty risk): the counterparty can fail to make the payment\nInterest Risk: the market interest rate might change\nInflation Risk: the inflation is high so the money isn’t worth much\n\n\nIncreased cash flow frequency usually means less risk.\n\ndef. Risk in finance is equivalent to Standard Deviation in mathematics.\n\nRisk is proportional to the duration of investment (See bond yield curve).\n"},"Risk-Neutral-Assumption":{"title":"Risk-Neutral Assumption","links":["Present-Value-Calculations","No-Arbitrage"],"tags":["Economics/Finance"],"content":"Risk-Neutral Assumption §\nA risk-neutral world is where there is no risk premium on return. This implies:\n\nAll Present Value Calculations are done at the risk-free rate\nAll assets have risk-free rate of return (=zero riNo-Arbitrageitrage Condition]] holds\n&amp; This is not really realistic, but simplifies calculations a lot.\n"},"Robert-Nozick":{"title":"Robert Nozick","links":[],"tags":["Economics/Game-Theory","People"],"content":""},"Rock-Paper-Scissors":{"title":"Rock Paper Scissors","links":[],"tags":["Economics/Game-Theory/Games"],"content":""},"Roy's-Identity":{"title":"Roy's Identity","links":["Utility-Function","Uncompensated-Demand-curve"],"tags":["Economics/Micro-Economics"],"content":"thm. Roy’s Identity. Relates income, price, and Indirect Utility Function to a own-price demand function.\nx1​(p1​,p2​,I)≡−∂v/∂I∂v/∂p1​​x2​(p1​,p2​,I)≡−∂v/∂I∂v/∂p2​​​​"},"SQL-Basics":{"title":"SQL Basics","links":["Relational-Algebra","Screenshot-2023-10-13-at-15.18.16.png"],"tags":["Computing/Data-Science"],"content":"**## Based on Relational Algebra\nSQL is more readable than Relational Algebra.\nSPJ Query §\nSelect-Project-Join (SPJ) query. Is what almost all queries are. in the form of:\nπattr1, attr2, ...​(σcondition​(relation1×relation2×…))\n…is equivalent to…\nSELECT attr1, attr2, ...\nFROM relation1, relation2, ...\nWHERE condition\n⇒ Think: you are checking {postgresql} condition on the whole cross product of relation1, relation2, …\nCanonical form of a single query\nSELECT ... AS ..., ... AS ...\nFROM Relation_1 r1, Relation_2 r2 -- Rename the table.\nWHERE ... AND ... -- Where condition\n\t(NOT) IN, (NOT) EXISTS, ALL, SOME (subtable) -- Nested Query\n\t-- exist doesn&#039;t need an attribute\nORDER BY ASC|DESC\nJoins, Unions and Differences §\n\nSELECT ...\nFROM ...\n-- Join query\n[INNER, OUTER] [LEFT, RIGHT, FULL] JOIN \n-- INNER JOIN equivalent to JOIN\n-- LEFT|RIGHT JOIN equivalent to LEFT|RIGHT OUTER JOIN\n(subtable) T -- rename \nON join_condition\n\n-- optional WHERE clause\nWHERE ...\n\n-- Set/Bag operation query\n(subtable) UNION, INTERSECT, EXCEPT (ALL) (subtable)\n\n\ntable references scoping is like other languages\nsubtables can be one tuple &amp; one column (=scalar), where you can do a direct comparison: {postgresql}WHERE age [&gt;=&lt;] (subtable)\n\n…but this will cause runtime error if the subtable has more than one value.\n\n\n{postgresql}SELECT DISTINCT to remove duplicates\n{postgresql}EXCEPT/INTERSECT/UNION ALL is a bag operation that removes by count; Example data\n\nConditionals §\nSELECT\nFROM\n&#039;string literal&#039; -- should be enclosed in single quotes\nWHERE string LIKE &#039;%foobar%&#039;-- pattern matching strings\n\n-- Null checking\nWHERE attr IS NULL\nWHERE attr IS NOT NULL\n-- etc.\nNull Handling Rules. §\n\n\n                  \n                  NULL and Unknown as a 0.5 value\n                  \n                \n\n\nComparing NULL with any value will result in UNKNOWN\nValues as numbers:\n\nTRUE=1\nFalse=0\nUNKNOWN=0.5\n\n\nOperations as functions:\n\n{postgresql}x AND y=min[x,y]\n{postgresql}x OR y=max[x,y]\n{postgresql}NOT x=1−x\n\n\n\nWhat about handling null results?\n\nSolution 1: {postgresql}SELECT COALESCE(col,0.0)\n\n⇒ if {postgresql}col = NULL, will return 0.0\n\n\nSolution 2: {postgresql}SELECT NULLIF(col,&#039;n/a&#039;)\n\n⇒ if {postgresql}col = &#039;n/a&#039;, will return NULL\n\n\n\nSQL Extensions §\nAggregation §\nSELECT COUNT(*)|COUNT(DISTINCT col)|AVG(col)|MIN(col)|MAX(col)\nFROM ...\nWHERE ...;\n\nAn aggregation in a HAVING clause applies only to the tuples of the group being tested.\nAny attribute of relations in the FROM clause may be aggregated in the HAVING clause, but only those attributes that are in the GROUP BY list may appear unaggregated in the HAVING clause (the same rule as for the SELECT clause).\n\nGrouping §\nSELECT aggr_func FROM [JOIN ON | UNION | ...] WHERE\n-- Group By\nGROUP BY col1, col2 -- will group if *both* columns are same\nHAVING condition_on_group\n\nCompute Order:\n\n{postgresql}GROUP BY\n`{postgresql}HAVING\n{postgresql}SELECT\n\n\naggr_func is computed for each group\n\ne.g. {postgreSQL}SELECT age, AVG(pop) FROM User GROUP BY age computes the average popularity for each age.\ne.g. {postgresql} SELECT uid, MAX(pop) FROM User is wrong ← aggregate function and column cannot be used together (which uid?)\ne.g. {postgresql}SELECT uid, age FROM User GROUP BY age is wrong ← which uid?\n“\n\n\n\nVariables §\nTwo forms of “variables”\n\nNamed Subqueries (will evaluate by macro expansion)\nViews (is a window into a table)\n\nProvides Logical data independence\n\n\n\n-- Named subqueries\nWITH subtable_name AS (subquery)\n\n-- Views\nCREATE VIEW view_name AS (subquery)\nDROP VIEW -- dropping a view\nData Input/Output §\nINSERT INTO ... (subquery) ...\nDELETE FROM ... [WHERE ...]\nUPDATE ... SET ... [WHERE ...]\nAdvanced Topics §\n\nTransaction\n\nAllows multiple queries to be treated as one (helps when making new tables with foreign key relations)\n\n\nObject-Relational Mapping\n\nConvert between object-oriented language’s object data into SQL-compatible tuples, and vice versa\n\n\nCasting\n\nConvert between datatypes {postgresql}CAST(num AS FLOAT)\n\n\n\nRecursion §\n\nYou can define new subtables with recursion\n\n{postgresql}WITH RECURSIVE subtable AS (recursive_query)\n\n\nSQL does fixed point recursion\n\n…i.e. recurses until the table doesn’t seem to change anymore\n\n\nLinear vs Non-linear recursion\n\nLinear: a single recursive call\nNon-linear: a tree-like recursion; multiple recursive calls\n\n\nMutual Recursion\n\nTwo tables are defined upon each other\n\n\n"},"SQL-Constraints":{"title":"SQL Constraints","links":[],"tags":["Computing/Data-Science"],"content":"Constraint Checking §\nSchema Declaration §\nCREATE TABLE User(\n\tuid INTEGER NOT NULL PRIMARY KEY, -- primary key\n\tage INTEGER NOT NULL UNIQUE -- not a key, but is unique\n\t\t    CHECK(age IS NULL OR age &gt; 0), -- constriant\n);\n\nCREATE TABLE Group(\n\tgid CHAR(10) NOT NULL PRIMARY KEY,\n\tname VARCHAR(100) NOT NULL UNIQUE\n);\n\nCREATE TABLE Member (\n\tuid INTEGER NOT NULL REFERENCES Group(uid), -- foreign key\n\tgid CHAR(10) NOT NULL, \n\tPRIMARY KEY(uid, gid) -- another way to write key\n\tFOREIGN KEY(gid) REFERENCES Group(gid) -- anoter foreign key\n);\n\nWhen constraint check fails during a query, will reject (maybe cascade, or set null)\nDeferred constraint checking: constraint check only happens for a full declaration or transaction\n{postgresql}CHECK will accept NULL values even if not clearly specified\n\nAssertions §\nCREATE ASSERTION assertion_name CHECK condition"},"SQL-Query-Optimization":{"title":"SQL Query Optimization","links":[],"tags":["Computing/Data-Science"],"content":"\n\n                  \n                  Problem: How to actually process this: \n                  \n                \n\n\nBasic Strategies §\n\nSQL Query rewriting\n\nMake everything into joins!\n\n\nDe-correlation: Correlated subqueries are de-correlated using “Magic” de-correlation\nIterated (=pipelined) algorithm processes\n\nProcess one tuple, up the chain, one at a time\nWill start producing results faster, but may not be fast in total\n\n\nBottom-up Evaluation\n\nProcess the bottommost query, then up one level, etc.\nUse temporary files to store intermediate results\n\n\n\nHeuristics-based Optimization §\n\nIdea: estimate size of intermediate results to calculate total operation count\n\nCardinality estimation\n\n\nGiven knowledge: ∣πA​R∣,∣R∣\nPrinciples:\n\nPreservation of Value\n\n\n\nSelection\n∣σA=val​∣∣σA=val​∣​≈∣R∣⋅∣πA​R∣1​=Distict A values in RSize of R​≈∣R∣⋅(1−∣πA​R∣1​)​​\n\ndivide by the “selectivity factor”\n\nConjunction, Disjunction (AND, OR operations)\n\nBest when A,B are independent columns\n\n∣σA=u∧B=v​∣∣σA=u∨B=v​∣​≈∣R∣⋅∣πA​R∣⋅∣πB​R∣1​≈∣R∣⋅(∣πA​R∣1​+∣πA​R∣1​−∣πA​R∣⋅∣πB​R∣1​)​ConjunctionDisjunction​​\nRange\n\nWithout max,min values: just say 31​\nWith max=hi(R.A) and min=lo(R.B)\n\n&amp; sometimes, highest and lowest is “invalid” → use second highest &amp; lowest\n\n\n\n∣σA&gt;v​∣≈∣R∣⋅max−minmax−v​\nJoins Estimation §\nNatural Join\n\nAssumption: containment—every tuple in smaller table joins with bigger table\n⇒ selectivity factor is the bigger one one\n\n∣R⋈S∣≈∣R×S∣⋅max(∣πA​R∣,∣πA​S∣)1​\nMulti-way Join\n\nAssumption: preservation of value sets—non-join attribute doesn’t lose values\n⇒ reduce by selectivity factor for every join\n\n∣R(A,B)⋈S(B,C)⋈T(C,D)∣≈∣R×S×T∣⋅selectivity of first join∣πB​R∣,∣πB​S∣1​​​⋅selectivity of second join∣πC​S∣,∣πC​T∣1​​​\nProjection over Join\n\nDue to assumption of preservation of value sets…\n…when R(A,B),S(B,C), A does not appear in S. Therefore we estimate:\n\n∣πA​(R⋈S)∣≈∣πA​S∣\n\n\n                  \n                  Nowadays people use histograms and ML for better estimation \n                  \n                \n\nJoin Plans §\nQ. given n relations to join, how to join?\n\nBrute Force: (n−1)!(2n−2)!​\nLeft-Deep Plan: n! \nGreedy: n2 \nDynamic Programming:\n\nNeed to consider: interesting orders (=sorted? deduped? etc.) need to be considered too!\n\n\n\n"},"SQL-Query-Processing-Algorithms":{"title":"SQL Query Processing Algorithms","links":["Sorting-Algorithms"],"tags":["Computing/Data-Science"],"content":"Notation §\n\nB(R): block count of table R\n∣R∣: tuple count of table R\n\n∣R∣&gt;&gt;B(R) because many tuples fit in a block\n\n\nM: number of available blocks in memory\nComplexity is always the number of I/Os\n\nSorting Based Algorithms §\nalg. External Merge Sort\n\nIdea: Merge Sort, but with limited memory \n\n\nWe are trying to sort relation R on disk with M blocks of main memory\nPass 0: we read M sequential blocks at a time, sort (using any Sorting Algorithms) and read onto disk into different runs. The number of runs is n.\nPass 1: We merge teach runs in memory. We need one block to write out the result, and n blocks to merge simultaneously.\n\nThus n=M−1\n\n\nRepeat passes until there is only one output run. This is the sorted run.\n\n\nComplexity\n\nNumber of passes: =⌈logM−1​⌈MB(R)​⌉⌉+1\nI/O Complexity: ≈O(B(R)logM​B(R))\nMemory requirement: M (as much as possible)\n\n\n\nalg. Sort-Merge Join\nIdea: while sorting the table, simultaneously join them in the process: \n\nRun external merge sort passes until the no. of total runs to merge is less than or equal to M−1\nMerge each table in-memory, and compare them to the condition. Join if necessary.\n\n\nComplexity for 2 passes:\n\nIO Complexity in 2 passes: O(B(R)+B(S))\nMemory requirement: M&gt;⌈MB(R)​⌉+⌈MB(S)​⌉=B(R)+B(S)​\n\n\nComplexity for &gt;2 passes: same as external merge sort\n! May degrade complexity if, e.g. the whole table needs joining.\n\nHash Based Algorithms §\nalg. Hash Join\n\nIdea: use a O(1) hash function that partitions the table into block size M−1 partitions, then merge: \n\n\nProbe: Partition both tables R,S into block size M−1 different partitions using the same hash function\nLoad one partition of R into memory. Stream one block of S at a time, and join.\n\n\nComplexity\n\nI/O: 3(B(R)+B(S))\nMemory requirement: M&gt;min[B(R),B(S)]​+1\n\n\n\nIndex Based Algorithms §\nEquality and Range: use B+ Trees\nIndex Nested Loop Join\nZigzag Join"},"SQL-Transaction-Guarantees":{"title":"SQL Transaction Guarantees","links":["Atomic","Directed-Acyclical-Graph","Readers-Writer-Locking"],"tags":["Computing/Data-Science","Computing/Algorithms"],"content":"SQL Transactions must be: ACID\n\nAtomic: Done or not done. Atomic\nConsistent: Don’t do partial writes\nIsolated: Transaction Serializability:= transactions should seem like they are executed in isolation\nDurable: Crashes should be recoverable\n\nIsolation §\nConcepts §\n\nSerial Schedule: execute transactions in order. Don’t interleave anything.\nConflicting Operations: two transactions conflict if any of the operations they have on same data conflict iff one of the operations is a write operation\n\nDirty Write: T1​.r(A) → T2​.w(A)\nDirty Read: T1​.w(A) → T2​.r(A)\n&amp; Two transactions are T1​,T2​ is conflict-serializable iff they have conflicts, but we can interleave some operations but still seems like it’s a serial schedule. SEREALIZABLE guarantee above is equivalent to this.\n\n\nConflicting Transactions:\n\nSee Isolation (database systems) - Wikiwand\nNon-repeatable Reads: T1​ reads X→ T2​ writes X and commits → T1​ reads X again (different value!)\nPhantom Reads: T1​ gets range → T2​ inserts between this range and commits → T1​ gets same range (different value!)\n\n\nPrecedence graph: a schedule with no cycles in the precedence graph (=graph is DAG) is conflict-serializable\n\nA path in an acyclic precedence graph is a conflict-serializable schedule\nExample: \n\n\n\nLevels of Isolation Guarantees §\n\nRead Uncommitted: lock &amp; release immediately\nRead Committed: don’t release write locks until commit\nRepeatable Reads: long duration locks\nSerializable: long duration locks on ranges\n\n\nSerializable Guarantee §\nImplemented thru Strict 2-phase locking (S2PL). Rules:\n\nOne writer, multiple readers: Readers-Writer Locking\n2-Phase Locking: For each transaction, you must lock everything first (=locking phase) then unlock everything together (=unlock phase): \nStrict Locking: Release write-locks (=exclusive-locks) only at commit or abort time\n\nGuaranteed recoverable (no cascading rollbacks)\nExample:\n\n\nRigorous (=Strong) Locking: Release all locks only at commit or abort time.\n\nPrevents Deadlocks\nAlso Recoverable.\nExample:\n\n\n\nRecovery §\nComputation Model: \nNaive Recovery §\n\nForce: on commit, (“force”) dirty flush to disk\nNo Steal: don’t (“stealthily”) flush before commiting\n\nSmarter: Undo/Redo Logging §\n\nlog start of transaction\nFor each write/delete, log the old and new values\nOn commit, write all logs to disk\nProperties\nWrite-ahead logging: before disk flush, log must be flushed first\nNo force: commit even without flushing\nSteal: flush to disk anytime (just log it!)\n\n\nFuzzy Checkpointing §\n\nLog begin checkpointing and log open transactions\n\nStart Checkpoint CHK1, Open transactions: S,T\n\n\nflush all current transactions (=transactions at “begin” point) at leisure\nLog finish checkpointing (and pointer for convenience)\n\n`Finish Checkpoint CHK1. Started at \n\n\n\nRecovery\n\nAnalyze &amp; Redo Phase\n\nFind last completed checkpoint Finished Checkpoint CHK1…\nGo to the start of that checkpoint Start Checkpoint CHK1, Open: S,T…\nAnalyze open transactions at time of crash U={S,T}\n\n! Start with checkpoint’s open transactions\nWhen you encounter close transactions Close S, remove from U={S​,T..,U}\nWhen you encounter new open transactions Begin T, add that to U={S,T..,addedU​​}\n\n\nAll operations between Start Checkpoint… and end of log is replayed\n\n\nUndo Phase\n\nFrom the last log item until whenever:\n\nFor each open transaction at time of crash T∈U\n\nUndo all of its operations in reverse order…\nUntil you reach Begin T\n\n\n\n\nStop when U is exhausted\n\n\n\nRecoverability §\nFor multiple transactions to be recoverable:\n\nT1 writes → T2 reads, then must T1 commits → T2 commits\nT1 writes → T2 writes doesn’t even matter\n\nEd discussion:\n"},"Scheduling-Problem":{"title":"Scheduling Problem","links":["Greedy-Algorithm","Dynamic-Programming","Subset-Sum"],"tags":["Computing/Algorithms"],"content":"Scheduling problem solution formulations\n\nSort by some quantity\nSchedule accordingly\nProve correctness by these strategies\n\nEvent Scheduling §\nQ. Interval Scheduling (Greedy Algorithm)\n\nDynamic Programming idea works\n\ndp[i] = max(dp[]+1\nO(n3) time\n\n\nGreedy Algorithm is even better\n\nIdea: sort by\n\n\n\nalg. Interval Scheduling with n rooms\nQ. Multiple Room Interval Scheduling\nQ. Interval Coloring\nQ. Interval Cover Problem\nJob Scheduling §\nQ. Minimize Makespan (=maximum time on any machine) of n machines\n\n\nNP-Hard problem (reduce from 21​-Subset Sum)\n\nalg. Approximate Greedy Makespan: Always choose the least loaded machine\n\nIs a 2-level approximation \nalg. Better Approximate Greedy Makespan: Assign from longest to shortest job, to least loaded machine every time.\n\n\nalg. Minimize lateness of jobs\n\n\nInterval Scheduling\n\nschedule n jobs\n\nno dependencies\n\n\n=&gt; how many jobs can you complete? (total time doesn’t matter!)\nconflict when:\n\nStart[i] &lt;\n\n\nDynamic Programming solution is easy\n\nOPT = max(1+OPT(jobs that don’t conflict with job 1) vs OPT(2..n))\n\n\n=&gt; but O(n3) or even with improvement O(n2)!\n\n\n\nGreedy solution:\n\njob that finishes first = job that leaves the most amount of remaining time\nimplementation\n\nSort by finish time\nchoose earlyest finish time\nchoose next earliest finish time that doesn’t conflict with most recently finished job\nDone!\n\n\n=&gt; O(nlogn) time (because sorting)!\nProof by induction: Exchange Argument\n\nbase: Y∗ optimal solution has y1​inY∗\n\nif you have x1​ that finish time earlier than y1​…\nremoving y1​ and adding x1​ is also optimal solution\n\n\n\n\n\n\n\nTry: dp solution\n\n\nvariation: make span algorithm (greedy is close to optimal, but not optimal)\n\n\ninterval scheduling, but with n meeting rooms\n\ngreedy still words!\n\n\n\nminimum total completion time.\n\n\nminimize lateness\n\nsort by ascending lateness&lt;&lt;\n\n\n"},"Securities-and-Exchange-Commission":{"title":"Securities and Exchange Commission","links":[],"tags":["Economics/Finance"],"content":"\nThe SEC is an independent agency of the United States federal government, and it is responsible for enforcing federal securities laws, proposing securities rules, and regulating the securities industry.\n"},"Security-(Finance)":{"title":"Security (Finance)","links":["Bonds-(Finance)","Futures","Capital-(Marxism)","Valorization,-Surplus-Value"],"tags":["Economics/Finance"],"content":"Bonds (Finance)\nFutures\nBasically capital to be valorized. Interest is surplus value."},"Sensuous-Activity":{"title":"Sensuous Activity","links":["Alienation","Private-Property","Capital-(Marxism)","species-being"],"tags":["Humanities/Marxism"],"content":"Sensuous Activity §\nSenses define reality…\n\nAll his human relations to the world - seeing, hearing, smelling, tasting, feeling, thinking, contemplating, sensing, wanting, acting, loving - in short, all the organs of his individuality, like the organs which are directly communal in form, are in their objective approach or in their approach to the object the appropriation of that object. This appropriation of human reality, their approach to the object, is the confirmation o f human reality.\n\n…but it’s estranged for now…\n\nThis material, immediately sensuous private property is the material, sensuous expression of estranged human life. Its move­ment - production and consumption - is the sensuous revelation of the movement of all previous production, i.e. the realization or reality of man.\n\n…and Private Property is a core “sensuous” response by capitalism…\n\nPrivate property has made us so stupid and one-sided that an object is only ours when we have it, when it exists for us as Capital (Marxism) or when we directly possess, eat, drink, wear, inhabit it, etc., in short, when we use it. […] Therefore all the physical and intellectual senses have been replaced by the simple estrangement of all these senses - the sense of having.\n\n…and removal of private property is the “emancipation” of our senses\n\nThe supersession of private property is therefore the complete emancipation of all human senses and attributes […] The senses have therefore become theoreticians in their immediate praxis. They relate to the thing for its own sake, but the thing itself is an objective human relation to itself and to man, and vice-versa.\n\nEmancipation of the Senses §\nThe socialized and trained “senses” determine the progress of the species-being\n\nTherefore this relationship reveals in a sensuous form, reduced to an observable fact, the extent to which the human essence has become nature for man or nature has become the human essence for man. It is possible to judge from this relationship the entire level of development of mankind. It follows from the character of this relationship how far man as a speciesbeing, as man, has become himself and grasped himself\n\nThis material, immediately sensuous private property is the"},"Sequence-Summation":{"title":"Sequence Summation","links":[],"tags":["Math/Calculus","Math"],"content":"Geometric Series: a,ar,ar2,…,arn\nSn​=a⋅1−r1−rn​\nIf it converges, the infinite sum:\nS∞​=1−ra​\nArithmetic Sequence: a,a+d,a+2d,…a+nd\nSn​=2n​⋅(2a+(n−1)d)\nOther Summations: See Convergence tests - Wikiwand\nProperties of Summations §\n(i=0∑n​ai​)2=i=0∑n​ai2​+i=j∑n​ai​aj​"},"Set-Cover":{"title":"Set Cover","links":[],"tags":["Computing/Algorithms"],"content":"Q. Set Cover. We have a universe set U and a bunch of sets S1​,…,Sn​ in the universe. (It is guaranteed that the union of all these sets will cover U). I want to cover all elements of the universe. Which sets should I choose do do this minimally?\n\ne.g. How many classes to visit to see all Duke students?\n\nalg. Approximate Set Cover.\n\nChoose the largest available set Si​\nRemove elements from that set from the universe\nRepeat until no element is left.\n"},"Set-Theory":{"title":"Set Theory","links":[],"tags":["Math"],"content":"Partitions and Probability §\ndef. Sets B1​…Bn​ is a partition of B if and only if:\n\nB1​…Bn​ are pairwise disjoint\nB1​∪…∪Bn​=B\n\nCOR. if B1​…Bn​ is pairwise disjoint P(B1​∪…∪Bn​)=P(B1​)+…+P(Bn​)\nCOR. if B1​…Bn​ is a partition of Ω then P(B1​∪…∪Bn​)=1\nSet Operations §\n\n∣A∣ Cadinality of set A\nA×B: Cartesian product of sets A,B\n2A: all the subsets of A [= power set of A]\n\n"},"Shepard's-Lemma":{"title":"Shepard's Lemma","links":["Expenditure-Minimization","Uncompensated-Demand-curve","Expenditure-Function","Cost-Minimization","Input-Demand-and-Output-Supply"],"tags":["Economics/Micro-Economics"],"content":"thm. Shepard’s Lemma (Expenditure Minimization). Relates Hicksian Demand and Expenditure Function\nh1​(p1​,p2​,uˉ)=∂p1​∂E​h2​(p1​,p2​,uˉ)=∂p2​∂E​​​\nthm. Shepard’s Lemma (Cost Minimization). Related conditional Input Demand and the Cost Function\nlc​(w,r,xˉ)kc​(w,r,xˉ)​=∂w∂C​=∂r∂C​​​"},"Short-selling":{"title":"Short-selling","links":[],"tags":["Economics/Finance"],"content":"= “selling an asset you don’t owe”\n= you borrow the asset to immediately sell it on the market;\n→ then when the price drops, you buy the share from the market to return it\n"},"Shortest-Path":{"title":"Shortest Path","links":["priority-queue","No-Arbitrage","Dynamic-Programming"],"tags":["Computing/Algorithms"],"content":"Summary of All Shortest Path Algorithms §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShortest Path (SP) AlgorithmsBFSA*Dijkstra’sBellman FordFloyd WarshallComplexityO(V+E)O(ElogV)O((V+E)logV)O(VE)O(V3)Recommended graph sizeLargeLargeLarge/MediumMedium/SmallSmallAll-PairsOnly works on unweighted graphsNoOkBadYesCan detect negative cycles?---✓✓SP on graph with unweighted edges✓(Best)✓✓✓(Bad)✓(Bad)SP on graph with weighted edgesMust expand graph✓(Best)✓✓✓(Bad)\nSingle Source Shortest Path (SSSP) §\nalg. BFS Shortest Path\n\nAssumption: Graph doesn’t have weights\nIdea: BFS, but every time you encounter a tense edge, relax it\n\n\nalg. Dijkstra’s Shortest Path\n\nAssumption. graph has non-negative edge weights\nIdea. BFS Shortest Path, but you choose the next vertex by how likely they are to be the shortest path (=maintain a priority queue based on path length)\n\nSee Dijkstra’s Algorithm - Computerphile - YouTube\n\n\nComplexity\n\nTime: O((∣V∣+∣E∣)⋅log∣V∣)\n\n\n\nalg. A-Star (A*) Shortest Path\n\nAssumption. graph has non-negative edge weights\nIdea. Dijkstra using priority queue, but the priority is calculated on a heuristic\n\nSee A* (A Star) Search Algorithm - Computerphile - YouTube\n\n\nComplexity.\n\nTime: O(∣E∣log∣V∣) using binary heap\n\n\n\nalg. Bellman–Ford Shortest Path\n\nIdea:\n\nRepeat BFS edge relaxation for ∣V∣−1 times. Get shortest path\nRepeat BFS edge relaxation again, for ∣V∣−1 times. But this time, if any cost values are updated, the node is part of a negative cycle. Mark cost to that node as ∞.\nSee Bellman Ford Algorithm | Graph Theory - YouTube\nAble to deal with negative edges/negative cycles\nOften used in finance for identifying No-Arbitrage opportunities.\n\n\nComplexity. Time: O(V⋅E)\n\nAll Pairs Shortest Path (APSP) §\nalg. Floyd-Warshall Shortest Path\n\nIdea: Dynamically Programmed algorithm. from i to j, compare the two paths:\n\ni→j using only vertices 1,…,k−1\ni→k→j, but only using vertices 1,…,k−1\nTake the smaller of the two.\n\n\nExample: \nDP Table: memo[k][i][j] is the shortest path from i→j using nodes 1..k\nSee also: Floyd–Warshall algorithm - Wikiwand\n\n\nComplexity. Time: O(V3), Space O(V2) by retaining most recent only\n\nTips and Tricks §\n\nIf the problem demands you multiply edge weights instead of summing them, use log of weights instead to transform it back into a problem with summation of edge weights. (See Example)\n"},"Signaling-Game":{"title":"Signaling Game","links":[],"tags":["Economics/Game-Theory"],"content":"\nSignaling game because one is attempting to signal to another authentically.\nHandicap principle - Wikiwand is an instance of the signaling game. Animals will signal their sexual prowess at the cost of actual function."},"Sliding-Window-Technique":{"title":"Sliding Window Technique","links":[],"tags":["Computing/Algorithms"],"content":"“Maximum consecutive something”/“Maximum Substring” ⇒ think sliding window.\n\nSliding windows can stretch or shrink in length\nIt’s a kind of greedy algorithm\nyou can do many things in O(1) time.\nIt can do flipping and stuff too!\n\nExample of complex sliding window problem:\n#1004 - Maximum Consecutive Ones with Flipping"},"Solipcism":{"title":"Solipcism","links":["The-Environment","Saint-maur"],"tags":["Sociability","Philosophy"],"content":"Every thought about giving advice # We are all different people\n\nPeople differ a lot in their levels of conscientiousness, extraversion, energy, ambition, curiosity, independence, risk-seeking, fame-seeking, neuroticism, conflict-aversion, obsessiveness, etc. By default, high conscientiousness people don’t understand low conscientiousness people. High extraversion people don’t understand low extraversion people. High energy people don’t understand low energy people. It’s actually worse than that because “high neuroticism” or “high ambition” can actually mean a ton of different things.\n\n\nWhile all theories dictate solitude is important for you in that it allows for space to think, to introspect, and generally to maneuver, when you are lacking The Environment that allows you to be by yourself you need to strategize with the resources available. Understanding exactly when and how you feel free and alone helps determine the environments you can generate to fulfill this need. These are the variables of solitude.\n1. The Environment §\n\nThe less people in The Environment, and the less they know you, you feel more free. In a train full of people you’re perfectly free to think any thought you want. The more anonymous they are the better.\nFamiliarity allows more freedom. When you have expectations fulfilled there is less need for processing—everything is as-is. This is why you can think better in a room that you completely control and design. You are perfectly familiar with all the furniture, and all the tools required for thinking are accessible.\nAccessible Thinking Tools. Notebooks. Phones. Computers.\n\n2. Perceptions §\n\nThe more anonymous the better. The less they know you the less you have to care for them. When they become mere background characters in your perception this is ideal\nBlock visual and auditory noise. Simply blocking noise out of sight and hearing helps generate a sense of freedom and solitude. Noise-canceling headphones and curtains will help in this regard.\n\n3. Interpretation §\n\nNotice how little people actually care about what you’re doing. Fundmental Misunderstanding; Self-Attention. The portion of thought assigned to you in another person’s brain is miniscule.\n\n\nThe woled isfounded on imperfection and misinformation\nWhen you’re Finally Comfortable Being Alone, then You Are Able to… §\nWhen you’re Finally Comfortable Being Alone, then You Are Able to Enjoy Company. §\nSaint maur 12 grade.\nDuke beginning. Vic? Zoe."},"Sorting-Algorithms":{"title":"Sorting Algorithms","links":["Priority-Queue"],"tags":["Computing/Algorithms"],"content":"\n\n                  \n                  Abstract \n                  \n                \nBest sort algorithm (merge sort) will take O(nlogn) at worst.\n\nQuick Sort §\nIdea:\n\nChoose pivot\nEverything left of pivot is smaller, everything right of it is bigger (O(n))\nCall quicksort on the left side and right side\n\nChoosing a good pivot\n\nRandom\n\nWith high probability pivot within 31​∼32​\nLikely Complexity: T(n)≤T(32​n)+cn=O(n)\n\n\nDSelect i.e. Median of medians approach \n\nRank is guaranteed to be between 41​∼43​ of the array slice\nComplexity: T(n)≤T(5n​)+T(43​n)+cn=O(n) ……first term: DSelect, second term quicksort\n\n→ T(n)&lt;A⋅n for some A≥20c (=O(n))\n\n\n\n\n\nMerge Sort §\nIdea:\n\nSplit into two\nCall mergesort on each left, right side\nmerge left, right side in O(n) time\n\nalg. Parallel Merge\nIdea: For the 8 elements shown below, can we find out how to place the element in parallel (8 simultaneous processes)? → We can!\n\nLeft half: binary search on right half for correct place\nRight half: binary search on left half for correct place\n\n\ndef PMerge(A[1..n], m)\n\t# Ind[i] stores the final index of the i-th element\n\t\n\t# left half\n\tparallel for i=1 to m\n\t\t# search in right half\n\t\tInd[i] &lt;- BinarySearch(in A[m+1..n] for A[i]) + i\n\t# right half\n\tparallel for j=m+1 to n\n\t\t# search in left half\n\t\tInd[i] &lt;- BinarySearch(in A[1..m] for A[j]) - m + j\n\tsync\n \n\t# place each item in their ordered location\n\tparallel for i=1 to n\n\t\tB[Ind[i]] &lt;- A[i]\n\tparallel for i=1 to n\n\t\tA[i] &lt;- B[i]\n\t\n\nSpan O(logn)\n\nthen we have…\nalg. Parallel Merge Sort\ndef PMergeSort(A[1..n])\n\t# base case\n\tif n &gt; 1\n\t\tm = ceiling(n/2)\n\tspawn PMergeSort(A[1..m])\n\tspawn PMergeSort(A[m+1..n])\n\tsync\n\tPMerge(A[1..m],A[m+1..n])\n\nSpan T∞​(n)≤T∞​(2n​)+O(logn)=O(log2n)\nWork W∞​≤2W∞​(2n​)+O(nlogn)=O(nlog2n)\n\nSecond term is work done by parallel merge procedure: correctly placing n items, each takes O(logn).\n\n\n\nalg. Inversion Counting. In an unsorted array A[1..n] count how many pairs A[i],A[j] are not in the correct order\n\nIdea: Mergesort, but during the merge step check how many times you need to reverse the pairs\n\n\nPartially Sorted §\nUses of partially sorted arrays:\n\nTop k items: e.g. Google Search, College Admissions\nk-th largest/smallest item\n\nk-th smallest item (=item of rank k) =&gt; use Tournament Tree"},"Species-being":{"title":"Species-being","links":["Humanism"],"tags":["Humanities/Marxism"],"content":"= Marxist Humanism\n\nMan is a species-being, not only because he practically and theoretically makes the species - both his own and those of other things - his object, but also - and this is simply another way of saying the same thing - because he looks upon himself as the present, living species, because he looks upon himself as a universal and therefore free being.\n\n\nFor in the first place labour, life activity, productive life itself appears to man only as a means for the satisfaction of a need, [But] the whole character of a species, its species-character, resides in the nature of its life activity, and free conscious activity constitutes the species-character of man. Life itself appears only as a means of life.\n\n\nThe object of labour is therefore the objectification of the specieslife of man: for man reproduces himself not only intellectually, in his consciousness, but actively and actually, and he can there­ fore contemplate himself in a world he himself has created.\n"},"Spontaneous-Organization":{"title":"Spontaneous Organization","links":["Value-of-Money"],"tags":["Economics/Game-Theory","Philosophy/Political-Philosophy"],"content":"Money is an example of spontaneous organization: Devonthink"},"Stable-Marriage-Problem":{"title":"Stable Marriage Problem","links":[],"tags":["Computing/Algorithms"],"content":"\n\n                  \n                  Problem Statement: \n                  \n                \nGiven n men and n women, where each person has ranked all members of the opposite sex in order of preference, marry the men and women together such that there are no two people of opposite sex who would both rather have each other than their current partners [=in a stable marriage].\n\n⇒ There is always a solution to this problem (Gale-Shapely Algorithm) in O(n2) time."},"Standardizing-a-Random-Variable":{"title":"Standardizing a Random Variable","links":["Normal-Distribution"],"tags":["Math/Probability"],"content":"\nAny random variable X can be standardized\n\n! It doesn’t have to be normally distributed\n\n\nFor a single random variable:\n\nY=SD(X)X−E(X)​\n\nFor X1​…Xn​\n\n"},"Stat-230-Probability":{"title":"Stat 230 Probability","links":["Probability","Conditional-Probability","Independence-(Math)","Random-Variable","Expected-Value","Variance","Distribution-(Math)","Exponential-Family","Binomial-Distribution","Multinomial-Distribution","Hypergeometric-Distribution","Poisson-Distribution","Exponential-Distribution","Uniform-Distribution","Normal-Distribution","Gamma-Distribution","Approximating-Distributions","Central-Limit-Theorem","Poisson-Limit-Theorem","Confidence-Intervals","Change-of-Variable-(Probability)","Joint-Distributions","Covariance","Correlation","Conditional-Distribution"],"tags":["Courses"],"content":"Probability Basics §\n\nProbability\nConditional Probability\nIndependence (Math)\nRandom Variable\nExpected Value, Variance Identities\n\nDistribution Manipulation §\n\nDistribution (Math)\n\nDistribution (Math)\nExponential Family\n\n\nDiscrete Distributions\n\nBinomial Distribution\nMultinomial Distribution\nHypergeometric Distribution\nPoisson Distribution\n\n\nContinuous Distributions\n\nExponential Distribution\nUniform Distribution\nNormal Distribution,\nGamma Distribution\n\n\nApproximating Distributions\nCentral Limit Theorem\nPoisson Limit Theorem\n\nAdvanced Probability §\n\nConfidence Intervals\nChange of Variable (Probability)\nJoint Distributions\nCovariance, Correlation\nConditional Distribution, Conditional Expectation\n"},"Stat-432-Statistics":{"title":"Stat 432 Statistics","links":["Estimator","Sufficiency","Likelihood-(Statistics)","Fisher-Information","Maximum-Likelihood-Estimator","Consistency","Moment-(Probability)","Confidence-Intervals","Hypothesis-Testing","Student's-t-test","Wilcoxon-Signed-Rank-Test","Wilcoxon-Rank-Sum-Test","Permutation-Test","Bernouilli-Distribution","Chi-Squared","Student's-T-Distribution"],"tags":["Courses"],"content":"\n\n                  \n                  All models are wrong, but some models are useful. \n                  \n                \n\n\nEstimator\nSufficiency\nLikelihood (Statistics), Fisher Information\nMaximum Likelihood Estimator\nConsistency\nMethod of Moment (Probability)\nConfidence Intervals\nHypothesis Testing\n\nStudent’s t-test\nWilcoxon Signed Rank Test\nWilcoxon Rank Sum Test\nPermutation Test\n\n\nDistributions\n\nBernouilli Distribution\nChi-Squared\nStudent’s T-Distribution\n\n\n"},"Statistical-Triple":{"title":"Statistical Triple","links":[],"tags":["Math/Statistics"],"content":"A probability triple where the outcome space is instead a sample space, defines a statistical experiment.\n(Ω,F,P) where Ω is the sample space, F=2Ω, and P:F→[0,1]\n\n\n                  \n                  Relationship between probability and statistics \n                  \n                \n\n"},"Stochastic-Calculus":{"title":"Stochastic Calculus","links":["Riemann–Stieltjes-integral","Reimann-Integral","Stochastic-Process"],"tags":["Math/Calculus"],"content":"def. Ito Integral. Let the following:\n\nBrownian Motion B={Bs​∣s≥0}, an Ito process X={Xs​∣s≥0}\n0=t1​&lt;t2​&lt;⋯&lt;tn​=t and thus ti​=nt​i\nThen the Ito integral Yt​ of X(t) with respect to B(t) is defined as such:\n\nY(t)=∫0t​X(s)dB(s):=n→∞lim​i=0∑n​X(ti−1​)[Bti​​−Bti−1​​]\n\nVisualization. Think of the 3D visualization of the Riemann–Stieltjes integral, but both f(x) and g(x) are zigzags.\nThe solution to the Ito integral is also a stochastic process Y={Yt​∣t≥0}. It also has the following properties because the integrator is Bt​\n\nYt​ is a martingale\nE(Yt​)=0\nlet Zt​=∫0t​X(s)2dB(s)\n\nIto Isometry: Var(Yt​)=E(Zt​)\nYt​∼N(0,E(Zt​))\n\n\n\n\n\nRemark. We often use shorthand notation (abuse of notation) to write an integral term like this:\n\ndBt​:=∫0t​1dBs​:=∑i=1∞​1⋅(Bti​​−Bti−1​​)\nXt​dBt​:=∫0t​Xs​dBs​:=∑i=0∞​Xt​(Bti​​−Bti−1​​)\ndt:=∫0t​1dt:=∑i=0∞​1(ti+1​−ti​)=t by definition of the Reimann Integral\nXt​dt:=∫0t​Xs​ds:=\nNow, these also exist but they are trivial (Ito Isometry):\ndBdt=dtdB=0\n(dt)2=0\n(dB)2=dt\n\nReference Table of Common Ito Integrals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic IntegralResultVariance∫0t​dBs​Bt​t∫0t​sdBs​tBt​−∫0t​Bs​ds31​t3∫0t​Bs​dBs​21​Bt2​−21​t21​t2∫0t​Bs2​dBs​31​Bt3​−∫0t​Bs​ds3t2∫0t​eBs​−21​sdBs​eBt​−21​t−1et−1\nMotivation. Unfortunately we cannot calculate ito intergrals directly. For example, as we saw in the above example finding ∫0t​B(s)dB(s) using just the definition was a laborious process. In order to make this easier, we use the following lemma.\nthm. Ito’s Lemma. (Ito’s Chain Rule) The stochastic calculus chain rule. Stated in two ways.\n\nX(t) is an Ito process dX(t)=a(X,t)dt+b(X,t)dB\nf(X,t) is a normal function\nThen:\n\ndf(X,t)=(ft​+afx​+2b2​fxx​)dt+bfx​dB\n\nfx​=∂x∂f​∣x=X​, ft​=∂t∂f​\n\nExample. Suppose we want to find the integral ∫0t​Bu2​−udBu​. Since it’s nearly impossible to find it from the definition of the Ito integral, let us use Ito’s lemma. Arbitrarily set an ordinary function f(x,t)=3x3​−tx.\n\n! Now, it is important to note that we normally don’t know what function to use, but in this case assume we are given a nice f(x,t) that will lead to the answer. Given this function, we then have:\nfx​=x2−t, fxx​=2x, and ft​=−x\nBecause B is a standard brownian motion it conforms to the form dB=0⋅dt+1⋅dB which means for Ito’s lemma a=0,b=1.\nThen, we use Ito’s lemma:\n\ndf(B,t)f(Bt​,t)−\\cancelto0f(B0​,0)3Bt3​​−tBt​​=(ft​+afx​+2b2​fxx​)dt+bfx​dB=\\cancelto0(−x+0+21​⋅2x)dt+(x2−t)dB=(B2−t)dB=∫0t​Bu​−udBu​​​\n■\nReference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotion: Xt​Differential: dXt​=udt+udBt​Bt​dBt​Bt2​2Bt​dBt​+dtBt2​−t2Bt​dBt​Bt3​3Bt2​dBt​+3Bt​dteBt​eBt​dBt​+21​eBt​dteBt​−21​teBt​−21​tdBt​e21​tsinBt​e21​tcosBt​dBt​e21​tcosBt​−e21​tsinBt​dBt​(Bt​+t)e−Bt​−21​t(1−Bt​−t)e−Bt​−21​tdBt​\nthm. Ito’s Product Rule. for two stochastic processes Xt​,Yt​\nd(Xt​Yt​)=Xt​dYt​+Yt​dXt​+dXt​dt​\n\nIto Processes §\ndef. Ito Process. An Ito process X(t)={Xt​∣t≥0} is a type of Stochastic Process that is defined by a certain a(Xt​,t),b(Xt​,t):\nX(t)=X(0)+∫0t​a(Xs​,s)ds+∫0t​b(Xs​,s)dB\n\na(Xt​,t) must be integrable in the ordinary sense\nb(Xt​,t) must be integrable in the stochastic sense\n\nIt actually needs to be integrable twice\nAs an abuse of notation we can write:\n\n\n\ndX=a(Xt​,t)⋅ds+b(Xt​,t)⋅dB\n\nThis is called a Stochastic Differential Equation (even if it’s not actually a differential equation).\nEssentially, a function of an Ito process is also an Ito process.\n\nProperties.\n\nE(dY∣Ft​)=(ft​+afx​+2b2​fxx​)⋅dt\nVar(dY∣Ft​)=(bfx​)2\n"},"Stochastic-Process":{"title":"Stochastic Process","links":["Stochastic-Calculus","Quadratic-Variation"],"tags":["Math/Statistics"],"content":"def. Time Series is a stochastic process indexed by discrete (integer) time points.\n\nSee discussion: Is a time series the same as a stochastic process? - Cross Validated\n\ndef. Stochastic Process is simply the sum of the sequence of random variables: X1​,…,Xn​:\nWk​={∑i=1k​Ri​w0​​if k&gt;0if k=0​\n\n\nRi​ are identically distributed.: Stationary\n\n\nE(Wk+1​∣W1​…Wk​)=Wk​ then: Martingale\n\n\nP(Wk+1​=wk+1​∣W1​=w1​,…Wk​=wk​)=P(Wk+1​=wk+1​∣Wk​=wk​): Markov\n\n\n                  \n                  random variable - Why do stochastic processes involve time? - Cross Validated\n                  \n                \n\n\n\nWeiner Process §\nStandard Weiner Process §\nMotivation. Assume there are random variables X1​… as:\nXi​={n​1​−n​1​​0.5 probability0.5 probability​\n\nlet Wk​ be a stochastic process such that:\n\nW(n)(t)=⎩⎨⎧​0R1​+⋯+Rt​affine extension on closest interval ​if t=0if t∈Nelse​\nThen we get the Weiner process as:\nB(t)=n→∞lim​W(n)(t)\ndef. Brownian Motion. (=Weiner process) B(t) (written as Bt​) is a set of random variables continuous-time indexed and has the following properties:\n\nIs a continuous process\nB0​=0\nBt​−Bs​∼d​N(0,t−s)\n\nThus, Bt​=d​N(0,t)\n\n\nAny interval Ba​−Bb​ and Bc​−Bd​ where [a,b] and [c,d] do not overlap is independent.\nNotation wise, we think of B={Bt​}t≥0​, i.e. a set of random variables indexed by t.\n\nRemark. Brownian Motion can also be defined as an Ito Process that satisfies the following stochastic differential equation:\ndB=μ⋅dt+σ2dB\nTrivially, in the case of standard brownian motion, μ=0,σ2=1 Thus dB=0⋅dt+1⋅dB.\nProperties. Brownian Motion satisfies the following properties:\n\nMartingale Property: Brownian motion is martingale: ∀s≤t,E(Bt​∣Fs​)=Bs​ where Fs​=σ(Bs​)\nMarkov Property:\n\n∀s≥0,{Bs+t​,t≥0} is independent of Fs​ and…\n{Bs+t​,t≥0}=d​{Bt​}\nAlternatively: P(Xu​∈A∣σ(Xs​))=P(Xu​∈A∣Xt​) where any combination s≤t≤u\n\n\nScaling Invariance: If Bt​ is a brownian motion, then aB(a2t)​ is also a brownian motion ∀a&gt;0\nQuadratic Variation property: [B]t​:=∑i=1n​(Bti​​−Bti−1​​)2=t\n\nWith Scale and Drift §\ndef. Weiner Process with Drift and Scaling (WPDS). Such is a Weiner process X(t) that has the following properties:\n\nIs a continuous process\nX(0)=x0​\nX(t+Δt)−X(t)=d​N(μΔt,σ2Δt)\nAny interval X(a)−X(b) and X(c)−X(d) where [a,b] and [c,d] do not overlap is independent.\n\n\nProperties:\n\nμ: drift; the higher, the more it climbs \nσ2: scaling; the higher, the more volatile (See y-axis:) \n\n\n\nthm. Standard WP to Scale and Drift. Given:\n\nB(t) is a standard Weiner process\nX(t) is a Weiner process with drift μ and scaling σ2, with initial value x0​\n⇒ Then the following relationship holds (two equivalent definitions)\n\nX(t)dX(t)​=d​x0​+μt+σB(t)=μ⋅dt+σ⋅dB(t)​​\nGeometric Brownian Motion §\nthm. Geometric Brownian Motion. Given X(t) is a WPDS μ,σ2,x0​ then the following is a geometric brownian motion with initial value S(0): (two equivalent definitions)\nG(t)dG(t)​=G0​exp[X(t)]=G0​exp[x0​+μt+σB(t)]=(μ+2σ2​)G(t)⋅dt+σG(t)⋅dB(t)​​\n\n\n                  \n                  Geometric to WPSD \n                  \n                \nThe following is equivalent:\n\ndS(t)S(t)​=(μ+2σ2​)S(t)⋅dt+σS(t)⋅dB(t)=S0​exp[(μ−2σ2​)t+σB(t)]​​\nProperties of Stochastic Processes §\ndef. Adapted process. A stochastic process {Xt​}t≥0​ is adapted to filtration set {Ft​}t≥0​ if ∀i,Xi​ is a Fi​-measurable function.\nIntuition. Recall that a sigma algebra F can be thought of as “resolution of information”. Xi​ is realized and its information is Fi​. The series of filtrations F1​⊂F2​⊂⋯⊂Ft​⊂⋯⊂F correspond to the series of random variables X1​,X2​,…; for each timestep, the information gets higher and higher resolution.\nMartingale Process §\nMotivation. Many stochastic processes, including the standard brownian motion Bt​ has the property that you can’t predict future trajectory based on information from its past trajectory. We formalize this as a martingale.\ndef. Martingale. A stochastic process {Xt​}t≥0​ on a filtered probability space (Ω,F,{Ft​},P) is a martingale w.r.t. its adapted filtration {Ft​} if:\n∀s≤t,E(Xt​∣Fs​)=Xs​\nThis is equivalent to saying\n\n∀s≤t,E(Xt​)=E(Xs​) by taking expectation on both sides of above\n∀s≤t,E(Xt​−Xs​)=0\n\nThis is a simple proof that brownian motion is a martingale; E(Bt​−Bs​=d​N(0,t−s))=0\n\n\n\nQuadratic Variation §\nMotivation. Quadratic variation is not about variation of probability distributions. It’s a way to measure how “shaky” a function (in this case, a Brownian motion) is in a given interval.\ndef. Quadratic Variation. For a stochastic process {Xt​}, its quadratic variation on interval [0,t], [X]t​ for 0=t0​&lt;t1​&lt;⋯&lt;tn​=t, is:\nn→∞lim​i=1∑n​(Xti​​−Xti−1​​)2"},"Stonewall":{"title":"Stonewall","links":[],"tags":["Philosophy/Queer-Theory","History"],"content":"\nStonewall riots: A series of demonstrations by members of the LGBTQ+ community against a police raid that took place at the Stonewall Inn in Greenwich Village, New York City, in June 1969. This event is considered a pivotal moment in the fight for LGBTQ+ rights.\n"},"Stream-of-Content":{"title":"Stream of Content","links":[],"tags":["Computing/Internet"],"content":"In the internet, the problem is not finding content, but finding good content. Therefore, pick sparingly what you consume.\n\nUse RSS feeds to choose good sources to read\nWhen you have a list of content, pick out what you want, like how you would scoop a bucket from the river.\n\nDon’t try to read the whole thing. It’s not efficient or useful.\nChoose what seems enticing or relevant\nApplies to articles, books, movies, tv shows, games, etc.\n\n\n"},"Strings-in-Rust":{"title":"Strings in Rust","links":[],"tags":["Computing"],"content":"\nAll rust string types use UTF-8\nstr is a string literal. You never use this\n\nEncoded into the program binary\n\n\n&amp;str is a string slice\n\nThis is the borrowed typ\n\n\nString is a string class\n\nIt has length, size, etc. all the information\nThis is the owned type\n\n\n"},"Student's-T-Distribution":{"title":"Student's T-Distribution","links":["Besset-Correction"],"tags":["Math/Common-Distributions"],"content":"def. Student’s T Distribution.\ntn−1​∼σ^/n​∑i=0n​(Xn​ˉ​−μ)2​\nwhere:\n\nXi​∼N(μ,σ2)\nσ^ is the Besset-corrected standard deviation estimator\n\n\n\n                  \n                  Tip \n                  \n                \nIt’s called a Student’s t because it was from a beer brewery company, and the company didn’t want the in-house statistician to publish it so he published it secretly.\nIntroduction to the t Distribution (non-technical) - YouTube\n"},"Student's-t-test":{"title":"Student's t-test","links":[],"tags":["Math/Statistics"],"content":"def. let X1​,…,Xn​∼iidN(μ,σ2). We are comparing two hypothesis:\n\n\nH0​:μ≤μ0​\n\n\nH1​:μ&gt;μ0​\nThen, let T=σ^/n​Xˉn​−μ0​​ where σ^=n−1E[(Xˉn​−Xi​)]​ is the besset-corrected variance estimator. We know that T∼tn−1​. Thus the γ=1−α-level student’s (=Gosset’s) t-test is:\n\n\nδ:{H0​H1​​else if T&gt;tn−1​(1−α)​"},"Subset-Sum":{"title":"Subset Sum","links":["Common-Graph-Problems"],"tags":["Computing/Algorithms"],"content":"Q. Subset Sum Problem. Given a set of real numbers X={x1​,…,xn​} is there a subset of X that sum exactly to a certain integer k\n\nReduce from Vertex Cover\n"},"Substitution-Effect-(SE)":{"title":"Substitution Effect (SE)","links":[],"tags":["Economics/Micro-Economics"],"content":"Imagine you travel to the Cayman Islands with your twin. You both start with the same income. You choose to rent a car with upfront cost, while your brother chooses to take a taxi. Remember, you both have the same income; only different opportunity cost of miles traveled.\ndef. Substitution effect is when you change the consumption of a good due to a change in opportunity costs.\nObserve that you are still on the same indifference curve; the utility is the same.\n\nThe size of the effect has to do with how substitutable the two goods are; the more subsitutable the two goods, the bigger the substitution effect is (obvious verbally and graphically):\n"},"Sufficiency":{"title":"Sufficiency","links":["Exponential-Family"],"tags":["Math/Statistics"],"content":"Recall Exponential Family\nSufficiency §\ndef. let statistic T=T(X); and Xi​ depends on unknown parameter θ. Then T is a sufficient statistic for θ if P[X=x∣T=t] does not depend on θ.\n\n\n                  \n                  Info \n                  \n                \nIf the distribution of Xi​’s depends on unknown parameter θ, but the distribution of Xi​ given T=t does not depend on θ, it must be the case that all information about θ is in T. Once the value of T is given, then θ becomes irrelevant in determining the value of X.\n\n\n\n                  \n                  Info \n                  \n                \nThere’s only one minimally sufficient statistic, and all other sufficient staistics are a function of this.\n\nthm. Fischer-Neymann factorization. T is a sufficient statistic iff:\n\ni.e. the joint density function can be factored into a a function of u,v where:\n\nu(X) does not depend on θ\nv(T(X),θ) depends on θ, and depend on X only through statistic T\n⇒ Intuitively, this menas when\n\n\n\n                  \n                  Info \n                  \n                \nVery convenient for determining if statistic is sufficient or not.\n\nlem 1. let θ^1​ an estimator for θ and T a sufficient statistic for θ\nthen θ^2​=E[θ^1​∣T] is also a sufficient estimator.\nthm. Rao-Blackwell theorem. Continuing from above,\nMSE[θ^2​]≤MSE[θ^1​]\n\n\n                  \n                  Info \n                  \n                \ni.e. if you throw more data into a statistic, it often becomes a better statistic.\n\n\nIn the exponential family of distributions, you can mostly do one iteration of Rao-Blackwell algorithm to get a pretty good estimator.\n\nBias of θ^ and Bias of Rao-Blackwellized E[θ^∣T=t] is the same\n\n\nIf T is a minimally sufficient statistic, E[θ^∣T=t] is the best you will do\n"},"Sustainable-Development-Goals":{"title":"Sustainable Development Goals","links":["United-Nations"],"tags":["Economics"],"content":"By the United Nations"},"Targeting-a-Niche":{"title":"Targeting a Niche","links":[],"tags":["Computing/Internet"],"content":"(DevonThink) &gt; The more precise and niche the words I input, the better the internet would ma… | Hacker News"},"Taxation":{"title":"Taxation","links":["Laffer-Curve"],"tags":["Economics"],"content":"Laffer Curve"},"Taylor-Approximation":{"title":"Taylor Approximation","links":[],"tags":["Math/Calculus"],"content":"f(x)≈f(a)+f′(a)(x−a)+2!f′′(a)​(x−a)2+3!f′′′(a)​(x−a)3+⋯+n!f(n)(a)​(x−a)n\nf(x)=n=0∑∞​n!f(n)(a)​(x−a)n"},"Technical-Notes-Index":{"title":"Technical Notes Index","links":["Nimf-Anthy-installation","Hacking-Flash-Apps","Yokosuka","Atsugi","lightdm---How-do-I-hide-a-particular-user-from-the"],"tags":["Computing"],"content":"Tasks for Atsugi-hamonikr §\n\n add volume keybindings to script\n make (sudo) script for nimf-anthy installation (see below)\n Wezterm configs (see)\n\n\nNimf-Anthy installation\nHacking Flash Apps\nSetting cinnamon settings in the command line\nMaizuru: Personal computer\nYokosuka: Personally managed server\nAtsugi: Remotely managed servers\nYokosuka\nAtsugi\nGeneral References §\nGitHub - tmux-plugins/tmux-resurrect: Persists tmux environment across system restarts.\nGitHub - pyoky/vimrc: The ultimate Vim configuration: vimrc\nHow To Configure WebDAV Access with Apache on Ubuntu 14.04 | DigitalOcean\nInstalling with Docker\nInstall Docker Engine on Ubuntu\nHTTP server on Docker with HTTPS\nlightdm - How do I hide a particular user from the"},"Technical-Rate-of-Substitution":{"title":"Technical Rate of Substitution","links":[],"tags":["Economics/Micro-Economics"],"content":"TRS=−∂f/∂k∂f/∂l​=−MPK​MPL​​\nIsoquant\n\nSlope of isoquant is the technical rate of substitution\n\nIsocost\n\nc(l,k)=wl+rk\n"},"The-Human-Condition":{"title":"The Human Condition","links":[],"tags":["Humanities"],"content":""},"The-Personal-Computer":{"title":"The Personal Computer","links":["Tools-and-Structures-Define-Your-Capacity","Living-With-the-Internet","Lifelong-Learning","Decentralization","Multidependence","Everything-is-a-File","Reliable-and-Scalable","(Article)-Evergreen-notes"],"tags":["Computing/Human-Interface","Logistics"],"content":"\nThe Personal Computer is an extension of the brain, a Bicycle of the Mind.\nThe personal computer is not a tool. Instead, it is a person you have a relationship with.\n\nThis is because you do not fully control your computer (!=definition of a tool). You influence your computer, and the computer influences you back.\nThe relationship can be positive or negative.\n\n\nThe personal computer is the interface and vehicle with which you explore the internet universe.\n(DevonThink) How To Become A Hacker This is the gospel for behaving within a culture of hackers. Things that particularly stand out is about asking good questions, discipline and competence, and Lifelong Learning.\n\nPrinciples §\n\nLifelong Learning\n\n(DevonThink) How do you remember all the Linux commands? - Quora\n(DevonThink) Why Windows Causes Stupidity - Why It Matters\n\n\nDecentralization\n\nKey components of your computer systems should follow decentralized models\ne.g. ActivityPub &gt; Twitter\ne.g. IPFS &gt; HTTPS\nThis also comes out of the principle of multidependence.\nOpen-source instead of closed-source\n\ne.g. Linux &gt; macOS\n\n\n\n\nOwn Your data\n\nServices that pay for you to access your own data will eventually disappear. Own your data.\nYour disk space should be ~1/3 empty. Keep it at that to allow for flexibility but not wasted space.\nPlaintext is the best format.\n\n\nEverything is a File\n\nDon’t use Windows objects, proprietary databases for notes or databases that you don’t own. Systems that don’t embrace this approach will eventually likely fail.\n\n\nSoftware Maturity\n\nInstall mature software only (Operating Systems, etc.)\nan extension of having Reliable and Scalable things\n\n\nUse Software As They’re Intended\n\nespecially: business software shouldn’t be used for personal things.\ne.g. Obsidian &gt; Notion (business pivot)\n\n\nKnow to Surface Data\n\nPhotos go into where you view photos the most (Apple Photos library). Documents go where documents are surfaced the most (DevonThink).\n⇒ Data should be surface-able, like (Article) Evergreen notes\n\n\nLiving with Generative AI\n\nA “language calculator”\nEverybody gets a 100 interns. (GPT: The Second Renaissance - No Boilerplate)\n\n\n"},"Time-Complexity":{"title":"Time Complexity","links":["Recurrence-Relation"],"tags":["Computing/Algorithms"],"content":"Assumptions:\n\nRAM is constant time access\nMultiplication, addition, etc. are constant time\nConstant time operations are O(1)\nWe concern ourselves with asymptotic behavior, i.e. as n→∞\n\nRecall there are only two models of computation\n\nTime complexity of loops: The time complexity of a loop is proportional to the number of iterations multiplied by the time complexity of the code inside the loop.\nTime complexity of recursion: The time complexity of a recursive function can be analyzed using a Recurrence Relation, which expresses the time complexity of the function in terms of its input size and the time complexity of its subproblems.\n\nRules of Computing Complexity §\n\nRule of sum: If an algorithm performs two precedural operations, and the total time taken by each operation is proportional to the size of the input n, then the time complexity is O(f(n)+g(n)), where f(n) and g(n) are the time complexities of the two operations.\nRule of product: If an algorithm performs two nested operations, and the time taken by the inner operation is proportional to the size of the input n, and the outer operation is performed n times, then the time complexity is O(f(n)∗g(n)), where f(n) is the time complexity of the outer operation and g(n) is the time complexity of the inner operation.\nBig O notation ignores constant factors: When calculating the time complexity of an algorithm, we can ignore constant factors, such as the time taken by basic arithmetic operations, since they do not affect the overall growth rate of the function.\nBig O notation ignores lower-order terms: When calculating the time complexity of an algorithm, we can ignore lower-order terms, such as constants or logarithmic terms, since they become insignificant as n grows larger.\n\nNotation §\nComplexity is most often denoted in poly-logarithmic time:\nO(nklogjn)\nTypes of Complexity Analysis\n\nBest case analysis\nWorst case analysis\nAverage case analysis\nAmortized Analysis\n"},"Tradable-Inflation-Protected-Securities-(TIPS)":{"title":"Tradable Inflation-Protected Securities (TIPS)","links":[],"tags":["Economics/Finance"],"content":"Tradable Inflation-Protected Securities (TIPS) are a type of bond issued by the treasury that are like bonds (issued by govn’t, pays coupon), but its principal changes based on inflation."},"Trade-Through":{"title":"Trade-Through","links":["Efficient-Market-Hypothesis"],"tags":["Economics/Finance"],"content":"is when a transaction occurs at a worse price than the market price. It’s usually prevented by regulation. A type of market inefficiency, a failure of the Efficient Market Hypothesis"},"Traffic-Routing":{"title":"Traffic Routing","links":["Maximum-Flow-Problem","Potential-Game","Nash-Equilibrium"],"tags":["Computing/Algorithms","Economics/Game-Theory/Games"],"content":"Similar to Maximum Flow Problem. An instance of a Potential Game.\nSelfish Traffic Routing §\nDefinitions §\n\nflow on edge e is xe​\nCongestion Function ce​(xe​) i.e. time taken\nIndividual Delay for Path P: ∑e∈P​ce​(xe​)\nTotal Delay for all paths: ∑∀cars​∑e∈P​xe​⋅ce​(xe​)\n\nPigou’s Example §\n\n\nNash Equilibrium when\n\nFor all users of edge a, ca​(xa​)≤cb​(xb​)\nFor all users of edge b, cb​(xb​)≤ca​(xa​)\n\n\nTotal delay =(0⋅1)+(1⋅1)=1\n\n\nGlobal Optimum when:\n\nTotal Delay​=(xa​⋅1)+(xb​⋅xb​)=1−xb​+xb2​​​\n⇒ Total delay minimized when half goes thru a, other half goes thru b\nBraess’s Paradox §\ni.e. adding a new road may worsen outcome for Nash Equilibrium (but not in global minimum)\n\n\nConsider before and after adding edge from b→a, a superhighway with zero delay cost.\nNE before adding blue edge:\n\n0.5 on top (s→a→t)\n0.5 on bottom (s→b→t)\nTotal Delay=23​\n\n\nNE after adding blue edge (superhighway):\n\nall passes thru green path (s→b→a→t)\nTotal Delay=2 (worse!)\n\n\n\nFor Social Planner… §\nObserve that congestion network is a convex optimization problem:\n\nTotal Delay function is a convex function\nConstraints (=conversation of flow) are linear:\n\n∀v inflow = outflow\n∑ flow at source/target is 1\nfe​≥0\n\n\n\nFor Obtaining Nash Equilibirum… §\nthm. Routing NE Uniqueness. For every routing problem there is only one NE.\nthm. Obtaining NE of Congestion Network. Minimizing the following function ϕ will yield the unique NE of any congestion network:\nϕ(f​)=2ae​fe2​​+be​fe​\nAtomic Traffic Routing §\n\nThis time, there are n cars (not infinitesimally small). If player i chooses path Pi​, then:\n\nme​: traffic (=# of cars) flowing thru edge e\nIndividual cost: ci​(Pi​,P−i​​)=∑e∈Pi​​ce​(me​)\nPotential Function: ϕ(P)=∑∀e​∑j−1me​​ce​(me​)\n\nThis is similar to the integral of costs for each edge: \n\n\n"},"Traveling-Salesperson-Problem":{"title":"Traveling Salesperson Problem","links":["Minimal-Spanning-Tree-Problem","Depth-First-Search"],"tags":["Computing/Algorithms"],"content":"\n\n                  \n                  Problem: \n                  \n                \nGiven a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city?\n\n\n\nBrute Force: O(∣V∣!)\n\n\nHeld–Karp algorithm: O(n2⋅2n)\n\n\nRecord: O(1.728n)\n\n\nTSP is not in NP, and is NP-Hard\n\n\nMetric TSP (MTSP) §\nalg. Tree-MTSP Approximation.\n\nRun MST\nRun DFS on the MST, record pre-time\nTraverse the graph in the pre-time order\n\n\nIs 2-approximation\n\nalg. Christofids MTSP Approximation"},"Tree":{"title":"Tree","links":["graph"],"tags":["Computing/Data-Structures"],"content":"def. Tree. A Tree is an undirected connected acyclical graph\n\nBalanced Tree\nBinary Search Tree\n"},"Turing-Machine":{"title":"Turing Machine","links":["Limits-of-Math-and-Computing"],"tags":["Computing/Formal-Languages"],"content":"A Turing machine:\n\nIs most powerful type of automation (probably.)\nCannot solve the halting problem.\nCan be used to prove Limits of Math and Computing.\n\n\n\n                  \n                  Turing Machine \n                  \n                \n\n&gt;M=(Q,Σ,Γ,δ,q0​,B,F)&gt;\n\nwhere the previously unencountered symbols are:\n\nQ: States\nΣ: Input Alphabet\nΓ: Tape Alphabet (superset of Σ)\nq0​: Start state\nF: Set of en states\nB: Blank (also written as □)\nδ:Q×Γ→Q×Γ×{L,R}\n\n\nConfiguration and transition of a TM is denoted:\nwqv⊢w′q′v′\nδ(q,v)=(q′,v,R)\n→ where the configuration is denoted as (state, write symbol, move left/right)\nIn this case the head is reading the first symbol of v, i.e. the right-side letter.\n\nTuring Machines as Language Recognizers/Acceptors §\nL(M)={w∣q0​w⊢vqf​w}\n⇒ Then w is in the language of the TM.\nYou can also think of it as the turing machine “halting” on the final state.\n\nIf TM halts on a final state, w is accepted\nIf TM halts on a non-final state w is not accepted\nIf TM doesn’t halt, w is not accepted\n\nTuring Machines as a Transducer [= Transformation on a language] §\ndef. A function f(w) is Turing-Computable if:\n∀w,  q0​w⊢∗qf​f(w)\nExample of a Turing Machine representing a turing-computable function:\n\nTuring Machine Building Blocks §\n\nM1​→M2​\nM1​→xM2​: run if x is the current output ← you can also have mutiple conditionals\n\nYou have the building blocks of commonly-used TMs.\n\ns: start, h: halt\nx: write symbol x onto tape\nL,R: Move left, right\nLa​,Ra​: Move left or right until you see a in tape ← make sure there is an a on tape or it won’t halt\nL¬a​,R¬a​Move left or right until you don’t see a in input\n→a,b}→w: symbols a,b are represented as variable w ← avoids having to write two identical machines for each symbol\n\nSome more advanced building blocks:\n\nC: Copy with a zero in the middle. e.g. abba→abba0abba. Tape head starts and ends in the beginning symbol.\nSL​,SR​ Shift left what is on the right, v.v. The symbol on the head is erased.\n\nExample of using building blocks to simplify a turing machine:\n\nTuring Machine Equivalents §\n\nTM with Stay option: δ:Q×Γ→Q×Γ×{L,R,S}\nMultitrack TM: One tape, but split into n cells: δ:Q×Γn→Q×Γn×{L,R}\n\nDiagram\n\n\n\nSemi-infinite TM: The tape is infinite only in one direction\n→ Proof by “folding over” the standard TM into multi-track.\nMulti-tape TM: δ:Q×Γn→Q×Γn×{L,R}n\nOff-line TM: Two-tape; one tape is input, the other the read/write tape. δ:Q×Γ2→Q×Γ×{L,R}2\nNon-deterministic TM\nNPDA with 2 stacks\n\nUniversal Turing Machine §\nEvery TM can be binary encoded by a binary number:\n\nThe Universal Turing Machine is a 3-tape turing machine that simulates a standard Turing Machine M. Each of the tapes are:\n\nTape A: Binary encoding of a simulation of M\nTape B: The tape of M\nTape C: M’s current state\n"},"Types-of-Demand-Curves-(MicroEcon)":{"title":"Types of Demand Curves (MicroEcon)","links":["Elasticity-of-Substitution","Uncompensated-Demand-curve","Marginal-Willingness-to-Pay","Cross-Price-Demand-Curve"],"tags":["Economics/Micro-Economics"],"content":"Demand Curves §\nDemand is a relationship between prices, income, and quantity consumed.\nExogenous:(p1​,p2​,I)↔Endogenous:(x1​,x2​)\nProperties relating these variables are:\n\nx1​↔I: Income Elasticity of Demand (YED)\nx1​↔x2​: Elasticity of Substitution (EoS)\np1​↔x1​: Own-price Demand Curves…\n\nUncompensated Demand curve\n\n= Ordinary Demand Curve\n= Marshalian Demand Curve\n\n\nCompensated Demand Curve\n\n= Marginal Willingness to Pay Curve\n= Hicksian Demand Curve\n\n\n\n\np1​↔x2​: Cross-Price Demand Curve\n\np1​↔x2​: Cross Elasticity of Demand (XED)\n\n\n"},"Types-of-Goods-(Economics)":{"title":"Types of Goods (Economics)","links":["Income-Effect-(IE)","Uncompensated-Demand-curve","Game-Theory","Prisoner's-Dillemma"],"tags":["Economics/Micro-Economics"],"content":"Economic Goods §\nNote:\n\nNormal Good ⊂ Ordinary Good\nGiffen Good ⊂ Inferior Good\n\nDepending on Price and Quantity Demanded §\n\nOrdinary Good: Follows the Law of Demand: Price∝1/QDemand​\nGiffen Good: Price∝QDemand​\n\n⇒ Cause: Income Decrease ⇒ Buy cheaper Giffen Good\nIrish Famine. the price of potatoes and meat increased subsequently. Compared to meat, potatoes could be much cheaper as a staple food.\n\n⇒ Potatoes were a Giffen Good\n\n\n\n\n\nDepending on Income and Quantity Demanded §\n\n\n                  \n                  Notation: \n                  \n                \n\nYED:= Income elasticity of demand-\nIncome Effect (IE) is important in understanding economic goods\n\n\n\nNormal Good: Income∝QDemand​\n\ni.e. ∂I∂x1​(p1​,p2​,Iˉ)​&gt;0 (derivative from Marshallian Demand)\nNecessity Good (Necessities)\n\n0&lt;∂I∂x1​​&lt;1\n\n\nLuxury Good\n\n1&lt;∂I∂x1​​\n…i.e. a normal good for which the proportional consumption increase exceeds the proportional Increase\n\n\n\n\nInferior Good: Income∝1/QDemand​\n\ni.e. ∂I∂x1​​&lt;0\n\n\nQuasilinear Good: QDemand​ doesn’t change on income change\n\ni.e. ∂I∂x1​​=0\n\n\n\nFor the set of all goods we can thus show the following Venn diagram:\n\nGame Theory Goods §\n\n\nCommon-pool resources are also called Common Goods.\nPublic Goods\n\nThe production of a public good is a Prisoner’s Dillemma situation.\nBasic examples: national defense, street lighting, a good schooling district, etc.\n\n\n"},"Types-of-Loans":{"title":"Types of Loans","links":["Annuity"],"tags":["Economics/Finance"],"content":"Types of Loans\n\nAmortizing Loan (=Annuity): borrower regularly pays back interest and principal\nSimple interest: borrower regularly pays back only interest. Principal is paid back altogether at the end.\n"},"UI-Design-Tips":{"title":"UI Design Tips","links":["dont-make-the-user-think","(Article)-Magic-Ink---Information-Software-and-the-Graphical-Interface"],"tags":["Computing/Human-Interface"],"content":"Principles §\ndont make the user think\n(Article) Magic Ink - Information Software and the Graphical Interface\nStart here §\nDetermine a color pallette\n\nWhat “atmosphere” are you going for? what\n"},"Uncompensated-Demand-curve":{"title":"Uncompensated Demand curve","links":["Homogenous-Function","Utility-Maximization","Types-of-Goods-(Economics)","Utility-Maximization-with-Endowments","Income-Effect-(IE)"],"tags":["Economics/Micro-Economics"],"content":"\n\n                  \n                  Abstract \n                  \n                \n\nThe canonical, standard demand curve when we normally say “demand of good x”\nIn theory, it should be the actual demand curves of individuals\nAll Marshalian demand curves are HD0 in (prices,income)\n\n\nHow to Derive OPDC §\nVisually:\n\nPlot the indifference map and budget line for goods x1​ and x_2=\\text{&quot;\\ of other goods”}.Chooseanoptimalbundlex_1atpricep_1.Plotthisontheprice−quantitygraph:A(x_1,p_1)$\nChange the price p1​→p1′​; i.e. change the gradient of the budget. See what substitution and income effects it produces.\nThen find the optimum bundle for the new price: C(x1′​,p1′​). Connect these points together.\n\nAnalytically:\n\nmaxx1​,x2​​ u(x1​,x2​) such that I=p1​x1​+p2​x2​ using Utility Maximization\nyou will get x1​(p1​,p2​,I),x2​(p1​,p2​,I)\n\ni.e. quantity demanded as a function of prices and income\nThis is the demand function → plot this to get demand curve\n\n\n\nAs stated above: OPDC must be HD0; check if it is!\nAnalysis of Goods §\n\nIn a demand curve, Price is the negative gradient of the budget line. (∵g=−p2​p1​​, and we defined p_2=\\1$)\n\nDepending on Price §\n∂p1​∂x1​​ &gt;0&lt;0​⟹Ordinary Good⟹Giffen Good​​\n\nOrdinary goods’s demand functions x1​ are homogenous in (p1​,p2​,I)\n\n→ Inflation in all prices as well as income doesn’t change anything\n\n\n\nDepending on Income §\n∂I∂x1​​ &gt;0=0&lt;0​⟹Normal Good⟹Quasilinear Good⟹Inferior Good​​\n\n\nGraph (a) shows a normal good’s OPDC.\nGraph (b) and (c) shows a inferior good’s OPDC.\nGraph (c) is the strange Giffen Good.\n\nLabor Supply Curve §\nSee also Utility Maximization with Endowments\nDeriving the Labor Supply Curve works similarly to the OPDC, with the following differences:\n\np1​ = (gradient of budget line) = (opportunity cost of leisure in dollars) = ∗∗w (wage)**\nThere’s no exogenous income. You’re endowed a bundle with zero consumption and certain hours of leisure.\n\n→ You can “sell off” your leisure hours at price w\n\n\n\nThus as wages increase (= price of leisure in dollars)\n\n→ the shift in the budget line happens from blue→pink in graph below…\n\n…with a stationary leisure endowment at the x-intercept and a clockwise rotation.\n\n\nThen after you draw the OPDC for leisure, you flip the curve horizontally.\n(since endowment=leisure hours+work hours.)\n\n\n\nGraph (a) shows the case where leisure is a normal good, but IE &gt; SE.\nGraph (b) is Normal good, IE &lt; SE.\nGraph (c) shows when leisure is an inferior good.\n\n\n\n                  \n                  Warning \n                  \n                \n\nWhen discussing labor supply, the Income Effect (IE) is called the wealth effect (WE). We’re not discussing why.\nBackward Bending Supply of Labor §\nIn real life, labor supply curves look like this:\n\nThis is because workers will trade-off leisure and work hours.\n→ the higher the person’s wage, they will work more; but at some point their wage is large enough and they start to enjoy life more. This is called the Backward Bending Supply of Labor.\nInteresting IRL Analysis §\ncdixon | How bundling benefits sellers and buyers\nDeriving Uncompensated Demand §"},"Unconstrained-Maximization":{"title":"Unconstrained Maximization","links":["Critical-Point"],"tags":["Math/Calculus"],"content":"\n\nThe first-order necessary condition for a maximum or minimum. If a function has a maximum or minimum at a certain point, then its derivative at that point is zero\n\nIf f(x) has a maximum or minimum at x=c, then f′(c)=0.\n\n\n\nThe second-order necessary condition for a maximum. If a function has a maximum at a certain point, then its second derivative at that point is less than or equal to zero:\n\nIf f(x) has a maximum at x=c, then f′′(c)≤0.\n\n\n\nThe second-order sufficient condition for a maximum. If a function’s first derivative is zero at a certain point and its second derivative at that point is less than zero, then the function has a maximum at that point:\n\nIf f′(c)=0 and f′′(c)&lt;0, then f(x) has a maximum at x=c.\n\n\n\nTo find the unconstrained maximum of a function, we can set its derivative equal to zero and solve for ‘x’. Then, we use the second derivative test to determine whether each solution is a maximum or minimum.\n\nFind unconstrained maximum of f(x)\n\nset f′(x)=0 and solve for ‘x’.\nUse f′′(x) to verify maxima.\n\n\n\n\n"},"Unemployment":{"title":"Unemployment","links":[],"tags":["Economics/Macro-Economics"],"content":"Unemployment u=N+UU​\nParticipation Rate p=populationN+U​\nVacancy Rate v=U#vacancies​\nMeasured using three variables:\n\nu the unemployment rate\np the participation rate\nv the vacancy rate\n\n…where…\n\nU is the number of registered unemployed people\nN is the number of employed people\nThe labor market is tight when Dlabor​&gt;Slabor​ and loose vice versa.\n"},"Uniform-Distribution":{"title":"Uniform Distribution","links":[],"tags":["Math/Common-Distributions"],"content":"Uniform Distribution §\ndef. Uniform Distribution. X has uniform distribution of it has a uniform density on interval (a,b):\nX∼Unif(a,b)fX​(x)={b−a1​0​x∈(a,b)else​FX​(t)=∫−∞t​fX​(x)dx=⎩⎨⎧​b−at−a​10​t∈(a,b)t&gt;bt&lt;a​​\nE(X)=2a+b​       SD(X)=23​b−a​\nEstimators §\nlet\n\nX∼Unif(a,b)\nX1​,…,Xn​∼iidUnif(a,b)\n\nLog likelihood:\nlnLn​(a,b∣x1​,...,xn​)=ln(b−a)n1​\nscore:\nsn​(a)=b−an​\nsn​(b)=b−a−n​\nMLEs:\na^MLE​=min(X1​,...,Xn​)\nb^MLE​=max(X1​,...,Xn​)"},"Univariate-Distribution-Relationship-Chart":{"title":"Univariate Distribution Relationship Chart","links":["Distribution-(Math)"],"tags":["Math/Statistics"],"content":"Useful resource for visualizing the relationship between distributions.\nURL: &lt;http://www.math.wm.edu/~leemis/chart/UDR/UDR.html&gt;\n"},"Unrestricted-Grammar":{"title":"Unrestricted Grammar","links":[],"tags":["Computing/Formal-Languages"],"content":"\n\ndef. If G is an Unrestricted Grammar, then L(G) is recursively enumerable [=TM equivalent]. This is equivalent to all productions following the form:\n\n\n(V∪T)+→(V∪T)∗\ndef. G is a Context-Sensitive Grammar if its productions follow the form:\n(V∪T)+→(V∪T)∗and∣LHS∣≤∣RHS∣andthere is no λ in the RHS"},"Use-value,-Exchange-value":{"title":"Use value, Exchange value","links":["Use-value,-Exchange-value","Fetishization","Emergent-Phenomena"],"tags":["Humanities/Marxism"],"content":"consumption. They constitute the material content of wealth, whatever its social form may be. In the form of society to be considered here they are also the material bearers [Triiger] of … exchange-value.\nThis common element exchange value cannot be a geometrical, physical, chemical or other natural property of commodities.\nBut clearly, the exchange relation of commodities is characterized precisely by its abstraction from their use-values.\nIf then we disregard the use-value of commodities, only one property remains, that of being products of labour.\nIf we make abstraction from its use-value, we abstract also from the material constituents and forms which make it a use-value. It is no longer a table, a house, a piece of yam or any other useful thing.\nWith the disappearance of the useful character of the products of labour, the useful character of the kinds of labour embodied in them also ~sappears; this in tum entails the disappearance of the different toncrete forms of labour.\netc. This is their plain, homely, natural form. However, they are only commodities because they have a dual nature, because they are at the same time objects of utility and bearers of value. Therefore they only appear as commodities, or have the form of commodities, in so far as they possess a double form, i.e. natural form and value form\nFetishization\nthing. But as soon as it emerges as a commodity, it changes into a thing which transcends sensuousness. It not only stands with its feet on the ground, but, in relation to all other commodities, it stands on its head, and evolves out of its wooden brain grotesque ideas far more wonderful than if it were to begin dancing of its own free will. 21\nThe mysterious character of the commodity-form consists therefore simply in the fact that the commodity reflects the social characteristics of men’s own labour as objective characteristics of the products of labour themselves,\nreligion. There the products of the human brain appear as autonomous figures endowed with a life of their own, which enter into relations both with each other and with the human race. So it is in the world of commodities with the products of men’s hands. I call this the fetishism which attaches itself to the products of labour as soon as they are produced as commodities, and is therefore inseparable from the production of commodities.\nformulas, which bear the unmistakable stamp of belonging to a social formation in which the process of production has mastery over man, instead of the opposite, appear to the political economists’ bourgeois consciousness to be as much a self-evident and nature-imposed necessity as productive labour itself.\nvalues. When they thus assume the shape of values, commodities strip off every trace of their natural and original use-value, and of the particular kind of useful labour to which they owe their creation, in order to pupate into the homogeneous social materialization of undifferentiated human labour.\ncrisis. There is an antithesis, immanent in the commodity, between use-value and value, between private labour which must simultaneously manifest itself as directly social labour, and a particular concrete kind of labour which simultaneously counts as merely abstract universal labour, between the conversion of things into persons and the conversion of persons into things*; the antithetical phases of the metamorphosis of the commodity are the developed forms of motion of this immanent contradiction.\nAs against this, the circulation of money as capital is an end in itself, for the valorization of value takes place only within this constantly renewed movement.\nCooperation\nmanufacture [Manufaktur] can hardly be distinguished, in its earliest stages, from the handicraft trades [Handwerksindustrie] of the guilds, except by the greater number of workers simultaneously employed by the same individual capital.\nWhen consumed in common, they give up a smaller part of their value to each single product; partly because the total value they part with is spread over a greater number of products, and partly because their value, although it is greater in absolute terms, is relatively Jess, looked at from the point of view of their sphere of action, than the value of separate means of production.\nJust as the offensive power of a squadron of cavalry, or the defensive power of an infantry regiment, is essentially different from the sum of the offensive or defensive powers of the individual soldiers taken separately, so the sum total of the mechanical forces exerted by isolated workers differs from the social force that is developed when many hands co-operate in the same undivided operation, such as raising a heavy weight, turning a winch or getting an obstacle out of the way. 4 In such cases the effect of the combined labour could either not be produced at all by isolated individual labour, or it could be produced only by a great expenditure of time, or on a very dwarf-like scale. Not only do we have here an increase in the productive power of the individual, by means of co-operation, but the creation of a new productive power, which is intrinsically a collective one. 5\nIf the labour process is complicated, then the sheer number of the co-operators permits the apportionment of various operations to different hands, and consequently their simultaneous performance. The time necessary for the completion of the whole work is thereby shortened. 9\nWhether the combined working day, in a given case, acquires this increased productivity because it heightens the mechanical force of labour, or extends its sphere of action over a greater space, or contracts the field of production relatively to the scale of production, or at the critical moment sets large masses of labour to work, or excites rivalry between individuals and raises their animal spirits, or impresses on the similar operations carried on by a number of men the stamp of continuity and manysidedness, or performs different operations simultaneously, or economizes the means of production by use in common, or lends to individual labour the character of average social labour - whichever of these is the cause of the increase, the special productive power of the combined working day is, under all circumstances, the social productive power of labour, or the productive power of social labour. This power arises from co-operation itself. When the worker co-operates in a planned way with others, he strips off the fetters of his his individuality, and develops the capabilities of his species.13 As a general rule, workers cannot co-operate without being brought together: their assembly in one place is a necessary condition for their co-operation. Hence wage-labourers cannot cooperate unless they are employed simultaneously by the same capital, the same capitalist, and therefore unless their labourpowers are bought simultaneously by him.\nHence, concentration of large masses of the means of production in the hands of individual capitalists is a material condition for the co-operation of wage-labourers, and the extent of co-operation, or the scale of production, depends on the extent of this concentration.\nproduction. That a capitalist should command in the field of production is now as indispensable as that a general should command on the field of battle.\nAs the number of the co-operating workers increases, so too does their resistance to the domination of capital, and, necessarily, the pressure put on by capital to overcome this resistance.\nwage-labourer. An industrial army of workers under the command of a capitalist requires, like a real army, officers (managers) and N.C.O.s (foremen, overseers), who command during the labour process in the name of capital.\nprocess.17 It is not because he is a leader of industry that a man is a capitalist; on the contrary, he is a leader of industry because he is a capitalist.\nHe pays them the value of 100 independent labour-powers, but he does not pay for the combined labour-power of the 100. Being independent of each other, the workers are isolated. They enter into relations with the capitalist, but not with each other.\nJust as the social productive power of labour that is developed by co-operation appears to be the productive power of capital, so co-operation itself, contrasted with the process of production carried on by isolated independent workers, or even by small masters, appears to be a specific form of the capitalist process of production.\nJameson\nAt this point, then, the quality of the various forms of human activity, their unique and distinct “ends” or values, has effectively been bracketted or suspended by the market system, leaving all these activities free to be ruthlessly reorganized in efficiency terms, as sheer means or instrumentality.\nWhat is unsatisfactory about the Frankfurt School position is not its negative and critical apparatus, but rather the positive value on which the latter depends, namely the valorization of traditional modernist high art as the locus of some genuinely critical and subversive, “autonomous” aesthetic production.\nFor all these reasons, it seems to me that we must rethink the opposition high culture/mass culture in such a way that the emphasis on evaluation to which it has traditionally given rise, and which-however the binary system of value operates (mass culture is popular and thus more authentic than high culture, high culture is autonomous and therefore utterly incomparable to a degraded mass culture)-tends to function in some timeless realm of absolute aesthetic judgment, is replaced by a genuinely historical and dialectical approach to these phenomena. S\nIndeed, this view of the emergence of mass culture obliges us historically to respecify the nature of the “high culture” to which it has conventionally been opposed: the older culture critics indeed tended loosely to raise comparative issues about the “popular culture” of the past.\nThe above reflections by no means raise, let alone address, all the most urgent issues which confront an approach to mass culture today. In particular, we have neglected a somewhat different judgment on mass culture, which also loosely derives from the Frankfurt School position on the subject, but whose adherents number “radicals” as well as “elitists” on the Left today. This is the conception of mass culture as sheer manipulation, sheer commercial brainwashing and empty distraction by the multinational corporations who obviously control every feature of the production and distribution of mass culture today. If this were the case, then it is clear that the study of mass culture would at best be assimilated to the anatomy of the techniques of ideological marketing and be subsumed under the analysis of advertising. R\nRather, class struggle, and the slow and intermittent development of genuine class consciousness, are themselves the process whereby a new and organic group constitutes itself, whereby the collective breaks through the reified atomization (Sartre calls it the seriality) of capitalist social life.\nmaterial. To rewrite the concept of a management of desire in social terms now allows us to think repression and wish-fulfillment together within the unity of a single mechanism, which gives and takes alike in a kind of psychic compromise or horse-trading, which strategically arouses fantasy content within careful symbolic containment structures which defuse it, gratifying intolerable, unrealizable, properly imperishable desires only to the degree to which they can again be laid to rest. This model seems to me to permit a far more adequate account of the mechanisms of manipulation, diversion, degradation, which are undeniably at work in mass culture and in the media. In particular it allows us to grasp mass culture not as empty distraction or “mere” false consciousness, but rather as a transformational work on social and political anxieties and fantasies which must then have some effective presence in the mass cultural text in order subsequently to be “managed” or repressed.\nNow the content of the partnership between Hooper and Brody projected by the film may be specified socially and politically, as the allegory of an alliance between the forces of law-and-order and the new technocracy of the multinational corporations: an alliance which must be cemented, not merely by its fantasized triumph over the ill-defined menace of the shark itself, but above all by the indispensable precondition of the effacement of that more traditional image of an older America which must be eliminated from historical consciousness and social memory before the new power system takes its place. T\nThis is the context in which the ideological function of the myth of the Mafia can be understood, as the substitution of crime for big business, as the strategic displacement of all the rage generated by the American system onto this mirror-image of big business provided by the movie screen and the various tv series, it being understood that the fascination with the Mafia remains ideological even if in reality organized crime has exactly the importance and influence in American life which such representations attribute to it. T\nThompson\n? If the transition to mature industrial society entailed a severe restructuring of working habits—new disciplines, new incentives, and a new human nature upon which these incentives could bite effectively—how far is this related to changes in the inward notation of time?\nIn a similar way labour from dawn to dusk can appear to be “natural” in a farming community, especially in the harvest mo\nClearly hunters must employ certain hours of the night to set their snares. Fishing and seafaring people must integrate their lives with the tides.\nLet us return from the timepiece to the task. Attention to time in labour depends in large degree upon the need for the synchronization of labour\nThe work pattern was one of alternate bouts of intense labour and. of idleness, wherever men were in control of their own working lives (The pattern persists among some self-employed—artists, writers, small farmers, and perhaps also with students—today, and provokes the question whether it is not\nimposition. This remains true to this day, and, despite school times and television times, the rhythms of women’s work in the home are not wholly attuned to the measurement of the clock.\nWe are entering here, already in 1700, the familiar landscape of disciplined industrial capitalism, with the time-sheet, the timekeeper, the informers and the finest\nPowell, in 1772, also saw education as a training in the “habit of industry”; by the time the child reached six or seven it should become “habituated, not to say naturalized to Labour and Fati\nIn all these ways - by the division of labour; the supervision of labour; fines; bells and clocks; money incentives; preachings and schoolings; the suppression of fairs and sports—.new labour habits were formed, and a new time-discipline was imposed.) It sometimes took several generations (as in the Potteries), and we may doubt how far it was ever fully accomplished: irregular labour rhythms were perpetuated (and even institutionalized) into the present century, notably in London and in the great ports.114"},"Utility-Function":{"title":"Utility Function","links":["Indirect-Utility-Function","Rationality-(Economics)","Monotonic-Transformation","Marginal-Rate-of-Substitution-(MRS)","utility-function","Utility-Function","Budget-Lines","Untitled-2-10.png"],"tags":["Economics"],"content":"for mapping (p1​,p2​,I)↦u, see the Indirect Utility Function\ndef. Utility Function. a utility function maps n goods to utility (happiness) that satisfies the assumption of Rational Taste including convexity.\nU(x):R2↦R\n⇒ such that if x1​​≻x2​​ then u(x1​​)&gt;u(x2​​)\nHomothetic, Quasilinear, Perfect Substitutes §\nIn addition to the constrains of Rational Taste we can also have these particular tastes that characterize a utility function.\ndef. Quasilinear Tastes. If a utility function has quasilinear tastes against good x1​, then the function (or a Monotonic Transformation of the function) is linear against x1​, e.g.:\nu(x1​,x2​)=x1​+f(x2​)\n\nSubstitutability §\ndef. Perfect Substitutes. If a utility function means goods x1​ and x2​ are perfect substitutes, then every indifference curve of the utility function is a linear function, e.g.\n\\displaylinesu(x1​,x2​)=x1​+x2​⟹setting u=uˉ,uˉ=x1​+x2​ is linear between x1​,x2​\n\ne.g. [[Untitled 7 2.png|5billsand10 bills]]\nSee Marginal Rate of Substitution (MRS)\n\nPerfect Compliments.\n\ne.g. ![[Untitled 6 2.png|Right shoe and left shoe|380]]\nFormula looks something like: u(x1​,x2​)=min(x1​,2x2​)\n\nIndifference Curves §\n\ndef. Indifference Curve. A set of bundles that an agent with rational taste is indifferent about\n\nIndifference curves are horizontal slices of a utility function.\n\ni.e. a level field of a scalar field defined by the utility function of two goods—u(x1​,x2​).\nIndifference curves from the same utility function cannot cross (obviously)\n\n\nNorth-east side is always better.\n\n∵ monotonic taste—if this is not the case, change the direction of the curve.\n\n\nThe convexity assumption causes ICs to bend to the origin\nMultiple indifference curves form one indifference map =[Utility Function].\nIndifference maps are considered the same when\n1. the order of the indifference curves is the same\n2. MRS at every point for every curve is the same\nBudget Lines are drawn on the same graph as ICs\nWhen tastes are strictly convex (a taste for variety) then:\n\n⇒ (A≿B)∧(B≿C) in the Graph\n\n\n\nAnalyzing Indifference Curve Shapes §\nu(x1​,x2​)=x1α​+x2​\n"},"Utility-Maximization-with-Endowments":{"title":"Utility Maximization with Endowments","links":["Positive-Leisure","Cobb-Douglas-Utility","Uncompensated-Demand-curve","Labor-Supply","Laffer-Curve"],"tags":["Economics/Micro-Economics"],"content":"Budget constraint with endowment\np1​x1​+p2​x2​=Endowmentsp1​e1​+p2​e2​​​+I\n\n⇒ Whether the change in price leads to a utility increase or decrease depends on where your original optimum is; i.e.\n\nWhether you are a net seller or net buyer of the good (optimum is left/right of endowment)\nWhich way the price ratio changes\n\nLeisure—Consumption Utility §\nSame equation as above, but with Consumption-Positive Leisure tradeoff\nConsumptionwL+pc​=Exogenous Income+Endowment=I+wT+pe​​\n…where…\n\nLabor quantities:\n\nw: wage (=price of leisure)\nL: quantity of leisure, in unit hours\nT: total time endowment, in unit hours\n\n\nConsumption quantities:\n\np: price of composite consumption good\nc: quantity of composite consumption good\ne: consumption endowment\n\n\n\nCobb-Douglas:\nmaxL,c​ u=Lαc(1−α) such that wL+pc=I+wT+pe\ntherefore we can plot the Ordinary Demand of Leisure (=Labor Supply)\n\nc=pα(I+wT+pe)​ Ordinary Demand of Consumption\nL=wα(I+wT+pe)​ Ordinary Demand of Leisure\n\nN:=T−L=T−wα(I+wT+pe)​ ==Ordinary Supply of Labor==\n\n\n\nTaxation §\nPercentage tax t on wage (=price of leisure)\n⇒ removing t from the wage\nw(1−t)L+pc=I+w(1−t)+pe\n\nTax Revenue (T−L)​Time worked​×wtHourly tax\nLaffer Curve\n"},"Utility-Maximization":{"title":"Utility Maximization","links":["Utility-Function","Budget-Lines","Constrained-Optimization","Monotonic-Transformation","Uncompensated-Demand-curve","Expenditure-Minimization","Homogenous-Function"],"tags":["Economics"],"content":"max u(x1​,x2​) such that I=p1​x1​+p2​x2​\nMaximization of the Utility Function against the Budget Constraints.\n\nUses Constrained Optimization\n&amp; If it makes it more convenient, do a Monotonic Transformation of the utility function. See Example.\nOptimal is where the budget line is the tangent to the Indifference Curve. This implies both gradients are same, i.e. −P2​P1​​=MRS\nThe result is the Ordinary Demand functions (one for each good)\nSee Expenditure Minimization for the opposite case\n! Always check if the resulting x1​,x2​ are positive. If not, it’s piecewise function to keep both positive.\n\nPerfect Substitutes §\nWith the form: u=(ax1​k+bx2​l)α\n\n⇒ Go to the corner that gives the highest utility (=buy only one good!)\nx1​={p1​I​0​if u(p1​I​,0)≥u(p2​I​,0)else​, x2​={p2​I​0​if u(0,p2​I​)≥u(0,p1​I​)else​\nPerfect Compliments §\nmaxx1​,x2​​ u(x1​,x2​)=min(x1​,2x2​)  such that I=p1​x1​+p2​x2​\n\n\nGet the formula that is the set of all kinked points in the utility function ⇒ x1​=2x2​\nGet the Constraint formula ⇒ I=p1​x1​+p2​x2​\nSolve for the two equations (=get the intersection)\n\nx1​=2p1​+p2​I​ , x2​=2p1​+p2​2I​\nQuasilinear Optimization §\nUse Lagrangian optimization, but beware that the blue line might happen (=maximum point lies outside x1​,x2​&gt;0):\n\n⇒ In this case, go to the red (*) corner solution.\nKinked Budget Constraint §\n\nCase 1: Inner kink\n\nLagrangian for blue section\nLagrangian for red section\nChoose the better one\n\n\nCase 2: Outer Kink\n\nLagrangian for blue and red section\nIf both solutions are unaffordable (=ICA​,ICB​ in the graph i.e. x2​&lt;0 in graph,), go to the kink.\n\n\n\nMore than Two Goods §\n\nCase 1: u(x1​,x2​,x3​)=x1α​x2β​x31−α−β​ ← Pure three-var cobb-douglas\n\n⇒ Use 3-var lagrangian\n\n\n==Case 2: u(x1​,x2​,x3​)=x1α​x21−α​+x3​==\n\nCheck that each term is Homogenous Degree of 1.\nTry the two-var lagrangian on the first term x1α​x21−α​ (assumping x3​ isn’t consumed)\nTry to maximize x3​ (assuming x1​,x2​ isn’t used)\nChoose the higher of the two utilities\n\nIf there is no concrete number, the cases differ on the conditions u(x1​,x2​,0)≶u(0,0,x3​)\n\n\n\n\n\nSolution for Case 2: \nmaxx1​,x2​,x3​​u=x1α​x21−α​+x3​  such that  I=p1​x1​+p2​x2​\n⎩⎨⎧​⎩⎨⎧​x1​=p1​αI​x2​=p2​1−αI​​x1​=x2​=0 and max x3​​If p1α​p21−α​αα(1−α)1−α​≥p3​1​Otherwise​\nGraphing a budget line with an indifference map, we can see that the bundle B1​ is where the consumer can achieve the most possible utility; where what is affordable = most possible utility\nTo find the bundle(=point) of maximum utility that is affordable, you can rephrase the problem as…\nWorked Example: Constrained Optimization Problem §\nUsing the Lagrange Method for max u(x1​,x2​) s.t. (budget line),\n∇u(x1​,x2​)=λ∇(p1​x1​+p2​x2​)p1​x1​+p2​x2​=I\nTo simplify further: ∇u(x1​,x2​)=λ∇(p1​x1​+p2​x2​−I) and thus let:\nL=∇u(x1​,x2​)−λ∇(p1​x1​+p2​x2​−I)\nto construct a set of equations where:\nδx1​δL​=0δx2​δL​=0p1​x1​+p2​x2​=I\nand solve the three equations. Note that the Lagrange method doesn’t work when:\n\nOne or more goods are non-essential, meaning that the budget line crosses the axes\n→ It’s a corner solution; i.e. the maximum point is at one of the intercepts, or at points where quantities of goods are negative\nTastes are non-convex, where there will be multiple solutions\nUtility Function are kinked or otherwise non-differentiable\n"},"Utility":{"title":"Utility","links":["Consumer-Surplus"],"tags":["Economics/Game-Theory"],"content":"Money. Utility. Consumer Surplus."},"VSCode-Extensions":{"title":"VSCode Extensions","links":[],"tags":["Computing"],"content":"\nangular language service\ncode spell checker\ncompare folders\ncss property sorter\ncss var complete\nerror lens\neslint\ngit lens\nhtml scss support\nindent rainbow\nintellicode\nintellicode api usage examples\nintellicode completions\njapanese language pack for visual studio code\nprettier - code formatter\nscss intellisense\nstylelint\ntodo highlight\nvim\nvscode-icons\npowershell\n"},"Valorization,-Surplus-Value":{"title":"Valorization, Surplus Value","links":[],"tags":["Humanities/Marxism"],"content":"Valorization is the addition of surplus value into capital.\n\nBegins with the curiosity:\n\nWhere did this M-C-M’ increase come from? ← magic! (actually capital?)\n\nPedagogical process of explaining valorization\n\nThe evolution of the money-owner to the capitalist\ncapitalist’s rant\n\n\n\n                  \n                  \\Delta M is the capitalist’s focus. C, or commodity, doens’t matter → overproduction, labor commoditization\n                  \n                \n\n\nHere is the answer:\n\nlabor’s use-val &gt; ex-val [=wages]\nCapitalist pays ex-val to buy the labor-hours. But what the laborer gives [=capitalist gets] is use-val (The distinguished character) for the amount of hours bought (by-hours)\nSurplus value is this difference\nA→B→C. C can be lengthened for abs. S-V. A-B can be shorted for rel. S-V\n\nImportance to marx’s pedagogy\n\npol-econ’s idea that wage = value of labor is an illusory description (Ch.19), they confuse the value of labor-power and value of labor i.e., the commiditized ex-val labor and the actual use-val of labor.\nTo pol-econ, wage = price ⇒ det. by demand/supply; if not the “natural price [=necessary price]”\n→ clarifies the source of M-C-M’, why is M&lt;M’ ← capital making capital\n→ reveals the justification/lawful pursuit of surplus-value, but why it doesn’t make sense on closer examination\n"},"Value-of-Money":{"title":"Value of Money","links":["Utility-Function","(Article)-Minimum-Viable-Superorganism--Melting-Asphalt","Future-Value-Calculations","Financial-markets","Gold-Standard"],"tags":["Economics/Finance"],"content":"\n\n                  \n                  A dollar now than a dollar tomorrow. \n                  \n                \n\nClass notes for detailed info.\nMoney’s Role §\n\nSolves the problem of the double coincidence of wants\n\nObserve the Edgeworth Box|420. This is a full barter system based on Indifference Curves.\n\n\nFunctions as a claim on future consumption (=store of value, claim on future consumption)\n\nI give you the perishable food now ⇒ you give me a claim on future consumption. \nDiscounting future value ← due to risk aversion and instant gratification (human psychology)\n\n\n\nSociology Perspective: §\nSee (Article) Minimum Viable Superorganism  Melting Asphalt\n\n\n                  \n                  Money is industrial-grade prestige status \n                  \n                \n\nTime Value of Money §\n⇒ Thus arises the time value of money and financial markets\n\nMoney is discounted using a discount rate (=interest)\nFinancial markets are essential for a store of value\n\nProperties Required in Physical Money §\n\nDivisible ⇒ numerical money. You can divide it into as much as reasonable in transaction\nScarce ⇒ gold/silver is often used\nDesirable ⇒ again, gold/silver.\nDurable ⇒ metallic goods\n\nSee the Gold Standard for more information"},"Variance":{"title":"Variance","links":["Covariance-&-Correlation"],"tags":["Math/Probability"],"content":"def. Variance of a random variable X:\nVar[X]=E[(X−μ)2]=E(X2)−[E(X)]2\nVar(X):=E((X−μ)2)≡∫−∞∞​(x−μ)2fX​(x)dx=E(X2)−[E(X)]2\nthm. (Variance Identities.)\n\nFor a single random variable X and constant n\n\nVar(n⋅X)=n2⋅Var(X) Quadratic Scaling\nVar(X+n)=Var(X) Translation Invariance\n\n\nFor any two random variables X,Y:\n\nVar(X+Y)=Var(X)+Var(Y)+2⋅E[(X−μx​)(Y−μy​)]\n\n\n…if X⊥Y (i.e. two are independent:\n\nVar(X+Y)=Var(X)+Var(Y)\n\n\n….if Independent and Identically distributed (i.i.d) random variables I1​,…,In​\n\nVar(I1​+⋯+In​)=n⋅Var(Ii​)\n\n\n\nSee also Covariance &amp; Correlation.\ndef. Standard Deviation of a random variable is:\nSD(X):=Var(X)​=E(X2)−[E(X)]2​\n\nFrom the properties of variance we have SD(aX+b)=∣a∣SD(X)+b\n\nStandardization. §\nMotivation. Sometimes it’s nice to have random variables to have E(X)=0 and Var(X)=0. Using translation invariance and quadratic scaling we can take any random variable and standardize it.\nthm. (Standardization) For random variable X with E(X)=μ and Var(X)=σ2,\nY=σX−μ​\n\nIts expected value (mean) is 0; i.e. E(Y)=0\nIts standard deviation is 1; i.e. Var(Y)​=1\n"},"Wealth-Effect-(WE)":{"title":"Wealth Effect (WE)","links":[],"tags":["Economics/Micro-Economics"],"content":"The Wealth effect is same as the income effect, except this time it is with endowments, so the math is a little different"},"Web-Dev":{"title":"Web Dev","links":[],"tags":["Computing"],"content":"Web Dev §\nThe Twelve-Factor App\nTop 5 Frontend Development Topics To Learn in 2019\ngetify/You-Dont-Know-JS\nTech stack rebuild for a new Facebook.com - Facebook Engineering\nLet’s Define Exactly What Atomic CSS is | CSS-Tricks\nQuick Start\nMonthly Getting Started / Web Dev Career Thread"},"Welfare-Theorems":{"title":"Welfare Theorems","links":["Pareto-Efficiency","Rationality-(Economics)"],"tags":["Economics/Micro-Economics"],"content":"thm. First Welfare Theorem. Market Equilibria ⊆ Pareto Efficiency\nthm. Second Welfare Theorem. If Preferences are convex, the Pareto Efficient Point can be supported as a market equilibrium."},"Wilcoxon-Rank-Sum-Test":{"title":"Wilcoxon Rank Sum Test","links":[],"tags":["Math/Statistics"],"content":""},"Wilcoxon-Signed-Rank-Test":{"title":"Wilcoxon Signed Rank Test","links":[],"tags":["Math/Statistics"],"content":""},"Worker-vs-Machine":{"title":"Worker vs Machine","links":["Proletatriat-(Marxism)","Industrial-Revolution"],"tags":["Humanities/Marxism"],"content":"Factories ← → Workers inversion/subject-object inversion, a moral indignation\n\nHumans as organs of the machine → machine dominates, is the agent; a quantity → qualitative change\n\nDifference between tool and machine [= the factory]\n\n\ntrained since childhood as functions of organs; transforms the person as an appendage\n\nEffects on the human body/soul\n\nDeprives work of all content ← extended alienation, abstraction of labor\n\nRevolts &amp; Machines\n\nMachine → labor revolt → suppressed by machinery (labor is easily replacable, putting machine v. worker against each other)\n\n\n\n                  \n                  industrial machine”, or is it more abstract “technological systems/organization systems”\n                  \n                \n\n\nit is the system not the machine that is at fault. Less of the literal machines, more of the productivity advantage/disadvantage\n"},"XPath-and-XQuery":{"title":"XPath and XQuery","links":["Extensible-Markup-Language","Regular-Expressions"],"tags":["Computing/Data-Science"],"content":"Query language for XML\nXPath §\n\nConsists of node (=element) and attribute\nuses and, or for conditionals (not |, &amp;, etc.)\n{xpath}/EMPS/EMP/PHONE[@type] checks for the existence of an attribute\n/bibliography/book/@ISBN returns all attribute values (ISBNs)\nfunctions:\n\n{xquery}x+y,x–y,x*y,x div y,x mod y\n{xquery}contains(a,b): a contains b\n{xquery}count($nodeset): number of child nodes in node set\n{xquery}position(): n-th node\nRegular Expressions matching {xquery}matches($string, $regex) e.g. {xquery}matches($input, &#039;H.*o W.*d&#039;)\n\n\nconditionals:\n\n{xquery}x=y note that one equals is fine, {xquery}x!=y\n{xquery}x&gt;y, {xquery}x &lt; y\n{xquery}and or not() works. (but &amp;,|,! does not!)\n\n\nAccess parent node as {xquery}child/.. without trailing backslash\nAccessing Attributes: {xquery}data($person/@name)\nFrequently used in conditions:\n\n{xquery}x + y\n{xquery}x - y\n{xquery}x * y\n{xquery}x div y\n{xquery}x mod y\n\n\nfunctions\n\n{xquery}contains(x, y): true if string x contains string y\n{xquery}count(node-set): counts the number of nodes in node-set\n{xquery}position(): returns the “context position” (roughly, the position of the current node in the node-set containing it)\n{xquery}last(): returns the size of the node-set containing the current node\n{xquery}name(): returns the tag name of the current element\n{xquery}sum(): returns sum of all matches\n\n\n\nXQuery §\nStandard format:\n&lt;result&gt;{\n\tfor $var in XPATH-QUERY (: Comments :)\n\tlet $var2 ...\n\twhere [condition]\n\tstable order by,\n\t\t$variable ascending,\n\t\t$variable descending\n\treturn &lt;elem&gt;{xquery}&lt;/elem&gt;\n}&lt;/result&gt;\nSort §\n\nConditionals §\n\nAxis Test §\n\n/: shorthand for child-or-self\n//: shorthand for descendant-or-self\n/..: shorthand for parent\none of self, attribute,\nparent, child, ancestor,† ancestor-or-self,† descendant, descendant-or-self,\nfollowing, following-sibling, preceding,† preceding-sibling,†\nnamespace\n†: These are reverse axes (=produce resulting node-sets in reverse document order)\nUse like: \n\nExistential Conditions (Exists some…) §\n\nDate Operations §\n\nDates will be printed as P dD T hH iM sS\n\nP just a P.\nd: Number of Days. D: constant\nT: constant\nh: Number of Hours, H: constant\ni: Number of Minutes, M: is constant\ns: Number of Seconds S: is constant\n\n\nDates can be\n\nsubtracted: {xquery}xs:date(&quot;1933-06-22&quot;) - xs:date(&quot;2000-01-01&quot;)\ncompared: {xquery}xs:date(&quot;1933-06-22&quot;) &gt;= xs:date(&quot;2000-01-01&quot;)\n\n\n\nDate functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear-from-dateTimeday-from-dateTimeminutes-from-dateTimeyear-from-dateday-from-dateminutes-from-timeyears-from-durationminutes-from-durationdays-from-durationmonth-from-dateTimehours-from-dateTimeseconds-from-dateTimemonth-from-datehours-from-timeseconds-from-timemonths-from-durationhours-from-durationseconds-from-duration\nTips and Tricks §\n\nNaming elements using variables:\n"},"Yield-Maturity-Curve-(Bond)":{"title":"Yield-Maturity Curve (Bond)","links":["Bonds-(Finance)","Federal-Funds-Rate"],"tags":["Economics/Finance"],"content":"Relationship between yield and maturity of a bond.\nInterest Rate Changes §\nThe government can change the federal funds rate, which influences the y-intercept of the yield curve:\n\n\nInflation ⇒ interest rate ↑ ⇒ firm borrowing ↓ ⇒ investment ↓\nRecession ⇒ interest rate ↑ ⇒ firm borrowing ↑ ⇒ investment ↑\n\n\n\n                  \n                  An inverted yield curve occurs as the bond market: \n                  \n                \nexpects a recession ⇒ expectes the government to increase the interest rate ⇒ long-maturity bonds are more appealing.\n\nMaking Money off Bonds §\n\nCredit Curves §\n\nGovernment bonds have the highest credit rating\nWhen comparing other types of bonds (e.g. corporate bonds), any bond with the same maturity has a higher yield than government bonds.\nSpread is the difference in yield between it and government bonds.\n\nSpread ∝ 1/Credit ∝ maturity, and is related to industry\n\n\n\n\n"},"Zero-Sum-Game":{"title":"Zero Sum Game","links":[],"tags":["Economics/Game-Theory"],"content":"def. Zero Sum Game. When each box in the payoff (cost) matrix sums to zero."},"index":{"title":"index","links":["Auction-Theory","Maximum-Flow-Problem","Stochastic-Calculus","CS-535-Algorithmic-Game-Theory","CS-330-Advanced-Algorithms","CS-334-Formal-Languages","CS-250-Architecture","ECON-205-Intermediate-Microeconomics-II","Math-581-Mathematical-Finance","Math-582-Financial-Derivatives","Econ-201-Intermediate-Microeconomics-I","Econ-204-Econometrics","Econ-210-Macroeconomics","Econ-371-Finance","Stat-432-Statistics","Stat-230-Probability","GSF-386-Politics-of-Sexuality","CulAnth-203-Marxism","Econ-361-Distributive-Justice","Phenomenology","Michel-Foucault","Judith-Butler"],"tags":["Courses"],"content":"\n\n                  \n                  Highlights \n                  \n                \n\nAuction Theory\nMaximum Flow Problem\nStochastic Calculus\n\n\nComputer Science §\n\n🌟 CS 535 Algorithmic Game Theory\n🌟 CS 330 Advanced Algorithms\n🌟 CS 334 Formal Languages\nCS 250 Architecture\n\nEconomics §\n\n🌟 ECON 205 Intermediate Microeconomics II\n🌟 Math 581 Mathematical Finance\nMath 582 Financial Derivatives\nEcon 201 Intermediate Microeconomics I\nEcon 204 Econometrics\nCS 535 Algorithmic Game Theory\nEcon 210 Macroeconomics\nEcon 371 Finance\nEcon 204 Econometrics\n\nMath §\n\n🌟 Stat 432 Statistics\n🌟 Math 581 Mathematical Finance\n🌟 Math 582 Financial Derivatives\nStat 230 Probability\n\nLiberal Arts §\n\nGSF 386 Politics of Sexuality\nCulAnth 203 Marxism\nEcon 361 Distributive Justice\n\nSee also:\n\nPhenomenology\nMichel Foucault\nJudith Butler\n"},"main-diagonal":{"title":"main diagonal","links":[],"tags":["Math/Linear-Algebra"],"content":"Main Diagonal\n\nAnti-diagonal\n"}}