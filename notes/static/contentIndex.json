{"(Article)-Equality-of-What-q":{"title":"(Article) Equality of What?","links":["Utilitarianism","Utility","Efficiency-(Statistics)","Microeconomics"],"tags":["article","Economics/Game-Theory","Philosophy/Political-Philosophy"],"content":"(DevonThink) Sen,Equality\n\nEquity is about asking “what should be equal?”\n\nLibertarians: Equality of Rights\nUtilitarians: Equal Weighting of Utility\nProgressive Taxation: Equality of Need Satisfaction\netc.\n\n\n⇒ The question then becomes: “Equality of What?”\n\nPursuit of equality in one domain often damages another domain\n\n⇒ arguments are often in the form of “Inequality in space A is acceptable, because it increases equality in space B.”\n\n\nCommon framing is between equality and liberty\n\ne.g. left: equality in needs. right: liberty and freedom\nThis is the wrong framing because there are lots of other candidates anyways\n\n\n\n\nAlternative framing: Efficiency (Statistics)\n\nEfficiency of utility ⇒ Pareto Optimum\nEfficiency of liberty ⇒ Libertarian Government\netc.\n\n\n"},"(Article)-Magic-Ink---Information-Software-and-the-Graphical-Interface":{"title":"(Article) Magic Ink - Information Software and the Graphical Interface","links":["Color-as-an-Extra-Dimension"],"tags":["Design","Computing/Human-Interface"],"content":"Notes on Magic Ink: Information Software and the Graphical Interface\n\n\n                  \n                  What You See is All You Get (WYSIAYG) \n                  \n                \n\n\nUI design is:\n\nHow information is presented: Graphic Design\n\nPersonal accounting, Health app visualization\n\n\nHow to manipulate and create: Industrial design\n\nPhotoshop, Illustrator\n\n\n\n\nUsers mostly want to learn something, not create something.\n⇒ Graphic design &gt; Industrial design.\n⇒ How information is presented &gt; How to create new things\nThus, paper graphic design is not only relevant, but is a baseline\n\n\n\n                  \n                  Example \n                  \n                \nIf a person is in the mood for a movie, what questions might she have?\n\nWhat movies are showing today, at which times?\nWhat movies are showing around a particular time?\nWhere are they showing?\nWhat are they about?\nAre they good?\n\nConsider this redesign.\n\n\n\nTimeline visualizes and invites time comparisons\n“What is showing” is boldly presented\nCinemas are color-coded for easy comparison\n\nContext-Sensitive Graphics §\n\nDigital interfaces are better than paper because information is context-sensitive.\n\nIt can link to more information\n(instead of having to search paper)\nIt can present data in different ways according to what the user needs\n(instead of being static data on paper)\n\n\n\nInteractivity is Bad §\n\nFor all information software, interaction == navigation in data-space\n\nContextual software should already know what user wants\nNavigation is effortful and bad.\n\n\nTo reduce manipulation…\n\nContext-Sensitive Specialized Controls:\nTight Feedback loops (immediate results)\n\n\n\n"},"(Book)-Godel-Escher-Bach":{"title":"Godel, Escher, Bach","links":["Rigor-Ambiguity-Axis","G.-W.-F.-Hegel"],"tags":["Computing/Formal-Languages"],"content":"In Rigor-Ambiguity Axis:\nIt’s a good analogy; a purely rigorous axiom system there’s statements that you cannot prove. We need to add the new axioms with deliberate shoice to resolve that contradiction (Synthesis) and generate new knowledge."},"(Book)-The-Case-for-Economic-Democracy":{"title":"The Case for Economic Democracy","links":[],"tags":["Economics/Game-Theory","Philosophy/Political-Philosophy"],"content":""},"(HowTo)-Cost-Minimization":{"title":"(HowTo) Cost Minimization","links":["Profit-Maximization","Lagrangian-Optimization"],"tags":["Economics/Micro-Economics"],"content":"See also Profit Maximization\nLong Run Cost Minimization §\nminl,k​ C=wl+rk such that x=f(x)\n\nLagrangian Optimization\nOptimal when Isoquant is tangent to the isocost\n\nHigher isoquant is not always better! it just means you’re producing more\n\n\n\n\n\n                  \n                  Warning \n                  \n                \nBeware when MC is monotonically decreasing. This means that the firm will produce zero or infinity, which is means it is a special case which you need to analyze separately.\n\n"},"(LLL)-Nomura-Traning":{"title":"(LLL) Nomura Traning","links":["Security-(Finance)","Bonds-(Finance)","Equity","Credit-Rating","Investment-Bank","Lifecycle-of-a-Trade","Interest-Rate-Arbitrage","Forward-Rate-Agreement","Probability","Expected-Value","Moment-(Probability)","Stochastic-Process","Quadratic-Variation","Stochastic-Calculus","Portfolio-Theory-(Risk)","Basel","Quantitative-Risk-Measurment","Fair-Value-Accounting","No-Arbitrage-Condition","Market-Risk","But-what-is-a-residual-q","Markov-Chain","Time-Series","Auto-Regressive-&-Moving-Average","Statistical-Inference-Using-ARMA","Auto-Regressive-&-Conditionally-Heteroskedatic","Data-Architecture","Cloud-Architecture","Hyperparameter-Search-Strategies","Student's-T-Distribution"],"tags":["Courses"],"content":"Finance §\nSecurity (Finance)\n\nFixed Income: Bonds\nEquity\nCredit Rating\nRole of Investment Banks\nLifecycle of a Trade\nFixed Income\n\nBonds: Govn’t (National/Municipal), Corporate\nStructured Products\nEmerging Market Debt\nRates Desk\n\nGovn’t Bonds\nBond futures\nInterest Rate Derivatives\n\n\n\n\nCredits Desk\nInterest Rate Arbitrage (Including Currency Carry Trade)\nPrime Client: institutional clients (hedge funds)\nCertificate of deposit\nForward Rate Agreement\n Prime client\n Total return swap\n credit default swap insurance\n Money market funds\n quantitative easing\n\nQuantitative Risk Management §\n\nProbability\n\nConditional Expectation for Stochastic Calculus\nMoment (Probability)\n\n\nStochastic Process\n\nQuadratic Variation\nStochastic Calculus\n\n\nPortfolio Theory (Risk)\n\nBasel\nQuantitative Risk Measurment\nFair Value Accounting\nRisk-Neutral Assumption\nMarket Risk\n\n\nProbabilistic/Statistical Models\n\nBut what is a residual?\nMarkov Chain\nTime Series\n\nAuto-Regressive &amp; Moving Average\nStatistical Inference Using ARMA\nAuto-Regressive &amp; Conditionally Heteroskedatic\n\n\n\n\n\nData Engineering §\n\nData Architecture\nCloud Architecture\n\nMachine Learning §\n\nTechnologies: Dask, Scikit Learn\nCross Validation\nHyperparameter Search Strategies\nModel Evaliation\n\nQQ Plot\nPermutation Feature Importance\nPDP &amp; ICE Plots Partial Dependence (PDPs) and Individual Conditional Expectation (ICE) Plots | Intuition and Math - YouTube\nLIME\n\n\nSpline functions\n\nQuestions:\n\na.s. convergence\nstrong/weak law of large numbers\nStudent’s T-Distribution, better understanding\n"},"(Proof)-Markov-Recurrence-by-Sum-Divergence":{"title":"(Proof) Markov Recurrence by Sum Divergence","links":["Hypergeometric-Distribution"],"tags":["Math/Probability"],"content":"let the indicators for (1) state being i at step n and (2) number of times process visits i:\nIn​={10​if Xn​=iif Xn​=i​ , Ni​=∀n∑∞​In​\nFirst we have:\nfi​​=P(n=1⋃∞​{In​=1}∣X0​=i)=P(N≥2∣X0​=i)=P(N≥k∣N≥k−1,X0​=i),∀k≥2​From E[Ni​]=E[∀n∑∞​In​]Strong Markov​​\nThe last line holds due to the strong markov property of process {Xt​}. Let stopping time T the time of the n−1st visit of i. Then:\n​P(N≥n∣N≥n−1,X0​=i)=P( visits once more... ∃t:XT+t​=i​​∣Ft​​ ...after n−1 times ​)=P(∃t:XT+t​=i∣XT​=X0​)=:fi​​Str. Mkv.​​\nThus we showed that the probability of process will be in state i for exactly m times will be:\nP(N≥m−1∣X0​=i)⋅P(∃t:XT+t​=i∣XT​=i)=fim−1​(1−fi​)\nThis is remeniscent of the Geometric Distribution, specifically:\nL(N∣X=0)∼Geo(p=1−fi​)\nA Geometric Distribution with mean 1−fi​1​\nNow looking at the definitions again:\n\nrecurrent if fi​=1⟺E[N∣X0​=i]=∞\ntransient if fi​&lt;1⟺E[N∣X0​=i]&lt;∞\nExploring this expectation further:\n\nE[N∣X0​=i]​:=E[∀n∑∞​In​∣X0​=i]=n=0∑∞​E[In​∣X0​=i]:=n=0∑∞​P[Xn​=i∣X0​=i]=n=0∑∞​Piin​​linearity​\n\nfi​=1⟺E[∑n=0∞​In​∣X0​=i]=∞\nfi​&lt;1⟺E[∑n=0∞​In​∣X0​=i]=\nFor an easier to calculate formulation, Then:\n\nE[n=0∑n​In​∣X0​=i]​=n=0∑​​​\n&lt;span style=&quot;float:right;&quot;&gt;■&lt;/span&gt;\n"},"(Self-Study)-Measure-Theory":{"title":"(Self-Study) Measure Theory","links":["Probability","Expected-Value","Stochastic-Process","Quadratic-Variation","Stochastic-Calculus","Dirac-Measure"],"tags":["Math"],"content":"\nProbability\n\nConditional Expectation for Stochastic Calculus\n\n\nStochastic Process\n\nQuadratic Variation\nStochastic Calculus\n\n\nDirac Measure\n"},"(Youtube)-How-We-Made-MuseScore-4---Music-App-Design-is-Challenging!":{"title":"(Youtube) How We Made MuseScore 4 - Music App Design is Challenging!","links":[],"tags":["youtube","Computing/Human-Interface"],"content":"How We Made MuseScore 4 - Music App Design is Challenging! §\n"},"2024-08-08":{"title":"2024-08-08","links":["tags/Computing/Linguistics/Python","Jupyter-Notebook-T&T"],"tags":["Periodic","Computing/Linguistics/Python"],"content":"Plans §\nYou need to start recruiting, and first you need to do you Akuna OA.\n\nRecruiting will never be a disadvantage, so start this week in any case\n\nBring your laptop to work the next few days, and work at work &amp; in library after work\nAkuna OA on wednesday.\n\n\n\nWork §\n\n Polish web UI with better scratchpad presentation, and finalizing image presentation\n\n Organize the webui files into a separate directory in repo\n\n\n Help out anniridh about llm.bind() not working\n mypy and pydantic for type checking\n Help annirudh of notebook examples\n Change ultipro password\n Change ultipro home address\n\nPython Development §\n#Computing/Linguistics/Python\nDevelopment Environment §\n\nmypy is good, but the cache will take very long to build\n\ntype checking in general is not in the spirit of python, but will help the dev experience (like typescript)\nErrors will not fully analyze when the something is blocking, e.g. the __init__.py issue\n\n\nlinters—use flake8; formatter—use black\npydantic is probably useful, but haven’t tried it yet\nnbQA is a bridge between python linters/formatters/type checkers and jupyter notebooks. (see also Jupyter Notebook T&amp;T)\n\nMultiline Strings §\ntriple quoted multiline strings (&quot;&quot;&quot;) will include the newline chars and tab chars in the string itself. To avoid this the model way is to do:\nmy_string = (\n    &quot;this is a&quot;,\n    &quot;multiline&quot;,\n    &quot;string.&quot;\n    )\nWhich will include the newline character but not the tab chars."},"2024-W29":{"title":"2024-W29","links":["2024-07-15","2024-07-16","2024-07-17","2024-07-18","2024-07-19","tags/Computing/Linguistics/SQL","Celine-Wei","Strategy","Trust","Dissociation"],"tags":["Periodic/Weekly","Computing/Linguistics/SQL"],"content":"\n月 2024-07-15\n火 2024-07-16\n水 2024-07-17\n木 2024-07-18\n金 2024-07-19\n\nWork §\nIssues with Dremio queries taking too long §\n#Computing/Linguistics/SQL\n\nSELECT DISTINCT is a red flag in general, because without an index it really takes long\nContinue with two things:\n\nUnderstanding data structure of udw_std_base\nChat continuation for agent\nIssues with the agent:\n\n\n\n\nStop word is inefficient because streaming is not implemented\nSQL quries are slower in python riskapi than in dremio\nIt doesn’t know which columns are important.\nBugs that I have no idea about:\nWhen I clean up the whitespaces from sql schema tool the agent bugs out and 504 response\nWhy there is no Observation:  string prepended\n\nColumns in udw_std_base to be concerned about: amount_usd,quantity, trade_maturity_date,notional_usd,notional, long_short,sation_name, normalized_rating_value,issuer_sap_rating, try with these\n\n Upload things to gitlab\n Contact Kathy about punching\n Contact carter’s assistant\n\nRisk Workbench §\n\nRun Desktop Coordinator (Staging)\nright\n\nThoughts §\nOutsourcing politics to Celine Wei §\nBecause she’s naturally more efficient in thinking about politics because that’s how she grew up. You will have to partially simulate political thought, and it will stress you out a lot (because in Asia politics is frowned upon).\nPlanning and Action repl loop §\nYou’ve had phases where each planning and action were respectively the main mode of Strategy. Now, you’ve experienced both:\n\nPlanning was main in high school where I studied, etc.\nAction was main since military where I wanted to “act without thinking” and thought that was the best type of pleasure\nThe conclusion is that both are equally important, and need to respect each other’s boundaries while Trust each otehr’s role in Strategy.\n\nSo good that you don’t have to care, you can Dissociation^9bc795 §\nWhile reading about why kernel developers argue so much about minor changes → it’s because they worked on it so hard, and they went through pain. But when you’re good at something and the thing you work on is reletively minor compared to your skills, then you don’t have to feel pain about something; you feel distant from it.\nYou’re scared to invest in concepts and in work. You want to dissociate, NOT because you’re scared of failure but that you will change because of how hard you worked on it.\nAlso why you feel more like an observer than a particiapnt of humanity (the quote)\nPlans §\n\nThursday—come in afternoon for risk talk (focus needed anyways)\nFriday—wfh. Less people anyways\n"},"Abstraction":{"title":"Abstraction","links":["Moore’s-law","Abstraction","Pipelining","Branching-(Computer-Science)","Instruction-Set"],"tags":["Computing/Computer-Architecture"],"content":"1.2. 8 Great Ideas from Computer Science §\n\nMoore’s law\nAbstraction\nOptimize for the Common Case\nParallelism → thruput increase\nPipelining → thuput increase\nPrediction\nMemory Hierarchy\nRedundancy\n\nAbstraction: Application Software &gt; Systems Software &gt; Hardware.\nAbstraction: High Level Language &gt; Assembly Language &gt; Binary Machine Language\nComponents of a Computer §\nComponents of a Computer: Input/Output, Memory, Datapath, Control. Datapath &amp; Control are sometimes combined as the CPU. Examples of I/O Devices: LCD displays (with frame buffers as pixels), touchscreens, etc.\nMemory is built from DRAM (Dynamic Random Access Memory) (↔ Sequential random access memory, which is old technology). Memory Hierarchy: Main Memory (DRAM) &gt; Secondary Memory (magnetic disks, flash memory).\nInstruction Set Architecture (=architecture) is the information developers need to develop a proper binary machine language program, including I/O, memory layout, etc. ABI (Application Binary Interface) is the combination of the instruction set and the OS interface.\n\nTransistor: a electric switch\nVISI (Very Large-Scale Integrated Circuit): ICs with millions and billions of trasnsistors\n(Moore’s law predicts the geometric increase in the number of transistors per area.)\n\nManufacturing process of ICs\nSilcon Ingot ⇒ Wafers ⇒ (processing steps) ⇒ Patterned Wafers ⇒ Diced Wafers (Dies) ⇒ Packaged Dies (patterned wafers, diced wafers and packaged wafers are each tested.)\n\nDefects can occur in each testing process. Yield measures the percentage of good dies from total # of dies on wafer.\nCost of manufactoring, due to defects, is generally proportional to the square of each die area. Which means it’s impractical to design very big dies.\n\nParallelism §\nImprovement of response time fo CPUs began to slow around 2002. Since then cpu architects started to use multicore processors to augment performance. But since programmers had to rewrite programs for them it was hard to adopt—due to the overhead of communication &amp; synchronization.\n1.9. Real Stuff: Benchmarking the Intel Core I7 §\nSPEC (System Performance Evaluation Cooperative) released standard benmarks for common programs. Types: integer benchmark / floating point benchmark. They give a score called a SPECratio per program which is calculated by: SPECratio=execution timereference time​ where reference time is from a reference processor and execution time is from the processor being benchmarked. The higher the better. Overall SPECratio of the CPU is a geometric mean of all programs’ SPECratios.\nSPEC also released a SPECpower power consumption benchmark. It measures average power consumption and ssj_ops (server side Java operations per second), at load increments from 10%, 20%, …, 100%. The final number given is:\noverall ssj_ops per watt=∑i=010​poweri​∑i=110​ssj_opsi​​"},"Ackerman-Function":{"title":"Ackerman Function","links":["Limits-of-Math-and-Computing"],"tags":["Computing/Algorithms","Math"],"content":"def. Ackerman Function. The Ackerman function satisfies the following recurrence relations:\n\nA(k,n)=A(k−1,A(k,n−1))\nA(k,1)=2\nA(1,n)=2n\n\n\nExample:\n\nA(2,n)=A(1,A(2,n−1))=⋯=A(1,A(1,…A(2,1)))=2n\nA(3,n)=2↑n\n\n\n\nProperties §\n\nGrows really fast\nAppears in Limits of Math and Computing\n"},"Activation-Functions-of-a-Neural-Network":{"title":"Activation Functions of a Neural Network","links":["Normal-Distribution"],"tags":["Computing/Maching-Learning"],"content":"RELU §\nREctifier Linear Units.\nf(x):=max{0,x}\nUsed as an activation function of a neuron in a neural network.\n\nRectifier since it removes\n\n\nGELU §\nGaussian Error Linear Unit\nf(x):=x⋅Φ(x)\nWhere Φ(x) is the CDF of the standard Normal Distribution.\nThe GELU function can be approximated by the following two approximations, the second one with less accuracy:\n\nf(x)≈0.5⋅x⋅(1+tanh[π2​​(x+0.044715x3)]\nf(x)≈x⋅σ(x) where σ(x) is the CDF of the logistic distribution\n"},"Agents-of-the-Macro-Economy":{"title":"Agents of the Macro Economy","links":["Profit-Function","Cobb-Douglas-Utility-(Two-Goods)"],"tags":["Economics/Macro-Economics"],"content":"Agents §\n\n\n                  \n                  We’re doing static analysis; i.e. it’s very LR or very SR. \n                  \n                \n\nNumerous agents of each sector are grouped into a representative agent.\n\nRepresentative household\nObjective: maximize utility over consumption (C) and Leisure (L) within the utility function\n→ Labor Supply SN and # of laborers N\n→ Goods Demanded D\nRepresentative Firm\nObjective: maximuze profit (π) over capital (K) and Labor (N) within the production function\n→ Labor Demand DN\n→ Goods Supplied S\nGovernment\nRole: collect taxes and pay transfers (NT); spend remaining on the goods market G\n\nHousehold Optimization §\n\nFor households the price of consumption goods and the price of leisure [=wage] is an exogenous variable; i.e. it cannot be controlled\nHouseholds have a time endownment of h which they can use for leisure (L) or consumption (C)\n\nThe Household Utility Function u(C,L)\nAssumptions:\n\n∂C∂u​,∂L∂u​&gt;0 …i.e. More consumption/leisure is always better\n∂C2∂2u​,∂K2∂2u​&lt;0\n…i.e. the Marginal Utility always diminishes\n∃∂C∂u​\n[= utility function is differentiable] …i.e. C,L are somewhat substitutable\n→ MRS=∂u/∂L∂u/∂C​∣uˉ​\n\nHousehold Budget Constraint\nwN+π−NT=C\n\nw is the representative wage\nN is the hours of labor a HH chooses\nπ is profit or dividends if the HH is also an owner of a firm\nNT is the net transfers (+transfers-taxes)\nC is consumption (in units: # of goods)\n\n\nFirm Optimization §\n\nThe representative firm employs Factors of Production (K,N) to produce YS=f(K,N) output\nProfit of the firm is π=f(K,N)−wN−rK\nThe firm is the average firm of the whole economy, which is unlike a microecon firm in many ways\n\nThe Firm Production Function f(z,K,N)\nAssumptions:\n\nConstant Returns to Scale (M&amp;A is useless)\nxλf(z,K,N)=f(z,λK,λN)\n→ This is probable, since output as a whole in general has CRS\nMore capital or more labor always means more output\n∂K∂Y​,∂N∂Y​&gt;0, equivalently MPK​,MPN​&gt;0\n→ Probable; throw another pencil into the economy, it’ll produce more\nDiminishing Marginal Product\n∂K2∂2Y​,∂K2∂2Y​&lt;0\nMarginal product of capital increase with more labor, and vice versa\n∂K∂N∂Y​&gt;0 (=positive cross-derivative)\n\nProfit Function\n∂K∂π​=∂K∂f​−r=0\n∂N∂π​=∂N∂f​−w=0\n↑ Must satisfy both conditions\n\n⇒ The Cobb-Douglas Production Function satisfies all these assumptions:\nf(z,K,N)=zKαN1−α where  0&lt;α&lt;1\n\nProfit π=zKαN1−α−wN−rK\nCapital Demand, ∂K∂π​=0 at z⋅α(NK​)α−1=r\nLabor Demand, ∂N∂π​=0 at z⋅(1−α)(NK​)α=w\n\nGovernment §\nThe government’s goals are more vague, threefold:\n\nAllocative: resources are used in certain proportions\nDistributive: income, wealth is distributed at tolerable equalities\nStabilization: business cycle is smoothed out\n\nAutomatic stabilizers; e.g. unemployment benefits\ngovernment purchases (G)\n\n\n"},"Alexander-Rosenberg":{"title":"Alexander Rosenberg","links":[],"tags":["Philosophy/Epistemology","People"],"content":""},"Alexandre-Kojeve":{"title":"Alexandre Kojeve","links":[],"tags":["People","Philosophy"],"content":"\nAlexandre Kojève (koh-ZHEV, French: [alɛksɑ̃dʁ kɔʒɛv]; 28 April 1902–4 June 1968) was a Russian-born French philosopher and statesman whose philosophical seminars had an immense influence on 20th-century French philosophy, particularly via his integration of Hegelian concepts into twentieth-century continental philosophy. As a statesman in the French government, he was instrumental in the formation of the European Union.\nWikipedia\n"},"Algebraic-Type-System":{"title":"Algebraic Type System","links":[],"tags":["Computing"],"content":"def. Algebraic Type System uses both:\n\nProduct Types: stuct Icon: {shape: Shape, x_pos: Int, y_pos: Int}\nSum Types: enum Shape: Square | Circle | Triangle\nIntuition. It’s called algebraic because:\n\nIconShape​∈Shape×Int×Int={Square,Circle,Triangle}​Product TypeSum Type​​\nThus looking at its cardinalities:\n∣Icon∣​=∣Shape∣⋅∣Int∣⋅∣Int∣=(∣Square∣+∣Circle∣+∣Triangle∣)⋅∣Int∣⋅∣Int∣=3⋅∣N∣⋅∣N∣​​"},"Algorithm-Problem-Tips":{"title":"Algorithm Problem Tips","links":["Internal/scratchpad","Problem-Sets","Python-Common-Operations","Time-Complexity","Priority-Queue","Hash-Table","Dynamic-Programming","Sliding-Window-Technique"],"tags":["Computing/Algorithms","Computing/Data-Structures","Meta-Learning"],"content":"During Practice §\n\nThere are only two ways to store data structures: as arrays (sequential storage) or as linked lists (linked storage).\n\n\nFirst of all, it should be clear that data structures are tools, and algorithms are methods to solve specific problems with appropriate tools.\n\n\nUse a plain-text editor as scratchpad\nRecord new techniques/algorithms\nIt’s okay to look at solutions after a certain period of trying.\nUse syntactic sugar &amp; libraries. Python has lots.\nComment above every line while coding\n\nDuring Testing §\n\nBrute Force → Better Data Structures/Algorithms → Even better time complexity\n\nAlways ask: can reduce Time Complexity even further?\n\nExponential is never good. Polynomial is not enough. Linear time!\n\n\nPriority Queue / Hashmap / Dynamic Programming / Sliding Window Technique\nTry thinking: What property doesn’t change? What variable is constant?\nTry thinking: Invert the search; find the contrapositive\n\n\nAre you {python}returning?\nEdge cases\n\ninfinite loops are probably not it\n⇒ make the while loops conditioned on index boundaries\n\n\nTest the program yourself on a text editor\nTry to defeat the problem (find very weird test inputs)\n"},"Algorithm":{"title":"Algorithm","links":["Complexity","External/Mathematical-Proof","Mathematical-Induction","Proof-by-contradiction","Testing-Principle"],"tags":["Computing","Math"],"content":"Loose definition\n\n\n                  \n                  Note \n                  \n                \nAn algorithm…\n\nfinishes in finite number of steps\nits answer is consistent\nit always gives the correct answer\n\n\nAlgorithm Design Principles §\n\nDesign\n\nWhat to compute\nHow it computes what we want it to compute\n\n\nCorrectness\n\nWhy does it compute what we want it to compute\n\n\nAnalysis &amp; Efficiency\n\nHow much resources does it use? (Complexity)\n\n\n\nMost algorithms follow the following two standard models of computation\n\nRecursion\nIteration\n\nVerifying Correctness §\n\n\n                  \n                  Warning \n                  \n                \nThere is no way to find out if an algorithm is correct.\n\n\nMathematical Proof\n\nDirect Proof\n\nMathematical Induction\n\n⇒ Recursive algorithms looks similar to proof by induction\n\n\n\n\nIndirect Proof [=Proof by contradiction]\n\n\nTest experimentally\n"},"Alienation-(Marx)":{"title":"Alienation (Marx)","links":["Proletatriat-(Marxism)","Alienation-(Marx)","Species-being"],"tags":["Philosophy/Marxism"],"content":"Commoditization of human labor §\n\nTrading labor\n\n\nDifferences of age and sex have no longer any distinctive social validity for the working class. All are instruments of labour, more or less expensive to use, according to their age and sex.\n\n⇒ The wage-laborer is homogenized into a single indistinguishable set\nEstrangement/Alienation (Marx) §\n\nAlienation of the Commodity, the production\n\n\nIt is the same in religion. The more man puts into God, the less he retains within himself. The worker places his life in the object; but now it no longer belongs to him, but to the object. […] not only that his labour becomes an object, an external existence, but that it exists outside him, independently of him and alien to him, and begins to confront him as an autonomous power; that the life which he has bestowed on the object confronts him as hostile and alien.\n\n\nAlienation of Human Creativity\n\n\nSo if the product of labour is alienation, production itself must be active alienation, the alienation of activity, the activity of alienation.\n\n\nHence the worker feels him­ self only when he is not working; when he is working he does not feel himself. […] It is therefore not the satisfaction of a need but a mere means to satisfy needs outside itself. Its alien character is clearly demonstrated by the fact that as soon as no physical or other compulsion exists it is shunned like the plague.\n\n\nJust as in religion the spontaneous activity of the human imagination, the human brain and the human heart detaches itself from the individual and reappears as the alien activity of a god or of a devil, so the activity of the worker is not his own spontaneous activity. It belongs to another, it is a loss of his self.\n\n⇒ Thus ultimately alienation from the Species-being"},"Amartya-Sen":{"title":"Amartya Sen","links":["Human-Development-Index","(Article)-Equality-of-What-q","Martha-Nussbaum"],"tags":["People","Philosophy"],"content":"\nDeveloper of the Human Development Index\nAuthor of (Article) Equality of What?\nFrequent interactions with Martha Nussbaum\n"},"Amia-Srinivasan":{"title":"Amia Srinivasan","links":[],"tags":["People","Philosophy/Ethics"],"content":""},"Analogical-Reasoning":{"title":"Analogical Reasoning","links":[],"tags":["Philosophy/Epistemology"],"content":""},"Analytic-Philosophy":{"title":"Analytic Philosophy","links":[],"tags":["Philosophy/Analytic"],"content":""},"Anarcho-Syndicalism":{"title":"Anarcho-Syndicalism","links":["Noam-Chomsky","Decentralization","Capital-(Marxism)","Karl-Marx"],"tags":["Philosophy/Political-Philosophy","Economics/Game-Theory"],"content":"Noam Chomsky\n\nAnarcho-: Decentralization of power\nSyndicalism: Worker’s union’s ownership of Means of Production\nA realistic alternative to Marxism\n"},"Annuity":{"title":"Annuity","links":["assets/Proof.png","Future-Value-Calculations"],"tags":["Economics/Finance"],"content":"Annuities are…\n\nMoney you borrowed: mortgages, etc. ⇒ pay regularly to pay off large amount\nMoney you lent: social security credit, etc. ⇒ give large money, get regular payments later\n\nAmortization is the same thing but from a different perspective\n⇒ You’re spreading a one-time cost into multiple years. Formulae are same if the recurring payments are of the same amount.\nTypes of Annuities\n\nOrdinary Annuity: payment occurs at the end of each period\nSimple Ordinary Annuity:\nCurrent balance=Prev. balance+Interest on previous balance−Payment\n\nFormulae for Constant Payment P §\n\nn: Term; time from first payment to last payment (in years)\nAn​: Present value of annuity of n terms.\nSi​: Total payment accrued after i periods (not a present value!)\ny=1+kr​ for formula simplification\n\n\nPreset value of annuity (=An​) formula. (see below for more)\n\n(i.e. the principal loan amount, mortgage loan amount, etc.)\n\n\n\nAn​:=(1+r/k)P​+(1+r/k)2P​+…(1+r/k)nP​\n\nPayment accrued after all n periods (=Sn​ =Future value of annuity after all payments made)\n(Demonstration, use geometric sum formula)\n\nSn​​=P+(1+kr​)P+(1+kr​)2P+⋯+(1+kr​)n−1⋅P=r/k(1+r/k)n−1​⋅P=y−1yn−1​⋅P​…(0)​​\n\nSimplified formula of PV using 1. and 2.\n\nAn​​=(1+r/k)nSn​​=r/k1−(1+r/k)−n​⋅P=y−11−y−n​⋅P​time valuefrom (0)​​\n\nRemaining balance after i periods (=Bi​)\n\nBi​​=(1+kr​)n−1(1+kr​)n−(1+kr​)i​An​=yn−1yn−yi​An​​​​\nPortion of the i-th payment Pi​ that goes to…\nPrincipal\nBi​=kr​yn−1yi−1​An​\n\nTotal Principal ⇒ ∑i=1n​Bi​=An​\n\nInterest\n\nEvery period, interest is fully paid.\nInterest is applied to the remaining balance B (not to be confused with B)\n\nIi​=kr​Bi−1​=kr​yn−1yn−yi−1​An​\n\nTotal Interest ⇒ ∑i=1n​Ii​=nP−An​\n\n⇒ Thus the total payment: ∑i=1n​Pi​=nP\n\nConsider k-periodic compounding (=k times every year)\nCost of Loan: ignoring the time value of money, sum of all interest payments nP−An​\nAs montly payment P increases, number of payment periods n decays exponentially.\n\n\nFormulae for Variable Interest Rates §\nVariables are same as above, except:\n\nPi​: payment after period i\nri​: interest rate during period i\n\n\nPresent value of annuity (An​)\n\nAn​​=l=1∑n​∏j=1l​(1+krj​​)Pl​​=∏j=1n​(1+krj​​)Sn​​​​"},"Apartheid":{"title":"Apartheid","links":["Nelson-Mandela"],"tags":["History"],"content":"\nNelson Mandela\nSouth Africa\n"},"Apple's-Philosophy":{"title":"Apple's Philosophy","links":["Get-Lost-in-the-Weeds","Ends-Disregards-the-Means","(Article)-Magic-Ink---Information-Software-and-the-Graphical-Interface","Alan-Kay","Control","first-impression","Design-as-Human-Interface"],"tags":["Design"],"content":"\n\n                  \n                  The Cook Doctrine\n                  \n                \nWe believe that we are on the face of the earth to make great products and that’s not changing.\n\nWe are constantly focusing on innovating.\nWe believe in the simple not the complex. Focus\nWe believe that we need to own and control the primary technologies behind the products that we make, and participate only in markets where we can make a significant contribution. Control\nWe believe in saying no to thousands of projects, so that we can really focus on the few that are truly important and meaningful to us. Focus\nWe believe in deep collaboration and cross-pollination of our groups, which allow us to innovate in a way that others cannot.\nAnd frankly, we don’t settle for anything less than excellence in every group in the company, and we have the self-honesty to admit when we’re wrong and the courage to change. And I think regardless of who is in what job those values are so embedded in this company that Apple will do extremely well.\n\n\n\n\n                  \n                  Abstract \n                  \n                \n\nApple’s focus is on consumer experience above all else.\nPrinciples §\nFocus §\n\nApple makes few products, but makes them really well\n\nMake only a few ads, but make them really well.\n\n\nAttention to Detail. Color-matching cables and stickers, black PCB, specific squircle curvature to maximize visual appeal.\n\nControl §\n\nVertical integration: control key technologies\n\nThe Grand Theory of Apple - YouTube\nApple is a technology company, only incidentally. Technology is a means to an end—delivering great experiences.\n\n\n\nExcellence §\n\nShip when the product is ready.\n\nArriving late to a market is okay, if the product delivers a convincing experience.\n\n\n\nDesign §\n\n(Article) Magic Ink - Information Software and the Graphical Interface\n\nApple Macbook Pro Retina 15 / 13 Official Promo Video - YouTube\n\nIf you never change anything, what you can engineer is really just incremental. But when you’re willing to change things, you open up a whole new world of design.\n\n\nThere are many design innovations […] that users won’t actually see, but they’ll certainly experience.\n\n\nAir is pulled into vents […] by fans with asymmetrically positioned fan blades. […] We position ours asymmetrically to spread the sound over a variety of frequencies which makes it seem quieter and less intrusive.\n\nIntroducing iOS 7 - Official Video - Apple (HD) - YouTube\n\nSimplicity is so much more than the absence of clutter and ornamentation. It’s about bringing order to complexity.\n\n\nDesign is more than how something looks. It’s how something feels and works, on so many different levels.\n\n\nApple often quotes Alan Kay:\n\nPeople who are serious about their hardware, should design their own software.\n\nMarketing §\nWhy Keynotes? Apple wants to fully Control the first impression of the device, and thereby implant a strong, purposefully constructed image in the press and ultimately the consumer’s mind.\nWhy Ads? Apple’s TV ads are (for most people) the first impression of the product. It’s the one opportunity to control the messaging.\nThe New iPad Pro — On Any Given Wednesday - YouTube\n\nSubtle sound effects of water while showing the river, the whoosh of the school bus or the crowded chatter of the coffee shop make the scene interactive and focused.\nEssentially, it’s message is: “wouldn’t you want to be that person?”\n\nApple marketing adheres to this messaging a lot. Phil Schiller said after unveiling the new (trash can) Mac Pro, showing a slide with a guy with three 4K displays: “you can be this guy.”\n\n\n\niPhone 8 (PRODUCT)RED™ Special Edition - Commercial - YouTube\n\nHow can a phone be sexy?\nThe music &amp; visuals match to produce a emphatic and resonant atmosphere\n\niPhone 11 and iPhone 11 Pro - Reveal - YouTube\n\nSame as the iPhone 8 commercial above. It brings character to the phones; the iPhone 11 is playful, alive, and tactile, while the Pro is austere, practical, and precise.\n\niPhone X - Announcement Video - YouTube\n\nIt interacts between the phone’s display showing a colorful wallpaper and the person’s face lit up by a similar gradient of colors.\n\nMessage: this phone blurs the boundaries between human and device.\nThe message is supplemented by the primary feature that assists this coalescene: Face ID\n\nPhil Schiller, in the Keynote, says specifically that the phone “recognizes you.”\n\n\n\n\n"},"Applications-of-Discrete-Markov-Chains":{"title":"Applications of Discrete Markov Chains","links":["Hypothesis-Testing"],"tags":["Math/Probability"],"content":"Gamber’s Ruin §\ngame. Gambler’s Ruin. Gambler starts out with i units. They have probability p of earning one unit, 1−p losing one unit, iid per round. Game ends when gambler reaches either 0 (lose) or a preset N (win). What’s the probability the gambler wins/loses the game?\nlet Pi​=P(reaches N∣X0​=i). Then:\n\nP0​=0, PN​=1\nPi​=pPi−1​+(1−p)Pi+1​\nThen identity: Pi​=pPi​+(1−p)Pi​ rearrange the second into the following:\n\nP2​−P1​P3​−P2​⋮Pi​−Pi−1​​=p1−p​(P1​−P0​)=p1−p​(P3​−P2​)=p1−p​(Pi+1​−Pi​)​=p1−p​P1​=(p1−p​)2P1​=(p1−p​)i−1P1​​​\nSumming all and canceling:\nPi​−P1​Pi​​=(p1−p​+(p1−p​)2+…(p1−p​)i−1)P1​=⎩⎨⎧​1−p1−p​1−(p1−p​)i​P1​iP1​​if p1−p​=1if p1−p​=1​​​\nTry substituting i=N for boundary condition PN​=1:\n1=PN​​=⎩⎨⎧​1−p1−p​1−(p1−p​)N​P1​N⋅P1​​if p1−p​=1if p1−p​=1​​​\n\nP1​=N1​ if 1−p=p\nP1​=1−((1−p)/p)N1−(1−p)/p​ if 1−p=p\nThus the general formula:\n\nPi​={1−((1−p)/p)N1−((1−p)/p)i​Ni​​if p=1−pif p=1−p=21​​\nHypothesis Testing For a Drug §\nThis can totally be used to construct a hypothesis test in the following way:"},"Approximating-Distributions":{"title":"Approximating Distributions","links":["Normal-Distribution","Poisson-Distribution"],"tags":["Math/Probability"],"content":"thm. A Normal Distribution with μ=np,σ=np(1−p)​ will approximate a Binomial distribution with n trials and success probability p when np&gt;1\nP(a≤X≤b)=k=a∑b​P(X=k)≃∫ab​fnormal​(μ=np,σ=np(1−p)​)dx\nREMARK. A Normal Distribution is equivalent to the following standard normal distribution:\n∫a−0.5b+0.5​Normal(μ,σ)dx=∫a+0.5b+0.5​σ2π​e−21​(σx−μ​)2​dx=∫σa−0.5−μ​σb+0.5−μ​​Normal(0,1)du=∫σa−0.5−μ​σb+0.5−μ​​2π​e−x2/2​dx\nwhere the factor of +0.5 to the integrand’s range is the continuity correction.\nthm. A Poisson Distribution with parameters λ=np will approximate a Binomial distribution with n trials and success probability p when np≤1.\nX∼B(n,p)⇒P(X=k)=k!λke−λ​=k!(np)ke−(np)​\nthm. A Binomial Distrubition with parameters p=NK​ will approxmiate a Hypergeometric Distribution when N&gt;&gt;n\nX∼Binomial(n,p)⇓P(X=k)≃(kn​)(NK​)k(NN−K​)n−k​​\nx1​×x2​∫f(x1​,x2​)dx"},"Approximation-Algorithm":{"title":"Approximation Algorithm","links":[],"tags":["Computing/Algorithms"],"content":"def. Approximation Factor. α(n)-level approximation. For optimization problem π, ALG is a α-level optimization algorithm iff for input I of size n:\nmax[OPT(I)ALG(I)​,ALG(I)OPT(I)​]=α(n)\n\nSmaller is better!\nConstant approximation factor: if approximation factor doesn’t depend on n, ALG is a constant α-level approximation\n"},"Artificial-Needs":{"title":"Artificial Needs","links":["External/Private-Property","Abstraction"],"tags":["Philosophy/Marxism"],"content":"Normally it’s human nature from which arises naturally the needs, and capital should be appropriated for\n\nUnder the system of Private Property their significance is reversed. Each person speculates on creating a new need in the other, with the aim of forcing him to make a new sacrifice, placing him in a new dependence and seducing him into a new kind of enjoyment and hence into economic ruin.\n\n\nMan becomes ever poorer as a man, and needs ever more money if he is to achieve mastery over the hostile being.\n\nand this extends to all aspects of human life\n\nMoreover, the worker has no more than a precarious right to live in it, for it is for him an alien power that can be daily withdrawn and from which, should he fail to pay, he can be evicted at any time. He actually has to pay for this mortuary.\n\n…ultimately our lives are deprived of natural human needs\n\nworker. Light, air, etc. - the simplest animal cleanliness - ceases to be a need for man. […] Dirt - this pollution and putrefaction of man, the sewage of civilization - becomes an element o f life for him.\n\n…and capital even justified this:\n\nThe fact that the multiplication of needs and of the means of fulfilling them gives rise to a lack of needs and of means is proved by the political economist:\n\n\nBy reducing the worker’s needs to the paltriest minimum necessary to maintain his physical existence and by reducing his activity to the most abstract mechanical movement.\nBy taking as his standard - his universal standard, in the sense that it applies to the mass of men - the worst possible state of privation which life (existence) can know.\n\n\n\n\n…and ultimately “saving” or “not feeling” is now a virtue:\n\nPolitical economy, this science of wealth, is therefore at the same time the science of denial, of starvation, of saving, and it actually goes so far as to save man the need for fresh air or physical exercise.\n\n\nThe less you eat, drink, buy books, go to the theatre, go dancing, go drinking, think, love, theorize, sing, paint, fence, etc., the more you save and the greater will become that treasure which neither moths nor maggots can consume - your capital The less you are, the less you give expression to your life, the more you have, the greater is your alienated life and the more you store up of your estranged life.\n\nMoney, Specifically §\n\nincreases. The need for money is for that reason the real need created by the modem economic system, and the only need it creates.\n\nAbstraction above the thing:\n\nJust as it reduces everything to its own form of abstraction, so it reduces itself in the course of its own movement to something quantitative.\n\nand thus this requires its production\n\nthis is manifested partly in the fact that the expansion of production and needs becomes the inventive and ever calculating slave of inhuman, refined, unnatural and imaginary appetites - for private property does not know how to transform crude need into human need. Its idealism is fantasy, caprice and infatuation.\n"},"Ascending-Price-Auction":{"title":"Ascending Price Auction","links":["VCG-Auction","Combinatorial-Auction","Rationality-(Economics)","Types-of-Goods-(Economics)"],"tags":["Economics/Game-Theory"],"content":"Motivation. The VCG Auction is DSIC and Welfare-maximizing. But it’s also very complicated to explain to all agents. Instead, let’s think of a simpler, intuitive idea: ascending price auctions. Now, in a Combinatorial Auction setting, there are two ways to ascend the price:\n\nAscending item price\nAscending bundle price\nBefore we proceed, we need to introduce definitions and properties.\n\n\ndef. Demand. For valuation function v, let item prices p1​,…,pn​. Agent demands bundle S∗ of this valuation if S the utility-maximizing bundle, i.e:\nS∗=argmaxS​v(S)−j∈S∑​pj​\nRemark. There may be multiple utility-maximizing bundle. Therefore we often collect all bundles that are utility-maximizing, and make it into a demand set D\nAscending Item Price Auction §\n\nAn Ascending item price auction has prices for each item, and increase the price of each item.\ndef. Walrasian Equilibrium. (WE) In a Combinatorial Auction, given the prices p1∗​,…,pj∗​, let goods in X partitioned into S1​,…,Sn​. This allocation this is a W.E. if:\n\nSi​ is demanded by i, (=Si​ maximizes i’s utility =Si​⊆Di​)\nIf pj∗​&gt;0 then item j has an owner.\n\nthm. First Welfare Theorem. (FWT) Given price p1∗​,…,pj∗​, if Allocation W.E. is welfare-maximizing.\nProof. Let S1∗​,…,Sn∗​ be the welfare-maximizing partition/price to a Combinatorial Auction, i.e. the W.E. Then consider any other partition T1​,…,Tn​. Then:\n■\nThus, we need only show that the allocation is a WE in order to show that it is welfare-maximizing. Now:\ndef. Ascending Item Price Auction.\n\nStart at pj​=0 for all items\nfor all agents, agent i “points to” all items (Di​) that maximizes μi​(Di​)=vi​(Di​)−∑j∈Di​​pj​ at that price\nIf there are two agents that choose the same item, increase that item’s price by a small amount, ϵ.\n\n&amp; You can choose which item price to raise first—different choices will lead to different final allocations.\n\n\nRepeat, until for all items there is only one agent pointing to it.\nGive that agent all the items they are pointing to at that price.\nWe define a property (subset of Rational Taste) to assist the proof:\n\ndef. Substitutes Property. A value function v satisfies the substitutes property if, the increase of one item’s price does not remove any other items from the bundle it demands.\nExample. In other words, there is no situation of “left shoe, right shoe” (complement goods). If the price of left shoe increases above what they want to pay, this person will drop out of the left shoe and right shoe both. This does not satisfy the substitutes property.\nthm. (Ascending Item Price Auction Maximizes Welfare) If all agent’s value function vi​ satisfies the substitutes property, and no two agent values the good within ϵ of each other the Ascending Item Price Auction terminates in a WE (and thus is welfare-maximizing via the FWT.)\nProof.\n\nAuction will terminate, because the item price is only increased when it is in someone’s demand set.\nAt termination, if the price of item j is non-zero, it is in somebody’s demanded bundle D)i​. This is because\n\nthe price was raised from zero because somebody was pointing to it\ntwo people cannot simultaneously drop out of it (±ϵ)\nThe raise of another item’s price does not cause any agent to drop j\n⇒ therefore if termination price pj∗​&gt;0 then it is in somebody’s demand bundle Di​ (Second property of WE).\n\n\nAt every point in time, all agents point to only their demanded bundle. (First property of WE)\n■\n\nAscending Personalized Bundle Price Auction. §\nAn Ascending bundle price auction has an individualized price for every bundle. We describe an alternative type of equilibrium and prove that such an allocation is maximizing. It is not DSIC.\ndef. Competitive Equilibrium. (CE) let personalized prices pi​(Si​) for every agent, for every bundle, and allocation S1​,…Sn​ be a partition of goods X. This price and allocation is a competitive equilibrium iff:\n\nSi​ is demanded by i, i.e. it utility maximizes for i, i.e. Si​⊆Di​\n\nThis is same as WE above.\n\n\nIt maximizes seller’s total revenue: maxS​∑∀i​pi​(Si​)\n\nthm. Competitive Equilibrium maximizes social welfare.\n■\ndef. Ascending Bundle Price Auction. \n\nlet pij​ be current prices, individualized for each bidder, for each bundle\nThen find the revenue-maximizing partition S1​,…,Sn​\n\nThere may be many, arbitrarily choose one.\n\n\nlet “Loser Set” L:={i∣Si​=∅}, i.e. agents that don’t get anything\n\nFor every loser i∈L get their demand bundle Di​ at these prices\nIncrease customized price of that bundle for i by ϵ, i.e. pi​(Di​)←pi​(Di​)+ϵ.\n\n\nRepeat from 2, until L=∅ or no agents demand anything.\nThis all seems very arbitrary, but it will magically terminate at the CE.\n"},"Asset-Classes":{"title":"Asset Classes","links":["Bonds-(Finance)"],"tags":["Economics/Finance"],"content":"Assets are the types of financial instruments that behave similarity.\n\nStocks\nFixed Income (=bonds)\nCash\nForeign Currencies (FX)\nReal Estate\nInfrastructure\nPrivate Equity\nCommodities\nCryptocurrency\n"},"Asset-Manager-(Finance)":{"title":"Asset Manager (Finance)","links":[],"tags":["Economics/Finance"],"content":"Performance of Funds §\nTypes of funds that are actively managed [= ∃ fund manager] includes Hedge funds and Mutual funds. Comparison:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHedge FundsMutual FundsRegulations on what to investLinientStrictRegulation on who can joinStrict (only rich people)Linient (anybody)Risk ToleranceHighLowShort-selling?CanCannot (regulation)"},"Assets-(Finance)":{"title":"Assets (Finance)","links":["Equity","Commodities","Derivatives-(Finance)","Futures","Real-Estate"],"tags":["Economics/Finance"],"content":"Assets are anything that’s tradable.\n\nEquity\nCommodities\nDerivatives (Finance)\n\nFutures\n\n\nReal Estate\n\n\n\n                  \n                  Note \n                  \n                \nA debt for one person is an asset for the lender.\n"},"Assumptions-in-Derivative-Pricing":{"title":"Assumptions in Derivative Pricing","links":["Options-(Finance)","Present-Value-Calculations","Assumptions-in-Derivative-Pricing"],"tags":["Economics/Finance"],"content":"\nPayoff: the gross outcome of an investment or trade. The amount you earned from that trade, regardless of commissions, extraneous costs, etc.\nProfits: the gross outcome of an investment or trade, including commissions, extraneous cost\n\n→ the distinction happens in Options (Finance). Payoffs don’t consider the option premium, while the profits do.\n\n\nReturn: short for “rate of return”. The percentage of profit (not payoff!) per original investment. Quoted in percentage (%) or log-returns.\n\nRisk-Neutral Assumption §\nA risk-neutral world is where there is no risk premium on return. This implies:\n\nAll Present Value Calculations are done at the risk-free rate\nAll assets have risk-free rate of return (=zero riAssumptions in Derivative Pricingitrage Condition]] holds\n&amp; This is not really realistic, but simplifies calculations a lot.\n\nNo Arbitrage Condition (=Law of One Price) §\nNo arbitrage condition means without any risk, there cannot be excess return (=return above risk-free rate).\nLaw of One Price states that:\n\nlet security A have payoff pA​, cost cA​ and B payoff pB​, cost cB​.\nIf pA​=pB​ then cA​=cB​\n"},"Atomic":{"title":"Atomic","links":["(Article)-Evergreen-notes"],"tags":["Computing","Meta-Learning"],"content":"Atomicity means indivisibility. Useful in a variety of contexts:\n\nProgramming &amp; Concurrency. atomic operations cannot have any operation happen between it during the operation i.e. it cannot be paused until completion\n(Article) Evergreen notes should be atomic for better, densely connected notes.\n"},"Atsugi":{"title":"Atsugi","links":[],"tags":["Documentation"],"content":"AWS John the Ripper Instance §\nKey name is `ssh-key`\nLogin as User `ec2-user` \nHamonikr Vs ff.sh §\nSetting a custom resolution via cmdline\nChanging cinnamon desktop wallpaper from the terminal\nAtsugi Linode VM §\nManagement Console\nIP: 172.104.89.111 (ports 22, 8080 open for sshd)\nDomain: atsugi.pyoky.me\nusers: root (pw: oniichan) \n\nUsing a keyboard with Astugi (Phone) is immensly beneficial.\nAtsugi Moving to Chrome §\nenv LANGUAGE=en_US /usr/bin/chromium-browser --password-store=basic\nConfig is in ~/.config/chromium (This is the one you have to zip.)"},"Attention-is-Currency":{"title":"Attention is Currency","links":["Nudging","Stream-of-Content"],"tags":["Computing/Internet"],"content":"Your attention is the currency of the modern, corporate internet.\n\nIt is valued, monetized, and traded (as with your personal information)\nChoose how to give attention\nBuild in Friction to keep yourself from being consumed by the wealth of information.\nThere is always a Stream of Content. Choose what is important and stands out. Quality &gt; Quantity.\n"},"Auction-Theory":{"title":"Auction Theory","links":["Vickery-Auction","Revenue-Maximizing-Auctions","VCG-Auction","Ascending-Price-Auction"],"tags":["Economics/Game-Theory"],"content":"def. Sealed Bid Auction. An auction is a game where there are a certain number of buyers, buyers each have their value, vi​ and bid, bi​. The auction process takes in the bids of all buyers b, and outputs the allocation x(b) and price p​(b).\n\nx=⟨x1​,…,xi​,…,xn​⟩ where xi​ is the allocation for buyer i\np​=⟨p1​,…,pi​,…,pn​⟩ where pi​ is the price charged for buyer i\ne.g. if there is one item to be sold, and the i-th player gets it, then:\n\nx=⟨0,…,0,1,0,…,0⟩\n\\vec{p}= \\langle 0,\\dots,0,\\9.37,0,\\dots0 \\rangle$\n\n\nNobody except i knowns their true value: vi​\n\ndef. Quasilinear Utility Model. Each bidder, after the auction process finishes, has utility and welfare:\nutilityμi​(b)​​=welfare=valuevi​xi​(b)​​−pi​(b)\nTypes of Auctions §\nRemark. An auction can be not DSIC and welfare-maximizing 🤯.\n\nSingle-Item\n\nDeterministic Valuations\n\nSecond Price (DSIC, Welfare-Max)\nFirst-Price (Revenue-Max)\n\n\nBayesian values\n\nVickery Auction with Reserve (DSIC if regular, Revenue-max)\n\n\n\n\nMulti-Item\n\nSingle-item Demand, Indivisible (=sponsored search)\n\nGeneralized Second Price (Welfare-Max)\nVirtual Second Price (Revenue-Max)\n\n\nCombinatorial Demand, Indivisible\n\nVCG Auction (DSIC, Welfare-Max)\nAscending Item Price → W.E. (Welfare-Max)\n\n\nCombinatorial Demand, Indivisible—Price discriminatory\n\nAscending Individual Bundle Price → C.E. (Welfare-Max)\n\n\nCombinatorial Demand, Indivisible, Bayesian\n\n? (Welfare-max)\n? (Revenue-max)\n\n\n\n\n\nProperties of an Auction §\nMotivation. An auction’s goal is to:\n\nMake sure each person tells the truth, i.e. vi​=bi​\nMaximize total utility, i.e. ∑μi​\nSometimes, to maximize seller revenue.\nTherefore, we want an auction that will satisfy the following two properties.\n\ndef. Welfare Maximizing (=Socially Efficient). An auction is welfare-maximizing iff Given all bidders are truthful, then it maximizes ∑∀i​vi​xi​(b)\ndef. Dominant Strategy Incentive Compatibility (DISC). Intuition: all bidders are incentivized to tell the truth. An auction is DISC iff:\n∀i,∀b,regardless of b−i​,utility for telling truthμi​(vi​,b−i​)​​≥utility for lyingμi​(bi​,b−i​)​​\ndef. Revenue Maximizing (=Optimal). See Revenue-Maximizing Auctions.\nMyerson’s Lemma (or, How to Be DSIC) §\nMotivation. Instead of directly showing that an auction mechanism is DSIC, it is enough for it to satisfy the properties of the following Myerson’s lemma.\nthm. Myerson’s Lemma. An auction is DSIC (=truthful) if and only if it satisfies the following to properties:\n\nGiven other agent’s bids b−i​, the allocation function xi​(bi​,) is a monotonically increasing function\n\ni.e. if bi′​&gt;bi​ then xi​(b’i​,b−i​)≥xi​(bi​,b−i​)\n\n\nGiven b−i​, the optimal price pi​(bi​)=bi​xi​(bi​)−∫0bi​​xi​(z)dz\n\nProof.\nProperty 1. Allocation is monotonic w.r.t. bid amount. (=bid more, get more)\nproof. By definition of DSIC, both of the following holds:\n\nreports bi​, actual value vi​: vi​xi​(vi​)−pi​(vi​)≥vi​xi​(bi​)−pi​(bi​) ⇒ vi​[xi​(vi​)−xi​(bi​)]≥pi​(vi​)−pi​(bi​)\nreports vi​, actual value bi​: bi​xi​(bi​)−pi​(bi​)≥bi​xi​(vi​)−pi​(vi​) ⇒ pi​(vi​)−pi​(bi​)≥bi​[xi​(vi​)−xi​(bi​)]\n\nThis is an argument from the fact that for any b,v DSIC guara ntees it’s the best possible\nFrom the two inequalities we get\n\n\n\nvi​[xi​(vi​)−xi​(bi​)]≥⋯≥bi​[xi​(vi​)−xi​(bi​)]​​\nThus (vi​−bi​)[xi​(vi​)−xi​(bi​)]≥0 ∎\nIntuition. vi​−bi​ and xi​(vi​)−xi​(bi​) have the same signs. Which means xi​ is monotonically increasing.\n\nProperty 2. Optimal Price.\nFrom the two inequalities in property 1 we get\nvi​[xi​(vi​)−xi​(bi​)]≥pi​(vi​)−pi​(bi​)≥bi​[xi​(vi​)−xi​(bi​)]\nWe take the limit by pushing vi​,bi​→v\nvxi′​(v)≥pi′​(v)≥vxi′​(v)\nThus vxi′​(v)=pi′​(v). And integrating both sides\nvxi′​(v)=pi′​(v)​⟹∫0v​z⋅xi′​(z)dz=∫0v​pi′​(z)dx⟹∫0v​zd(xi​(z))=pi​(v)⟹pi​(v)=v∫0v​1d(xi​(z))−∫0v​1⋅xi​(z)dz⟹pi​(v)=vxi​(v)−∫0v​xi​(z)dz​substitute zintegration by parts​​\n■\nIntuition. The geometric intuition for this is that the price for the allocated person is the rectangle minus the area under the graph, i.e. the blue region.\n"},"Auto-Regressive-&-Conditionally-Heteroskedatic":{"title":"Auto-Regressive & Conditionally Heteroskedatic","links":["Stylized-Facts-of-Financial-Return-Series","Normal-Distribution","Student's-T-Distribution","Time-Series","Stochastic-Recurrence-Relation"],"tags":["Math/Statistics"],"content":"\ndef. Skedaticity = Variance\ndef. Heteroskedatic = Different Variance\nMotivation. There are in practice many time series where there are clusters of high volatility, and clusters of low volatility. See for example Stylized Facts of Financial Return Series. Thus we have a model to address that.\ndef. Auto-regressive Conditionally Heteroscedatic (ARCH) Process. Let:\n\n\n“Random Source” {Zt​} be SWN(0,1)\n\nWe may/may not model a distribution for this. If we do it’s often Normal or Student’s T.\n\n\n“Volatility Setting” {σt​} be a always-positive process defined as: σt2​=α0​+∑i=1p​αi​Xt−12​\n\nBy this definition, at time t with information Ft​, σt+1​ is already known, i.e. pre-visible.\n\n\n{Xt​} is a process Xt​=σt​Zt​\n\nProperties.\n\n\nAssuming Zt​⊥Ft−1​ → E[Xt​∣Ft−1​]=0\n\nThus Martingale Difference process\n\n\nAssuming {Xt​} is white noise → Var[Xt​∣Ft−1​]=σt2​\n\n…and since σt2​ is weighted sum of past Xt​’s…\nThus the process makes volatility clusters\n\n\n\nStochastic Difference Equation §\nConsider the squared of the ARCH(1) process:\nXt​=α0​Zt2​+α1​Zt2​Xt−12​\nThis is analyzed as a Stochastic Recurrence Relation, and solution given by:\nXt2​=α0​i=1∑∞​(α1i​j=0∏i​Zt−j2​)\nFor this to be stationary:\nthm. ARCH(1) is a white noise (=cov-stationary &amp; uncorrelated) iff α1​&lt;1"},"Auto-Regressive-&-Moving-Average":{"title":"Auto-Regressive & Moving Average","links":[],"tags":["Math/Statistics"],"content":"Autoregressive §\nMotivation. You want to predict a time series, e.g. milk consumption per month. It is the most natural to say “future value is a weighted sum of past (realized) values.” The autoregressive model formalizes that.\n\ndef. Autoregressive Process of order p. (AR(p)). {Xt​} is an AR(p) process if it is modeled\nXt​=known at Ft−1​ϕ1​Xt−1​+ϕ2​Xt−2​+⋯+ϕp​Xt−p​​​+ unknown ϵt​​​\nwhere {ϵt​}∼WN(0,σϵ2​) which is called “innovation,” the “new” factor or information.\nIntution. Current value is the weighted sum of past p values, plus some new thing that happened this time.\nMoving Average §\nMotivation. But you might consider a more intelligent model, which is able to look back at the prediction errors in past data to predict current data.\n\ndef. Moving Average Process of order q. {Xt​} is a MA(q) process if it is modeled:\nXt​=known at Ft−1​θ1​ϵt−1​+θ2​ϵt−2​+⋯+θq​ϵt−q​+​​+ unknown ϵt​​​\nwhere {ϵt​}∼WN(0,σϵ2​). We can also get its theoretical ACF as:\nρ(h)=⎩⎨⎧​∑i=0q​θi2​∑i=0q−∣h∣​θi​⋅θi+∣h∣​​0​−q≤h≤qelse​\nIntuition. Current value is the weighted sum of past q innovations and nothing else.\nARMA §\nNow for the kicker:\ndef. Autoregressive Moving Average process of order p,q. {Xt​} is an ARMA(p,q) process if it is modeled:\nXt​= learn from past data i=1∑p​ϕi​Xt−i​​​+ learn from past err i=1∑q​θi​ϵt−i​​​+ only new info ϵi​​​\nInvertibility, Causality §\nthm. ARMA causality/invertibility condition. let ARMA(p,q) process as above, and let:\nϕ~​(z)θ~(z)​=1−ϕ1​z−⋯−ϕp​zp=1+θ1​z+⋯+θq​zq​​\nnow, if:\n\nϕ~​(z),θ~(z) has no common roots\nϕ~​(z) has roots z outside the unit complex circle i.e. ∣z∣&gt;1 → Causality\nθ~(z) has roots z outside the unit complex circle i.e. ∣z∣&gt;1 → Invertibility\nIf causal, this process is causal with coefficients obtained from the following:\n\ni=0∑∞​ψi​zi=ψ~​(z)θ~(z)​\nExample. ARMA(1,1) has form Xt​=ϕ1​Xt−1​+θ1​ϵt−1​+ϵ, with ∣ϕ1​∣&lt;1, and complex polynomials:\nϕ~​(z)θ~(z)​=1−ϕ1​z=1+θ1​z​​\nAnd z=ϕ1​1​, ∣ϕ1​∣&lt;1 so we have a causal form. The coefficients obtained by:\ni=0∑∞​ψi​zi​=1−ϕ1​z1+θ1​z​=(1+θ1​z)(1+ϕ1​z+ϕ12​z2+⋯)=(1+θ1​z)j=0∑∞​ϕ1j​zj  =j=0∑∞​ϕ1j​zj+θ1​j=0∑∞​ϕ1j​zj+1\\  =1+i=1∑∞​(ϕ1i​+θ1​ϕ1i−1​)zi =ψ0​1​​+i=1∑∞​ψ1​,ψ2​,…ϕ1i−1​(ϕ1​+θ1​)​​zi.​Geo. Sum expandindex start, from 0 to 1 ​​\nThus the causal form is:\nXt​=ϵt​⋅1+i=1∑∞​ϕ1i−1​(ϕ1​+θ1​)ϵt−i​\nARIMA §\nMotivation. Problem is when there is “drift” (=data generally moves up). This is corrected by using not the time series itself, but the “difference series” of that time series.\ndef. Difference Operator. For process {Xt​}t∈N​ the difference operator ∇ is defined as:\n∇Xt​∇dXt​​=Xt​−Xt−1​={∇Xt​∇d−1(Xt​−Xt−1​)​if d=1if d&gt;1​​​\n\nthis has nothing to do with the gradient operator in calculus ∇\nNow we can define:\ndef. Autoregressive Integrated Moving Average (ARIMA).\n\n\nWe have some series {Xt​}t∈N​ that keeps on drifting\nSowe difference it d times, Yt​=∇dXt​ and got a new time seiries {Yt​}\n{Yt​} can be modeled as a ARMA(p,q) time series.\nThen, {Xt​} is a ARIMA(p,d,q) time series\nExample. Let price of security {St​}, and log-prices {lnSt​}. Then obviously the log-returns {lnSt​−lnSt−1​=lnSt−1​St​​} is the difference series ∇1lnSt​. If the log-returns series can be modeled as an ARMA(p,q) then the original log-prices is an ARIMA(p,1,q) model.\n"},"Autoencoders":{"title":"Autoencoders","links":[],"tags":["Computing/Maching-Learning"],"content":""},"BEM-Method":{"title":"BEM Method","links":[],"tags":["Computing"],"content":"…for naming CSS selectors.\nblock__name-of-content__status--on { ; }\n// e.g.\ndiv-input__first-name__input--enabled { ; }\nhttps://www.devbridge.com/articles/implementing-clean-css-bem-method/"},"Backtesting":{"title":"Backtesting","links":["Testing-Principle"],"tags":[],"content":"An implementation of the Testing Principle, i.e. using the predictive model and testing it on past data to see if it’s effective."},"Balance-Sheet":{"title":"Balance Sheet","links":["Money-(Medium-of-Exchange)"],"tags":["Economics/Finance"],"content":"def. Balance Sheet is a table showing all the firm’s assets and liabilities.\nExample.\nwhere the terms are standardized by Accounting standards - Wikipedia like GAAP (U.S.) or IFRS (int’l)\n\nAssets\n\nCurrent Assets (=liquidated within one year)\n\nCash &amp; Equivalents: Liquid assets on hand by a firm\nRecieveables: unofficial transactions, e.g., regular grocery purchases, etc.\nInventories: Items not sold\n\n\nNon-current Assets (=long-term held assets)\n\nProperty, Plant, &amp; Equipment (PP&amp;E), i.e. Durable capital assdets\n\n\n\n\nLiabilities\n\nCurrent Liabilities\n\nPayables: unofficial transactions (counterpart to Recieveables)\nShort-term borrowings: quick borrowings, credit line from bank, commercial paper, etc.\n\n\nNon-current Liabilities\n\nLong-term debt: corporate bonds, bank lones\n\n\n\n\nStockholder’s Equity: money received by purchasers of stock\n\nThis is not the market price of stock. This is equity price at time of issuance (book value)\nBut during stock buybacks, firms buy at market price, so stockholder’s equity will cancel out at a different rate than during book valueation\nRetained Earnings are profits held and not paid to equity owners for reinvestment\n\n\n\n\n\n                  \n                  Assets and (liabilities + equity) must always sum to zero. When assets depreciate, the equity decreases accordingly \n                  \n                \n\nNote that:\ndef. Book Value (=Total Equity) is by definition Assets less Liabilities\n\nThis is totally separate from the Market Value ⇒ hard to measure the “value” of a firm\nsoft skills (e.g. brand image, reputation) are not reflected in book value (the market may better reflect intangibles).\nRemark\n"},"Balance-of-Payments":{"title":"Balance of Payments","links":[],"tags":["Economics/Macro-Economics"],"content":"Trade balance"},"Banker's-Rule":{"title":"Banker's Rule","links":[],"tags":["Economics/Finance"],"content":"\n“Exact Time”: number of days from t0​\n“Ordinary interest”: 1 year=360 days,1 month=30 days\n"},"Bankrolling":{"title":"Bankrolling","links":[],"tags":["Economics/Finance"],"content":"\nBankrolling refers to the act of providing financial support or funding to a person, organization, or project. It involves supplying the necessary monetary resources to enable the individual or entity to carry out their activities or achieve their goals. Bankrolling often implies providing substantial financial backing or resources to ensure the success or sustainability of the supported venture.\n"},"Bankruptcy":{"title":"Bankruptcy","links":[],"tags":["Economics/Finance"],"content":"def. Bankruptcy is a legal instrument to protect when somebody/company has too much libabilities it cannot pay"},"Base-&-Superstructure":{"title":"Base & Superstructure","links":["Phenomenology"],"tags":["Philosophy/Marxism"],"content":"Phenomenology"},"Basel":{"title":"Basel","links":["Investment-Bank","Insurance-Company","Bankruptcy","Quantitative-Risk-Measurment","Credit-Rating"],"tags":["Economics/Finance"],"content":"Motivation. Basel is an agreement made by a bunch of countries’ (G10) representatives to regulate Investment Banks and insurance companies to prevent them from going Bankrupt.\nFramework §\nBasel forces banks to measure three risks quantitatively:\n\nCredit risk = everything in banking book.\nMarket risk = everything in trading book.\nOperational risk\nUsing the banks’ positions and how much risk they’re taking, it calculates the amount of liquid assets (=capital) the bank must be holding. This is called:\ndef. Minimum Capital Charge (Basel). Amount of assets a bank must be holding. Has two tiers based on how liquid, how risk-free they are:\nTier 1: Shareholder equity + retained ernings. (no other claims)\nTier 2: Any other positions. (will be claimed by others in case of bankruptcy)\n\nMethodology §\nCredit risk (=banking book) is measured by the Risk-weighted assets (RWA) method\nRWA=f⋅∀i∑​(asset value)i​×(riskiness weight)i​\nwhere f=8% in Basel 2.5.\nTo calculate the riskiness weights banks can choose:\n\nStandardized Approach (SA): regulation specifies weight values per asset type\nInternal Models Approach (IMA): banks calculates its own weights, but must consider (1) probability of counterparty default and (2) expected loss in case of default\nMarket risk (=trading book) is measured by either:\nRisk-wegithed Assets = Standard Approach (SA): same as credit risk, weights also similarly supplied\nValue-at Risk (VaR) = Internal Models Approach (IMA). Calculates probability that loss over time period t=(1 day, 10 days) is over amount K with probability x% See ^4j1myg\n\nStressed Var (SVaR), …over time period t=(12 months during 2008 crisis)…\nIncremental Risk Charge (IRC), default and rating change risk\n\n\n"},"Basis-Points":{"title":"Basis Points","links":[],"tags":["Economics/Finance"],"content":"100bp=1%"},"Beige-Book":{"title":"Beige Book","links":["Federal-Reserve"],"tags":["Economics/Finance"],"content":"The Beige Book is a report published by the Federal Reserve that provides qualitative information on current economic conditions across the 12 Federal Reserve Districts. It is published eight times per year and is used by policymakers, economists, and investors to gain insights into the state of the economy. The report relies on information gathered from various sources, including surveys of businesses and interviews with key contacts in each district."},"Bernouilli-Distribution":{"title":"Bernouilli Distribution","links":[],"tags":["Math/Statistics"],"content":"\n\n                  \n                  Note \n                  \n                \nAn experiment with two outcomes, success and failure, with probability p and (1−p) respectively.\n\n\nE(X)=p\nVar(X)=pq\n"},"Bertrand-Price-Competition":{"title":"Bertrand Price Competition","links":[],"tags":["Economics/Micro-Economics"],"content":"Firms will choose price as the strategic variable.\n\nMarket Demand x(p)=A−bp where A,b are constants\nSimultaneous game\nFirm with lower price captures all of demand\n\nCase: Constant and Same Marginal Cost §\n\nMarginal Cost is constant for both firms: c(x)=cx, MC=c\nBoth firms will attempt to undercut each other.\n\nBRC1​:p1​(p2​)=max(p2​−ϵ,MC)\nBRC2​:p2​(p1​)=max(p1​−ϵ,MC)\nNE={p1​=c,p2​=c} where both firms have zero profit\n\n\n\n\nCase: Different but Constant Marginal Cost §\n\nMarginal Cost is constant, but different for both firms\n\nc1​(x)=c1​x, c2​(x)=c2​x, c1​&lt;c2​\n\n\nThen…\n\nBRC1​:p1​(p2​)=max(p2​−ϵ,c1​)\nBRC2​:p2​(p1​)=max(p1​−ϵ,c2​)\nNE={p1​=c2​−ϵ,p2​=c2​}\n\n&amp; firm 1 can set undercut the price because they still have c1​&lt;c2​−ϵ\n\n\nFirm 1 will make positive profit; firm 2 will make zero profit.\n\n\n\nCase: Sequential Move §\n\nChange assumption: Dynamic game; firm 1 sets price first, then firm 2 sets price. (c1​&lt;c2​)\n\nThen, there cannot be a best response curve. Instead, consider the following subgame perfect Nash equilibirum:\n\n\n\n12​:p1​=c2​−ϵ:p2​={min(p2M​,p2​−ϵ)c2​​if c2​&lt;p1​if c2​≥p1​​(can undercut price)(can’t b/c MC)​​​\n- The following is a non-subgame perfect Nash equilibirum. The non-credible threat is when firm 2 threatens &quot;I&#039;m just gonna lose money if you don&#039;t listen.&quot;. Given a constant $k$, \n\n12​:p1​=p2M​+k:p2​={p2M​0​if p1​ sets what I wantif p1​ isn’t what I want​​​\n\nDifferent order: Dynamic game; firm 2 sets price first, then firm 1 sets price. (c1​&lt;c2​ still assumed)\n\nThen, SPNE\n\n\n\n21​:p2​=c2​:p1​(p2​)={min(p1M​,p2​−ϵ)c1​​if p2​&gt;c1​else p2​&lt;c1​​​​\nIf Product Can Be Differentiated §\nWhen we assume that products can be differentiated (=diversified), then we can improve on the Bertrand model;.\n\nConsumers’ preferences are uniformly distributed across differentiation space R\nFirms can produce a product with a certain value P of differentiated characteristic P∈R\nA consumer with preference C will pay a cost when consuming P which is proportional to the distance Cost=∣P−C∣\n\n\nAssume there exists a possibility of differentiation space (∃R∈[0,1])\nHowever firms are not allowed change product characteristics.\nThe only strategic variable is price.\n\n\n\nFirst consider firm 1’s BRC:\n\nIf firm 1 gives away products for free p1​=0\n\n…firm 2 can still charge a price since some consumers will prefer their products\n…and p2​&gt;MC because firm 1 will exit otherwise.\n\n\nIf firm 2 charges a price p1​&gt;0\n\n…firm 2 can charge an even higher price since some consumers will prefer their products\n\n\nTherefore the BRC is a positively sloping curve\n\n\nFirm 2’s BRC is symmetrical:\n\n→ Equilibrium is at the intersection of the two BRCs, and at this point p1∗​=p2∗​.\n→ Equilibirum price is above marginal cost, unlike pure Bertrand Price Competition\n\n\n\n\n\n                  \n                  Info \n                  \n                \n\nDifferentiation “softens” price competition. Therefore firms want more differentiation\nDetermining Firm 2’s Best Response Curve: §\n\n\nwhen firm 1’s price &lt; MC, firm 2’s best response is set price at MC\nwhen firm 2’s price &gt; MC, firm 2’s best response is the undercut firm 1’s price by a very small amount (ϵ)\n\n\nFirm 2’s BRC is symmetric\n→ Firms’ prices will unravel to both pricing to p = MC (Bertrand price = MC)\n→ At p=MC, market will purchase 2xM, and firms produce xM each.\n\np_{1}&amp;=max[MC_{2}-\\epsilon,MC_{1}] \\\\\np_{2}&amp;=max[MC_{1}-\\epsilon,MC_{2}]\n\\end{align}\nHotelling Model §\n\nProducts can be differentiated by one characteristic variable which ranges from 0 to 1.\nDifferentiation is the only strategic variable; i.e. price, etc. cannot be changed.\n\n\n\n\n                  \n                  Tip \n                  \n                \nThe graph and its corresponding explaination in the textbook is wrong; the following is the correct explaination.\n\nThinking about Firm 1’s BRC:\n\nwhen firm 2 produces a product with characteristic &lt; 0.5, then firm 1 will produce a product that is just closer to 0.5 [=larger] to capture more consumers\nwhen firm 2 produces a product with characteristic &gt; 0.5, then firm 1 will produce a product that is just closer to 0.5 [=smaller] to capture more consumers\nwhen firm 2 produces a product with characteristic = 0.5, then firm 1 will produce a product with characteristic = 0.5.\n\n\n\n                  \n                  Info \n                  \n                \nIn a single axis (1-dim. differentiation space)\n\n\n→ Firm 2’s BRC is symmetrical; this unravels to the equilibrium where both firms produce product with characteristic = 0.5\nCircle Model §\nThe circle model combines the Price Competition and Differentiation.\n\nThis is a sequential game where firms will enter [first move] and then it will compete with price [second move]\n\nFirms who enter incurs entry cost (an economic cost) of FC when they enter.\n\n\nPrice and differentiation are the two strategic variables.\n\nProduct differentiation space is around a circle whose circumference is 1.\n\n\n\nThen the following conclusions:\n\n\n                  \n                  Info \n                  \n                \n\nEach firm is respresented as a point on the circle.\n\n\nFirms will enter as long as FC&gt;π …①\nFirms compete with their immediate neighbors\n→ Firms will space themselves out around the circle\n\nThe number of firms entering N and the equilibrium price given that N firms have entered p are determined in the following order:\n\nFC↦N∗ where N∝FC1​ (∵ ①)\nN∗↦p∗ where p∝N1​ but p&gt;MC. Specifically:\n\n"},"Besset-Correction":{"title":"Besset Correction","links":["Bias-(Statistics)","Normal-Distribution"],"tags":["Math/Statistics"],"content":"Review and intuition why we divide by n-1 for the unbiased sample | Khan Academy - YouTube\nNaiive sample variance estimator:\ns^2=n∑i=1n​(xi​−xˉ)​\n⇒ Sample variance is biased. (btw, this is MLEs)\nBesset Corrected sample variance estimator:\ns^2=n−1∑i=1n​(xi​−xˉ)​\n\nLarger than the naiive sample estimator\n"},"Better-Gradient-Descent-Methods":{"title":"Better Gradient Descent Methods","links":["Estimator","Bias-(Statistics)","No-Regret-Dynamics"],"tags":["Computing/Maching-Learning","Math/Calculus","Math/Linear-Algebra"],"content":"Motivation. While gradient descent is accurate it is computationally intensive. We shall consider heuristics to improve the computational speed.\ndef. Gradient Descent. We will use the following notation/characterization of gradient descent.\nwk+1​=wk​−αk​[N1​i=1∑N​∇fi​(wk​)]\nwhere\n\nα1​,…,αk​,… are learning rates on step k\nfi​ is the loss function for datapoint i and weghts wk​\n\nMotivaiton. The first step to optimize is to avoid calculating the gradients of all datapoints i=1,…,N.\ndef. Stochastic Gradient Descent. For each step, we instead randomly choose one datapoint, j to update:\nwk+1​=wk​−α∇^fj​(wk​)\nThis works because  expectation of gradient E[∇^fj​]​​= average gradient N1​i=1∑N​∇fi​​​\nMotivation. This is better, but this leads to lots of steps k. We can group or “batch” data together to reduce that, while also not calculating all gradients like classical GD. The gradient used like this, is an unbaised Estimator of the true gradient calculated over all datapoints.\ndef. Batch Gradient Descent. For each step, choose a random batch of size n, {j1​,…,jn​} and average them.\nwk+1​=wk​−αk​[n1​i=1∑n​∇fji​​(wk​)]\nMomentum §\nIntuition. Momentum is a way to “dampen” the oscillations when taking many tiny steps, and to reduce the number of steps required for convergence (by a factor of a square root).\nThis dampening, implemented for stochastic/batch gradient descent, can be considered as a bias-variance-tradeoff of the gradient estimator.\n\nalg. Nesterov’s Accelerated Gradient Descent (NAG). This is convoluted but it’s optimized to guarantee a certain rate of convergence, see below. This is also the original implementation of momentum.\n\nlet λ0​=0,λk​=1+21​⋅1+rλk−12​​\nlet γk​=λk+1​1−λk​​\nfor steps k=1.., setting w1​=t1​:\n\nMomentum: tk+1​=wk​−β1​∇fj​(wk​)\nDescent: wk+1​= weighted sum of momentum(1−γk​)tk+1​+γk​tk​​​\n\n\n\nthm. (informal statement). Step k will minimize:\n distance to optimal at step k f(wk​)−f(w∗)​​≤k21​ constant 2β∣∣w1​−w∗∣∣22​​​\ni.e., the distance to optimal drops quadratically wrt k\nalg. GD with Momentum. A more intuitive way to incorporate momentum. Setting β∈[0,1] to how much you want to accumulate prior momentum:\n\nfor k=0…:\n\nMomentum: mk+1​=βmk​+∇fi​(wk​)\nDescent: wk+1​=wk​−αmk+1​\n\n\n\nHigher-order Methods §\nIntuition. Curvature is useful for gradient descent.\nAdaptive Learning Rates §\nIntuition. We adapt the learning rates for each parameter. Small learning rate for frequently updated parameters, and large learning rates for infrequently updated parameters. This works because:\n\nFrequently updated parameters can be overfit, and infrequently updated ones underfit\nBy adjusting learning rates inversly to frequency/magnitude of update we reduce overand underfitting, while also improving convergence rates\n\nalg. GD with Adaptive Learning Rates. (Adagrad) It is proven that it reduces regret bounds\n\nfor k=0,…\n\nSum-of-Squares accumulation: vk+1​=vk​+ sqared gradient ∇f⊙∇f​​\nAdaptive rate: αk​=γ⋅1/(ϵ+vk+1​​) (element-wise divide)\nDescent: wk+1​=wk​−αk​⊙∇f\nThe sum of squares will grow fast for parameters with high gradients and/or frequent updates. The learning rate is the reciprocal, and thus diminishes. v.v.\n\n\n\nalg. Root-Mean-Squared Propagation. (RMSprop). Adaptive learning rates using exponential moving average. Set a learning rate sequance α1​,…,αk​,… such that αk​→0 at a rate k1/2. Then:\n\nfor k=0,…\n\nSum-of-Squares accumulation: vk​+ sqared gradient ∇f⊙∇f​​\nDecaying SoS (=“preconditioner”) bk​=γ⋅1/(ϵ+vk+1​​) (element-wise divide)\nDescent: wk+1​=wk​−αk​bk​⊙∇f\n\n\n\nAdaptive Learning Rate + Momentum §\nalg. Adaptive Learning Rate with Momentum (ADAM). Integrate both the adaptive learning rate (into second moment) and momentum (first moment).\n\nfor k=0,…\n\nFirst moment: mk+1(1)​=β1​mk(1)​+(1−β1​)∇f\n\nDebias: m~k+1(1)​=1−β1(k+1)​mk+1(1)​​\n\n\nFirst moment: mk+1(2)​=β1​mk(2)​+(1−β2​)∇f⊙∇f\n\nDebias: m~k+1(2)​=1−β2(k+1)​mk+1(2)​​\n\n\nDescent: wk+1​=wk​−αk​m~k+1(1)​/(ϵ+m~k+1(2)​​)\n\n\n\n\n\n                  \n                  Conclusion \n                  \n                \nJust use ADAM.\n"},"Beveridge-Curve":{"title":"Beveridge Curve","links":["Unemployment"],"tags":["Economics/Macro-Economics"],"content":"Beveridge Curve §\n:= description of the inverse correlation between Unemployment and job openings\n\n\n\nIt may shift due to: increased in unemployment benefits, participation rate changes, etc.\nBeveridge curve\n\nThe curve, named after William Beveridge, is hyperbolic-shaped and slopes downward, as a higher rate of unemployment normally occurs with a lower rate of vacancies.\n\n\n\nThe more inwards, the more efficient the labor market is.\n\ni.e. the matchmaking between employer and employee is done well.\n\n\n"},"Bias-(Statistics)":{"title":"Bias (Statistics)","links":[],"tags":["Math/Statistics"],"content":"\n\n                  \n                  Note \n                  \n                \nBias is the difference between the expected value [=mean] of an estimator and the ground truth.\n\nBias[θ^]=E[θ^]−θ0​\nthm. Linear Transformation Preserves Bias. If θ^ is unbiased, g(θ^) is unbiased if g is a linear transformation.\n→ For estimators without bias, we can compute the precision to evaluate how good it is."},"Big-Data":{"title":"Big Data","links":["Database-Management-System","Media/Web/(Article)-Big-Data-Is-Dead"],"tags":[],"content":"\nApache Iceberg is an open-source relational dbms that is designed for huge amounts of data\nGoogle Bigquery and AWS Snowflake are data warehouse systems, meaning that they manage the big data dbms\n\n(Article) Big Data Is Dead. Beware decision-making where"},"Big-Oh-Notation":{"title":"Big-Oh Notation","links":[],"tags":["Math/Calculus","Computing/Algorithms"],"content":"Alternative formulation of parts of calculus.\ndef. Big-Oh Notation. for functions f,g, we say f(n)=O(g(n)) iff:\n\nDefinition 1:\n\n∃c∈R+ ∀n∈N+,f(n)≤c⋅g(n)\n\nDefinition 2:\n\nn→∞lim​sup(f(n))&lt;∞\n\n⇒ Intuitive Understanding: f=O(g)≈f&lt;g\n\ndef. Big-Omega Notation. For functions f,g we say f=Ω(g) iff g=O(f)\ndef. Big-Theta Notation\nf=O(g) and f=Ω(g)⟹f=Θ(g)"},"Binomial-Distribution":{"title":"Binomial Distribution","links":[],"tags":["Math/Common-Distributions"],"content":"Binomial Distribution §\ndef. Binomial Distribution. For random variable X which denotes the number of successes within n trials with success probability p:\nXP(X=k)​∼Binom(n,p)=(kn​)⋅pk⋅(1−p)n−k​\n\nE[X]=np\nVar[X]=n⋅p(1−p)\n"},"Binomial-Option-Pricing-Model":{"title":"Binomial Option Pricing Model","links":["No-Arbitrage","Present-Value-Calculations","Binomial-Security-Pricing-Model","Stochastic-Process","Approximating-Distributions","Normal-Distribution"],"tags":["Economics/Finance"],"content":"Binomial Option Pricing Model §\nQ. What is the fair price (=No-Arbitrage) of a European call option at time t0​?\n\nYou can’t really just use Present Value Calculations because it’s a derivative.\nUsing the same Binomial Security Pricing Model but in options.\n\n\nFind the purchase of stock and borrow of money that would have the equivalent payoff as the call option (=reproducing portfolio)\nUse that to price the stock\nThen, according to the No-Arbitrage that must be the same as the current price of the call option\nDo this for n periods for a binomial tree\n\nCall Option Details §\nPayoff (=price of call option at time t1​)\nC(t1​)={max[S(t1​)⋅u∗​K,0]max[S(t1​)⋅d∗​K,0]​probability pprobability 1−p​\n\nu,d is the uptick factor and the downtick factor\n\nReproducing Portfolio Details §\n\nΔ(t): number of stocks you purchase\n\nwill increase due to dividend payouts: Δ(t1​)=Δ(t0​)eq(t1​−t0​)\nq is the dividend rate\n\n\nb: amount you borrow at time t0​\n⇒ Value of portfolio\n\nV(t0​)V(t1​)​=Δ(t0​)⋅S(t0​)−b=Δ(t1​)⋅S(t1​)−ber(t1​−t0​)​​\nCalculating Fair Call Price §\n⇒ Equate V(t1​)=C(t1​) and solve for Δ(t),b to obtain the reproducing portfolio.\nΔ(t0​)b​=(u−d)S(t0​)Cu​(t1​)−Cd​(t1​)​e−q(t1​−t0​)=u−ddCu​(t1​)−uCd​(t1​)​e−r(t1​−t∗0)​​\n\nCalculate V(0):\n\nV(t0​)=e−q(t1​−t0​)(u−d)S(t0​)Cu​(t1​)−Cd​(t1​)​S(t0​)−e−r(t1​−t0​)u−ddCu​(t1​)−uCd​(t1​)​\n\n⇒ Thus V(t0​)=C(t0​) and simplifying:\n\nC(t0​)=e−r(t1​−t0​)[u−de(r−q)(t1​−t0​)−d​Cu​(t1​)+u−du−e(r−q)(t1​−t0​)​Cd​(t1​)]\nGeneralizing into n terms §\n\nWith the No-Arbitrage we have E[C(t1​)]=C(t0​)e(r−q)(t1​−t0​).\nThen define the risk-neutral probability of an uptick such that\n\np∗​:=u−de(r−q)(t1​−t0​)−d​\nthm. Risk-Neutral Binomial Call Pricing Formula\nC(t0​;n)=e−(nh)ri=0∑n​(in​)p∗i​(1−p∗​)n−iCui​dn−i​​(tn​)\nwhere\n\nCuidn−i​(tn​):=max[C(t0​)uidn−i−K,0]\nun​≈eσhn​​\ndn​≈e−σhn​​\nh:=t1​−t0​\nr,q is the risk-free rate and dividend rate\n\nBy taking n→∞ we see\n\nS(t) becomes a Stochastic Process\nBinomial sum can be approximated by a Normal Distribution\n"},"Binomial-Security-Pricing-Model":{"title":"Binomial Security Pricing Model","links":["Bernouilli-Distribution","Binomial-Distribution","Dividend-Discount-Model","Central-Limit-Theorem"],"tags":["Economics/Finance"],"content":"\nModel Definition §\n\nConsider time interval [t0​,tf​]\n\ntotal time τ=tf​−t0​\nn intervals\nhn​: duration of one interval (=ntf​−t0​​)\n\n\nSt​: price of security at time t.\n\nS0​ is given as a constant\n\n\nGross return Sj−1​Sj​​\n\nIt’s called gross because its in the form of 1.xx or 0.xx, not Sj−1​Sj​−Sj−1​​\nIs defined as a Bernouilli Distribution:\n\n\n\nSj−1​Sj​​={un​dn​​with probability pn​with probability (1−pn​)​(stock uptick)(stock downtick)​\n\nN: number of upticks. Is a random variable with Binomial Distribution N∼Binom(n,pn​)\n\nP(N=k)=(kn​) pnk​ (1−pn​)n−k\n\n\nSample Space Ωn​\n\nΩ={U,D}\nΩ2​={UU,UD,DU,DD}\nΩ3​={UUU,UUD,…,DDD}\nΩn​ is for n-period binomial tree\nPr(ω):=pnN(ω)​(1−pn​)N(ω)\n\n\nFinal Price Sn​=S0​ unk​ dnn−k​\n\nSn​ is a random variable. P(Sn​=S0​ unk​ dnn−k​)=P(N=k)\ni.e. probability that price will be equal to there being k upticks is the probability that there will be k upticks. (no shit)\n\n\nExpectation of final price: E(Sn​)=S0​(pn​un​+(1−pn​)dn​)n\nTo find number of upticks k from final price Sn​ we use\n\nk=ln(S0​dnSn​​)/ln(du​)\n\nDividends: D(tj−1​,tj​)=qS(tj−1​)hn​\n\nSee Dividend Discount Model for dividends in non-binomial tree model\nTotal Return (incl. dividends): R(tj−1​,tj​)=S(tj​)S(tj​)−S(tj−1​)+D(tj−1​,tj​)​=Rj​+qhn​\nCapital Gains Return (excl. dividends): Ri​:=S0​S1​−S0​​\n(Dividends Return qhn​)\n\n\n\nAssumptions &amp; Definitions §\n\nLog returns: Yn,j​:=ln(Sj−1​Sj​​) Is also a random variable\n\nLog normal is another indicator of how well the stock is performing\nIt is similar in value to the percentage return\nSee What does the average log-return value of a stock mean? - Personal Finance &amp; Money Stack Exchange for the intuition\n\n\nun​dn​=1 i.e. the stock ticking up then down is same as no movement at all\nInstantaneous Rate of Return\n\nm=limn→∞​hn​E[R(t0​,t1​)]​\n\n\nDrift: Instantaneous Expected Log-Return\n\nμ=limn→∞​Yn,j​=limn→∞​hn​E[lnS0​S1​​]​\n\n\nLog Variance: Instantaneous Variance of Log-Return = Volatility (σ)\n\nσ2=limn→∞​σn2​=limn→∞​hn​Var[lnS0​S1​​]​\n\n\nDividend Rate q\n\nLemmas §\n\nμn​hn​=pn​lnun​+(1−p)lndn​\nσn2​hn​=pn​(1−pn​)[lndn​un​​]2\nμ=m−q−2σ2​\n(Proofs in notes)\n\nthm. Parameter Triple. Given a security with μ,σ2,m,q we determine that:\n\nun​≈eσhn​​\ndn​≈e−σhn​​\npn​≈21​(1+σμ​hn​​)=un​−dn​e(m−q)hn​−dn​​\n\nContinuous Time Model §\nModel Definition §\nInstead of assuming an uptick-downtick gross return is a Bernouilli Distribution, we instead think of the log returns. (Use Lindenberg CLT)\nSn​(t)​=S0​exp(lnS0​Sn​​)=S0​exp(j=1∑n​lnSj−1​Sj​​)=S0​exp(Yn,j​)=S0​exp(μt+σt⋅Z)​Lindenberg CLTY∼N(nμn​hn​,nσn2​hn​)Z∼N(0,1)​​\n⇒ Thus we have\nln(S0​St​​)∼N(μt,σ2t)\nProperties\n\nS(t) is a lognormal random variable with\n\ni.e. S(t)=S0​eμt+σt​Zt where Z is the standard normal random variable\n\n\nln(St−1​St​​) are i.i.d.\n"},"Binomial-Theorem-1":{"title":"Binomial Theorem 1","links":[],"tags":["Math"],"content":"(x+y)n=k=0∑n​(kn​)xn−kyk=k=0∑n​(kn​)xkyn−k."},"Binomial-Theorem":{"title":"Binomial Theorem","links":[],"tags":["Math"],"content":"(x+y)n=k=0∑n​(kn​)xn−kyk=k=0∑n​(kn​)xkyn−k."},"Binomial-Tree-Model-of-Security-Pricing":{"title":"Binomial Tree Model of Security Pricing","links":["Bernouilli-Distribution","Binomial-Distribution","Dividend-Discount-Model","Central-Limit-Theorem"],"tags":["Economics/Finance"],"content":"\nModel Definition §\n\nConsider time interval [t0​,tf​]\n\ntotal time τ=tf​−t0​\nn intervals\nhn​: duration of one interval (=ntf​−t0​​)\n\n\nSt​: price of security at time t.\n\nS0​ is given as a constant\n\n\nGross return Sj−1​Sj​​\n\nIt’s called gross because its in the form of 1.xx or 0.xx, not Sj−1​Sj​−Sj−1​​\nIs defined as a Bernouilli Distribution:\n\n\n\nSj−1​Sj​​={un​dn​​with probability pn​with probability (1−pn​)​(stock uptick)(stock downtick)​\n\nN: number of upticks. Is a random variable with Binomial Distribution N∼Binom(n,pn​)\n\nP(N=k)=(kn​) pnk​ (1−pn​)n−k\n\n\nSample Space Ωn​\n\nΩ={U,D}\nΩ2​={UU,UD,DU,DD}\nΩ3​={UUU,UUD,…,DDD}\nΩn​ is for n-period binomial tree\nPr(ω):=pnN(ω)​(1−pn​)N(ω)\n\n\nFinal Price Sn​=S0​ unk​ dnn−k​\n\nSn​ is a random variable. P(Sn​=S0​ unk​ dnn−k​)=P(N=k)\ni.e. probability that price will be equal to there being k upticks is the probability that there will be k upticks. (no shit)\n\n\nExpectation of final price: E(Sn​)=S0​(pn​un​+(1−pn​)dn​)n\nTo find number of upticks k from final price Sn​ we use\n\nk=ln(S0​dnSn​​)/ln(du​)\n\nDividends: D(tj−1​,tj​)=qS(tj−1​)hn​\n\nSee Dividend Discount Model for dividends in non-binomial tree model\nTotal Return (incl. dividends): R(tj−1​,tj​)=S(tj​)S(tj​)−S(tj−1​)+D(tj−1​,tj​)​=Rj​+qhn​\nCapital Gains Return (excl. dividends): Ri​:=S0​S1​−S0​​\n(Dividends Return qhn​)\n\n\n\nAssumptions &amp; Definitions §\n\nLog returns: Yn,j​:=ln(Sj−1​Sj​​) Is also a random variable\n\nLog normal is another indicator of how well the stock is performing\nIt is similar in value to the percentage return\nSee What does the average log-return value of a stock mean? - Personal Finance &amp; Money Stack Exchange for the intuition\n\n\nun​dn​=1 i.e. the stock ticking up then down is same as no movement at all\nInstantaneous Rate of Return\n\nm=limn→∞​hn​E[R(t0​,t1​)]​\n\n\nDrift: Instantaneous Expected Log-Return\n\nμ=limn→∞​Yn,j​=limn→∞​hn​E[lnS0​S1​​]​\n\n\nLog Variance: Instantaneous Variance of Log-Return = Volatility (σ)\n\nσ2=limn→∞​σn2​=limn→∞​hn​Var[lnS0​S1​​]​\n\n\nDividend Rate q\n\nLemmas §\n\nμn​hn​=pn​lnun​+(1−p)lndn​\nσn2​hn​=pn​(1−pn​)[lndn​un​​]2\nμ=m−q−2σ2​\n(Proofs in notes)\n\nthm. Parameter Triple. Given a security with μ,σ2,m,q we determine that:\n\nun​≈eσhn​​\ndn​≈e−σhn​​\npn​≈21​(1+σμ​hn​​)=un​−dn​e(m−q)hn​−dn​​\n\nContinuous Time Model §\nModel Definition §\nInstead of assuming an uptick-downtick gross return is a Bernouilli Distribution, we instead think of the log returns. (Use Lindenberg CLT)\nSn​(t)​=S0​exp(lnS0​Sn​​)=S0​exp(j=1∑n​lnSj−1​Sj​​)=S0​exp(Yn,j​)=S0​exp(μt+σt⋅Z)​Lindenberg CLTY∼N(nμn​hn​,nσn2​hn​)Z∼N(0,1)​​\n⇒ Thus we have\nln(S0​St​​)∼N(μt,σ2t)\nProperties\n\nS(t) is a lognormal random variable\n\ni.e. S(t)=S0​eμt+σt​Zt where Z is the standard normal random variable\n\n\nln(St−1​St​​) are i.i.d.\n"},"Bivariate-Ordinary-Least-Squares-Regression":{"title":"Bivariate Ordinary Least Squares Regression","links":["Mean-Squared-Error"],"tags":["Math/Statistics"],"content":"Motivation. Let’s say that there is a relationship between GDP per capita and life expectancy. Maybe god has declared a perfect formula describing this relationship:\nGDP Per Capita=200⋅Life Expectancy+1000+Noise\nWhile we humans may never truly know the parameters of the formula, 200 and 1000, we can still make a good guess about it. Therefore, assuming this is a linear relationship, we have the Bivariate Ordinary Least Squares Model.\nYi​=β0​+β1​Xi​+ϵi​\nwhere (X1​,Y1​),…,(Xn​,Yn​) are observations (=regressors). The OLS algorithm will minimize the squared sum of residuals:\nminβ1​^​,β0​^​​i=1∑N​ϵi​^​2\nwhere ϵi​^​:=Yi​−=Y^(β0​^​+β1​^​Xi​)​​. Note the square.\nthm. Parameter OLS Estimator for N observations (Xi​,Yi​)\nβ1​^​β0​^​​:=∑i=1N​(Xi​−Xˉ)2∑i=1N​(Xi​−Xˉ)(Yi​−Yˉ)​:=σX2​σXY​​=ρXY​σX​σY​​:=Yˉ−β1​^​Xˉ​​\nProperties.\n\nPredictor: Yi​^​=β0​^​+β1​^​Xi​\nResidual: ϵ^:=Yi​−Y^ is the estimator for the error term, i.e. how good the predictor is.\n\nN1​∑iN​ϵi​^​=0 i.e. the mean of residuals is zero in OLS algorithm.\n\n\nOLS results in assuming that X is exogenous, i.e. ρX,ϵ​=0.\n\nρ\n\n\nRegression Variance: σ^2=N−k∑i=1N​ϵi​^​2​=N−k∑i=1N​(Yi​−Yˉ)2​ where k is the number of parameters (k=2 in this case)\n\nbasically the Mean Squared Error. The lower the better.\nσ^ is also the standard error of the residuals.\nIn Stata, σ^ is called the Root MSE\n\n\n\nEvaluation of Estimators.\n\nMean of β1​^​: E(β1​^​)=β1​+ρX,ϵ​σX​σϵ​​\n\nThus bias is ρX,ϵ​σX​σϵ​​\nIf ρX,ϵ​=0 then exogenous (this is the definition of exogeniety)\nIf ρX,ϵ​&gt;0 then there’s some 3rd factor positively correlated with X, thus bias is positive.\nIf ρX,ϵ​&lt;0 then v.v.\n&amp; Thus the bias characterizes exogeniety\n\n\nVariance of β1​^​: Var(β1​^​)=N⋅Var(X)σ^2​\n\nThis is also called precision\nVar(β1​^​)​ is also called standard error of β1​^​.\n! For random variables, Var(X)​ is called standard deviation. For estimator random variables, it is called standard error. An abuse of terminology.\n\n\n\nOther Data when Regression is Run §\nThere are a bunch of other variables that may matter, that is not included in the above core set of variables of the regression. Here are a few:\ndef. Coefficient of Determination (R-Squared). Intuition: The proportion of the variation in Y^ that can be determined from X. We define it as:\nR2:=1−SStot​SSres​​\nwhere\n\nSSres​:=∑i​ϵi​^​2=∑i​(Yi​−Yi​^​)2 residual sum of squares\nSStot​:=∑i​(Yi​−Yˉ)2 total sum of squares\n\nExample. A R2 of 0.67 means that around 67% of the variation in Y^ is explained by X. Therefore, the higher it is, the better the regression is (=has more explanatory power).\nRemark. It can also be shown that:\nR2=ρY,Y^2​\nMeasurement Error §\nMotivation. There are measurement errors in every data; it can be both in the independent variable X or the dependent variable Y. We can characterize measurement error (in this case a measurement error in X) as:\nXi​=Xi∗​+vi​\nwhere vi​ is randomly distributed with E(vi​)=0 and std. dev. σv​, which is the measurement error. In this case, the regression in the model Yi​^​=β0​^​+β1​^​Xi​+ϵ^i​ will also change, into:\nβ1​^​=n→∞​β1​σv2​+σX∗2​σX∗2​​\nWe can extract a couple of facts from this relationship:\n\nAttenuation bias: the greater the σv​, the closer β1​^​ is to zero.\nif σv​=0 then β1​^​=β1​, i.e. no measurement error\n\nAlternatively, if there is a measurement error in Yi​, then:\n\nMeasurement error: Yi​=Yi∗​+vi​ where E(vi​)=0 and std. dev. σv​\nError term vi​ is absorbed by error term ϵi​^​\nThis will increase σ^ (variance of regression) and thus Var(β1​^​), but does not bias the estimator.\n\nIssues to Watch out for §\nHeteroskedasticity is when the variance of data is different for some subsets of data than other subsets. ⇒ Use heteroskedatic-consistent standard errors by using robust in stata. This does not affect value of β1​^​ (how!) and does not bias it.\nAutocorrelation often occurs in time series data where the error terms are sticky. For example, attendance at a NY yankees game will be sticky, because people who watched a good year will probably come back next year, even if the yankees aren’t as good as the year before. → used lagged variables"},"Black-Scholes-European-Option-Pricing-Formula":{"title":"Black Scholes European Option Pricing Formula","links":["Normal-Distribution","Binomial-Option-Pricing-Model","Forwards","No-Arbitrage","Bonds-(Finance)"],"tags":["Economics/Finance"],"content":"thm. BSM Formula for European Call Options The Fair price of a European call option is C(t) given by\nC(t)d+​d−​τ​=e−τqS(t)N(d1​)−e−τrKN(d−​)=στ​ln(S(t)/K)+[(r−q)+2σ2​]τ​=d+​−στ​=στ​ln(S(t)/K)+[(r−q)−2σ2​](τ)​=T−t​where...time to expiration​​\nwhere\n\nt is entry date, T is execution date\nr is the risk-free rate\nK is the strike price\nN(d) is the CDF of the Standard Normal Distribution\nS(t) are the price of the underlier, modeled as log-normal R.V.s (=geometric brownian motion) as S(t)=S0​eμt+σt​Bt​\n\n\n\n                  \n                  There are other derivations of BSM, notably: \n                  \n                \n\nUsing the Binomial Option Pricing Model\n[[Risk-Neuk-Neutral%20Derivation%20of%20BSM.md)tfolio\n\n\nProof (Derivation) Sketch. We use a similar strategy to when we priced Forwards—by constructing a portfolio whose value is always equal to the derivative we want to price, and then using the Law of One Price to find its current price.\nWe first outline the replicating portfolio as a combination of cash and a certain number of securities. We have bt​ units of cash and nt​ units of cash. We model them as the following:\n\nCash value: Bt​\n\n! Not Bt​. It’s just denoted B because it’s a bond with risk-free\nbt​ units; changes along time. (How? See below)\nBt​=B0​ert where r is the risk-free rate\n\nThus dBt​=rBt​dt\n\n\n\n\nStock value: St​\n\nnt​ units; changes along time. (How? See below)\nStochastic process with dSt​=(m−q)St​dt+σSt​dBt​\n\nThis is a simple geometric brownian motion process. m is the instantaneous rate of return, and q is the dividend yield\n\n\nSimplify the stock paying the dividend by re-modeling it as StC​=St​eqt\n\n\nStock value with dividend: StC​\n\nsame nt​ units\nStochastic process with dStC​=(m−q)St​dt+σSt​dBt​\nThus we have the following value of our portfolio Vt​\n\n\n\nVt​=bt​Bt​+nt​StC​=must bef(St​,t)\nValuing the Self-fiancing Portflio\nThis portfolio must be self-financing; i.e. we must use cash to buy stock, or sell stock to buy cash, in order to exactly track the value of the derivative. This condition translates to: \n before dt nt​St+dtC​+bt​Bt+dt​​​ change in asset value (dnt​)St+dtC​​​dVt​​= after dt nt+dt​St+dtC​+bt+dt​Bt+dt​​​= change in cash value −(dbt​)Bt+dt​​​=nt​dStC​+bt​dBt​​self-financing cond.compact formtrust the textbook​​\nNow, expanding this using the properties of dStC​, dBt​ , and also that bt​=must beBt​f(St​,t)−nt​StC​​, we simplify to:\ndVt​=​ A nt​(m−r)StC​+rf(St​,t)​​​dt+ B nt​σStC​​​dBt​\nValuing the Security Itself\nAlternatively, we can use Ito’s lemma to value f(St​,t) directly. (Recall that we always have ∂x∂f​(St​,t):=∂x∂f​(x,t)∣x=St​​\ndf(St​,t)=​ C ∂t∂f​+(m−q)St​∂x∂f​(St​,t)+21​σ2St2​∂x2∂2f​(St​,t)​​​dt+ D σSt​∂x∂f​​​(St​,t)dBt​\nEquating the two\nNow, due to the fact that an Ito process as a unique representation in the differential form, we know that A=C and B=D in the above formulae. Using the latter:\nnt​=StC​St​​:=Δf​∂x∂f​(St​,t)​​\nUsing the former:\nnt​(m−r)StC​+rf(St​,t)=set​∂t∂f​+(m−q)St​∂x∂f​(St​,t)+21​σ2St2​∂x2∂2f​(St​,t)\nAnd substituting the former nt​ into the latter, we have the partial differential equation:\n21​σ2St2​∂x2∂2f​(St​,t)+(r−q)St​∂x∂f​(St​,t)+∂t∂f​(St​,t)−rf(St​,t)=0\nNow, this PDE will satisfy any derivative; we can use any boundary conditions on it. But if we solve it for a European Call option, we will get the BSM formula.\n\nf(St​,t):=C(St​,t) starting at t and ending at T. Strike price is K\nC(x,T)=max{x−K,0}\nC(0,t)=0 and C(x,t)→x→∞e−q(T−t)\n\n■"},"Black-Scholes-Merton-Derivative-Pricing-Formula":{"title":"Black-Scholes-Merton Derivative Pricing Formula","links":["Stochastic-Process","Dividend-Discount-Model","Interest-Rate"],"tags":["Economics/Finance"],"content":"thm. BSM Derivative Pricing. Generalized pricing of any derivative. Using different constraints, one can price any derivative.\n∂S∂f​+2σ2S2​∂S2∂2f​+(r−q)S∂S∂f​−rf=0\nwhere…\n\nf[S(t),t] is the price of the derivative at time t\nS(t) is a Geometric Brownian Motion s.t. dS=(m−q)S⋅dt+σS⋅dB(t) with B(t) a standard brownian motion\n\nσ is the scale of process S\nμ is the drift of process S\nm is the rate of return of process S\nq is the dividend rate (any dividend is immediately reinvested)\n\n\nr is the Discount Rate\n"},"Bonds-(Finance)":{"title":"Bonds (Finance)","links":["Loans","Monetary-Policy","Tradable-Inflation-Protected-Securities-(TIPS)","Treasury","Investment-Bank","Bonds-(Finance)"],"tags":["Economics/Finance"],"content":"Motivation. When a company needs money to invest in a new project, Loans are the simplest way. But banks are wary because once a loan is made they have to keep it to maturity and can’t offload it. We want a loan that is tradable.\ndef. Bonds are loans that are tradable, often of low principle amount, that pays coupon payments (instead of interest, like in loans.)\nthr. Buying a bond is a way of betting on the current/future interest rates, because Price∝1/Yield, and price of a bond perfectly matches the market yield (linked to federal Fed Funds Rate).\n\nTradable Inflation-Protected Securities (TIPS) are inflation protected bonds.\n\nCash Flow Structure of a Bond §\n\n\nLender lends money to the government (usually US Treasury)\nCoupon payments pay regularly:\n\nSemi-annually for US Treasury\nAnnually for other countries\n\n\nPrincipal (=par, base) amount is paid at the maturity;\nTime To Maturity (TTM) is time from now to when the principal pays\nYield\n\nCurrent Yield\nYield to Maturity: Expectation Hypothesis defines yield to maturity as the expected rate of return if held til maturity.\n\n\n\nGovernments are the borrowers of money in this transaction.\n\nGovernment “issues” bonds regularly, which is\n\nPrimary Market: Treasury sells bonds (millions of $ worth) mostly to private Investment Banks\nSecondary Market: Investment banks sell smaller chunks to private investors / bond holders sell to each other\n\n\nGovernments pay coupon and principal through\n\nTax revenue\nIssuing more bonds\n\n\n\nBonds are an important part of a portfolio because…\n\nAre near-riskless investments as they generate stable cash flows. The only risks are:\ninflation risk—when the government doesn’t have the money, they’ll just print more money. This causes inflation which devalues the bonds.\nAre very liquid, since many people are willing to buy bonds in most scenarios, even in recessions.\n\nBond Pricing §\nthm. The price of a bond is the following:\nB(n)=ry​/21−(1+ry​/2)−n​C+(1+2ry​​)nM​\nwhere…\n\nB(n) is the current market price of the bond\nn=2τ is the periods left to maturity (where τ is the years left to maturity)\n\nnot periods passed! periods left\n\n\nM is the Principal amount\n\nNormally use 100or1000\n\n\nC is the per-period (=semi-annual) coupon payment\n\nnormally, coupon is quoted at annual coupon rate rC​ (in percent) so C=2rC​​M\n\n\nry​ (or y) is the yield to maturity (≈discount rate)\n\nr is the current yield. r=B(n)2C​\nn is the number of coupon payments (semi-annual for Treasury bonds)\n\n\n\nThe Three Rates of Bonds\n\nry​: Yield to Maturity—“What’s the return rate for the coupons if I hold until maturity?”\nr: Current Yield—“Coupon rate, but at the current bond price”\nrC​: Coupon Rate—“Coupon rate (at principal price).”\n\n\nPrice and rates:\n\nTrades at premium B(n)&gt;M ⇔ ry​&lt;r&lt;rC​\nTrades at discount B(n)&lt;M ⇔ ry​&gt;r&gt;rC​\n\n\n\nObserve…\n\n\nB(n)∝yield1​.\n\n\nlimn→0​B(n)=M\n\n\nCausality: Exogenous factors → Δyield → Δprice.\n\n\n\nIn a portfolio of bonds…\n\n\nMarket Value=100Price of par​×# of pars\nThe causality of the variables of a bond. The market’s expected rate of return of a bond is the yield. With this yield we can calculate its price.\nPrice—Yield Curve (mathematical) §\nPrice and Yield are inversly correlated → Price-yield curve looks like this:\n\nDuration is the gradient of this price-yield function:\nDuration:=dydP​\nDollar Duration (DV01) is the change in price due to 1% change in yield [=$1 change in yield per par]\nDV01:=±1%⋅yΔP​\n\nDollar Duration can be only used to approximate price changes for small changes in yield (~50bp)\nDuration is used to estimate risk (Price sensitivity against yield = DV01 risk\nDollar Duration is quoted as an absolute value. (We all know that it’s mathematically negative)\nDV01 Risk:= Δ1%yieldΔMarket Value​ = “Δ market value for 1% yield change” = 100#par​⋅DV01\nDV01 has no relationship with volatility\n\n\n\n                  \n                  Example calculation \n                  \n                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime to MaturityDV01Yield Vol.e.g. change in yieldchange in price2 year$1.810%p20bp$0.3610 year$7.25%p10bp$0.72\nYield(–Maturity) Curve (empirical) §\n\nDeterminors of the shape of the yield curve:\n\nYield ∝ Maturity. The farther away the cash flows are, the more risky it is.\nInterest Rate changes. Yield ∝ 1/Price, and yield is linked to Fed Funds Rate. Thus buy at high yield, sell at low yield. Consider\n\nBond investors think a recession is looming in 1yr → Treasury will increase rates in 1yr → sell 1yr mat bonds\nLeft intercept is the fed rates &amp; short-mat bonds → high movement\nright side is long-mat bonds → low movemen\n\n\nLiquidity Preference Theory. Shorter maturity bonds are more liquid simply because more of them exist in the world. Higher liquidity means less risk.\n\nRisk &amp; Volatility §\ndef. Price[rate of return] volatility = SD[Price] (in $)\ndef. Yield volatility = SD[Yield] (in %p)\ndef. DV01 Risk = Change in MV per change in 1% yield\n\nYield Vol &gt; RoR Vol ← mathematical relationship (price formula)\nMaturity ∝ Price Vol.\n"},"Boolean-Decision-Problem":{"title":"Boolean Decision Problem","links":[],"tags":["Computing/Algorithms"],"content":"Q. CNF Satisfiability Problem. (=CNF-SAT =Circuit Satisfiability) Given a conjunctive normal form, is there a way to set the variables to True or False such that the formula evaluates to True?\n(x1​∨¬x2​∨x3​)∧(¬x1​∨x4​)∧(x2​∨¬x3​∨x5​∨¬x6​)\n\nLiteral: xi​ or xi​ˉ​\nClause: (x1​∨x2​∨x3​ˉ​) a set of or-connected literals\n\nthm. Cooke-Levin Theorem. The Boolean Satisfiability problem is NP-complete.\n\nThe first problem to be determined to be NP-complete.\n\nQ. 3-Satisfiability Problem. (3-SAT)\n\nThe CNF-SAT problem, but every set of ORs has only three variables\n3-SAT ≤p​ 4-SAT\n\nalg. Reduction CNF-SAT ≤p​ 3-SAT\n\nIdea: a∨b≡(a∨c)∩(b∨cˉ)……(1)\n\n\nConstruction\n\n∃{x1​…xn​} variables that satisfies Q:q1​…qm​ where qi​ is a clause\nQ′:q1′​…qm′′​ constructed to satisfy (1).\n\nm′&gt;m (obviously)\nNew variables v1​…vk​ thus Q has variables {x1​…xn​,v1​…,vk​}\n\n\n\n\nCNF-SAT ⇒ 3-SAT\n\nlet X1​…Xn​∈{0,1} that satisfies Q. Then these values will satisfy Q′ regardless of vi​ ’s values, so choose them arbitrarily. These values satisfy Q′\n\n\n3-SAT ⇒ CNF-SAT\n\nlet X1​…Xn​,V1​…Vn​∈{0,1} that satisfies Q′. Then X1​…Xn​ will satisfy Q.\n\n\nThus proven.\n\nQ. DNF Satisfiability Problem. Given a disjunctive normal form, is there a way to set the variables to True or False such that the formula evaluates to True?\n\nExists a Polynomial time algorithm\nOnly one clause need be satisfied\n\nQ. Tautology. Given a Boolean formula Q, is Q always True regardless of what the variables are set to?\n\nThe verification procedure for True itself is NP-Complete, and it is exactly the CNF Satisfiability problem\nThe verification procedure for False is also NP-hard\nThe whole problem is not in NP (and is Co-NP Complete)\n"},"Brain-Anatomy":{"title":"Brain Anatomy","links":[],"tags":["Biology/Neuroscience"],"content":""},"Branching-(Computer-Science)":{"title":"Branching (Computer Science)","links":[],"tags":["Computing/Computer-Architecture"],"content":"\n20% of instructions are branches\n30% of branches take an extra cycle\naverage CPI is 1+0.2*(0*.*3***1) = 1.06 cycles\n"},"Budget-Lines":{"title":"Budget Lines","links":["assets/Screen_Shot_2022-09-01_at_15.34.06.png","assets/Screen_Shot_2022-09-01_at_15.31.59.png"],"tags":["Economics/Micro-Economics"],"content":"\n⇒ Similar to the Possibility Frontier (PPF), but like a “consumption possibility frontier”.\n\n\n                  \n                  Info \n                  \n                \n\nYou will often plot all other consumed goods into one combined good, as the **“ofotherconsumption”∗∗withunit.\nI=p1​x1​+p2​x2​—Equation of the budget constraint\n\ny-intercept: p2​I​ → if you buy only x2​\nx-intercept: p1​I​ → if you buy only x1​\ngradient: −p2​p1​​\n\n→ Opportunity cost of one unit of x1​ in terms of x2​.\nnegative, since for additional consumption of x1​ you lose your ability to consume x2​.\n\n\n\nChange in Budget Lines §\n\nExogenous change in income I\nChnage in price p1​ or p2​ Graph\n\nWhen price of good x1​ decreases (i.e. p1​ decreases) intercept p2​I​ stays the same, gradient p2​p1​​ increases. Vice versa when p2​ changes.\n⇒ Real income changes\n\n\nEndowments\n\nEndowments are a particular basket of goods you get for free; it’s a single point in the graph—in this case, (5 pants, 10 shirts) is the endowment.\n\n\nProgressive Taxes\nInter-temporal Budgets\n"},"Buffett-Indicator":{"title":"Buffett Indicator","links":[],"tags":["Economics/Finance"],"content":"Buffett Indicator Valuation Model\n\nThe Buffett Indicator (aka, Buffett Index, or Buffett Ratio) is the ratio of the total United States stock market to GDP.\n\nGDPStock Market ∑MCAP​\n\n\n\n1: Stock is overvalued → Bubble\n\n\n&lt;1: Stock is undervalued → Buy Now\n"},"Byte-Pair-Encoding":{"title":"Byte-Pair Encoding","links":["GPT-2-Architecture"],"tags":["Computing/Maching-Learning"],"content":"Compression algorithm which is also used for GPT-2 Architecture\nByte Pair Encoding Tokenization - YouTube\nConstructing a Vocabulary §\n\n\nSplit the text into a Corpus, which is simply just split across whitespace &amp; punctuation\nSplit the Corpus into individual letters\n\nAdd all letters to the set Vocab\n\n\nPair up every possible letters from the Corpus. Observe its frequency\n\nAdd the most frequent pair into the set Vocab as a token.\nKeep track of the merge rules\n\n\nRepeat until the set Vocab reaches a desired size.\n\nTokenizing a text using a prexisting vocabulary §\nGo through the merge rules one by one, any apply greedily, until you can’t anymore. This is enough to tokenize the word."},"CAPM-Model":{"title":"CAPM Model","links":["Security-(Finance)","Regression","Risk-(Finance)","Market-Beta","Leverage","Portfolio-Theory-(Markowitz)","Measuring-Security-Performance","Lagrangian-Optimization"],"tags":["Economics/Finance"],"content":"Gentle Introduction §\ndef. The Capital Asset Pricing Model (CAPM) is a method of predicting the returns of an asset, by using regression analysis between it and the market’s returns.\n\nThe regression line has the equation:\nri​−rf​=α+β⋅(rm​−rf​)\nData correlates between market excess return (rm​−rf​) and asset excess return (=ri​−rf​). The statistics produces the following data:\n\nCoefficient of Determination [=R-value] 0≤R2≤1, measures the goodness of fit. Bigger means the model is a better fit.\nStandard deviations [=Risk (Finance)] σm​,σi​\nCorrelation coefficient −1&lt;ρ&lt;1, measures the degree of correlation between the two variables\nMarket Beta [=slope ≈1] β:=σm​ρi,m​⋅σi​​, which measures the sensitivity of one variable to another. It doesn’t deviate much from 1.\nAlpha [=intercept ≈1], which measures the excess return against all odds (of the prediction of the CAPM model.) It should, theoretically, always be zero. But it is not (See Jenson’s Alpha below.)\n\nIf we plot riskless assets with a riskful asset:\n\n\nrf​ is the riskless return, rp​ is the riskful asset’s return, and w is the weight of riskful assets in portfolio.\nσp​ is the volatility for the riskful asset, and σf​ the volatility of riskless asset, is zero by definition\nGradient of the blue line is σp​rp​−rf​​ ← this is a constant. Thus the blue frontier is linear.\n\n⇒ With riskless assets, you can short-sell the riskless asset to go beyond the risk/return:\n\n(2) is achieved by short-selling the risk-free asset and purchasing more of the riskfull asset.\n\nThis is also called Leverage.\nThis means that the weight on the risk-free asset is negative\n\n…and the weight on the risk-full asset is &gt;1\n\n\nIf the risk-free rate ever eclipses the risk-ful rate, disaster insues. (This is what happened during the financial crisis of 2008; banks were highly levered.)\n\nCombination of Risk-ful—Risk-free &amp; Risk-ful—Risk-ful §\n\n\n\n                  \n                  6~7%.\n                  \n                \n\n\nNote that the axes are not market returns, but **excess returns**. This means usually the y-intercept will be zero, unless as we later discuss $\\alpha &gt;0$.\n\nSensitivity vs. Asset Return. If the data shows the asset’s return is higher, then the sensitivity should increase.\n\n\nM is when the market is measuring against itself.\n\n\nRisks. In the CAPM model, there are two types of risks we care about.\n\nSystematic Risk [=market risk, beta risk]. This is the risk due to the market recessions.\n\nDiversification cannot solve beta risk.\nHigh beta implies higher returns in good times, worse returns in bad times.\n\n\nIdiosyncratic Risk. This is the risk due to individual firms’ characteristics (tech, organization, etc.)\n\nIdiosyncratic risk ⟶n→∞​0 i.e. diversification solves this\nUncorrelated with market risk.\n\n\n\n\n\n                  \n                  Industries that do well despite a recession: Pharmacuticals, Consumer necessities, Defense industries. \n                  \n                \n\nTechnical Reference §\nUse terminology from Portfolio Theory (Markowitz).\n\nMarket Beta: correlation of portfolio with market.\nCapital Allocation Line (CAL)\n\nSecurity (σi​,μi​)\nRisk-free security (0,μf​)\nEquation of Possible Portfolios (=Capital Allocation Line): μp​=σi​μi​−μf​​σp​+μf​\n\nThis is a line ⇒ Capital Allocation Line.\nSlope: σi​μi​−μf​​=Portfolio RiskExcess Return​=Sharpe Ratio\nHigher slope (=higher Measuring Security Performance) is better (more return per unit risk)\n\n\n\n\nCapital Market Line (CML)\n\nMarket Security (σi​,μM​)\nRisk-free security (0,μf​)\n\n\n\nInvestor Utility Function\n\nRed: risk-seeking\nBlack: risk-neutral\nBlue: risk-averse\n\n\n\n\nLagrangian Optimization problem maxμ,σ​E[u(Rp​)] s.t. g(μ,σ)=k\n⇒ to get optimal portfolio Rp∗​ \n\n\n\n\n"},"CS-250-Architecture":{"title":"CS 250 Architecture","links":["Abstraction","Performance-(Computing)","Power-Consumption-(Computing)","Moore’s-law","Instruction-Set","Instructions-(Computer-Science)","Branching-(Computer-Science)","Pipelining","Cache"],"tags":["Courses"],"content":"\nAbstraction\nPerformance (Computing)\n\nPower Consumption (Computing)\nMoore’s law\n\n\nInstruction Set\n\nInstructions (Computer Science)\nBranching (Computer Science)\nPipelining\n\n\nCache\n"},"CS-316-Database-Systems":{"title":"CS 316 Database Systems","links":["Database-Management-System","Relational-Algebra","Entity-Relationship-Model","Database-Design-Theory","SQL-Basics","SQL-Constraints","Extensible-Markup-Language","Document-Type-Definition","XPath-and-XQuery","MongoDB-Reference","Physical-Data-Organization","Database-Indexing","SQL-Query-Processing-Algorithms","SQL-Query-Optimization","SQL-Transaction-Guarantees"],"tags":["Courses"],"content":"Theoretical Foundations §\n\nDatabase Management System\nRelational Algebra\nEntity-Relationship Model\nDatabase Design Theory\n\nSerializable Query Language §\n\nSQL Basics\nSQL Constraints\n\nNoSQL Databases §\n\nExtensible Markup Language (XML)\nDocument Type Definition\nXPath and XQuery\nMongoDB Reference\n\nQuery Processing §\n\nPhysical Data Organization\nDatabase Indexing\nSQL Query Processing Algorithms\nSQL Query Optimization\nSQL Transaction Guarantees\n"},"CS-330-Advanced-Algorithms":{"title":"CS 330 Advanced Algorithms","links":["CS330-HW03","CS330-HW04","CS330-HW05","CS330-HW07","CS330-HW08","CS330-HW10","Time-Complexity","Recurrence-Relation","Peak-Element","Undominated-Points","Sorting-Algorithms","Priority-Queue","Parallel-Algorithms","Matrix-Multiplication","Inner-Product","Dynamic-Programming","Longest-Common-Sequence","Knapsack-Problem","Matrix-Chain-Multiplication","Depth-First-Search","Directed-Graph","Directed-Acyclical-Graph","Shortest-Path","Greedy-Algorithm","Scheduling-Problem","Huffman-Text-Compression-Algorithm","Minimal-Spanning-Tree-Problem","Ackerman-Function","Maximum-Flow-Problem","Computational-Tractability","Boolean-Decision-Problem","Common-Graph-Problems","Subset-Sum","Linear-Programming","Approximation-Algorithm","Set-Cover","Traveling-Salesperson-Problem","K-Clustering-Problem","Hashing-Algorithms"],"tags":["Courses"],"content":"Homeworks\n\n\nCS330 HW03\n\n\nCS330 HW04\n\n\nCS330 HW05\n\n\nCS330 HW07\n\n\nCS330 HW08\n\n\nCS330 HW10\n\n\nAsymptotic Analysis\n\nRecurrence Relation\n\n\n\nDivide and Conquer\n\nPeak Element\nUndominated Points\nQuick Sort\n\nTournament Tree\n\n\nMerge Sort\n\n\n\nParallel Algorithms\n\nParallel Sum\nMatrix Multiplication\n\nParallel Inner Product\n\n\nParallel Merge Sort\n\n\n\nDynamic Programming\n\nFibonacci Sequence\nLongest Common Sequence\nKnapsack Problem\nMatrix Chain Multiplication: increasing in gaps\n\n\n\nDepth First Search\n\nReachability and Connectivity\nDirected Graph\n\nPreand Post-time\nStrongly Connected Component algorithm (Kosaraju’s)\nCycle detection → DAG\nTopological Sort of DAG\n\n\n\n\n\nShortest Path\n\nSingle Source Shortest Path\n\nBFS Shortest Path (vertex creation, alarm clock method)\nDijkstra’s\nA*\nBellman-Ford\n\n\nAll-Source Shortest Path: Floyd-Warshall\n\n\n\nGreedy Algorithm\n\nScheduling Problem\nHuffman Text Compression Algorithm\nMinimal Spanning Tree Problem\nAckerman Function\n\n\n\nMaximum Flow Problem (=Min-cut, Edge matching)\n\nMin Cut\nEdge Matching\n\n\n\nComputational Tractability\n\nPolynomial Reduction\nBoolean Decision Problem\n\nCooke-Levin Theorem\nCNF-SAT ≤p​ 3-SAT\nVertex Cover ≤p​ Subset Sum\nSubset Sum ≤p​ Knapsack Problem\n\n\nCommon Graph Problems\n\nIndependent Set\nClique\nVertex Cover\nTriangle Cover\n\n\nLinear Programming\n\n\n\nApproximation Algorithms\n\nLoad Balancing\nSet Cover\nTraveling Salesperson Problem\n\nChristofides Approximation (1.5-level)\n\n\nK-Clustering Problem\n\nK-Max: Maximum distance to closest center\nK-Means: Mean per center\n\nLlyod’s Approximation (2-level)\nK-Means++ Approximation (&lt;2-level)\n\n\n\n\n\n\n\nHashing Algorithms\n\nUniformity\nUniversality\nLinear Congruence Hashing\nMultiply Shift Binary Hashing\n\n\n"},"CS-334-Formal-Languages":{"title":"CS 334 Formal Languages","links":["Finite-Automata","Regular-Expressions","Regular-Grammar","Regular-Languages","Pushdown-Automata","Context-Free-Grammar","Context-Free-Language","Context-Free-Language-Parsing","Turing-Machine","Recursively-Enumerable-Languages","Unrestricted-Grammar","Formal-Grammar","Formal-Languages","Chompsky-Heirarchy"],"tags":["Courses"],"content":"Finite Automata §\n\nFinite Automata\nRegular Expressions\nRegular Grammar\nRegular Languages\n\nPushdown Autotmata §\n\nPushdown Automata\nContext-Free Grammar\nContext-Free Language\nContext-Free Language Parsing\n\nTuring Machines §\n\nTuring Machine\nRecursively Enumerable Languages\nUnrestricted Grammar\n\nBackground Knowledge §\n\nFormal Grammar, Formal Languages\nChompsky Heirarchy\n"},"CS-535-Algorithmic-Game-Theory":{"title":"CS 535 Algorithmic Game Theory","links":["Game-Theory","Equilibria-in-Game-Theory","Zero-Sum-Game","Potential-Game","Traffic-Routing","No-Regret-Dynamics","Auction-Theory","Vickery-Auction","Sponsored-Search-Auction","Revenue-Maximizing-Auctions","Combinatorial-Auction","VCG-Auction","Ascending-Price-Auction","Optimal-Stopping-Problem","Fairness-(Economics)","Utility-Function","Fractional-Allocation","Integer-Allocation","Ordinal-Allocation","Property-Exchange","Stable-Marriage-Problem","Ski-Rental-Algorithm","Online-Matching","CS535-HW1","CS535-HW2","CS535-HW3","CS535-HW4","CS535-HW5","CS535-Midterm","(Paper)-Roughgarden,-Transaction-Fee-Mechanism-Design"],"tags":["Courses"],"content":"Games, Learning, and Dynamics §\n\nGame Theory\n\nEquilibria in Game Theory\nZero Sum Game, min-max theorem\n\n\nPotential Games\n\nTraffic Routing\n\n\nNo-Regret Dynamics\n\nAuctions §\n\nAuction Theory (directory of auctions)\n\nVickery Auction\nSponsored Search Auction\nRevenue-Maximizing Auctions\nCombinatorial Auction\n\nVCG Auction\nAscending Price Auction\n\n\n\n\nOptimal Stopping Problem\n\nAllocations §\n\nFairness (Economics) (directory of allocation methods)\n\nUtility Function\n\n\nCardinal Allocation\n\nFractional Allocation\nInteger Allocation\n\n\nOrdinal Allocation\n\nProperty Exchange\nStable Marriage Problem\n\n\nOnline Algorithms\n\nSki Rental Algorithm\nOnline Matching\n\n\n\n\n\nCS535 HW1\nCS535 HW2\nCS535 HW3\nCS535 HW4\nCS535 HW5\nCS535 Midterm\n(Paper) Roughgarden, Transaction Fee Mechanism Design\n"},"CS-675-Deep-Learning":{"title":"CS 675 Deep Learning","links":["Norm-(Math)","Information-Theory","List-of-Machine-Learning-Datasets","Neural-Networks,-Backpropagation-and-Gradient-Descent","Better-Gradient-Descent-Methods","Training-Tricks","Logistic-Model","Probabilistic-Generative-Models","Convolutional-Neural-Networks","Object-Detection","Linear-Factor-Models","Probabilistic-Principle-Component-Analysis","Independent-Component-Analysis","Slow-Feature-Analysis","Sparse-Coding-and-Dictionary-Learning","Autoencoders","Variational-Autoencoders","Restricted-Boltzmann-Machines","Generative-Adversarial-Network","CS-675-HW1","CS-675-HW2","CS-675-HW5"],"tags":["Courses"],"content":"\n\n                  \n                  That&#039;s cute. But I don&#039;t care why it works—if it works, it&#039;s useful. If it doesn&#039;t, it&#039;s just cute. \n                  \n                \n\nBackground §\n\nMathematics\n\nNorm (Math)\nInformation Theory\n\n\nTraining Basics\n\nList of Machine Learning Datasets\nNeural Networks, Backpropagation and Gradient Descent\nBetter Gradient Descent Methods\nTraining Tricks\n\n\n\nSupervised Models §\nSupervised models minimize the difference (often cross-entropy) between data distribution and ideal target distribution\n\nClassifiers\n\nLogistic Model\nProbabilistic Generative Models\n\n\nVision\n\nConvolutional Neural Networks\nObject Detection\n\n\n\nUnsupervised Models §\nUnsupervised models all aim to cleverly extract abstract features from data without labels.\n\nLinear Factor Models\n\nProbabilistic Principle Component Analysis\nIndependent Component Analysis\nSlow Feature Analysis\nSparse Coding and Dictionary Learning\n\n\nAutoencoders\n\nVariational Autoencoders\n\n\nGraphical Models\n\nRestricted Boltzmann Machines\n\n\n⭐ Generative Adversarial Network (GANs)\n\nTime Series Modeling §\n\nAutoregressive Moving Average Models\n\n\nWritten Homework Sections §\n\nCS 675 HW1\nCS 675 HW2\nCS 675 HW5\n\nNotes to Improve §\n\n Finish up MLE derivation &amp; understanding for Probabilistic Generative Models\n Finish bias derivation in NN\n Finish optimizing the logistic regression ^02k7nz\n R-CNN, Faster R-CNN etc.\ndeep learning is just a catalogue of tools, and finding which tool to use is the key intuition\n"},"CS-675-HW1":{"title":"CS 675 HW1","links":[],"tags":[],"content":"Problem 3 §\nFrom U1​,U2​ to R,Θ §\nConsider that\n\nU1​=2πΘ​, dΘdU1​​=2π1​\nU2​=exp(−2R2​), dRdU2​​=Rexp(−2R2​)\nUsing change of variables  we have:\nfΘ​(θ)=2π1​\nfR​(r)=rexp(−2r2​)\n\nFrom R,Θ to Z1​,Z2​ §\nNote that:\n\nZ12​+Z22​=R2(cos2Θ+sin2Θ)=R2 thus R=Z12​+Z22​​\n\ndZ1​dR​=Z1​(Z12​+Z22​)−1/2=RZ1​​\nSymmetrically dZ2​dR​=RZ2​​\n\n\nZ1​Z2​​=RcosΘRsinΘ​=tanΘ thus Θ=arctan(Z1​Z2​​)\n\ndZ1​dΘ​=1+Z12​Z22​​1​⋅Z2​(−1)Z1−2​=−Z12​+Z22​Z2​​=−R2Z2​​\nSymmetrically dZ2​dΘ​=Z12​+Z22​Z1​​=R2Z1​​\nNow, calculate the jacobian for a multivariate change of variables:\n\n\n\n∣J∣​=​∂z1​∂r​∂z1​∂θ​​∂z2​∂r​∂z2​∂θ​​​=​RZ1​​−R2Z2​​​RZ2​​R2Z1​​​​=R3Z12​​+R3Z22​​=R1​​​\nAnd thus the join probability being:\nfΘ,R​(θ,r)​=fΘ​(θ)⋅fR​(r)⋅∣J∣=2π1​rexp(−2r2​)⋅r1​=2π1​exp(−2z12​+z22​​)=2π​1​exp(−2z12​​)⋅2π​1​exp(−2z22​​)=fZ1​​(z1​)⋅fZ2​​(z2​)​​\nThis show both that:\n\nfZ1​​,fZ2​​ is the standard normal pdf\nZ1​,Z2​ are independent because the joint pdf is a simple product of each pdf. ∎\n"},"CS-675-HW2":{"title":"CS 675 HW2","links":[],"tags":["Courses"],"content":"Problem 1 §\nLet h3​=y^​ for consistencey in notation. Assume vectors are column vectors. First we calculate:\n∂h3,i​∂ℓ​∂h3​∂ℓ​​=N−2​(yi​−h3,i​)=−N2​(y−h3​)⊤=letδ3⊤​​shape 1×D3​​​\nThen we consider the tree of computation as in class. Let ak​=Wk⊤​hk−1​+bk​\n\n∂W3⊤​∂h3​​=h2​ to get ∂W3⊤​∂ℓ​=h2​δ3⊤​ 🔴\n∂b3​∂h3​​=1 to get ∂b3​∂ℓ​=δ3⊤​ 🔴\n∂h2​∂h3​​=W3​ to get ∂h2​∂ℓ​=W3​δ3⊤​=letδ2​\nNext layer:\n∂a2​∂h2​​=∂(⋅)∂g(⋅)​\n∂h1​∂a2​​=W2⊤​ to get ∂h1​∂ℓ​=W2⊤​g′(⋅)δ2​=letδ1​\n∂W2⊤​∂h2​​=h1​ to get ∂W2⊤​∂ℓ​=h1​δ2⊤​ 🔴\n∂b2​∂h2​​=1 to get ∂b2​∂ℓ​=δ2⊤​ 🔴\nSimilarly:\nget ∂W1⊤​∂ℓ​=h0​δ1⊤​=xδ1⊤​ 🔴\nget ∂b1​∂ℓ​=δ1⊤​ 🔴\n\nResults marked with 🔴"},"CS-675-HW5":{"title":"CS 675 HW5","links":[],"tags":["Courses"],"content":"Problem 2-1 §\n(coding problems follow below)\nThe energy function:\nE(v,h;θ)=−(i∑​j∑​σi​Wij​hj​vi​​−i∑​2σi2​(vi​−bi​)2​+j∑​αj​hj​)\nAnd the joint distribution p(v,h)=Zexp(−E(v,h))​\np(vi​=X∣h) §\nIsolate terms involving vi​:\nE(vi​,h)​=−(j∑​σi​Wij​hj​vi​​−2σi2​(vi​−bi​)2​)+…=vi​(j∑​σi​Wij​hj​​)−2σi2​(vi​−bi​)2​=−2σi2​(vi​−μi​)2​+…​​\nwhere\nμi​=bi​+σi​j∑​Wij​hj​.\nAnd noting Z as the regularization, p(vi​∣h)is Gaussian:\np(vi​∣h)=2πσi2​​1​exp(−2σi2​(vi​−μi​)2​).\np(hj​=1∣v) §\nIsolate terms involving hj​:\nE(v,hj​)=−(i∑​σi​Wij​hj​vi​​+αj​hj​)+…\nFor hj​∈{0,1}:\nexp(−E(v,hj​))={exp(0),exp(∑i​σi​Wij​vi​​+αj​),​hj​=0hj​=1​.\nThus the conditional p(hj​=1∣v):\np(hj​=1∣v)=σ(i∑​σi​Wij​vi​​+αj​),\nwhere σ(x)=1+exp(−x)1​ is the sigmoid function."},"CS-Course-Requirements-(Done)":{"title":"CS Course Requirements (Done)","links":["CS-675-Deep-Learning"],"tags":["Courses"],"content":"Bachelor of Science (BS) Degree\nCourse Substitutions for CS Majors or Minors\nGraduation with Distinction\nPrerequisites (Done) §\n\nOne of the following introductory COMPSCI courses or equivalent:\n\nCOMPSCI 101L (Introduction to Computer Science)\nCOMPSCI 102 (Interdisciplinary Introduction to Computer Science)\nCOMPSCI 116 (Foundations of Data Science)\n\n\nMATH 111L (Introductory Calculus I) or equivalent\nMATH 112L (Introductory Calculus II) or equivalent\n\nBase Requirements (Done) §\n\nCOMPSCI 201 (Data Structures and Algorithms)\n==COMPSCI 230 (Discrete Math for Computer Science) see substitutions==\nCOMPSCI 250 (Computer Architecture)\nCOMPSCI 330 (Design &amp; Analysis of Algorithms)\n\nOne Of the following COMPSCI Courses on Systems: (Done) §\n\nCOMPSCI 310 (Introduction to Operating Systems) or 510 (Advanced Operating Systems)\nCOMPSCI 316 (Introduction to Databases) or 516 (Database Systems)\nCOMPSCI 350 (Digital Systems, cross-listed as ECE 350) or 550 (Advanced Computer Architecture, cross-listed as ECE 552)\nCOMPSCI 351 (Computer Security) or 551 (Advanced Computer Security)*\nCOMPSCI 356 (Computer Network Architecture) or 514 (Computer Networks)\n\nTwo Courses in MATH/STA: (Done) §\n\nOne STA course at or above STA 111**, including the cross-listed MATH 230\nOne of MATH 202, 216, 218, or 221***\n\nFive Electives at 200-level or Higher (beyond Those Counted towards the Requirements above): (1 left) §\n\nThree COMPSCI courses that are not independent study courses\n\nCS 334 Formal Languages\nCS 545 Algorithmic game theory\n==CS 675 Deep Learning==\n\n\nTwo in COMPSCI (independent study possible), MATH, STA, or a related area approved by the Director of Undergraduate Studies\n\nMath 212 (this is allowed, see below)\nSTAT 432 Stat. Inference &amp; Learning\n\n\n\nBoth courses have been offered as a 290 and 590 course with the same name, and will satisfy this requirement.\n\nSTA 111 will not be offered after Summer 2020. We recommend you take STA 199 or higher.\n\n**MATH 212 does not count towards this requirement, but can count towards an elective."},"CS330-HW03":{"title":"CS330 HW03","links":[],"tags":["Courses"],"content":"Problem 2 §\nSubproblem (a) §\nIdea: a majority element must be majority in either of the two halves of an array\nAlgorithm:\n# array to check\nA[1..n]\n \nfunction Count(l, r, elem)\n\t# base case\n\tif l = r\n\t\treturn 1\n \n\t# divide array into two, and count there\n\tm = ceiling((l+r)*0.5)\n\tl_count = PCount(l, m, elem)\n\tr_count = PCount(m+1, r, elem)\n \n\t# return the sum of left and right counts\n\treturn l_count + r_count\n \nfunction MajorityElem(l,r)\n\t# base case\n\tif l = r\n\t\treturn A[l] \n \n\t# check left, right half majority\n\tm = ceiling((l+r)*0.5)\n\tleft_majority = MajorityElem(l, m)\n\tright_majority = MajorityElem(m+1, r)\n \n\t## check consensus\n \n\t# check no consensus first; if one is null return the non-null\n\tif left_majority == NULL and right_majority != NULL:\n\t\treturn right_majority\n\tif left_majority != NULL and right_majority == NULL:\n\t\treturn left_majority\n \n\t# there cannot be a both no consensus case\n \n\t# consensus agrees\n\tif left_majority == right_majority:\n\t\treturn left_majority\n\t\n\t# consensus disagrees\n\telse if left_majority != right_majority:\n \n\t\t# count the majority\n\t\tleft_count = Count(l, r, left_majority)\n\t\tright_count = Count(l, r, right_majority)\n\t\t\n\t\t# if tied, no consensus\n\t\tif left_count == right_count\n\t\t\treturn NULL\n\t\t\n\t\t# winner exists, return the winner\n\t\treturn left_count &gt; right_count ? left_majority : right_majority\n\t\t\n\nCount function: T(n)≤2T(2n​)+O(1)=O(n)\nMajorityElement function: T(n)≤2T(2n​)+2⋅O(n)\n\nFirst term: recursive calls\nSecond term: calls Count\nSolution: T(n)=O(nlogn) (same recurrence relation as mergesort)\n\n\nCorrectness (Brief argument):\n\nBase case: in n=1 the element is majority element\nIH: Assume MajorityElement(k/2) is correct.\nIS:\n\nIf one of them has no majority but the other ones does, the one with majority must be the true majority because of an element exists in more than half of the total array it must be that in one of the halves it will be the majority.\nIf both halves have a majority, then the one that has more counts in the total array wins\nThere cannot be a both halves no-majority case as explained in (1)\n\n\n\n\n\nSubproblem (b) §\nParallel version of algorithm\n# array to check\nA[1..n]\n \nfunction PCount(l, r, elem)\n\t# base case\n\tif l = r\n\t\treturn 1\n \n\t# divide array into two, and count there\n\tm = ceiling((l+r)*0.5)\n\tspawn l_count = PCount(l, m, elem)\n\tspawn r_count = PCount(m+1, r, elem)\n\tsync\n\t\n\t# return the sum of left and right counts\n\treturn l_count + r_count\n \ndef MajorityElem(l,r)\n\t# base case\n\tif l = r\n\t\treturn A[l] \n \n\t# check left, right half majority\n\tm = ceiling((l+r)*0.5)\n\tparallel left_majority = MajorityElem(l, m)\n\tparallel right_majority = MajorityElem(m+1, r)\n \n\t## check consensus\n \n\t# check no consensus first; if one is null return the non-null\n\tif left_majority == NULL and right_majority != NULL:\n\t\treturn right_majority\n\tif left_majority != NULL and right_majority == NULL:\n\t\treturn left_majority\n \n\t# there cannot be a both no consensus case\n \n\t# consensus agrees\n\tif left_majority == right_majority:\n\t\treturn left_majority\n\t\n\t# consensus disagrees\n\telse if left_majority != right_majority:\n\t\t# count the majority\n\t\tleft_count = PCount(l, r, left_majority)\n\t\tright_count = PCount(l, r, right_majority)\n\t\t\t\t\n\t\t# if tied, no consensus\n\t\tif left_count == right_count\n\t\t\treturn NULL\n\t\t\n\t\t# winner exists, return the winner\n\t\treturn left_count &gt; right_count ? left_majority : right_majority\n \n\nPCount analysis\n\nSpan: T∞​(n)≤T(2n​)+O(1)=O(logn)\nWork: W∞​(n)≤2T(2n​)+O(1)=O(n)\nmajority element determination can be run in parallel\n\n\nSpan: T∞​(n)≤T(2n​)+O(logn)=O(log2n)\n\nFirst term: parallel recursive call\nSecond term: compare the majority\nSolution\n\nUsing the master theorem, we see logb​a=log2​1=0 and the for the residual term ccrit​=0.\nThis is the second case from which we infer T∞​(n)=O(log2n)\n\n\n\n\nWork: W∞​≤2T(2n​)+O(n)=O(nlogn)\n\nProblem 3 §\nfunction PCountAndArrange(l,r) =&gt; int\n\t# base case; if zero, return count 1; else return count 0\n\tif l=r\n\t\treturn A[l] == 0 ? 1 : 0\n \n\t# recursive call &amp; count\n\tm = ceiling((l+r)*0.5)\n\tk = spawn PCountAndArrange(l, m)\n\tn = spawn PCountAndArrange(m+1, r)\n\tsync\n\tzerocount = k + n\n \n\t# rearrange the left half zeros with right half numbers\n\tparallel for i = m-k+1 .. m\n\t\tB[i+k] = A[i]\n\tparallel for i = m .. m+k\n\t\tB[i-k] = A[i]\n\tparallel for i = m-k+1 .. m+k\n\t\tA[i] = B[i]\n \n\t# return the count\n\treturn zerocount\n\t\n\nProcessor count: O(n)\n\nPCountAndArrange recursive calls take O(n) processors\nrearrange takes O(n) processors for each parallel for loop\n\n\nSpan: T∞​(n)≤T(2n​)+O(1)=O(logn)\nWork: W∞​(n)=2T(2n​)+O(n)=O(n)\n\nFirst term for recursive call\nSecond term for rearranging\n\n\n\nProblem 4 §\n# A[(int, int)] is the structure of the array\n \nfunction FindMaximal()\n\tmax_y = -Infinity\n\t\n\t\n\t# Sort the points in decreasing order by x-coordinate\n\tReverseMergeSort(A[])\n\tmax_y = A[0]\n\tresult = [A[0]]\n\t\n\t# traverse in reverse x direction (right to left)\n\tfor p in A\n\t\t# if y is bigger than anything seen before it&#039;s maximal\n\t\tif p.y &gt; max_y then\n\t\t\tresult.append(p)\n\t\t\tmax_y = p.y\n\t\t\t\n\treturn result\n\nReverseMergeSort O(nlogn)\nFindMaximal T(n)=O(n)+O(nlogn)=O(nlogn)\nCorrectness (Brief argument):\n\nThe rightmost element must be the maximal element\nIf the current element has a higher y value than any other element on the right side of it, it must be a maximum\nIf the current element has a lower y value than another point on the right of it, it is dominated by that point and thus cannot be a maximum\n\n\n\n# A[(int, int)] is the structure of the array\n \nfunction isDominant((x1,y1),(x2,y2))\n\t# first is dominant\n\tif (x1 &gt;= x2 and y1 &gt; y2) or (x1 &gt; x2 and y1 &gt;= y2)\n\t\treturn 1\n\t# second is dominant\n\tif (x2 &gt;= x1 and y2 &gt; y1) or (x2 &gt; x1 and y2 &gt;= y1)\n\t\treturn -1\n\t# none are dominant\n\telse\n\t\treturn 0\n\t\t\n \nfunction PSortedSubtract(A[1..n],B[1..m])\n\tparallel for i = 1..n\n\t\tif BinarySearch[A[i],B] == False\n\t\t\tC.append(A[i])\n \nfunction PFindDoms(l, r)\n\t# base case\n\tif l=r\n\t\treturn l\n\t\n\t# get indicies of domannt points from left and right half\n\tm = ceiling((l+r)*0.5)\n\tldoms = spawn PFindDoms(l,m)\n\trdoms = spawn PFindDoms(m+1,r)\n\tsync\n \n\tnondoms = []\n\tparallel for i = l..m\n\t\tparallel for j= m+1..r\n\t\t\t# either one is dominant drop the non-dominant\n\t\t\tif isDominant[A[i],A[j]] == 1\n\t\t\t\tnondoms.add(j)\n\t\t\tif isDominant[A[i],A[j]] == -1\n\t\t\t\tnondoms.add(i)\n\t\t\t\t\n\tPMergeSort(nondoms)\n\t\n\treturn SortedSubstract((ldoms + rdoms), nondoms)\n \nfunction FindMaximal()\n\t# mergesort based on the x values of the array\n\tXMergeSort(A)\n\treturn FindDoms(l, r)\n \n\nisDominant span: O(1)\nPSortedSubtract\n\nspan: T∞​(n)=O(logn)\nwork: W∞​(n)=O(nlogn)\n\n\nPFindDoms\n\nspan: T(n)=T(2n​)+O(1)+O(log3n)+O(logn)=O(log4n)\n\nfirst term: parallel recursion\nsecond term: parallel comparison of dominant points\nthird term: PMergeSort\nfourth term: SortedSubtract\nSecond case using master theorem: O(log4n) time.\n\n\n\n\n"},"CS330-HW04":{"title":"CS330 HW04","links":[],"tags":["Courses"],"content":"Problem 2 §\nfunction VC(v)\n\t# including v\n\tin = 1\n\tfor child c of v:\n\t\tin += min(c.in, c.ex)\n\t\n\t# not including v\n\tex = 0\n\tfor child c of v:\n\t\tex += c.in\n\t\t\n\tv.in = in\n\tv.ex = ex\n\t\n\treturn min(v.in, v.ex)\n\nBase case: for a leaf node leaf, leaf.in = 1, leaf.ex = 0\nIH: for node v with leaves u1..uk, assume ui.in, ui.ex is correct\nIS: for node v,\n\nCase including: VC including v does not require u1..uk to be included, but including it may be more minimal. IH guarantees ui.in, ui.ex is correct so take the minimum\nCase excluding: VC excluding v requires all of u1..uk to be included, so in that case sum all the ui.in values to guarantee counting all cases where children are included\n\n\nTime complexity: T(n)≤#children⋅T(#childrenn​)+O(1)=O(n)\n\nProblem 3 §\ndef PalindromeDecomposition\n \n\t# memo[n][n] is a 2-d array whose value is all &quot;N&quot;\n\t\n\t# in increasing order of gap\n\tfor gap=1..n\n\t\tfor i=0..n-gap-1\n\t\t\tj = i + gap\n\t\t\t\n\t\t\t# base case\n\t\t\tif i==j\n\t\t\t\tmemo[i][j] = Y\n\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t# if edges are same, check the inner\n\t\t\tif A[i] == A[j]\n\t\t\t\tif memo[i+1][j-1] == Y\n\t\t\t\t\tmemo[i][j] == Y\n\t\t\t\telse\n\t\t\t\t\tmemo[i][j] == N\n\t\t\t\t\t\n\t\t\t# if edges are diff, not palindrome\n\t\t\telse A[i] != A[j]\n\t\t\t\tmemo[i][j] == N\n\t\n\t# memo_count[n][n] is a 2-d array whose value is all 0.\n\t\n\t# in increasing order of gap\n\tfor gap=1..n\n\t\tfor i=0..n-gap-1\n\t\t\tj = i + gap\n\t\t\t\n\t\t\t# if the whole length i-j is palindrome, 1\n\t\t\tif memo[i][j] == Y\n\t\t\t\tmemo_count[i][j] = 1\n\t\t\t\t\n\t\t\t# if the whole length isn&#039;t palindromic...\n\t\t\tminPcount = +infty\n\t\t\tfor k=0..j-i:\n\t\t\t\t# get the minimum palindrome split of all possible splits\n\t\t\t\tmin(minPcount, memo[i][i+k-1] + memo[i+k][j])\n\t\t\tmemo_count[i][j] = minPcount\n\t\n\treturn memo_count[0][n]\n \n\n\nPart 1:\n\nBase case: a length-1 string is a palindrome\nIH1: substring A[i+1][j−1] is a palindrome\n\nIS1: then if A[i]=A[j], the string A[i][j] is a palindrome\n\n\nIH2: substring A[i+1][j−1] is not a palindrome\n\nIS2: then the string A[i][j] cannot be a palindrome\n\n\n\n\n\nPart 2:\n\nBase case: If the string A[i][j] is a palindrome, then it is the minimum palindromic split of count 1\nIH: palindromic substring count has been computed correctly from all proper substrings of A[i][j]\nIS: the minimum palindromic substring if A[i][j] is the minimum of the sums A[i][i+k−1]+A[i+k][j] for k∈[i,j]\n\n\n\nThus proved\n\n\nPart 1 span: O(n2) as filling a 2-d array with O(1) calculation per subarray\n\n\nPart 2 span: O(n3) as filling a 2-d array with O(n) calculation per subarray\n\n\n⇒ Total time complexity: O(n3)\n\n"},"CS330-HW05":{"title":"CS330 HW05","links":[],"tags":["Courses"],"content":"Problem 1 §\nV = [] # all vertices\nE = [] # all edges\n# A[1..m] holds the preference list\n \ndef graphCreation():\n\t# for every friend\n\tfor k = 1..m:\n\t\tprevNode = None\n\t\tfor i = 1..len(A[k]):\n\t\t\tnode = A[k][i]\n\t\t\tif node not in V:\n\t\t\t\tV.append(node)\n\t\t\t# if this is the first node\n\t\t\tif prevNode != None:\n\t\t\t\t# add the edge\n\t\t\t\tE.append((node, prevNode))\n\t\t\tprevNode = node\n \npost = [] # post-time\nvisited = [] # add visited verticies\nclock = 0\n \n# in-class algorithm\ndef DFS(u):\n\tvisited[u] = True\n\tclock += 1\n\tfor (u -&gt; v) in E:\n\t\tif visited[v] == False:\n\t\t\tDFS(v)\n\tclock += 1\n\tpost = (u, clock)\n\t\ndef AllDFS(V, E):\n\tfor v in V:\n\t\tif visited[v] == False:\n\t\t\tDFS(v)\n \n# in-class proof\ndef TopologicalSort():\n\ttopological = Mergesort(post) # reverse merge sort of post-time\n\t# will be sorted by clock time:\n\t# e.g. [(v1, 0), (v4, 3), (v2, 7), ...]\n\treturn [v for v, time in topological]\n\t\t\t\t\nHigh-Level Algorithm §\nThis problem resembles the computation of a Topological Sorting on a Directed Acyclic Graph (DAG) that is created from the preference lists, which may contain contradictions.\n\nWe create a graph where each vertex represents a restaurant and each directed edge from vertex i to j signifies that restaurant i is preferred over restaurant j by some friend. We let n be the number of vertices (restaurants), m the number of edges (preferences), and N the total length of all preference lists.\nWe perform a Depth-First Search (DFS) on this graph to detect a cycle. If there is a cycle (contradictions in the preference lists), we report that no such ordering of restaurants exists.\nIf there is no cycle, we carry out a Topological Sort on this graph which gives the required ordering of restaurants.\n\nProof of Correctness §\n\nLet’s represent the preference list of each friend as a directed path in the graph. The direction from i to j (i→j) represents that restaurant i is preferred over restaurant j. Therefore, if there is an ordering that satisfies all friends’ preferences, the directed graph has to be acyclic.\nWe perform DFS on the graph, and if we find a back edge (which forms a cycle), it means the preferences are contradictory, and no solution exists. So, our DFS serves as a mechanism to validate if such an ordering can exist.\nIf DFS completes without detecting a cycle, we then apply Topological Sorting on this graph. The use of DFS ensures that all vertices are explored and included in the final ordering.\n\nRunning Time §\n\nCreating the graph takes O(N) time as we are examining each preference and creating an edge for it.\nRunning a DFS on the constructed graph takes O(n+m) time, where n is the number of vertices, and m is the number of edges, visited exactly once.\nThe Topological sorting takes O(nlogn) time as uses mergesort via post-time\n⇒ Total running time of the algorithm is O(N)+O(n+m)+O(nlogn)=O(nlogn+m+N).\n\nProblem 2 §\nV = [] # all verticies\nE = [] # all edges, undirected graph\nfor c in Constraints:\n\t# unpack the constraint into the two verticies of a graph\n\tu, v = c\n \n\t# add both verticies\n\tif v not in V:\n\t\tV.append(v)\n\tif u not in V:\n\t\tV.append(u)\n \n\t# add the edge if it doesn&#039;t exist\n\t# note that this is an undirected graph\n\tif both (u, v) and (v, u) not in E:\n\t\tE.append((u, v))\n \n \n \nvisited = []\nteam = [-1] * len(V) # create a team array initialized with -1\n# there is team 0 and team 1\nbipartite = True # assume true until disproven\n \ndef partitionDFS(u):\n\tvisited.append(u)\n\tfor all neighbours v of u:\n\t\t# if same team, not bipartite\n\t\tif v in visited and team[v] == team[u]:\n\t\t\t\tbipartite = False\n\t\t\t\treturn\n\t\t# if different team and not in visitied\n\t\telif v not in visited:\n\t\t\t# assign v the opposite team as u\n\t\t\tteam[v] = 1 - team[u]\n\t\t\tpartitionDFS(v)\n \ndef AllDFS():\n\tfor v in V:\n\t\tif v not in visited:\n\t\t\tteam[v] = 0 # start coloring from team 0\n\t\t\tpartitionDFS(v)\n \nIdea §\nThis problem can be seen as a variant of Bipartite Graph Checking problem, where every edge represents the constraint that two players cannot be in the same team.\n\nWe construct a graph where each vertex corresponds to a player and each edge corresponds to the constraint between players. Let n be the number of vertices (players) and m be the number of edges (constraints).\nWe then perform a Depth-First Search (DFS) on this graph aiming to 2-color the graph where color should alternate between vertices. If during this process we encounter a vertex that has already been visited and has the same color as the current vertex, it means players with existing constraints are in the same team, thus, no valid partition can exist.\nIf the DFS completes without detecting such a situation, the created 2-color partitions are the required team allocations.\n\nProof of Correctness §\nThe underlying premise for this solution relies on the properties of Bipartite Graphs.\n\nIf it’s possible to divide players into two distinct groups such that no two players from the same team have constraints between them, the graph is bipartite.\nDuring the 2-color DFS check, if we come across an already visited and colored node that has the same color as the current node, we conclude that the graph isn’t bipartite - thus, such a partition doesn’t exist. If DFS completes without such contradictions, it means the graph is bipartite and we’ve found a valid partition.\n\nRunning Time §\n\nConstructing the graph takes O(m) time as every constraint creates an edge in the graph.\nRunning DFS across all vertices and edges of our graph takes O(n+m) time.\n⇒ Total running time of the algorithm: O(m)+O(n+m)=O(n+m).\n"},"CS330-HW07":{"title":"CS330 HW07","links":[],"tags":["Courses"],"content":"Problem 1 §\nAlgorithm §\n# each item is in format (time, start/end, event_index)\n# order priosity: smaller time -&gt; start -&gt;\nQ = sort(L, R)\nEvents = []\n\nwhile Q has elements:\n\n\tif Q.next is a start time\n\t\tt &lt;- Q.dequeue\n\n\t\t# if there is a tie in start times\n\t\twhile Q.next.start_time == t.start_time:\n\t\t\tt &lt;- max_end_time(t, Q.dequeue)\n\t\t\t\n\t\t# t is now the earliest starting time with maximum duration\n\t\n\telse Q.next is an end time\n\n\t\t# get latest end time\n\t\twhile Q.next is an end time\n\t\t\tt &lt;- Q.dequeue\n\t\t\t\n\t\t# t is now the latest end time\n\t\t\n\tadd t&#039;s event to Events\n\t\n\tend if\nend while\n\nComplexity §\n\nSorting: O(nlogn)\nLoop: O(n)\n⇒ Total: O(nlogn)\n\nCorrectness §\n\nBase case\n\nLet Y∗=(y1​,…) the optimal solution\nAssume Y∗’s y1​ is not the earliest start time……(1)\nAlgorithm chooses earliest start time x1​.\n\n[x1​.start,y1​.start] is in total cover, but not in Y∗’s cover.\nThis means Y∗ cannot be optimal. This is a contradiciton\nThus Assumption (1) must be wrong\n\n\n\n\nInductive Step\n\nLet Y∗=(x1​…xk−1​,yk​,…) is the optimal solution\nY∗’s yk​ is the optimal next choice after xk−1​\nCase 1: next item in priority queue is an end time\n\nAlgorithm chooses xk​ where…\n\nxk​.start&lt;xk−1​.end\nxk​.end&lt;yk+1​.start\namong such points, max(xk​.end)\n\n\nAssume yk​ is an event that doesn’t satisfy the maximum endtime requirement……(2)\n\nThen, [yk​.end,xk​.end] is in total cover, but not in Y∗’s cover. This is a contradiction.\nThus assumption (2) must be wrong\n\n\n\n\nCase 2: next item in priority queue is a start time\n\nAlgorithm chooses xk​ with the earliest starting time and longest duration\nThis is at least as optimal as yk​, because…\n\nxk​.start≤yk​.start\nxk​.end≤yk​.end\nThus the cover of xk​ is as large as yk​\n\n\nThus xk​ is as optimal as yk​\n\n\n\n\nQED\n\nProblem 2 §\n// Idea: Reverse the order of weights in the graph,\n// then run minimum spanning tree.\n\n// First, reverse the weight ordering of the graph\n\n// negate weights...\nfor e in E\n\te&#039; &lt;- e\n\te&#039;.weight &lt;- -e.weight\n\tadd e&#039; to E&#039;\n\nlet m be the edge with minimum weight in E&#039;\n\n// then monotonically increase so all weights are positive \n// (to use MST algorithm)\nfor e in E&#039;\n\te&#039;.weight += m.weight + 1\n\n// Then, run MST algorithm (Prim or Kruskal)\nV_mst, E_mst &lt;- MST(V, E&#039;)\n\n// the minimum spanning tree in (V, E&#039;) is\n// same as the maximum spanning tree in (V, E)\n\n// Return the edges that are not in the MST.\nreturn E \\ E_mst\nCorrectness §\n\nThe problem is equivalent to getting a tree whose sum of edge weights is maximal.\n\nThis is because the remaining graph must be acyclic (a tree) and…\n…because the sum of removed weights is minimal thus the sum of remaining weights must be maximal.\n\n\nWe can convert the original graph in question to a weight order reversed graph:\n\nSuppose E={e1​,e2​,…,em​} where ei​ has weight wi​\nThen, E′={e1′​,…em′​} where ei′​ has weight wm−i+1​\n\n\nThen, we can leverage the correctness of the MST algorithm (prim’s or kruskal’s) in order to get the minimum spanning tree of this new weight-order-reversed-graph.\nThe edges in this MST is the maximum spanning tree in the original graph\nTherefore, removing these edges we are left with the subset F which is minimal\n\nComplexity §\n\nnegating weights: O(m)\nmonotonically raising weights positive: O(m)\nMST algorith: O(mlogn)\nTotal: O(mlogn)\n"},"CS330-HW08":{"title":"CS330 HW08","links":[],"tags":["Courses"],"content":"Problem 1 §\nAs outlined in This Ed Discussion I assume I will have available the whole flow function at my disposal.\n// Find the min-cut edges\n// let f[e] the max-flow capacity by the flow function\n// let e.cap be the capacity of edge e\n\nRESIDUAL(V, E, f):\n    let r_E = [] // for residual edges\n    for edge e (u-&gt;v) in E:\n        forward_res = e.cap - f[e] // forward residual capacity\n        if forward_res &gt; 0:\n            add edge e (u-&gt;v) with e.cap = forward_res to r_E\n        if f[e] &gt; 0:\n            add edge e&#039; (v-&gt;u) with e&#039;.cap = f[e] to r_E // Only add reverse edge if flow &gt; 0\n    return r_E\n\n// Run DFS to find reachable vertices in residual graph\nreachable = DFS(s, r_E)\nunreachable = V - reachable\n\n// Find min-cut edges\nmin_cut_E = []\nfor a in reachable:\n    for b in unreachable:\n        if edge e (a-&gt;b) in E:\n            add e to min_cut_E\n\nINCREMENT(e):\n    if e not in min_cut_E:\n        f[e] = f[e] + 1\n    else:\n        r_E = RESIDUAL(V, E, f)\n        path = BFS(s, t, r_E) // augmenting path in res graph\n        if path exists:\n            augment_flow(path, 1) // flow augmentation from class\n            // Augment flow along path by 1\n    return f\n\nDECREMENT(e):\n    if f[e] &gt; 0 and e in min_cut_E:\n        f[e] = f[e] - 1\n    else:\n        r_E = RESIDUAL(V, E, f)\n        path = BFS(s, t, r_E) // augmenting path in res graph\n        if path exists:\n            augment_flow(path, -1) \n            // Decrease flow along path by 1\n    return f\n\nCorrectness §\n\nThis leverages the correctness of the max-flow and min-cut algorithm\n\n\nIncrement function\n\nIf the increment is in not in a min-cut the graph is still bottlenecked at the min-cut, and the maximum flow cannot increase. The flow function is the same\nIf the increement is in the min-cut, there may be more flow to be had. T\n\nhere is need only to construct and update using the residual network once because the increment is by 1, which means any path found by BFS on the residual graph is guaranteed to increase the max-flow.\nAlternatively, if the BFS doesn’t find any path in the residual network, this means that the graph has a different min-cut due to the capacity change. But the flow function wouldn’t change because there would be nothing to update (again, leveraging the correctness of the max-flow augmentation step.)\n\n\n\n\nDecrement function\n\nIf the decrement is in a min-cut the graph’s bottleneck is reduced. The max flow will be reduced by 1\nIf the decrement is not in a min-cut the graph’s bottleneck is not reduced, but there may be a change in flow. Use the flow augmentation step (symmetric to the increment in-min-cut situation) to update the flow function\n\n\n\nComplexity §\n\nResidual calculation: O(V+E) but assume V&lt;E thus O(E)\nUsing BFS shortest augmenting path: O(E)\nTotal: O(E)\n\nProblem 2 §\nN = Set of visitors, where n in N is the set of demographics\n// e.g. N[0] = {male, 40-50}\n\nM = Set of advertisers and their preferences and number of ads\n// e.g. M[0] = {male, female}, M[0].showCount = 3\n\n// Construct graph G = (V,E) such that\nfor n in N:\n\tadd n to V\nfor m in M:\n\tadd m to V\n\n// for every advertiser...\nfor m in M:\n\t// check if each visitor has any acceptable demographic\n\tfor n in N:\n\t\tfor dem in M:\n\t\t\tif n contains dem:\n\t\t\t\tadd (n-&gt;m,1) to E // directed edge n-&gt;m weight 1\n\n// Add start and end vertex for max-flow\n\nadd s, t to V\nfor n in N:\n\tadd (s-&gt;n, 1)\nfor m in M:\n\t// by allowing bigger capacity here, required # of visitors\n\t// accessed by the advertisers\n\tadd (m-&gt;t, m.showCount)\n\n// Calculate the max-flow with algorithm from class\nMaxFlow(G)\n\nfor every vertex m such that (m-&gt;t) in E:\n\tif edge (m-&gt;t) is at capacity:\n\t\tcontinue\n\telse // not at capacity; advertiser isn&#039;t satisfied\n\t\treturn False\nreturn true\n\nCorrectness §\n\nIdea is to transform the problem into the matching edges problem described in class, which is a variant of the max-flow problem\n\n\nVisitor is guarateed to be shown only one ad, because s→n∈N has capacity 1, meaning that for any edge n→m∈M only one of them can be used in max-flow (recall all edges n→m for any n∈N,m∈M has capacity 1)\nAdvertiser is guaranteeed to be shown their ad only to the right demographic group, due to the edge matching conditions in line 13-19.\nIf all edges m→t where m∈M is at capacity, it means that the ads have the shown as much as the advertiser requested (showCount). This is because all edges n→m for any n∈N,m∈M has capacity 1, and thus max-flow must use showCount number of n→m edges for max-flow.\n\nThus, if the max-flow algorithm ends up using the full capacity of these edges, the conditions of the problem are feasible\nElse, the advertisers requested showCount has not been satisfied and the conditions are not feasible.\n\n\n\nComplexity §\n\nlet ∣N∣=n,∣M∣=m. Note that both max(∣M[i]∣),max(∣N[i]∣)=O(k) where k is the number of demographic groups\nGraph Construction: line 8-11 O(n+m), line 14-19 O(n⋅m⋅max(∣M[i]∣)⋅max(∣N[i]∣))=O(nmk2)\nMax-flow using BFS shortest augmenting path: O((n+m)(nm+n+m))\n\nn+m+2 is the number of vertices\nnm is the number of edges between n∈N,m∈M\nn+m is the number of edges between s→n, m→t\n⇒ Total O(nm(n+m))\n\n\nChecking m→t is at max capacity: O(m)\nTotal: O(nm(n+m+k2))\n"},"CS330-HW10":{"title":"CS330 HW10","links":[],"tags":["Courses"],"content":"Problem 1 §\nAlgorithm is one that assigns every one element to the set with the smaller sum.\n(a) §\n\nWe first establish that the cost of the algorithm ALG=∑x∈S​x where S is the bigger one of A,B.\n\nlet X={x1​…xn​} where xi​ is the i-th processed element by ALG.\n\n\nALG=∑x∈S​x−xj​+xj​ where xj​ is the last item added to set S\n\n! Looking at the first term, we can establish ∑x∈S​x−xj​≤21​(∑i&lt;j​xi​)……(1)\n\ni.e. Sum in S before xj​≤Average of total before xj​\nWe know this because Sum in S before xj​ is the smaller sum of A,B, and thus the average is bigger than the minimum of the two.\n\n\nThen we also establish ∑i&lt;j​xi​≤∑i&lt;j​xi​+∑i&gt;j​xi​=∑x∈X​x−xj​\n\ni.e. Sum before xi​ ≤ sum before xj​ and sum after xj​ = total sum excluding xj​\n! This implies 21​∑i&lt;j​xi​≤21​(∑x∈X​x−xj​)…….(2)\n\n\n&amp; (1) and (2) together shows ∑x∈S​x−xj​≤21​(∑x∈X​x−xj​)=21​∑x−21​xj​……(3)\n\n\nWe thus show ALG≤21​∑x−21​xj​+xj​=21​∑x+21​xj​……(4)\n\nWe know from the lecture (makespan, lemma 1 and 2) that 21​∑x≤OPT and xj​≤OPT\n\nfirst, because optimal solution cannot be smaller than the total / 2\nsecond, because optimal solution cannot be smaller than any element\n\n\n\n\nThus we establish ALG≤OPT+21​OPT=23​OPT\n\n\n\n\nThus proven\n\n(b) §\n\nWe first investigate the trivial cases when X has respectively 1,2 and 3 elements\n\nCase n=1\n\nOPT=x1​, ALG=x1​ thus α=1\n\n\nCase n=2\n\nOPT=max[x1​,x2​], ALG=max[x1​,x2​] thus α=1\n\n\n\n\nWe then investigate the case when n≥3\n\nAs we also know that x1​&gt;x2​&gt;x3​&gt;⋯&gt;xn​ in the order of assignment by ALG, we can state both that:\n\nix1​+⋯+xi​​≥xi​ thus x1​+⋯+xi​≥ixi​……(1)\nxj​≥n−jxi+1​+⋯+xn​​ thus xj​≥xi+1​+⋯+xn​……(2)\n(1)−(2) yields ∑k=1…i​xk​−xj​≥\n\n\n\n\n\nProblem 2 §\nCode §\nimport csv\nfrom typing import List, Tuple\nimport time\nfrom math import radians, cos, sin, asin, sqrt\nimport random\n \n# Function to load data from CSV file\ndef load_data(riders_filepath) -&gt; List[Tuple[float, float]]:\n    passengers = []\n    with open(riders_filepath, &#039;r&#039;) as file:\n        csv_reader = csv.reader(file)\n        next(csv_reader, None)  # Skip the header\n        for row in csv_reader:\n            sourceLat = float(row[1])\n            sourceLon = float(row[2])\n            passengers.append((sourceLat, sourceLon))\n    return passengers\n \n# Assuming the Haversine function is replaced by a simple Euclidean distance for this simulation\ndef euclidean_distance(point1, point2):\n    return sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n \n# 2-approximation algorithm for k-center clustering using Euclidean distance\ndef k_center_clustering_euclidean(passengers, k, n):\n \n    # sample n elements from passengers\n    passengers = random.sample(passengers, n)\n \n    start_time = time.time()\n \n    # Choose an arbitrary point as the first center\n    hubs = [random.choice(passengers)]\n    for _ in range(1, k):\n        # Find the next hub as the point farthest from any existing hub\n        next_hub = max(passengers, key=lambda p: min(euclidean_distance(p, hub) for hub in hubs))\n        hubs.append(next_hub)\n \n    end_time = time.time()\n    \n    # Assign each point to the closest hub and calculate the cost\n    assignments = {hub: [] for hub in hubs}\n    for passenger in passengers:\n        closest_hub = min(hubs, key=lambda hub: euclidean_distance(passenger, hub))\n        assignments[closest_hub].append(passenger)\n \n    \n    # The cost is the maximum distance of any point to its assigned hub\n    cost = max(euclidean_distance(passenger, closest_hub)\n               for closest_hub, passengers in assignments.items() for passenger in passengers)\n    \n    return hubs, assignments, cost, end_time - start_time\n \n# Example passengers data (replacing the real CSV loading)\npassengers = load_data(&quot;./passengers.csv&quot;)\nprint(len(passengers))\n \n# Run the algorithm for different values of k and record the results\nprint(&quot;K&quot;)\nresults = {}\nfor k in range(50,200,10):\n    hubs, assignments, cost, runtime = k_center_clustering_euclidean(passengers, k, 100)\n    print(k, &quot;,&quot;, runtime)\n \nprint(&quot;N&quot;)\nfor n in range(1000, 5000, 100):\n    hubs, assignments, cost, runtime = k_center_clustering_euclidean(passengers, 10, n)\n    print(n, &quot;,&quot;, runtime)\n \nResults §\n\nProblem 3 §\nimport csv\nfrom typing import List, Tuple\nimport time\nfrom math import sqrt\nimport random\nimport numpy as np\n \n# Function to load data from CSV file\ndef load_data(riders_filepath) -&gt; List[Tuple[float, float]]:\n    passengers = []\n    with open(riders_filepath, &#039;r&#039;) as file:\n        csv_reader = csv.reader(file)\n        next(csv_reader, None)  # Skip the header\n        for row in csv_reader:\n            sourceLat = float(row[1])\n            sourceLon = float(row[2])\n            passengers.append((sourceLat, sourceLon))\n    return passengers\n \n# Euclidean distance function\ndef euclidean_distance(point1, point2):\n    return sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n \n# Function to initialize centroids randomly\ndef initialize_random(passengers, k):\n    return random.sample(passengers, k)\n \n# Function to initialize centroids using k-means++\ndef initialize_kmeans_plusplus(passengers, k):\n    centroids = [random.choice(passengers)]\n    # probabilistic initialization\n    for _ in range(1, k):\n        distances = [min(euclidean_distance(p, c) for c in centroids) for p in passengers]\n        probabilities = [d ** 2 for d in distances]\n        total = sum(probabilities)\n        probabilities = [p / total for p in probabilities]\n        next_centroid = random.choices(passengers, weights=probabilities, k=1)[0]\n        centroids.append(next_centroid)\n    return centroids\n \n# Assign passengers to the nearest centroid\ndef assign_passengers(passengers, centroids):\n    assignments = {}\n    for passenger in passengers:\n        closest_centroid = min(centroids, key=lambda centroid: euclidean_distance(passenger, centroid))\n        if closest_centroid in assignments:\n            assignments[closest_centroid].append(passenger)\n        else:\n            assignments[closest_centroid] = [passenger]\n    return assignments\n \n# Update centroids based on the mean of assigned points\ndef update_centroids(assignments):\n    centroids = []\n    for points in assignments.values():\n        centroids.append(tuple(np.mean(points, axis=0)))\n    return centroids\n \n# Calculate the quality of solution\ndef calculate_quality(assignments):\n    total_distance = 0\n    for centroid, points in assignments.items():\n        total_distance += sum(euclidean_distance(centroid, p) ** 2 for p in points)\n    return total_distance / len(assignments)\n \n# k-means algorithm\ndef k_means(passengers, k, initialize_func):\n    # Initialize centroids\n    centroids = initialize_func(passengers, k)\n    assignments = {}\n    for _ in range(20):  # fixed number of iterations for simplicity\n        # Assign passengers to the nearest centroid\n        assignments = assign_passengers(passengers, centroids)\n        # Update centroids\n        centroids = update_centroids(assignments)\n    return centroids, assignments\n \n# Run the k-means algorithm with different initializations\ndef run_k_means(passengers, k_values):\n    results = {k: {&#039;random&#039;: int, &#039;k-means++&#039;: int} for k in k_values}\n    for k in k_values:\n        for _ in range(3):  # Repeat the experiment 3 times\n            temp_results = {init: {&#039;runtime&#039;: 0, &#039;quality&#039;: 0} for init in (&#039;random&#039;, &#039;k-means++&#039;)}\n            for init, func in [(&#039;random&#039;, initialize_random), (&#039;k-means++&#039;, initialize_kmeans_plusplus)]:\n                start_time = time.time()\n                centroids, assignments = k_means(passengers, k, func)\n                runtime = time.time() - start_time\n                quality = calculate_quality(assignments)\n                temp_results[init][&quot;runtime&quot;] += runtime\n                temp_results[init][&quot;quality&quot;] += quality\n        # average the results\n        results[k][&#039;random&#039;] = (temp_results[&#039;random&#039;][&#039;runtime&#039;] / 3, temp_results[&#039;random&#039;][&#039;quality&#039;] / 3)\n        results[k][&#039;k-means++&#039;] = (temp_results[&#039;k-means++&#039;][&#039;runtime&#039;] / 3, temp_results[&#039;k-means++&#039;][&#039;quality&#039;] / 3)\n    return results\n \n# Example passengers data (replacing the real CSV loading)\n# Here we need to load the data from the file provided by the user\n# passengers = load_data(&quot;/path/to/passengers.csv&quot;)\n \npassengers = load_data(&quot;./passengers.csv&quot;)\nprint(len(passengers))\n \n# Define k values to test\nk_values = [2, 4, 6, 8]\n \n# Run the k-means algorithms and print the results\nresults = run_k_means(passengers, k_values)\nprint(results)\n \nResults §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of ClustersInitializationRuntimeQuality2random0.098172.265082k-means++0.097902.265084random0.156750.771634k-means++0.159210.721646random0.205300.395366k-means++0.215130.350478random0.265860.233318k-means++0.289570.20059"},"CS535-HW1":{"title":"CS535 HW1","links":[],"tags":["Courses"],"content":""},"CS535-HW2":{"title":"CS535 HW2","links":["No-Regret-Dynamics"],"tags":["Courses"],"content":"(DevonThink) Assignment 2\nProblem 1 §\nIn class algorithm uses regret bound of ≤2TlnN​. Now consider this modified RWM algorithm. The game ends at time τ but we have no knowledge of it.\n\nInitially pit​=N1​∀i∈X\nLet interval bound Ti​=2i. We start in the interval T0​&lt;t≤T1​\n\nIn this interval, we set η=min(Ti​lnN​​,21​)\nThen within this interval the regret ≤2Ti​lnN​\n\n\nOnce we reach the end of this interval, we move onto the next interval.\nWe reach the end of the algorithm when time is τ. We are in the interval Tk​&lt;t&lt;Tk+1​ where k is the largest integer that satisfies 2k≤τ.\nNow, the regret at this point can be obtained by summing all regret from previous and current intervals:\n\nRegret​≤i=1∑k​22ilnN​≤22k+1lnN​=2Tk+1​lnN​​​\nThus we have regret bound 2Tk+1​lnN​ at time interval τ≤Tk+1​. \nProblem 2 §\nProblem 3 §\nThis is formulated as the online prediction problem, with the seller having the option to choose a price k1​,k2​,…,kk−1​,kk​ with the loss function what represents the foregone profit (that could have been captured).\nlit​={vi​−kvi​​if sold, but at lower than willing to payif not sold​\n\nLoss is zero when price is set to perfectly match the current buyer’s willingness to pay.\nThen, there are n iterations of the game (since there are n buyers), with k different options for the seller. Thus the regret bound for a RWM Formulation of the game will be ≤2nlnk​.\n\nProblem 4 §\nProblem 5 §\nBNE §\nThe utility of player i is\nμi​={vi​−bi​0​if gets itemelse​\nThen, assume other person bids bj​=2vj​​ where vj​∼Unif(0,1). Then, to maximize their expected utility player i will:\nmaxbi​​E(μi​)=max(vi​−bi​)⋅ i’s bid is greater P(bi​&gt;2vj​​)​​\nWhere P(bi​&gt;2vj​​)=2bi​ since vj​ is a uniform distribution. Then solve the maximization problem:\nmaxbi​​(vi​−bi​)2bi​\nSolving this problem will yield bi​=2vi​​. This is shown symmetrically for the other agent.\nRevenue §\nThe revenue of the seller is\nRE(R)​={2v1​​2v2​​​given v1​&gt;v2​given v1​&lt;v2​​=E(2v1​​∣v1​&gt;v2​)+E(2v2​​∣v1​&lt;v2​)=2⋅∫01​2v1​​⋅=f(v1​)f(v1​&gt;v2​∣v1​)​p(v1​&gt;v2​)=2v1​​p(v1​∣v1​&gt;v2​)​​dx=​​"},"CS535-HW3":{"title":"CS535 HW3","links":[],"tags":["Courses"],"content":"Problem 4 §\nLet agents 1 (Alice) and 2 (Bob). The optimal case is when 1 gets a and 2 gets b if α&gt;β, and vice versa, thus maximum welfare for efficient allocation is\n by agent 2 2​​+ by agent 1 max(α,β)​​\n1 will demand both initially when pa​=pb​=0 but will drop out once\n2−(pa​+pb​)​&lt;max(α−pa​,β−pb​)​​\nCase α−pa​&gt;β−pb​. Then\npa​+pb​pb​pb​​&gt;2−α+pa​&gt;2−α&gt;1​as 1&lt;2−α&lt;2 since α∈(0,1)​​\nAnd 1 will not demand b anymore. This will lead to two cases:\n\nIf pa​&lt;pb​ auction continues since both chooses a. 1 will drop out eventually, as pa​ will be raised until pa​&gt;α while 2 still demands a as long. Total welfare is 2.\nIf pb​&lt;pa​ then 1 will drop out, 2 will get b with total welfare 2.\nSame can be shown for α−pa​&lt;β−pb​.\n\nProblem 5 §\n​​"},"CS535-HW4":{"title":"CS535 HW4","links":[],"tags":["Courses"],"content":"Problem 1 §\nProblem 2 §\nSubproblem 1 §\nSimply reporting ∞ utility on every item will result in the agent getting all items and thus maximum utility.\nSubproblem 2 §\nMechanism A will calculate MNW x∗=argmaxx​∑∀i​lnUi​(x) with the final allocation for agent i being:\nxiF​=xi∗​∏j=i​Uj​(x−i∗​)∏j=i​Uj​(x∗)​\nwhere, per the problem x−i∗​=argmaxx​∑j=i​lnUj​(x), i.e. running MNW without agent i. For agent i, their final utility is\nUiF​=Ui​(xi∗​∏j=i​Uj​(x−i∗​)∏j=i​Uj​(x∗)​)=Ui​(xi∗​)⋅∏j=i​Uj​(x−i∗​)∏j=i​Uj​(x∗)​\ndue to the utilities being additive utilities.\nOn the other hand, a VCG auction will allocate x∗=argmaxx​∑∀i​lnbi​(x) where bi​ is the reported utility that may not be truthful. We consider agent i, while fixing everybody else’s reported utilities b−i​​. The payment charged to agent i is:\npi​​=j=i∑​lnbj​(x−i∗​)−=∑j=i​lnbj​(x∗)​∀j∑​lnbj​(x∗)−lnbi​(x∗)U​​​=lnj=i∏​bj​(x−i∗​)−lnj=i∏​bj​(x∗)=−ln(∏j=i​bj​(x−i∗​)∏j=i​bj​(x∗)​)​per definition of VCG​\nThus for agent i their final utility is\nUiF​​=lnUi​(x∗)−pi​=lnUi​(x∗)+ln(∏j=i​bj​(x−i∗​)∏j=i​bj​(x∗)​)=ln(Ui​(x∗)∏j=i​bj​(x−i∗​)∏j=i​bj​(x∗)​)​per the question​\nSubproblem 2a\nNow, the correspondence comes from the fact that the subtraction of final utility due to VCG payment corresponds to the scaling down of utility in mechanism A.\nSpecifically:\nlnfi​=lnj=i∏​Uj​(x∗)−lnj=i∏​Uj​(x−i∗​)≤0​​\nSince product of optimal utility allocation with j divided by Uj​(x∗), cannot be better than product of optimal utility allocation without j in the first place (adaptation of logic from lecture). Thus we have fi​≤1.\nSubproblem 2b\nThe correspondence between utilities will be:\nlnUiF,Mech.A​=UiF,VCG​\nAnd since ln(x) is monotonic maximizing one will maximize the other. (Used in next proof)\nSubproblem 2c\nConsidering the VCG mechanism, agent i will attempt to maximize their own final utility, expressed as\nUiF,VCG​maxUiF,Mech.A​​=lnUi​(x∗)− i cannot influence lnj=i∏​bj​(x−i∗​)​​+lnj=i∏​bj​(x∗)⟺max UiF,VCG​⟺max​lnUi​(x∗)+lnj=i∏​bj​(x∗)​⟺maxln∀j∏​bj​(x∗)=max∀j∑​lnbj​(x∗)​establised in 2bsince b−i​​ is fixedNash welfare max.​​\nWhich is to say, in mechanism A it is also in i’s best interest to maximize the nash welfare objective. Thus mechanism A is DSIC. ■\nSubproblem 2d\nAs outlined in the problem statement, for any allocation x^:\n∀i∑​Ui​(x∗)Ui​(x^)​j=i∑n−1​(Uj​(x∗)Uj​(x−i∗​)​−1)​≤n≤1​remove specific agent iset x^:=x−i∗​​​\nNow, using the hint from problem statement, set di​:=Uj​(x∗)Uj​(x−i∗​)​−1 Then since ∑j=in−1​di​&lt;1, ∏j=in−1​(di​+1)≤(1+n−11​)n−1. Then:\n(n−1n​)n−1n→∞lim​(n−1n​)n−1e1​​≥j=i∏n−1​Uj​(x∗)Uj​(x−i∗​)​=∏j=i​Uj​(x−i∗​)∏j=i​Uj​(x∗)​=fi​1​≥n→∞lim​fi​1​≤n→∞lim​fi​​​\n■\nProblem 3 §\nSubproblem 1 §\nLet allocation A be EF1, i.e. ∀i∀j∃g s.t. vi​(Ai​)≥vi​(Aj​∖{g}). Now, if we sum over all agents j that is not i on both sides:\n∀i∃g​ s.t. (n−1)vi​(Ai​)nvi​(Ai​)​≥j=i∑​vi​(Aj​∖{gij​})≥j=i∑​vi​(Aj​∖{gij​})+vi​(Ai​)=j=i∑​(vi​(Aj​)−vi​({gij​}))+vi​(Ai​)=j=i∑​vi​(Aj​)+vi​(Ai​)−j=i∑​vi​(gij​)=∀j∑​vi​(Aj​)−j=i∑​vi​(gij​)​adding vi​(Ai​) to both sides​\nTo bound each of these terms, we use:\nj=i∑​vi​(gij​)​≤j=i∑​maxgij​∈Aj​​vi​(gij​)≤(n−1)maxg​vi​(g)​take item i values most from everybody elsejust take maximum value item​​\nThen returning to our original inequality:\nnvi​(Ai​) value of EF1 allocation to i vi​(Ai​)​​​≥=vi​(⋃∀j​Aj​)∀j∑​vi​(Aj​)​​−(n−1)maxg​vi​(g)≥ value of proportional allocation to i nvi​(⋃∀j​Aj​)​​​− some constant nn−1​maxg​vi​(g)​​​​\n■\nProblem 4 §\n\nlet Xij​ be the the utility of i getting item j. Per the question this is Unif[0,1].\nlet Y−ij​ be the largest utility an agent gives for item j, i.e. Y−ij​:=maxk=i​Xkj​.\nFrom this we know that the probability that agent i will get item j is P(Xij​&gt;Y−ij​)\nWe first calculate i’s expected utility, Ui​ from getting its own bundle:\n\nE(Ui​):=​E​∀j∑​Xij​ i bid max? 1Xij​&gt;Y−ij​​​​​=∀j∑​=n+1n​ , see explaination belowE(Xij​∣Xij​&gt;Y−ij​)​​=n1​P(Xij​&gt;Y−ij​)​​=m⋅n+1n​⋅n1​​​\nNote that\nE(Xij​∣Xij​&gt;Y−ij​)​=E(Xij​ given it is largest bid for j)=E(n-th smallest bid from n i.i.d uniform r.v. 0 to 1)=n+1n​​Per the given hint​​\nIn a similar fashion, we can calculate i’s expected utility from getting another agent k’s bundle, Uik​:\nE(Uik​)​:=E​∀j∑​Xij​⋅1Xkj​&gt;Y−kj​​​=∀j∑​E(Xij​∣ doesn’t matter Xkj​&gt;Y−kj​​​)⋅P(Xkj​&gt;Y−ij​)=m⋅21​⋅n1​​​\nNow, since the product Xij​⋅1Xi​&gt;Y−ij​​ themselves are independent random variables for all i, bounded from [0,1], and considering that Hoeffding’s inequality doesn’t require identical distribution), we can use it to state:\nP(Ui​≥E(Ui​)−t)P(Uik​≤E(Uik​)+t)​≤e−2t2/∑∀items​(1−0)=e−2t2/m≤e−2t2/m​Equivalently​​\nReplacing E(Ui​),E(Uik​) with the values we calculated above, as well as set t:=2(n+1)m​ we get:\nP(Ui​≥2(n+1)m​)≤e−m/2(n+1)2P(Uik​≤2(n+1)3m​)≤e−m/2(n+1)2​​\nNow, the probability that i will envy k can be stated P(Ui​&lt;Uik​). Using the union bound inequality:\nP(Ui​&lt;Uik​)​≤P(Ui​≤2(n+1)m​and Uik​≥2(n+1)3m​)≤P(Ui​≤2(n+1)m​)+P(Uik​≤2(n+1)3m​)≤2e−m/2(n+1)2​​\nThen, we calculate the probability that there will be no envy between any two agents i,k:\nP(i,k⋃​Ui​&lt;Uik​)​≤n total agents, i∑​​​n total agentsk∑​​​P(Ui​&lt;Uik​)=n22e−m/2(n+1)2⟶m→∞​0​​\n■"},"CS535-HW5":{"title":"CS535 HW5","links":[],"tags":["Courses"],"content":"Problem 1 §\nProof by induction\nProblem 2 §\nThe following case has all n women settle for their last proposer: m1​ prefers w1​ most, m2​ prefers w2​ most, etc. s.t. mi​ prefers wi​ the most. Then every woman settles for their last proposer.\nProblem 3 §\nSubproblem 1 §\nWhere ∣H∣&gt;∣S∣, a matching π is stable if :\n\nthere exists no unmatched pair (s∈S,h∈H) such that both would prefer to be with each other than their current match in π\nthere exists no hospital slot that would prefer an unmatched student s rather than its current match.\n\nSubproblem 2 §\nWe can extend this problem to create ∣h∣−∣s∣ new dummy hospital slots in set D, and add to every student s’s preference list:\n preexisting preference ⋯​​≻s​∈D, in random orderh1​≻s​h2​≻⋯≻h∣D∣​​​\nAnd also for every h∈D construct a randomly ordered preference set of all students as well, and then match students S with H′=H∪D.\nBy the same proof that we discussed during class (Gale-Shapely), this algorithm always has a stable matching including the dummy hospitals. When we remove the dummy hospitals, this will not create an unstable matching. This is because in every student’s preference ordering the dummy hospitals are lower than any real hospital in all student’s preferences, therefore no real-matched pair (s,t∈H∖D) will be unstable.\nProblem 4 §\nSubproblem 1 §\nWe run the Gale-Shapely algorithm on a partition with preferences by breaking ties arbitrarily. In this situation it is trivial to see that a variation on the lemma from class holds:\nLemma. If a woman w rejects man m, in all future scenarioes, w is engaged to man m′ who is not less preferred than m (i.e. w may be indifferent between m,m′).\nThen, proof by contradiction. Suppose the Gale-Shapely algorithm terminates in a match with a strongly unstable match (m1​,w2​),(m2​,w1​) but w1​≻m1​​w2​ and m1​≻w1​​m2​. Then, since m1​ proposes in order of his preferance he must have proposed and gotten rejected by w1​ before proposing and being accepted by w2​. By the lemma, since w1​ rejected m1​ she must have the preference m2​⪰w1​​m1​, but this is a contradiction.\n■\nSubproblem 2 §\nCounterexample. Suppose matching m1​,m2​ and w1​,w2​, whose preferences are such that:\n\nm1​: w1​≻w2​\nm2​: w1​∼w2​\nw1​: m2​≻m1​\nw2​: m2​≻m1​\nConsider matching (m1​,w1​),(m2​,w2​). This weakly unstable because (m2​,w1​) is a better match since m2​ is indifferent and w1​ prefers m2​.\nThe only other possible matching (m1​,w2​),(m2​,w1​) is also unstable because w2​ will prefer m2​ and m2​ is indifferent about this.\n\nProblem 4 §\nSubproblem 1 §\nConsider an algorithm which, on timestep i, moves distance (−2)i−1; in other words, moves right 1 step, then left 2 steps, then right 4 steps, then left 8 steps, etc. Suppose also that the car is located at point k∈R. We can then say:\n\nmovei​:=(−2)i−1, the movement at time i\ntraveli​:=2i−1, the total travel distance after time i\nAnd position (=displacement) after time i:\n\n\\frac{1}{3}(1-2^{i}) &amp; \\text{if i is even}  \\\\\n\\frac{1}{3}(1+2^{i}) &amp; \\text{if i is odd} \n\\end{cases}\nWe consider both cases:\n\nIf k&gt;0, the worst case is when on timestep i we have searched until point k−1 and then turn left on timestep i+1, then meet k−1 before timestep i+2. In this case:\n\nSince we move right on the first step, and k is set as one step to the right of posi​ i must be odd\nk=posi​+1, which means i=log2​(3(k−1)−1)\n\n\nIf k&lt;0, the worst case is symmetric:\n\ni must be even\nk=posi​−1, which means i=log2​(1−3(k+1))\nIn both cases the total travel distance is\n\n\n\n​=traveli+1​+∣posi+1​∣+∣k∣={−10+8k−316​−8k​if k&gt;0,i is evenif k&lt;0,i is odd​​​\nSince the optimal algorithm which knows k will always have a cost ∣k∣:\nk−10+8k​−k−16/8−8k​​&lt;8&lt;8​k&gt;0k&lt;0​​\nThis is a 9-competitive algorithm.\nComputation:\n\nSubproblem 2 §\nConsider an algorithm which with 21​ chance chooses the first step to be positive, and with 21​ to be negative. After the decision the algorithm deterministically behaves like in subproblem 1. We consider the case when k&gt;0 (the analysis when k&lt;0 is symmetric):\nWhen ∣posi​∣&lt;k&lt;∣posi+1​∣, we have two cases for right-first or left-first:\n\nMoved right on the i-th step: the total travel distance is: traveli+1​+∣posi+1​∣+∣k∣\nMoved left on the i-th step: the total travel distance is traveli​+∣posi​∣+∣k∣\nSince each case have 21​ chance, the expected travel distance is:\n\nE[total travel]​=21​(traveli+1​+∣posi+1​∣+∣k∣)+21​(traveli​+∣posi​∣+∣k∣)​​\nTo bound this competitiveness we recall ∣posi​∣&lt;k&lt;∣posi+1​∣. We try to find k which maximizes the competitive ratio. Since i≥1:\n\\frac{d}{dk}\\frac{\\mathbb{E}[\\text{total travel}]}{|k|}=\\begin{cases}\n\\frac{8-11\\cdot 2^i}{6 k^2} \\text{if $i$ is even} \\\\\n\\frac{4-11\\cdot 2^i}{6 k^2} \\text{if $i$ is odd}\n\\end{cases} &lt;0\n$$(computation below) Thus regardless of if $k$ is positive or negative, the worst case scenario is when $k$ is smaller, or $k=|\\text{pos}_{i}|$. Then:\n$$\\begin{align}\n\\frac{\\mathbb{E}[\\text{total travel}]}{|k|}&amp;=\\frac{\\mathbb{E}[\\text{total travel}]}{|\\text{pos}_{i}|}\n \\\\\n&amp;=\\frac{1}{2|\\text{pos}_{i}|}(\\text{travel}_{i+1}+|\\text{pos}_{i+1}|+|\\text{pos}_{i}|)+\\frac{1}{2|\\text{pos}_{i}|}(\\text{travel}_{i}+2|\\text{pos}_{i}|) \\\\\n&amp;= \\begin{cases}\n\\frac{13\\cdot 2^i-10}{2 \\left(2^i-1\\right)}&amp; \\text{if $i$ is even}\\\\\n\\frac{13\\cdot 2^i-2}{2 \\left(2^i+1\\right)} &amp; \\text{if $i$ is odd}\n\\end{cases}\\\\\n\\lim_{ i \\to \\infty } \\frac{\\mathbb{E}[\\text{total travel}]}{|k|}&amp;\\leq 6.5&lt;7\n\\end{align}\nTherefore, the algorithm is 7-competitive. ■\nComputation of derivative (divided between i even or odd):\n\nComputation of competitive ratio (divided between i even or odd)"},"Cache":{"title":"Cache","links":["LRU-algorithm"],"tags":["Computing/Computer-Architecture"],"content":"# of Frames​=Block SizeCache Size​=(# of sets)×(# of ways)=(2# of index bits)×(assoc.)​\n\nMost popular caching method: LRU algorithm\n"},"Capital-(Marxism)":{"title":"Capital (Marxism)","links":[],"tags":["Philosophy/Marxism"],"content":"Defining Capital §\nCapital is both\n\nstored up labor (as labor produces capital goods), and\npower to command labor:\n\n\nThe capitalist possesses this power not on account of his personal or human properties but in so far as he is an owner of capital. His power is the purchasing power of his capital, which nothing can withstand.\n\nConsider Capital profit and labor wage as fundamentally different products:\n\nprofits are in proportion to the amount of capital, but\nwage is determined by the amount of required subsistence:\n\n\nFirstly, the profits of capital are regulated altogether by the value of the stock employed, […] Furthermore, in many large factories the whole labour of this kind is committed to some principal clerk, whose wages never bear any regular proportion to the capital of which he oversees the management.\n\n\n[Laborer] becomes an appendage of the machine, and it is only the most simple, most monotonous, and most easily acquired knack, that is required of him. Hence, the cost of production of a workman is restricted, almost entirely, to the means of subsistence that he requires for maintenance,\n"},"Cardinality":{"title":"Cardinality","links":["Zermelo-Frankle-Set-Theory-with-Axiom-of-Choice-(ZFC)","Limits-of-Math-and-Computing"],"tags":["Math"],"content":"Referencing How Infinity Works (And How It Breaks Math) - YouTube\ndef. ℶ0​ is the cardinality of natural numbers\ndef. ℶ1​ is the cardinality of real numbers\n\nℶn+1​=2ℶn​ i.e. ℶn+1​ is the cardinality of all the subsets of ℶn​\n\nContinuum Hypothesis §\nhyp. There is no set whose cardinality is between that of integers and real numbers.\n\nExpressed using above notation: ℶ1​=?ℵ1​\nThis hypothesis is proved to be unprovable within ZFC, according to Limits of Math and Computing\n"},"Cartels-and-Collusion":{"title":"Cartels and Collusion","links":["Prisoner's-Dillemma"],"tags":["Economics/Game-Theory","Economics/Micro-Economics"],"content":"Cartels and the Natural Incentive to Defect §\ndef. Cartels are when firms collude to act as a single monopolistic firm.\nHowever, cartels cooperating is not a stable set of strategies, because it has a tendency to unravel, as it is a Prisoner’s Dillemma situation.\nWe can analyze this for two firms in the following two ways:\n\n\n\nUsing the profit for the hypothetical monopoly firm (graph a)\n\nTwo firms each produce 21​xM, and thus together produce xM at pM.\nIf I secretely produces one more unit of good:\n…both will lose red area together [=half each]\n…I will gain the profit of selling the additional units [blue area]\n\n→ Thus I am incentivized to defect and produce more.\n\n\nUsing residual demand for one of the firms\n\nAssume the other firm is diligently cooperating to produce their 21​xM.\nI will face a residual demand curve [=Dr] which is shifted left 0.5xM\nmy best option is to produce at MRr=MC which is 0.75xM, bigger than the promised 0.5xM\n\n→ Thus I am incentivized to defect and produce more\n\n\n\n\n                  \n                  Prisoner’s Dillemma situation:※\n                  \n                \n\n\nAside on Prisoner’s Dilemma §\n\n\n                  \n                  Quote \n                  \n                \ndef. Prisoner’s Dillemma (PD) is any game where the following payoff structure holds…\n…where T&gt;R&gt;P&gt;S:\n\n\nFinitely Iterated PD §\nIn a finitely iterated PD, the Subgame Perfect Nash Equilibrium is to defect every game.\n\nSolving from the last game:\n\nLast Game: Knowing there will be no more games [= same as one-shot game], your dominant strategy is defection.\n2nd to last: Knowing that next game both will defect, your dominant strategy is defection.\n…\nThe whole game unravels into a defection.\n\nInfinitely Iterated PD §\nGame design with:\n\nInfinitely many games\nprobability that one will meet [γ]\ndiscount parameter [PV=rFV​]\n\nIn this case there are multiple Nash Equilibirum Strategies\n\nALL C is a BR to ALL C → (ALL C, ALL C) is an NE\nTRIGGER is a BR to TRIGGER → (TRIGGER, TRIGGER) is an NE\nTrigger Strategy:= strategy where the opponent’s current action triggers my future behavior. e.g. “I will coop. until opp. defects, and defect always after that.”\nTFT is a BR to TFT → (TFT, TFT) is an NE\n\n\n\n                  \n                  These strategies are subgame perfect. Let’s prove that for (TRIGGER, TRIGGER): \n                  \n                \n\n\nIf we’ve always cooperated before, this subgame is same as original game\n→ (TRIGGER, TRIGGER) is an NE\nIf there was non-cooperation before, this subgame has NE:\n→ (ALL D, ALL D) is an NE\n\n→ All subgames have NE (TRIGGER, TRIGGER).\n∴ (TRIGGER, TRIGGER) is a SPNE"},"Cash-Sweep":{"title":"Cash Sweep","links":[],"tags":["Economics/Finance"],"content":"when money is automatically moved into a bank account based on a certain threshold. The money is then put into a higher interest-earning account such as a high interest saving account, money market mutual funds, or short-term certificates or can be used to payoff debt."},"Central-Limit-Theorem":{"title":"Central Limit Theorem","links":[],"tags":["Math/Probability"],"content":"\n\n                  \n                  Intuition: Selecting from a box of 1 to 10, \n                  \n                \n\nAs the sample size (n) increases, observe the following:\n\nμ is constant\nVar is increasing\nThe distribution apporoaches a bell curve\n\n\nthm. Law of Averages (=Law of Large Numbers) let X1​,…,Xn​ be random variables that are i.i.d. If E(X)=μ, and Xˉn​=nX1​+⋯+Xn​​ then for any small value of ϵ:\nn→∞lim​[P(∣X−μ∣&lt;ϵ)]=1\n\nthm. Central Limit Theorem (for averages) let X1​,…,Xn​ be random variables that are i.i.d. If E(X)=μ, SD(X)=σ and Xˉn​=n∑Xi​​ then for a big value of n:\nXˉn​∼Normal(μ,nσ2​)\n∵ for Xˉn​ where n is just a constant\n\nMean: E(Xˉn​)=E(nX1​+⋯+Xn​​)=n1​⋅n⋅E(Xi​)=μ\nVariance: Var(Xˉn​)=Var(nX1​+⋯+Xn​​)=n21​⋅n⋅Var(Xi​)=nσ2​\nStd. Dev: SD(Xˉn​)=Var(Xˉn​)​=n​σ​\n\nthm. Central Limit Theorem (for sums) let X1​,…,Xn​ be random variables that are i.i.d. If E(X)=μ, SD(X)=σ and Sn​=X1​+⋯+Xn​ then for a big n:\nSˉn​∼Normal(nμ,nσ2)\n∵ for Sn​ where n is just a constant\n\nMean: E(Sn​)=E(X1​+⋯+Xn​)=n⋅E(Xi​)=nμ\nVariance: Var(Sn​)=Var(X1​+⋯+Xn​)=n⋅Var(Xi​)=nσ2\nStd. Dev: SD(Sn​)=Var(Sn​)​=n​⋅σ\n\n\n\n                  \n                  Abstract \n                  \n                \nTo Summarize, for a big enough value of n, and Xˉn​ the average and Sn​ the sum:\n\nP(a&lt;Xˉn​&lt;b)≃∫ab​Normal[μ,n2σ​]=∫σ/n​a−μ​σ/n​a−μ​​Normal[0,1]=Φ(σ/n​b−μ​)−Φ(σ/n​a−μ​)P(a&lt;Sn​&lt;b)≃∫ab​Normal[nμ,nσ2]=∫n​σa−nμ​n​σb−nμ​​Normal[0,1]=Φ(n​σb−nμ​)−Φ(n​σa−nμ​)\nrmk. Binomal Approximtion using Normal Distribution. let the following:\n\nX∼Binom[n,p]\nIndicator functions s.t. X=I1​+⋯+In​ where Ii​ defines the i-th event is successful.\nNote that I1​,…,In​ are all i.i.d.\nthen…\nE(Ii​)=p\nVar(Ii​)=E(Ii2​)−[E(Ii​)]2=E(Ii​)−p2=p−p2\n(∵∀I=Indicator, E(I2)=E(I))\nFor a large enough n, X∼Normal[np,np(1−p)]\n(∵I1​,…,In​ are i.i.d., see the additional rule for expectated value and variance)\n\nLindenberg CLT §\nthm. Lindenberg Central Limit Theorem. Given random variables X1​…Xn​, with each E(Xk​)=μk​, Var(Xk​)=σk2​and the following conditions:\n\nThey are independent (No need to be identically distributed)\nlimn→∞​∑σk2​1​E((Xk​−μk​)21∣Xk​−μk​&gt;ϵ∑σk2​​) i.e. variance is not too big\n⇒ Then\n\n∑k=1n​σk2​​∑k=1n​(Xk​−μk​)​⟶n→∞d​N(0,1)"},"Centralized-Power":{"title":"Centralized Power","links":[],"tags":["Philosophy/Political-Philosophy"],"content":""},"Change-of-Variable-(Probability)":{"title":"Change of Variable (Probability)","links":["Normal-Distribution"],"tags":["Math/Probability"],"content":"Univariate Case §\nthm. Linear Change of Variable. let X s.t. P(X=x)=fX​(x). let Y=aX+b. Then…:\nfY​(y)=∣a∣1​fX​(ay−b​)\n⭐ thm. Change of Variable (bijective function). let X s.t. P(X=x)=fX​(x). let Y=g(X) where g(x) is an inversible function (=bijective). Then:\nfY​(y)=∣dy/dx∣fX​[g−1(y)]​\n\nThe bottom is a x-axis reflective version of fX​\nThe left is a 90 degrees counterclockwise rotation of fY​\nThe graph is of Y=g(X) where g(x)=x​\n\nthm. Change of Variable (injective function). let:\n\nX,Y,fX​,fY​,Y=g(X)\nDomain(X) is partitioned [x1​,x2​),[x2​,x3​),… s.t. g(x) is bijective in each interval.\n\nthen fY​(y) can be defined on interval [xi​,xi+1​)…:\nfY​(y)=x:y=g(x)∑​∣dy/dx∣fX​[g−1(y)]​ , when y∈g([xi​,xi+1​))\nMultivariate Case §\nthm. Change of Variables. (bijective function). let X with fX​(x) and Y=g(X), where g:Rn↦Rn is a bijective function with continuous derivatives. Then the joint probability density function of Y=(Y1​,Y2​,…,Yn​)⊤ is:\nfY​(y)=∣det jacobian ∂y∂x​​​∣fX​(g−1(y))​\nExample. See the Box-Mueller Transform"},"Chebyshev's-Inequality":{"title":"Chebyshev's Inequality","links":[],"tags":["Math/Statistics"],"content":"\nthm. Chebyshev’s Inequality. let X s.t. E[X]=μ, Var[X]=σ2 where σ is not infinite. Then:\n\n∀ϵ&gt;0,P(∣X−μ∣&gt;ϵ)≤ϵσ2​\n\nIntuitively, it means that “only few” data points are “far away” from the mean for any “reasonable” distribution:\n\n“only few”: “≤ϵσ2​”\n“far away”: “∣X−μ∣&gt;ϵ”\n“reasonable”: σ is not infinite.\n\n\n"},"Chi-Squared":{"title":"Chi-Squared","links":["Gamma-Distribution"],"tags":["Math/Common-Distributions"],"content":"χn2​∼i=1∑n​Zi2​\nalso related to the Gamma Distribution:\n\nΓ(n,2)∼χn2​\n"},"Chinese-101":{"title":"Chinese 101","links":["Pinyin-Rules"],"tags":["Courses"],"content":"\nPinyin Rules\n\n"},"Chinese-102-Favorite-Clothes":{"title":"Chinese 102 Favorite Clothes","links":[],"tags":["Courses"],"content":"我最喜欢的衣服是牛仔裤。这条有点大（是加大号的），但我觉得很合适。这是一条贵裤子，九十九块钱。我是今年夏天在纽约的时候买的。我喜欢这条牛仔裤，因为合适很多衬衫。"},"Chinese-102-Summar-vacation-introduction":{"title":"Chinese 102 Summar vacation introduction","links":[],"tags":[],"content":"你好，我叫孙鹏凯。我是四年级。\n我今年夏天 (xiàtiān) 去了纽约做金融 (jīnróng) 工作。跟同事(1/4)一起出去喝酒。我很喜欢纽约的 jazz bar.\n我见面我的高中的同学。我，他，和他的大学的同学常常一起出去玩。因为他们是 design school 的学生，我们常常去 art museum了.\n我的女朋友来看我，我们一起去了 Boiler Room 音乐节 (yīnyuè jié)\n\n"},"Chinese-102-Weather-in-Yokohama":{"title":"Chinese 102 Weather in Yokohama","links":[],"tags":[],"content":"Screenshot 2024-09-15 at 13.48.45.png\nScreenshot 2024-09-15 at 13.49.13.png\n我的家在横滨，在日本。横滨四季分明，天气常常暖和。在横滨，今天的天气不太好，因为现在下雨，温度是三十度。明天比今天更热。下个星期的天气比这个星期冷一点。以前，夏天凉快，现在夏天非常热了。"},"Chinese-102":{"title":"Chinese 102","links":["Chinese-102-Summar-vacation-introduction","Chinese-102-Favorite-Clothes","Chinese-102-Weather-in-Yokohama","Chinese-102-Thanksgiving-Writing","Chinese-102-Favorite-Chinese-Restaurant"],"tags":["Courses"],"content":"\nChinese 102 Summar vacation introduction\nChinese 102 Favorite Clothes\nChinese 102 Weather in Yokohama\nChinese 102 Thanksgiving Writing\nChinese 102 Favorite Chinese Restaurant\n"},"Chinese-203":{"title":"Chinese 203","links":[],"tags":[],"content":"\n孙子 sūnzi\n鲲鹏 kūnpéng (朋友的朋，百鸟的鸟)\n** kǎixuán\n\nPart 1:\n我姓张，工场张。\n开学以前，老生都会来帮新生搬家。\n他是在中国出生，长大的研究生。\nPart 2\n住在校内宿舍的好处是什么？A：住在校内宿舍的好处是安全的，而且是认识新朋友的。\n你的家离学校很远吗？你是怎么来学校的？A：我的家在日本，离学校太远。我坐飞机来学校。\nPart 3\n住在校外不见得比较安全。\n他可能把电脑落在那辆出租车上了！"},"Chompsky-Heirarchy":{"title":"Chompsky Heirarchy","links":["Formal-Grammar","Formal-Languages","Turing-Machine","Unrestricted-Grammar","Recursively-Enumerable-Languages","Linear-Bound-Automata","Context-Sensitive-Grammar","Recursive-Languages","Pushdown-Automata","Context-Free-Grammar","Context-Free-Language","Finite-Automata","Regular-Expressions","Regular-Grammar"],"tags":["Computing/Formal-Languages"],"content":"The Chompsky Hierarchy is the levels of capabilities of automata=grammar=languages.\nCapabilities of Automata §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType of AutomataMemoryCan…Can’t..Finite AutomataNonerecognize integersrecgz. arith. expr.Push-down AutomataStackrecgz. arith. expr.compute arith. expr.Turing MachineInfinitecompute arith. expr.determine halting probl.\nThe Hierarchy §\n\nTuring Machine == Unrestricted Grammar == Recursively Enumerable Languages\nLinear Bound Automata == Context Sensitive Grammar == Recursive Languages\nPushdown Automata == Context-Free Grammar == Context-Free Language\nFinite Automata == Regular Expressions == Regular Grammar\n\n"},"Chompsky-Normal-Form":{"title":"Chompsky Normal Form","links":[],"tags":["Computing/Formal-Languages"],"content":""},"Circular-Flow-of-Income":{"title":"Circular Flow of Income","links":[],"tags":["Economics/Macro-Economics"],"content":""},"Class-Traitors":{"title":"Class Traitors","links":[],"tags":["Philosophy/Marxism"],"content":""},"Cobb-Douglas-Utility-(Two-Goods)":{"title":"Cobb-Douglas Utility (Two Goods)","links":["Uncompensated-Demand-curve","Utility-Function","Marginal-Willingness-to-Pay","Expenditure-Function"],"tags":["Economics"],"content":"def. Two-goods Cobb-Douglas Utility function:\nu(x1​,x2​)=x1α​x21−α​\nlet the income:\nI(p1​,p2​)=p1​x1​+p2​x2​\nUtility Maximization §\nmaxx1​,x2​​u=x1α​x21−α​  such that  I=p1​x1​+p2​x2​\nOrdinary Demand: §\n⎩⎨⎧​x1​(p1​,p2​,I)=p1​αI​x2​(p1​,p2​,I)=p2​(1−α)I​​\nIndirect Utility: §\nv(p1​,p2​,I)​:=u(x1​(p1​,p2​,I),x2​(p1​,p2​,I))=p1α​ p21−α​(1−α)1−α αα​⋅I​​\nExpenditure Minimization §\nminx1​,x2​​I=p1​x2​+p2​x2​  such that  u=x1α​x21−α​\nCompensated Demand: §\n⎩⎨⎧​h1​(p1​,p2​,uˉ)=(p2​p1​​α1−α​)αuˉh2​(p1​,p2​,uˉ)=(p1​p2​​1−αα​)1−αuˉ​\nExpenditure Function: §\nE(p1​,p2​,uˉ)​:=p1​h1​+p2​h2​=(1−α)1−α ααp1α​ p21−α​​⋅uˉ​​"},"Cobb-Douglas-Utility":{"title":"Cobb-Douglas Utility","links":["Uncompensated-Demand-curve","Utility-Function","Marginal-Willingness-to-Pay","Expenditure-Function"],"tags":["Economics"],"content":"def. Two-goods Cobb-Douglas Utility function:\nu(x1​,x2​)=x1α​x21−α​\nlet the income:\nI(p1​,p2​)=p1​x1​+p2​x2​\nUtility Maximization §\nmaxx1​,x2​​u=x1α​x21−α​  such that  I=p1​x1​+p2​x2​\nOrdinary Demand: §\n⎩⎨⎧​x1​(p1​,p2​,I)=p1​αI​x2​(p1​,p2​,I)=p2​(1−α)I​​\nIndirect Utility: §\nv(p1​,p2​,I)​:=u(x1​(p1​,p2​,I),x2​(p1​,p2​,I))=p1α​ p21−α​(1−α)1−α αα​⋅I​​\nExpenditure Minimization §\nminx1​,x2​​I=p1​x2​+p2​x2​  such that  u=x1α​x21−α​\nCompensated Demand: §\n⎩⎨⎧​h1​(p1​,p2​,uˉ)=(p2​p1​​α1−α​)αuˉh2​(p1​,p2​,uˉ)=(p1​p2​​1−αα​)1−αuˉ​\nExpenditure Function: §\nE(p1​,p2​,uˉ)​:=p1​h1​+p2​h2​=(1−α)1−α ααp1α​ p21−α​​⋅uˉ​​"},"Collective-Effervescence":{"title":"Collective Effervescence","links":[],"tags":["Sociology"],"content":""},"Collective-Memory":{"title":"Collective Memory","links":["Social-Organization","Social-Institution","Meetings-as-a-Realization-of-Social-Organization"],"tags":["Sociology"],"content":"Collective Memory is a shared understanding of the past events of a Social Organization or Social Institution.\n\nCollective memory serves to “get people on the same page” about the past. (For getting people on the same page about the present, we use Meetings (Sociology).)\n"},"Color-as-an-Extra-Dimension":{"title":"Color as an Extra Dimension","links":[],"tags":["Computing/Human-Interface"],"content":""},"Combination-(Probability)":{"title":"Combination (Probability)","links":[],"tags":["Math/Probability"],"content":"Notation §\ndef. Number of Choosing k from n items:\n(xn​)=⎩⎨⎧​(n−k)!⋅k!n!​0​n≥xelse​\nNOTATION. N⋅(N−1)⋅(N−2)⋯(N−k)=Nk−1​"},"Combinatorial-Auction":{"title":"Combinatorial Auction","links":["VCG-Auction","Ascending-Price-Auction"],"tags":["Economics/Game-Theory"],"content":"Motivation. When a government auctions of wireless spectrums to AT&amp;T, Sprint, etc., they are not simply allocating one item per bidder. Instead the wireless carriers may want two spectrums but not one individually, or they might want all the spectrums to monopolize. We model these cases as a combinatorial auction.\ndef. Combinatorial Auction.\n\n1…n bidders\nM={1…m} items\nΩ={(S1​,…,Sn​)∣is partition of M} ← “outcomes”\nEach bidder i has valuation vi​ for every Si​∈2M; i.e. every possible allocation they can get\n\nThe following type of auction mechanisms are applicable in a combinatorial auction situation.\n\nVCG Auction\nAscending Price Auction\n"},"Combined-Income-&-Substitution-Effects":{"title":"Combined Income & Substitution Effects","links":["Income-Effect-(IE)","Substitution-Effect-(SE)"],"tags":["Economics/Micro-Economics"],"content":"Income Effect (IE)\nSubstitution Effect (SE)\nCombined Income/Substitution Effects §\nIn cases where price of one good changes, income and substitution effects happen together. Imagine when gasoline prices rise:\n\n\nGraph (a) shows the change in the budget line due to the increase in price of gasoline.\nGraph (b) shows the compensated budget line to maintain the level of utility.\nGraph (c) shows the ultimate, uncompensated bundle C.\n\nThe process for both effect is as follows\n\nMove along the same utility curve from bundle A → B; substitution effect\nMove to the new budget line from bundle B → C; income effect\n→ this determines if good is normal (homo-thetic or not), quasi-linear, inferior; see below)\n\nIncome and Substitution Effects on Types of Goods §\nThe above case was when gasoline was a normal good. When gasoline is inferior the move due to income effect (B → C) is to the right. The degree to which this occurs coincidentally determines if the good is regular inferior, or (the strange case of) a Giffen good.\n\nObserve that for both graphs B → C is a rightward move, because lowering income (blue → red) increased consumption, meaning this is an inferior good. If C has more of gasoline than A, it is the strange Giffen good.\nBringing it Together §\nThe final is combining all of these movements together. In the following graph, the consumer consumes only shirts and pants. Then they’re given a coupon that discounts the price of shirts (blue → red). Using the compensated budget we determine the substitution effect (b, A → B). Then, depending on whether shirts/pants are normal, inferior (or quasilinear) we can determine where the final income effect (B → C) will lie.\n\nMathematically… §\nAssume:\n\nUtility curve uA​=u(x1​,x2​)\nBudget line I=p1​x1​+p2​x2​\nYour bundle A(x1A​,x2A​),\n\nThen occurs a change in price p1​→p1′​, causing a change in the budget line from I→I′. Then, consider how much you should compensate your income, to stay in the same indifference curve with utility uA​. That is, what is the minimum income that I can spend to reach a certain indifference curve?\nmin p1′​x1​+p2​x2​ s.t.uA​=u(x1​,x2​)\nThus you find the bundle B(x1B​,x2B​), with the size of the substitution effect x1A​→x1B​\nSince this compensated budget is a hypothetical scenario, we will need to move back to the original budget.\nmax u(x1​,x2​) s.t.I′=p1′​x1​+p2​x2​\nto find bundle C(x1C​,x2C​)."},"Commodities":{"title":"Commodities","links":["Futures"],"tags":["Economics/Finance"],"content":"\nOften traded at the Chicago Mercantile Exchange (CME)\nTraded in Futures\n"},"Common-Graph-Problems":{"title":"Common Graph Problems","links":["Traveling-Salesperson-Problem","Minimal-Spanning-Tree-Problem","Common-Graph-Problems"],"tags":["Computing/Algorithms"],"content":"See also:\n\nTraveling Salesperson Problem\nMinimal Spanning Tree Problem\n\nQ. Clique Problem. Given a graph G=(V,E) what is the maximum set of vertices C such that all vertices v in C are fully connected, i.e. for every u,v∈C, there exists edge (u,v)∈E\n\n\nNP-complete problem\n\nQ. Independent Set Problem. (=Stable set =anti-clique) Given a graph G=(V,E) what is the maximum set of vertices C such that no edges connect any two vertices in this set? \n\nComplement with vertex cover.\n\nC is an maximal independent set ⇔ V−C is a minimal vertex cover\nReduction both ways is trivial\nthm. 3-SAT ≤p​ Independent Set\n\n\nConstruction: each clause to a fully connected tri-vertex component, and connect the variable and its negations between tri-vertex components. \n∃x1​…xn​ that satisfies Q where Q has k clauses ⇔ ∃ Independent Set of size k.\n\n\n\nQ. Vertex Cover. Given G=(V,E) find the minimal subset C⊆V such that it covers all edges in the graph.\n\n\ne.g. graph that has a vertex cover comprising 2 vertices (bottom), but none with fewer.\nNP-complete problem\nC is a minimal vertex cover ⇔ V−C is a maximal Independent Set\n\nRed is vertex cover, and white is independent set: \n\n\n\nalg. Approximate Vertex Cover.\n\nChoose any edge e that connects vertices u,v\nremove edge e from graph, as well as any edges that connected to u and v\n\nu,v are added to the vertex cover set\n\n\nRepeat until no edges remain\n\n\nIs a 2-approximation\n\nalg. Approximate Greedy Vertex Cover.\n\nChoose vertex with maximum degree\nAdd this as part of vertex cover. Remove edges connected to this vertex\nRepeat until no edges remain\n\n\nIs not optimal.\n\nQ. Triangle Cover. Given G=(V,E) find the minial subset of vertices that it covers at least one vertex per triangle, for every triangle in graph.\n\nTriangle Cover is NP-complete (reduce from vertex cover)\n\nQ. Dominating Set. Given G=(V,E) find the minial subset S of vertices such that, every vertex in G is either in S or is a neighbor of S\nQ. Critical Vertices. Given connected graph G=(V,E) find all vertices that when removed will disconnect the graph."},"Communism":{"title":"Communism","links":["External/Private-Property"],"tags":["Philosophy/Marxism"],"content":"“Crude” Communism §\n\nThe first positive abolition of private property - crude com­ munism - is therefore only a manifestation of the viieness of private property trying to establish itself as the positive com­ munity.\n\n\nPhysical, immediate possession is the only purpose of life and existence as far as this communism is concerned; the category of worker is not abolished but extended to all men […] The crude communist is merely the culmination of this envy and desire to level down on the basis of a preconceived minimum. It has a definite, limited measure.\n\n\n(2) Communism (a) still of a political nature, democratic or despotic; (b) with the abolition of the state, but still essentially incomplete and influenced by private property, i.e. by the estrange­ meant of man.\n\nActual Communism §\n\nCommunism is the positive supersession of Private Property as human self-estrangement, and hence the true appropriation of the human essence through and for man\n\n\nit is the.genuine resolution of the conflict between man and nature, and between man and man, the true resolution of the conflict between existence and being, between objectification and self-affirmation, between freedom and necessity, between individual and species. It is the solution of the riddle of history and knows itself to be the solution.\n\nand thus\n\nIf we characterize communism itself - which because of its character as negation of the negation, as appropriation of the human essence which is mediated with itself through the negation of private property\n"},"Comparable-Company-(Comps)-Analysis":{"title":"Comparable Company (Comps) Analysis","links":["Income-Statement","Measuring-Security-Performance","Price-to-Earnings-Ratio","Balance-Sheet","Free-Cashflow"],"tags":["Economics/Finance"],"content":"Comps Analysis:= comparing companies with multiples.\n\n\n                  \n                  More growth potential = Better.\n                  \n                \n\nBut make sure that the companies being compared are similar in size/industry/geographic regions.\nMultiples Analysis §\n:= using the ratios of accounting line items (like EBITDA, income, etc.) to evaluate the potential growth of a firm.\n\nRatios are calculated from estimations of future balance sheet line items.\nWe use rate of return to compare investments, not price. r=P1​P2​−P1​​. You can’t compare price.\nThe following are commonly used multiples:\nMeasuring Security Performance\nPrice to Earnings Ratio\n\nAccounting Sheets §\n\nBalance Sheet\nIncome Statement\nFree Cashflow\n"},"Compensating-and-Equivalent-Variation":{"title":"Compensating and Equivalent Variation","links":["Utility-Function"],"tags":["Economics/Micro-Economics"],"content":"Q. Coupon Exchange Problem. Josh and Andrew have identical utility functions u(x1​,x2​) where x1​ is milkshakes and x2​ is toys.\n\nTheir incomes are the same\nJosh has a 50% off coupon for x1​.\n⇒ Can Josh and Andrew work out a trade where Andrew buys the coupon?\nFigure out\nLeast Josh would take for the coupon using the expenditure function\nMost Andrew would pay for the coupon (also using the expenditure function)\n⇒ Trade occurs when (least Josh would sell for) &lt; (most Andrew would pay for)\n\n\n\n                  \n                  Idea \n                  \n                \nCV and EV - YouTube\nOh no! the price of good x1​ increased!\n\n🥺 She’s now at a lower Indifference Curve. How much should we give as compensation to get her back on the original indifference curve? ⇒ Compensating Variation\n😈 She’s at a lower inx_{1}$ increase, we just take money away from her directly? ⇒ Equivalent Variation\n\n\ndef. Compensating Variation (CV). When a price of a good changes, the Compensating variation is the change in expenditure (±) required to maintain the utility.\ndef. Equivalent Variation (EV). An individual may trade a change in income (±) for a change in prices.\n\n\n\nCompensating Variation (CV) is how much money an individual would have to get (or have taken away) to be indifferent to the change in prices"},"Computational-Tractability":{"title":"Computational Tractability","links":["CS-330-Advanced-Algorithms","Chompsky-Heirarchy","Turing-Machine","Independent-Set-Problem","Common-Graph-Problems","Subset-Sum","Traveling-Salesperson-Problem","Hamiltonian-Path/Cycle"],"tags":["Computing/Algorithms","Computing/Formal-Languages"],"content":"Definitions §\nthm. Satisfiability Reducibility. All problems can be reduced to boolean satisfiability problems.\n\nOptimization problems: choose some k, then ask “is there a solution ≤k?, ≥k?” =&gt; Then do a binary search on [0,k] or [k,upper bound]\n\ndef. Computationally tractable problems. There exists a polynomial time algorithm O(nk) for the solution. The set of all computationally tractable problems is denoted P.\n\nMost problems in CS 330 Advanced Algorithms\n\nthm. Polynomial Reduction. Problem π1​:X1​↦{0,1} that is an element of P can be reduced to problem π2​:X2​↦{0,1} problem which is also in P, if there exists a function f:X1​↦X2​ such that:\n\nπ1​(x1​)=1⟺π2​(f(x1​))=1\nf(x) takes polynomial time to complete\n\nf(x) can be a many-to-one function\n\n\n\n\nThis is denoted π1​≤p​π2​\nVisually: \nImplications of when π1​≤p​π2​\n\nπ2​ is at least as hard as π1​.\nP-time:\n\nIf π2​ is a P-time problem and\n! If the input size to π2​ which is ∣f(x)∣ is bounded by ∣x∣c\nThen π1​ is also a P-time problem\n\n\nNP-hardness:\n\nIf π1​ is NP hard → π2​ is also NP-hard\n\n\n\n\n&amp; Techniques for reduction of π1​≤p​π2​\n\nProcess:\n\n\nIf π1​,π2​ is a min/maximization problem, convert it to a decision problem\nIf construction is needed, construct from π1​ to π2​: domain G→G′\n\n! The construction may not suppose you know the answer π1​.\nIf it’s a graph, it should apply to any graph\nIf it’s a set, it should apply to any set\n\n\nIf there is something like “exists solution of size k” in π1​, this k will probability be present in π2​ as well\n\n\n\n\nProve that if ∃ answer to π1​ in domain G, that implies ∃ answer to π2​ in domain G′.\nProve that if ∃ answer to π2​ in domain G′, that implies ∃ answer to π1​ in domain G.\nProven!\n\n\n\nComplexity Classes §\nComplexity classes are an extension of the Chompsky Heirarchy.\n\n\n\nP (Polynomial Time)\nNP (Nondeterministic Polynomial Time) The certificate exists that can be verified in polynomial time. A non-deterministic Turing Machine can solve it in polynomial time because it has many paths.\nco-NP\n\nproblem π∈NP⟹πc∈co-NP:luc_check_circle:\nproblem π∈NP⟺π∈Co-NP is an open problem\nSee Whats the difference between NP and co-NP - Stack Overflow\n\n\nNP-complete: Problems where any NP problem can be reduced into in polynomial time.\n\nIf there is any NP-complete problem that can be shown to be solved in polynomial time, then P=NP.\nthm. Cook-Levin Theorem. CNF-SAT is NP-complete.\n&amp; To prove a decision problem π is NP-complete\n\nShow π∈NP i.e. certificate can be checked in poly-time\nShow π0​≤p​π where π0​ is known NPC problem by…\n\nfrom solution I0​ of π0​, construct solution I of π\nshow that I0​ is solution for π0​ if and only if I is solution for π (both ways needed to show that non-solutions are non-solutions)\n\n\n\n\n“Completeness” can apply to any complexity class. It means that it is the “hardest” problem in that class.\n\n\nNP-hard: Problems that are at least as hard as all NP (and NP-complete) problems\n\nEquivalently, Problems that are as hard or harder than all NP problems\n\n\n\nList of Problems and Reductions §\nNP-Complete Problems §\n\nCNF-SAT (Cooke-Levin Theorem)\n3-SAT\nIndependent Set Problem\nCommon Graph Problems Problem\nSubset Sum Problem\nCommon Graph Problems\nTraveling Salesperson Problem\n\nCycle\n\n\n\nKarp’s 21 NP-complete problems - Wikiwand\nCategory:NP-complete problems - Wikipedia\n(DevonThink) NP-complete Reductions"},"Compute-Architecture":{"title":"Compute Architecture","links":["Abstraction","Kubernetes","Terraform-&-OpenTofu"],"tags":["Computing/System-Design"],"content":"Motivation. Compute needs need to be scalable beyond single/few nodes. One workload needs to scale up/down, and also need to be processed in parallel.\nDepending on the level of Abstraction offered by the technology product, we divide into:\n\nServerless: AWS Lambda\nSoftware as a service (SaSS)\nPlatform as a service (PaSS): AWS managed containers\nInfrastructure as a service (IaaS): AWS EC2, Virtual networks\n\n\nOrchestration §\nMotivation. Preparing the infrastructure, dividing the workload, etc. takes a lot of work. Orchestrators automate this process:\n\nKubernetes: orchestrator for docker containers\nTerraform &amp; OpenTofu: IaaC, prepares environments as declared\n"},"Computer-Architecture":{"title":"Computer Architecture","links":["Instructions-(Computer-Science)","Instruction-Set"],"tags":["Computing/Computer-Architecture"],"content":"Term Definitions §\n\nInstruction: a single operation a processor can perform\nInstruction Set\nArchitecture: An abstract specification of a computer’s hardware—an interface—in order to be able to write software for it. (a contract between HW and SW to interact)\nMicro-architecture: the actual implementation of the architecture design\nComputer: machine that follows simple instructions deterministically\n\n\n\n                  \n                  technology of the time, programming language design, operating system design, the applications to write, and the history of the architecture design.\n                  \n                \n\n\nOSes and languages may also be influenced by the design of the architecture\n\nThe Five Component of a Computer are: Processor(CPU) / Control / Datapath / Memory / IO.\nMoore’s Law determines the trend of\n\nC Programming Language §\nUnix invented by: Ken Thompson\nC language by: Dennis Ritche\n(Think of it as: Newton, to create a new branch of science—physics—, invented calculus to make it possible.)\n\n\n                  \n                  The contraints of technology of the time shaped the design of C. \n                  \n                \n\nLimitations of: compiler size / code size / performance / portability\n→ became a portable assembly language\nDidn’t consider: security / robustness / maintainability / legacy code"},"Conditional-Distribution":{"title":"Conditional Distribution","links":["Joint-Distributions"],"tags":["Math/Probability"],"content":"def. Conditional Distribution. let X,Y be jointly distributed. Then the conditional probability distribution of Y given a specific value of X is the conditional probability distribution.\n\nX,Y are discrete:\n\npmfY∣X=x​(y):=P(Y=y∣X=x)=P(X=x)P(Y=y∧X=x)​\n\n\nX,Y are continuous:\n\n\nfY∣X=x​=fX​(x)fX,Y​(x,y)​\n\nif X⊥Y:\n\nfY∣X=x​(y)=fY​(y)\n\n\n                  \n                  Visually: \n                  \n                \n\n…the red line height divided by the orange line is the p.d.f conditional distribution fX∣Y=y​(x).\n…the orange line is the value of the marginal distribution fY​ at Y=y.\n\nthm. Continuous Multiplication Rule. let X,Y be jointly distributed. Then:\nfX,Y​(x,y)=fX∣Y=y​(x)⋅fY​(y)\nthm. Rule of Average Conditional Probability. let X,Y, Then:\nfX​=∫−∞∞​fX∣Y=y​⋅fY​dyP(X=x)=∫P(X=x∣Y=y)⋅fY​(y)dy"},"Conditional-Probability":{"title":"Conditional Probability","links":["Conditional-Distribution","Stochastic-Calculus","Expected-Value"],"tags":["Math/Probability"],"content":"See also: Conditional Distribution\ndef. Conditional Probability. The probability of event A given that event B has occurred is:\nP(A∣B):=P(B)P(A∩B)​\n\nIf Ω’s elements are all equally likely: P(A∣B)=#B#(A∩B)​\nMultiplication Rule: P(A∩B)=P(A∣B)⋅P(B)=P(B∣A)⋅P(A)\nCompliment Rule: P(A∣B)=1−P(AC∣B)\n\nthm. Chained Conditional Probability. For three events A,B,C:\nP(ABC)=P(A)⋅P(A)P(AB)​⋅P(AB)P(ABC)​=P(A)⋅P(B∣A)⋅P(C∣AB)\ndef. Bayes’ Theorem. For events A, B:\nP(B∣A)=P(A)P(A∣B)⋅P(B)​⇔P(B∣A)P(A)=P(A∣B)P(B)\nVisualization.\n\n…for Stochastic Calculus §\nlet (Ω,F,P) be a probability space. We define:\ndef. Conditional Probability against Events of A,B∈Ω as P(A∣B):=E[1A​∣B], the conditional expected value, which is defined as an integral/sum ∫B​1A​dP\ndef. Conditional Probability against"},"Confidence-Intervals-and-Hypothesis-Testing-in-OLS-Linear-Regression":{"title":"Confidence Intervals and Hypothesis Testing in OLS Linear Regression","links":["Confidence-Intervals","Hypothesis-Testing","Central-Limit-Theorem","Student's-t-test","Student's-T-Distribution"],"tags":["Math/Statistics"],"content":"See also: Confidence Intervals and Hypothesis Testing\nMotivation. Assume we have our estimators for our sample size N using OLS, β0​,β1​^​^​. Now, assuming we have the true population data (impossible in real life) and take 100 samples of size N from the whole population, we get 100 different tuple of estimators (β0​^​,β1​^​). If we plot these on a graph, we get an approximate bell curve. This is due to the Central Limit Theorem. Knowing this fact, we can deduce if there is a correlation between X and Y.\n\nRemark. N≥30 is the minimum required for CLT. N≥100 is a conservative requirement for CLT to apply.\nRemark. We will only look at β1​^​ since it is the more important parameter.\nHypothesis Testing §\ndef. The Null hypothesis in regression is H0​:β1​=0, i.e. there is no correlation.\ndef. Regression T-test. See Student’s t-test. A T-test is a test for rejecting the null hypothesis. let the T-statistic T=Var(β1​^​)​β1​^​−β1Null​​. Then\n{H0​H1​​if ∣T∣&gt;Kotherwise​\n\nThe cutoff value K is determined by how powerful (=α) you want the test to be. This is determined by the Student’s T-Distribution.\n\nThis is because T=d​tN−1​\n\n\nThis is Student’s t-test but with only one random variable.\nNormally, we set the cutoff K=2, i.e. two standard deviations away. This is around an α=0.05 test.\n\nTable of common T-Statistic values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue of TSignificanceSidedness∥T∥&gt;1.95P(β1​=0)&lt;0.05Two-side∥T∥&gt;2.58P(β1​=0)&lt;0.01Two-side\nA Large-sample critical value is simply using the idea that as N→∞ the T→dN→∞​N i.e. the T statistic becomes a normal distribution. Thus the cutoff values are using the standard normal table.\nIntuition. \nConfidence Interval §\nIntuition.\nBias:\nstandard error of regression: mean squared of residuals; also the estimator for the error\nStandard error of…\n\nResiduals → standard error of regression\nbe\nVaraince of β1​^​ is also the variance of CLT limit with the same sample size\n\nRemark. Substantive significance does not imply statistical significance, nor does vice versa. The two are irrelevant."},"Confidence-Intervals":{"title":"Confidence Intervals","links":["Fisher-Information"],"tags":["Math/Statistics"],"content":"def. Confidence Interval. For X distributed with an unknown parameter θ, a γ-level confidence interval is an interval in which the probability of θ0​ being in the interval is γ:\n​P(L&lt;θ0​&lt;U)=γ⟹2L+U​±2U−L​⟹Confidence Interval=[L,U]​​\nwhere L,U are both random variables (=a statistic) from X.\ndef. Observed Information. For X1​,…,Xn​∼iidf(x1​,…,xn​;θ), observed information is:\nJ(θ)=−l′′(Xi​;θ)\nAnd for X1​,…,Xn​∼iidf(x;θ)\nJn​(θ)=i=1∑n​−l′′(Xi​;θ)\nRemark. This is Fisher Information, but gathered on data (Xi​)\nthm. (approximating a confidence interval using Fisher Information) In general, if θ^MLE​ can be found and the log-likelihood is twice-differentiable, an approximate CI of level γ=1−α can be approximated by:\nθ^MLE​±Jn​(θ^)​z1−2α​​​\nthm Delta Method. Let unknown parameter θ of r.v. X1​,…,Xn​’s distribution. If Fisher’s approximation [=asymptotically efficient and asymptotically unbiased] conditions are satisfied, the following holds for all g which g’(θ)=0:\nn​(g^​MLE​−g0​)d⟶​n→∞​N(0,[g′(θ)]2⋅σ2)\nand due to Fisher’s Approximation (asymptotic normality):\nn​(g^​MLE​−g0​)d⟶​n→∞​N(0,I(θ0​)g′(θ0​)2​)\nExample. To construct a standard normal distribution’s confidence interval of level γ=1−α:\nP(z2α​​&lt;Z&lt;z1−2α​​)=1−α"},"Consistency":{"title":"Consistency","links":["Chebyshev's-Inequality"],"tags":["Math/Statistics"],"content":"def. Consisteny. An estimator θ^(X1​,…,Xn​) is consistent for parameter θ if:\n∀ϵ&gt;0,  θ^  p⟶​n→∞​  θi.e.n→∞lim​P(∣θ^−θ∣&lt;ϵ)=1n→∞lim​P(∣θ^−θ∣&gt;ϵ)=0\n![[スクリーンショット 2023-02-21 15.40.01.png|center|250]]\ndef. Asymptotic Unbiasedness. An estimator θ^ is asymptotically unbiased if:\nE[θ^n​]⟶n→∞​ θ\nRemark. You can use the Chebyshev’s Inequality for determining Consistency."},"Constrained-Optimization":{"title":"Constrained Optimization","links":["Budget-Lines","Monotonic-Transformation","Utility-Function","Utility-Maximization"],"tags":["Economics/Micro-Economics","Math/Calculus"],"content":"Lagrangian Method §\nAlg. Lagrangian Optimization.\nLet:\n\nf(x) the target function to optimize\ng(x)=c is the constraint function\nλ is the Lagrange multiplier.\n\n\n(when optimizing for budget constraint) Do a Monotonic Transformation on the Utility Function to make the function easier to manipulate\nThe Lagrangian function is constructed to find the maximum or minimum of a target function subject to constraints:\n\nThe Lagrangian: L(x,λ)=f(x)−λ(g(x)−c)\nλ is an unknown constant\n\n\nThe first-order necessary conditions (also known as KKT conditions) are found by taking the derivative of the Lagrangian with respect to all variables and the Lagrange multipliers, and setting them equal to zero:\n\nFor all i, ∂xi​∂L​=0\n∂λ∂L​=0\n\n\nFeasibility condition:\n\nIs it in the feasible region: g(x)=c?\n\n\nSolve for x,λ ← this is the optimal point\n\n\n\n                  \n                  Example \n                  \n                \nWorked Example\n![[Pasted image 20230905153050.png|Worked Example|625]]\nUtility Maximization\n"},"Consumer-Sentiment-Index":{"title":"Consumer Sentiment Index","links":[],"tags":["Economics"],"content":"University of Michigan performs a monthly telephone survey about how the households feel about the economic climate."},"Consumer-Surplus":{"title":"Consumer Surplus","links":["Marginal-Willingness-to-Pay"],"tags":["Economics/Micro-Economics"],"content":"Consumer Surplus §\nMarginal Willingness to Pay Curve leads naturally to the concept of consumer surplus. Follow the following line of logic:\n\nThe area under the MWTP curve is the Total Willingness to Pay (TWTP)—the amount the consumer was willing to pay to get q amount of goods.\nSince they didn’t have to actually pay that amount (they only paid p×q), they’re better off.\nThe quantitative amount that they’re better off is the Consumer Surplus\n\n\nDeadweight Loss due to Taxation, Analyzed with MWTP Curves §\n\nConsier a distortionary tax on housing prices, per square feet. The blue line shows the price p per sqft of housing. The red line shows the post-tax budget line, and A is the optimal point.\n\n→ What if instead the government took away a lump sum instead of a distortionary tax? The amount taken away is L and the green line shows the new budget. In this hypothetical, ideal case, the optimal is B.\n→ In this case, the distortionary tax collected for bundle A is vertial distance T. As DWL:=L−T, the vertical distance shows the Deadweight Loss for the taxation. (The lump sum was better.)\n\nAlternatively, consider graphing the MWTP for utility uA, using points A,B.\n\n→ With the distortionary tax—optimal bundle: A, CS=a. Govn’t revenue is T=h×(p+t)=b\n→ With the lump sum tax—optimal bundle: B, CS=a+b+c\n\nHow come the consumer has more surplus as B in the bottom graph? Aren’t they indifferent in MWTP?\n\n→ The consumer instead lost the collected lump sum tax, L; ∴L=(a+b+c)−(a)=b+c\n→ ∴ DWL=L−T=(b+c)−(b)=c\n"},"Contagion-(Finance)":{"title":"Contagion (Finance)","links":["Leverage"],"tags":["Economics/Finance"],"content":":= a propogation of bear markets, due to the fact that banks lend and borrow from each other\n→ The higher the Leverage, the greater the degree of contagion in markets"},"Context-Sensitive-Grammar":{"title":"Context Sensitive Grammar","links":[],"tags":["Computing/Formal-Languages"],"content":""},"Context-Free-Grammar":{"title":"Context-Free Grammar","links":["Pushdown-Automata","Context-Free-Language-Parsing"],"tags":["Computing/Formal-Languages"],"content":"Section: Pushdown Automata\ndef. a Context-Free Grammar (CFG) is a grammar whose production rules consist of:\nA→x\n…where A is a variable, and x is a string of variables and terminals.\n\n\n                  \n                  CFGs can check for syntactically correct programs \n                  \n                \n\n\nSimple Grammar: a CFG where any pair of (var,term) appear in no more than one rule.\nAmbiguous Grammar: there exists a string in the language that has more than one derivation.\n\ndef. a Parse Tree [=Derivation Tree] shows the derivation steps of a single string from a start symbol according to the rules of a grammar.\n\n\nThe leaves of the tree read right-to-left is the yield of the tree—in this case, aaϵb is the yield.\nThe yield should not contain the λs. (obviously)\n\ndef. Leftmost Derivation. Replace the leftmost variable in every step.\ndef. Rightmost Derivation. Replace the rightmost variable in every step.\nNormal Forms §\ndef. Chomsky Normal Form. A CFG is in CNF iff all rules are in the form:\nAA​→BC→a​two varsone terminal​​\n\nthm. All CFGs can be expressed in CNF1\ndef. Greibach Normal Form. A CFG is in GNF iff all rules are in the form:\n\nA→ terminal a​​ only vars BCD…​​\n\nthm. Determinability. If the CFG doesn’t contain rules of the form A→λ or A→B, then we can determine for all w if it is in the language of the CFG.\nthm. a Context-Free Grammar is equivalent to an NPDA.\nNPDA → CFG §\nalg. Given NPDA, you can define an equivalent CFG in the following steps:\n\nFor any transitions that pop/push more than one variable onto the stack:\n\nUse a new state to split out the transition, so that one transition only push/pops one variable onto the stack\n\n\nThen, for each transition arc:\n\nIf the transition arc is a pop arc that pops A off the stack, i.e.:\nfrom state i to j is in the form t,A;λ,\nthen construct a production rule for the grammar as:\n(qi​,A,qj​)→t\nIf the transition arc is a push arc that pushes A onto the stack i.e.:\nfrom state i to j is in the form t,γ,Aγ\nthen construct a production rule for the grammar as:\n(qi​,γ,qk​)→t(qj​Aql​)(ql​γqk​), where qk​,ql​ is every possible combination of states in the PDA(!)\n\n\nFinally, of all the rules generated, if any variables on the right side of the arc don’t appear on the left side, that rule is useless.\n\nCFG → NPDA §\nalg. Given a CFG, you can build an LL-NPDA in the following steps. This is a top-down design:\n\nThere are 3 total states in the NPDA: 0, 1, 2\nMove from state 0 to state 1 by adding the start variable S onto the stack\nThe middle state has loops for every rule of the CFG. Pop off the LHS, add the RHS to stack while processing any beginning terminals\nMove from state 1 to final state 2 in a λ-transition\n\n\nalg. Given CFG, you can build an LR-NPDA in the following steps. This is a bottom-up design:\n\nThere are 3 total states in the NPDA: 0, 1, 2\nState 0 has loops for:\n\n…every rule of the grammar. Each rule will pop the RHS and push the LHS of the rule\n…every terminal of the grammar. Each terminal will be pushed onto the stack while reading that terminal.\n\n\n\n\n\nContext-Free Language Parsing\nFootnotes §\n\n\nLinz ↩\n\n\n"},"Context-Free-Language-Parsing":{"title":"Context-Free Language Parsing","links":[],"tags":["Computing/Formal-Languages"],"content":"LL Parsing §\ndef. LL(k) Parsing. Left-to-Right, Left-most derivation Parser with k lookaheads.\nalg. Given CFG G, LL(k) parsing occurs in the following steps:\n\nCreate the First(),Follow() sets for each of the each of the variables in the grammar.\n\nFirst() is the set of terminals (including λ) that can lead the string derived.\n\nCreated by checking the variable on the left side of the production: A→aX.\nCommon sense will be enough.\nDon’t forget the check variables that go to λ i.e. disappear. λ is only included in the FIRST set if the whole variable can disappear to λ.\n\n\nFollows() is created by checking the variable on the right side of the production: A→aX.\n\nThe start variable always has the $\\\n\n\n\n\n\n      - Use the first sets to make life simpler. $\\lambda$ is always removed.\n2. Create the $LL(1)$ parse table.\n   1. For each variable, check the rules.\n   2. There must be an entry for **every element of the first set.** check which rule makes the most sense [= FIRST set of the RHS of the rule is the lookahead symbol]\n   3. If FIRST set includes a $\\lambda$ (and the variable can goto) $\\lambda$, ⇒ There must be a $\\lambda$ entry for **every element of the follows set…**\n   4. …however, if the FIRST set includes a $\\lambda$ **but** the variable **cannot** goto $\\lambda$, **find another rule that makes sense to put there.**\n3. Use the table to do the necessary steps for parsing the string.\n\n![Untitled](Untitled%203%201.png)\n\n## LR Parsing\n\ndef. $LR(k)$ Parsing. **L**eft-to-Right, **R**ight-most **Reverse** derivation parser with $k$ lookaheads.\n\nalg. Given CFG $G$, $LR(k)$ parsing occurs in the following steps:\n\n1. Set up the grammar so that it’s ready to parse:\n   1. Change the start symbol to $S&#039;\\rightarrow S$. This is rule #0.\n   2. Number the rules $1$ through $n$.\n2. Calculate the FIRST and FOLLOW sets of variables.\n3. Construct a $LR$ parsing DFA in the following steps:\n   1. The start state has **rule number 0 and rules for $S$**. Place markers ($\\_$) in the beginning.\n   2. For each rule, “process” one _symbol_ at a time.\n      Processing means that to transition to another state…\n      …with arc labeled *symbol\n      …*to create a new state with the marker jumped over that _symbol_.\n   3. You have a new state with one rule that has the jumped marker. Now, add to the state **the closure of that rule.** The closure of a rule is calculated as such:\n      1. If the marker is in front of a variable, the derivation rules for that variables is included\n      2. if the market is in front of a terminal, don’t do anything\n4. Construct a $LR$ Parsing table from the DFA to use during parsing:\n   1. Do the following for every state of the DFA [=every row of the parse table]:\n      1. For every transition arc, put an entry in the table, for that **state number** and **arc label:**\n      2. If it’s just a transition arc, put a **“shift to state #” or sN**\n      3. If it’s a final state, only the entries for the elements of FOLLOW should be populated;\n         put a **“reduce by rule number #” or rN** and the rule number is the rule of the state (since it’s a final state, there should be only one rule)\n      4. If it’s a final state but its rule is $S’\\rightarrow S$, it’s special; only populate the “$” entry.\n         put a **“accept” or acc** in that place.\n      5. If the state contains a $**\\lambda$-rule,** then always put a **“reduce by rule #” or rN,** where the rule number is that lambda rule ← this should be in the **“$” column\\*\\*\n\n![Untitled](Untitled%204.png)"},"Context-Free-Language":{"title":"Context-Free Language","links":["Pushdown-Automata"],"tags":["Computing/Formal-Languages"],"content":"Section: Pushdown Automata\nClosure Properties §\nContext-Free Languages are closed under:\n\nUnion\nConcat\nStar\nRegular Intersection (←think: you have states &amp; one stack to combine. It makes sense)\n\nPumping Lemma §\nthm. Pumping Lemma for CFL.\n\n\n\n                  \n                  v,y. Divide it up into different cases and derive contradictions in all of them.\n                  \n                \n\n\nTransforming Context-Free Languages for Easy Parsing §\nYou can do the following modifications on CFLs:\n\nAdd Lambda\nRemove λ-productions. ← Every CFG with λ productions can be made into one w/o λ-productions.\nSubstitution from A→wBv and B→y1​∣y2​∣..∣yn​ into A→wy1​v∣wy2​v∣…∣wyn​v\nRemoving Left-Recursion [e.g. A→Ax] ← Left-recursion is bad, because top-down parsers will get into an infinite loop\n\nUse the above modifications to produce CFLs that do not have useless productions.\nalg. Removing useless productions in CFLs. These steps must go in order!\n\nIdentify useless variables\nUseless variables are ones that: 1. cannot be reached by the start symbol 2. cannot derive a terminal\nRemove useless productions\ni.e. remove all productions that contain a useless variable\nRemove all λ-rules\n\ncreate a set of variables that can ultimately derive a λ\nfor every production that uses those variables on the RHS, derive the λ now.\n\n\nRemove unit productions\n\ndraw the variable dependency graph (1 level)\nFor every production that uses unit production on the RHS, consolidate all into one; i.e. derive it now.\n\n\n\nNormal Forms §\n\n\ndef. Chompsky Normal Form\nEither: one terminal, or two variables. Easy to convert any grammar into CNF.\n\n\nA→aA→BC\n\n\ndef. Geribach Normal Form\nAll rules are the form:\n\n\nV→σV∗\n\nAll λ-free CFGs can be expressed in GNF.\n"},"Continual-Procession-of-Technology":{"title":"Continual Procession of Technology","links":["Moore’s-law"],"tags":["Computing"],"content":"Moore’s law"},"Control-Problem":{"title":"Control Problem","links":[],"tags":["Computing"],"content":""},"Controlled-Experiments":{"title":"Controlled Experiments","links":["Dummy-Variables","Two-Stage-Lease-Squares-Regression"],"tags":["Math/Statistics"],"content":"Motivation. In theory we should be able to estimate the causal effect of a controlled experiment using Difference of Means model:\nYi​=β0​+β1​isTreated+ϵi​\nBut we have the following issues in really (ABC issues):\n\nAttrition\nBalance\nCompliance\n\nBalance §\n\ndef. Balance means treatment and control groups are same in all other factors\ndef Blocking means to categorize into blocks (e.g. men and women) and randomize within groups.\n\nBlocking is harder to implement when there are lots of categories, and sample size is limited.\nTherefore, before running regression check for balance by running the Balance Test:\n\n\n\nXi​=γ0​+γ1​ isAssigedTreatment Zi​​​+νi​\nwhere Xi​ is a covariate that we’re worried is not randomized enough.\n\n😊 If γ1​ is not statistically significant, it means treatment and unobserved factors are not correlated.\n😞 If γ1​ is statistically significant then (even if it was randomized) then ρT,ϵ​=0 it’s a failure of randomization\n\nCompliance §\nMotivation. Some people don’t comply, even through they’re offered treatment. What if people who don’t comply are different from people who comply? If this is the case, then focussing on just the treated &amp; compliant people would be not indicative of the true effect of the treatment.\n\n\nZ is the binary “was going to treat this person”\nT is the binary “is compliant person”\n\nIntention to Treat Approach §\nWe attempt to resolve this issue by not regressing against treated &amp; compliant people, but all people who were “supposed to be treated” (=intent to treat), i.e. T=1∧Z=1\n\nNon-ITT: Yi​=β0​+β1​ hasComplied Ti​​​+ϵi​\nITT: Yi​=δ0​+δ1​ ITT Zi​​​+νi​\n\nThis is a conservative estimate. ∣δ1​∣≤∣β1​∣; the lower the compliance, the lower the coefficient. (Full compliance implies ”=“)\n\n\n\n2SLS Approach §\nAlternatively, a better approach to dealing with non-compliance is using a 2SLS. You can use the ITT variable Z as the IV, since it satisfies the conditions that make a good IV:\n\nInclusion condition: Z is correlated with T\nExclusion condition: Z is uncorrelated ϵ because treatment is randomly assigned\n\n\nThen, the first stage (reduced form): Ti​^​=γ0​+γ1​Z+γ2​X2i​+νi​\nThen the second stage:Yi​=β0​+β1​Ti​^​+β2​X2i​+ϵi​\n\nAttrition §\ndef. Attrition means dropping out.\nThe following regression assesses if treatment is correlated to attrition:\ndidAttritioni​=δ0​+δ1​isTreatedi​+νi​\n→ if δ1​ is statistically significant the attrition is correlated to treatment, thus we need to control for it.\nMethods to resolve attrition:\n\nAdd covariates that are significant when we test (interactive attrition test): didAttritioni​=δ0​+δ1​Zi​+δ2​X2i​+δ3​Zi​×X2i​, where Z is the intention-to-treat binary variable.\n\nThis should also be significant γ1​^​in the balance test: X2i​=γ0​+γ1​Zi​+νi​ \nAdd the significant covariates to the final analysis\n\n\nTrim either dataset so the attrition rate is the same\nSelection model. (not discussed)\n"},"Convex-Programming":{"title":"Convex Programming","links":[],"tags":["Computing/Algorithms","Math/Calculus"],"content":""},"Convolutional-Neural-Networks":{"title":"Convolutional Neural Networks","links":[],"tags":["Computing/Maching-Learning"],"content":"Intuition. We aim to take advantage of the features of human vision to inspire the architection of computer vision model. Notably human vision has:\n\nLocal connectivity, i.e. near things are related\nInvariant in many transformation, i.e. rotating, stretching, flipping objects doesn’t make them different objects\nthe RGB channels of an image are not natural, it’s for human eyes only.\n\nComponents §\nConvolution Layer §\nIntuition. The term comes from a function convolution, where two functions are “combined.” 3b1b from 4:45 shows taking a moving average of a function f, which is a type of convolution. The moving average (function g) filters the original function f. This is denoted as f∗g.\nIn computer vision, we only consider discrete convolution, which is also called a kernel. How Blurs &amp; Filters Work - Computerphile - YouTube visualizes the convolution well. Also see: \nwhere the left xi​ is the original “image” and the right is the filtered image. kij​ is the kernel applied that results in cell (i,j) in the output. If you don’t want the image to get to smaller, you can also zero-pad the image’s border:\n\nPooling Layer §\nPooling layers don’t have special parameters. They are just a simple kernel that takes either the maximum or the average of the input: \nNormally, CNNs alternate between convolutional and pooling layers.\n"},"Coordination-Failure-Business-Cycle-Model":{"title":"Coordination Failure Business Cycle Model","links":["Real-Business-Cycle-Model","Production-Function"],"tags":["Economics/Macro-Economics"],"content":"Motivation. The Real Business Cycle Model is good and all, but what is the Production Function doesn’t follow some of our assumptions? Notably, the assumption of decreasing returns to scale? Consider the following production function of the form:\nY=zKαNβ\nwhere α+β≥1, such that there is more increasing returns to scale.\n\nThese are called complementaries, similar to complementary goods, that more labor is able to collaborate to produce more than the sum of its parts.\nThis will be reflected in the labor demand function as the firms will change its hiring practicies:\n\nNow, when the complementarties are strong enough for the labor demand function to be steeper than the labor supply, we can apply the Keynesian Coordination model\nCoordination Model §\n\nWe observe the two different equilibria:\n\n(Blue) High interest rates r+, low employment N−, low production Y−\n(Orange) Low interest rates r−, high employment N+, high production Y+\nThis model predicts that business cycles will be jumps between two equilibria, caused by:\n\n\nChanges in confidence in the economy\nChanges in perception of volatility\netc.\n"},"Cornout-Quantity-Competition":{"title":"Cornout Quantity Competition","links":[],"tags":["Economics/Micro-Economics","Economics/Game-Theory"],"content":"Firms will choose quantity as the strategic variable.\n\n\nDetermining Firm 2’s Best Response Curve:\n\nWhen firm 1’s quantity = 0, firm 2’s best response is to produce xM (=monopoly quantity)\nWhen firm 1’s quantity &gt; 0, firm 2’s best response derived by the reduction in demand in graph (c):\n\nD (=full market demand) is shifted left by x1​ (=firm 1’s produced quantity) to Dr; Firm 2’s BR is to optimize at MC=MR\n→ This shows that when Firm 1 produces 2xM, D will shift left so much that MR = MC at x1​=0.\n\n\n\n\nFirm 1’s BRC is symmetrical, as shown in (b)\n\n→ Firms produce xC (= cournot quantity)\n→ 21​xM&lt;xC&lt;xB\n\n\n\n\n\n                  \n                  Info \n                  \n                \n\nCornout Price pC converges to competitive prices p=MC as the number of firms increase. (Take my word for this; it’s proven mathematically but not shown above)\nSequential Move: Stackleberg Competition §\n\nStackleberg Leader is Firm 1, Stackleberg Follower is Firm 2\n\n\nFirm 1 considers Firm 2’s expected BRC before making a move:\nProfit maximize knowing firm 2’s BRC:\n\nGraph (a) is the expected Firm 2’s BRC\nWhen Firm 1’s quantity &gt;x∗ then firm 2 produces nothing\n→ Firm 2 faces the full market demand ⇒D=Dr\nWhen Firm 1’s quantity ∈[0,xM] then firm 2 faces the residual demand Dr which is calculated by:\n\nat x1​=xM,\n\n…then BR: x2​=21​xM and Σx=23​xM\n…thus Dr=D−21​xM\n…equivalently Dr(xM)=D(23​xM)\n\n\nat x1​=0,\n\n…then BR: x2​=xM and Σx=xM\n…thus Dr=D−xM=0\n…equivalently Dr(x1​[=0])=D(xM)\n\n\n\n\n\nSequential Move: Entry Deterrence §\nGame Structure §\n\nFirst move is Firm 2’s entry decision\nSecond move is either Cournot Quantity, Stackleberg Quantity competition (Bertrand isn’t considered)\nIf firm 2 enters it must pay a Fixed Cost (FC) which is an economic cost to them (=matters in profit calculation).\n\n\n\nFirm 2 chooses entry;\n\n…if it enters it must pay FC, and two players cornout compete;\n…if it doesn’t its profit is zero, and Firm 2 is a monopoly\n\n\nFirm 2 chooses entry;\n\n…if it enters it must pay FC, and two players stackleberg compete with firm 1 as stackleberg leader;\n…if it doesn’t its profit is zero, and Firm 2 is a monopoly\n\n\nFirm 1 chooses quantity first;\n\n…Firm 2 enters, paying FC, and Firm 2 gets stackleberg follower profit\n…Firm 2 doens’t enter, and Firm 1 gets profit from its original quantity\n\n\n\n⇒ The game structure of (c) allows for strateigic entry deterrance by firm 1:\nEntry Deterence §\n\nFirm 1 may deter entry to maximize profit by setting a certain quantity\nFC is an exogenous variable\n→ firm 2’s entry decision will depend on how big FC is\n→ thus, firm 1’s entry deterrence will depend on how big FC is\n\n\n(a) is the profit against a choice of production quantity for a monopoly.\n→ Then, see (b):\n\nIf FC&gt;FC: Second firm will not enter no matter\n…when FC=πM\nIf FC​&lt;FC&lt;FC: First firm will deter entry by increasing production\n…when FC​=(x s.t. π(x)=πSL)\n…i.e. Firm will deter entry until π drops so much that it’s better to compete (and get Stackelberg profit)\nIf FC&lt;FC​: normal stackleberg competition\n…where firm 1 is Stackelberg Leader (SL) and get πSL\n…and firm 2 is Stackelberg Follower (SF) and get πSF\n"},"Correlation":{"title":"Correlation","links":["Covariance"],"tags":["Math/Statistics"],"content":"def. Correlation (Correlation Coefficient). Correlation is Covariance, but standardized (unitless)\nρXY​​=Corr(X,Y)=Cov(σX​X−μX​​,σY​Y−μY​​)=σX​σY​1​Cov(X−μX​,Y−μY​)=σX​σY​Cov(X,Y)​​\nthm. Correlation Identities.\n\n−1≤ρ≤1\nρXY​=1⇔∃a,b∈R∧a&gt;0∣Y=aX+b\nρXY​=−1⇔∃a,b∈R∧a&lt;0∣Y=aX+b\nρaX,bY​=Corr(X,Y) i.e. linearly invariant\n\nFor R.V. X,Y, the following means the Same Thing §\n\nX,Y are uncorrelated\nCov(X,Y)=0\nVar(X+Y)=Var(X)+Var(Y)\nE(X⋅Y)=E(X)⋅E(Y)\n\nIndependence [X⊥Y] implies all of the above, but any of the above does not imply independence."},"Cost-Function":{"title":"Cost Function","links":[],"tags":["Economics/Micro-Economics"],"content":"\n\n                  \n                  wl+rk. Needs derivation by optimization.\n                  \n                \n\nC:(w,r,xˉ)↦C\n\nProperties\n\nHD1 in input prices\nIncreasing in input prices\nIncreasing in output quantity\n\n\n"},"Cost-Minimization":{"title":"Cost Minimization","links":["Profit-Maximization","Lagrangian-Optimization"],"tags":["Economics/Micro-Economics"],"content":"See also Profit Maximization\nLong Run Cost Minimization §\nminl,k​ C=wl+rk such that x=f(x)\n\nLagrangian Optimization\nOptimal when Isoquant is tangent to the isocost\n\nHigher isoquant is not always better! it just means you’re producing more\n\n\n\n\n\n                  \n                  Warning \n                  \n                \nBeware when MC is monotonically decreasing. This means that the firm will produce zero or infinity, which is means it is a special case which you need to analyze separately.\n\n"},"Covariance-&-Correlation":{"title":"Covariance & Correlation","links":[],"tags":["Math/Probability"],"content":""},"Covariance":{"title":"Covariance","links":["Variance"],"tags":["Math/Statistics"],"content":"Motivation. Let us imagine a tuple of data generated by two independent RVs: (x1​,y1​),…,(x4​,y4​), as graphed below. \n\nVar(X)=2 for both\nVar(Y)=1 for both\n…but the ‘spread’ of these two are clearly different!\nThere, we observe that what matters is the ‘quadrant’ or ‘direction’ of the spread that matters, and that is captured by multiplication of the two coordinates. Thus:\nIntuition. Covariance is the “average quadrant” of the direction of spread.\nThe covariance matrix from 5:06\n\ndef. Covariance. Covariance measures the joint variability of two R.V.s; let X,Y.\n\nWhen X,Y show similar behavior, Cov(X,Y)&gt;0\nWhen X,Y show opposite behavior, Cov(X,Y)&lt;0\n\nCov(X,Y):​=E((X−μX​)(Y−μY​))=E(X⋅Y)−E(X)⋅E(Y)​\n\nWhen X⊥Y, then Cov(X,Y)=0; but Cov(X,Y) does not imply X⊥Y\nCovariance is a generalization of Variance: Var(X)=E((X−μX​)2)=Cov(X,X)\n\nthm. Relationship between Covariance and Variance. let X,Y. then:\nVar(X+Y)​=Var(X)+Var(Y)−2⋅E((X−μX​)(Y−μY​))=Var(X)+Var(Y)−2Cov(X,Y)​​\n\nWhen X⊥Y then Var(X,Y)=Var(X)+Var(Y)\n\nthm. Bilinearity of Covariance.\n\nCov(aX,aY)=ab⋅Cov(X,Y)\nCov(X,Y+Z)=Cov(X,Y)+Cov(X,Z)\n\nthm. Summed Variance. let X1​,…,Xn​. Then\nVar(i∑​Xi​)=i∑​Var(Xi​)+2∀j,k s.t.j&lt;k∑​Cov(Xj​,Xk​)\nE.G. second summation has (23​) terms:\nVar(X1​+X2​+X3​)​=Var(X1​)+Var(X2​)+Var(X3​)+2⋅Cov(X1​,X2​)+2⋅Cov(X2​,X3​)+2⋅Cov(X1​,X3​)​\nCovariance Matrix §\ndef. Covariance Matrix is a collection of covariances for X1​,…,Xn​ RVs:\nΣ=​σ1,1​σ2,1​⋮σn,1​​σ1,2​σ2,2​⋮σn,2​​⋯⋯⋱⋯​σ1,n​σ2,n​⋮σn,n​​​\nwhere σi,j​ is the covariance Cov(Xi​,Xj​)\n\nAlways symmetric\nDeterminant gives\n"},"Cramer-Rao-Lower-Bound-(CRLB)":{"title":"Cramer-Rao Lower Bound (CRLB)","links":[],"tags":["Math/Statistics"],"content":"def. Cramer-Rao Lower Bound. Given unbaised estimator θ^,\nVar(θ^)≥I(θ)1​\nthm. There is no other unbiased estimator that has lower preceision than the CRLB.\n\nThus CRLB is a statement about the bound on the best possible precision [=efficiency] we can get for unbiased estimators.\nIf an estimator reaches the CRLB, it is deemed most efficient.\n\nthm. Cramer-Rao Lower Bount for Biased Estimators. If θ^ has a\nIf:  E[θ^]=g(θ)⇒Var(θ^)≥I(θ)g′(θ)​"},"Credit-Rating":{"title":"Credit Rating","links":[],"tags":["Economics/Finance"],"content":"def. Credit Rating is a third-party validation of the creditworthiness of an entity. In the US three companies issue credit ratings with following ratings\n\nAnything below BBB is considered Junk or High-Yield (a euphamism)\n\n"},"Critical-Point":{"title":"Critical Point","links":[],"tags":["Math/Calculus"],"content":"\n\n                  \n                  Critical Point \n                  \n                \nA critical point on function f(x) is all xi​ where dxd​f(xi​)=0\n"},"Critical-Vertex":{"title":"Critical Vertex","links":["Graph"],"tags":["Computing/Algorithms"],"content":"Graph"},"Cross-Price-Demand-Curve":{"title":"Cross-Price Demand Curve","links":[],"tags":["Economics/Micro-Economics"],"content":"Relates price of one good p1​ to the quantity demanded of another good x2​"},"Curvature-(Math)":{"title":"Curvature (Math)","links":[],"tags":["Math/Calculus"],"content":""},"Data-Architecture":{"title":"Data Architecture","links":["Database-Management-System","Relational-Algebra","MongoDB-Reference"],"tags":["Computing/System-Design"],"content":"def. Extract, Transform, Load. (ETL) is the standard workflow to process data (no shit)\n\nE: pull data from source system\nT: clean, standardize, merge, reshape, process data\nL: store data into target system\nMotivation. The biggest problem to designing data systems is scalability and reliability; out of those scalability can only be solved by architecture (as opposed to reliability, which can be managed in HW/driver leve). There are two main ways data is stored and processed:\n\n1. Database Management System (DBMS) §\nTraditional SQL databases using Relational Algebra\n\nTypes:\n\nRelational: PostgreSQL, MySQL\nObject/NoSQL: MongoDB\nVector: Pinecone, Faiss\n\n\nScaling:\n\nVertical: Add more CPU/RAM to machine\nHorizontal\n\nSharding: store partial rows on each machine, with a shared key\nReplication: One write master, many read replicas\n\n\n\n\n\n2. File + Distributed File System (DFS) §\nCreate a DFS cluster, and store data on specialized row/columnar storage. Processing is done in a distributed way.\n\nTypes:\n\nMapReduce: Hadoop with Avro (row store file format)\nDAG execution: Spark with Parquet (column store file format)\nDFS Tech: Apache HDFS\n\n\nScaling:\n\nHorizontal Scaling: Add more nodes to the cluster\n\n\n"},"Data-is-Everything":{"title":"Data is Everything","links":["(Article)-Magic-Ink---Information-Software-and-the-Graphical-Interface","Personal-Computing","Functional-Programming","Turing-Machine"],"tags":["Computing"],"content":"Observe:\n\n(Article) Magic Ink - Information Software and the Graphical Interface\n\n⇒ UI Design is information design (not interactivity).\n\n\nEverything is a File (in Personal Computing)\nFunctional Programming treats programs as manipulation of data.\nTuring Machine take input and produce output on tape.\n\nAn “effective method” [=algorithm] is just procedures on data\n\n\n\nThus data is what drives every computable process.\n\n\n                  \n                  Abstract \n                  \n                \nEverything is Data. Data is Everything.\n"},"Data-driven-or-Truth-driven":{"title":"Data-driven or Truth-driven","links":["Limits-of-Math-and-Computing","Turing-Machine","Phenomenology","Data-is-Everything","Perspective-Projections-Model","Analytic-Philosophy"],"tags":["Mental-Models","Math","Computing"],"content":"Data-Algorithm structure in the top-down approach; Truth-Proof in the bottom-up approach.\n\nThey merge at Godel’s Incompleteness Theorem\n\nRelation between Gödel’s incompleteness theorem, the halting problem and universal Turing machines - Computer Science Stack Exchange\nLimits of Math and Computing.\nThink: the UTM takes a binary encoding of a TM and can simulate that TM. This is algorithm that is taken as input data to another algorithm.\n\n\nAxioms are fundamental bits of data. Using proofs we derive more data [=conjectures, theorems].\n\nData-Algorithm §\nPhenomenology and to a greater extent whole of Continental Philosophy. Data is Everything. Perspective Projections Model\nTruth-Proof §\nBertrand Russell, G. E. Moore and all of Analytic Philosophy\nAlso see (DevonThink) How to safely think in systems. | Irrational Exuberance"},"Database-Design-Theory":{"title":"Database Design Theory","links":["Relational-Algebra"],"tags":["Computing/Data-Science"],"content":"How to reduce a Relational Model to make it more compact.\nArmstrong’s Axioms §\n\nReflexivity: Y⊆X⟹X→Y\nAugmentation: X→Y⟹XZ→YZ\nTransitivity: X→Y and Y→Z⟹X→Z\n\nSecondary Rules §\n\nDecomposition: X→YZ⟹X→Y,X→Z\nComposition: A→B,X→Y⟹AX→BY\nUnion: X→Y,X→Z⟹X→YZ\nPseudo-transitivity: X→Y,YA→Z⟹XA→Z\nIdentity: X→X\nExtensivity: X→Y⟹X→XY\n\nFunctional Dependency §\ndef. Functional Dependency. If for all tuples in a relation that has the same values for attribute X and attribute Y, then Y is functionally dependent on X.\n∃f:X↦Y⟹X→Y ...read &quot;Y Depends on X&quot;\ne.g. an address relation with street, city, state, zip\n\nzip → (city, state)\n\nZip code determines city and state\n\n\n(zip, state) → zip\n\nThis is a Trivial dependency:= RLHS⊇RHS\n\n\nzip → (state,zip)\n\nThis is non-trivial, but not complete,\nComplete Non-trivial dependency:= LHS∩RHS=∅\n\n\n\nalg. BCNF decomposition procedure. Given a set of dependencies F and relation R, BCNF decomposition determines a minimally redundant (=row-redundant) Relation.\n\nChoose (non-determinisitcally) a dependency in F, let X→Y which is non-trivial (=X is not a superkey)\nDecompose using X into two relations R1​,R2​\n\nR1​ has attributes X∪Y\nR2​ has attributes X∪(attr(R)−Y)\n\n\nRecurse procedure for R1​,R2​\n\n\nIt’s a non-deterministic process (=there can be multiple ways to decompose a relation).\n\ndef. Closure. Given dependencies F, the closure Z∗ of set of attributes Z is all attributes determined by Z through those dependencies\n\nFinding a Key Using Functional Dependencies §\nalg. Given set of functional dependencies F on relation R, find the keys of the schema\n\nFor each dependencies LHS→RHS, compute the closure of LHS such that LHS→LHS∗.\n\nIf LHS∗ doesn’t contain all the attributes of R, then augment LHS such that it reaches all the attributes.\n\ni.e., let Remain=attr(R)−LHS∗\nthen, it must be LHS∪Remain→attr(R)\n\n\nThen, LHS∪Remain is a superkey of R. let this be S.\nCan you reduce this superkey S?\n\nTry taking out one attribute at a time from S to create S′\nIs S′ reducible?\n→ Repeat until we reach something unreducible.\n⇒ The final S′ that is unreducible is a key.\n\n\n\n\nRepeat for all dependencies in F.\n\nMulti-value Dependency §\ndef. Multi-value Dependency (MVD). Y is a multi-value dependency of X if…\n\nfor all tuples with the same values of X…\n…if we swapped all the Y values of them…\n…those entries also exist in the database. then:\n\nX↠Y\nIn colloquial terms, we can say that for every determined value of X, all tuples with values of Y “associated” with that X must exist too.\n\ndef. Trivial MVD. Given relation R(A,B,C):\n\nA,B↠C: Obvious. LHS∪RHS=attrs(R)\nA,B↠A: Obvious. LHS⊇RHS\n\ndef. 4th Normal Form (4NF). if every MVD in relation R is X↠Y such that X is always a superkey, then relation R is in the 4th Normal Form. Example\nMultivalue Dependency Rules §\n\nComplementation: X↠Y⟹X↠attrs(R)−X−Y\nAugmentation: X↠Y,B⊆A⟹XA↠YB\nTransitivity: X↠Y,Y↠Z⟹X↠Z−Y\n\nAutomatically means X↠Y,Y↠Z⟹X↠Z\n\n\nReplication: X→Y⟹X↠Y\nCoalescence: If 𝑋 ↠ 𝑌 and 𝑍 ⊆ 𝑌 and there is some 𝑊 disjoint from 𝑌 such that 𝑊 → 𝑍, then 𝑋 → 𝑍\n\nChase §\nProving theorems about chase. Example: \nalg. Tuple Generation. To enforce X↠Y on table R\n\nFor a determined X=xi​\n\nEnumerate all available Y=y1​,y2​,…,y?​\nEnumerate all available\n\nlet Q:=attr(R)−X−Y\nEnumerate all available Q=q1​,…,q?​\n\n\nAre all combinations Y×Q available in the table?\n\nIf not, add missing ones\n\n\n\n\nFor another determined X=x2​, repeat\nRepeat for all X=x1​…x?​\n\nalg. Chase. To prove X↠Y…\n\nInitialization: add two tuples (x,y1​,…),(x,y2​,…) to the initial table\nFor each MVD (order doesn’t matter):\n\nTuple Generation: see above\n\n\nFor each FD (order doesn’t matter):\n\nYou may infer equalities, e.g. e1​=e2​\n\n\nDoes X↠Y fully available? (=Y×Remaining Attrs all in table)?\n\nYes → proven\nNo → disproven (counterexample)\n\n\n"},"Database-Indexing":{"title":"Database Indexing","links":[],"tags":["Computing/Data-Science"],"content":"\n\n                  \n                  Every trip to the disk is a trip to pluto. \n                  \n                \n\nSequential Index §\n\nCan be dense or sparse\n\nPrimary Index: Key indices are often sparse, because the rows are ordered using the key index\nSecondary Index: Index on other attributes are often dense b/c/ v.v.\n\n\nYou can manually create indexes in SQL:\n{postgresql}CREATE INDEX indexname ON tablename(columnname)\nMulti-column index: Index (A,B) on R(A,B,C) has keys that have A,B concatenated and sorted together.\n\nSee: sql server - what does a B-tree index on more than 1 column look like? - Stack Overflow\nQuote: “With most implementations, the key is simply a longer key that includes all of the key values, with a separator. No magic there;-). In your example the key values could look something like:&quot;123499|John Doe|Conway, NH&quot;,&quot;32144|Bill Gates| Seattle, WA&quot;\n\n\n\nIndex Sequential Access Method (ISAM) §\n\nLookup: \nInsert / Delete: \n\nB+ Trees §\n\nArchitectural Improvement on ISAM\n\nEach block can fan-out a specific amount n\nEach block containes n−1 entries to indicate ranges\nExample: \n\n\nLeaves are sequential → Range queries are possible: \nGuaranteed to be well-balanced (=all nodes are at least half-full)\n\nGuaranteed by recursive node splitting (on insert) and recursive node coalescing or entry stealing (on delete)\nExample: \n\n\nPerformance: access time is O(height)=O(logfanout​#Tuples)\n\n→ You can fit this in memory (4-level B+ Trees are often good enough)\n…recall you also need to node-split of node-coalesce sometimes too\n\n\n"},"Database-Management-System":{"title":"Database Management System","links":["Performance-(Computing)"],"tags":["Computing/Data-Science"],"content":"DBMS must be able to…\n\nStore data persistently.\nHandle query of data by the user\nUpdate data as requested\n\nWhat do databases aim to be?\n\nPhysical Data Independence\n\nHow data is physically stored is abstracted away by the DBMS\n\n\nConcurrency Control\n\nMultiples accesses must be performant and conflict-free\n\n\nRecovery\n\nFailures should be handled and recovered\n\n\nPerformance (Computing)\n\nMust be performant with lots of accesses\n\n\n"},"DataviewMermaid":{"title":"DataviewMermaid","links":[],"tags":[],"content":"Mermaid Output §\n//YAML Settings\nlet c = dv.current();\nlet direction = c.Direction ?? &quot;LR&quot;; \nlet showAsCode = c.ShowCode ?? false;\nlet subGroups = c.SubGroupNames ?? [];\nlet removeOrphans = c.RemoveOrphans ?? false;\nlet keepLinksWithoutSource = c.KeepLinksWithoutSource ?? true;\nlet keepLinksWithoutDest = c.KeepLinksWithoutDes ?? true;\n\nvar nodeQueries = c.Nodes ?? [&#039;TABLE WITHOUT ID &quot;[[No Nodes]]&quot;, &quot;No Nodes&quot;, &quot;[&quot;, &quot;]&quot;, &quot;red&quot; FROM &quot;&quot; LIMIT 1&#039;];\nvar linkQueries = c.Links ?? [];\nvar styles = c.Styles ?? [];\n\n//DEFAULTS\nlet def = {open: &quot;[&quot;, close: &quot;]&quot;, style: &quot;default&quot;};\nstyles.push(&quot;classDef red fill:#f2b5bd88,color:#c94f60,stroke:#c94f60,stroke-width:1px&quot;);\nstyles.push(&quot;classDef orange fill:#efc5b388,color:#c76a43,stroke:#c76a43,stroke-width:1px&quot;);\nstyles.push(&quot;classDef yellow fill:#f2d9b588,color:#b68035,stroke:#b68035,stroke-width:1px&quot;);\nstyles.push(&quot;classDef green fill:#bde0bd88,color:#3c9f3e,stroke:#3c9f3e,stroke-width:1px&quot;);\nstyles.push(&quot;classDef mint fill:#c9e4d688,color:#24a864,stroke:#24a864,stroke-width:1px&quot;);\nstyles.push(&quot;classDef aqua fill:#c0dede88,color:#339999,stroke:#339999,stroke-width:1px&quot;);\nstyles.push(&quot;classDef blue fill:#cbcde188,color:#6d73b0,stroke:#6d73b0,stroke-width:1px&quot;);\nstyles.push(&quot;classDef purple fill:#d4c0e388,color:#8c59b1,stroke:#8c59b1,stroke-width:1px&quot;);\nstyles.push(&quot;classDef pink fill:#e6c1d788,color:#b54a88,stroke:#b54a88,stroke-width:1px&quot;);\nstyles.push(&quot;classDef grey fill:#d3cfcf88,color:#7e7273,stroke:#7e7273,stroke-width:1px&quot;);\nstyles.push(&quot;classDef default fill:#88888888,color:#000,stroke:#000,stroke-width:1px&quot;);\n\n//--------------\n//SHOULDN&#039;T NEED TO CHANGE CODE BELOW HERE\n//--------------\n\n//FRONTMATTER STRING\nlet mFront = (showAsCode ? &quot;```\\n&quot; : &quot;```mermaid\\n&quot; );\nmFront += &quot;graph &quot; + direction + &quot;\\n\\n&quot;;\n\n//CREATE ARRAY OF ALL NODES\nvar nodesArray = [];\nfor (var q = 0; q &lt; nodeQueries.length; q++) {\n\tlet DQLResults = await dv.tryQuery(nodeQueries[q]);\n\tDQLResults.values.forEach(node =&gt; {\n\t\tlet ref = getNodeRefName(node[0]);\n\t\tlet ID = getLegalCharacters(ref);\n\t\tlet display = cleanLabel(node[0], node[1]);\n\t\tif  (!node[4]) {node[4] = &quot;default&quot;}\n\t\tlet nodeObj = {qID: q, ID:ID, link:ref, name, display:display, \n\t\t\t\t\t\topen:node[2], close:node[3], style:node[4], linked: false}\n\t\tnodesArray.push(nodeObj);\n\t});\n};\n\n//CREATE MERMAID STRING FOR LINKS (CHECK FOR ORPHANS)\nnodesArray = dv.array(nodesArray);\nvar mLinks = &quot;%%---LINKS---\\n&quot;;\nfor (var q = 0; q &lt; linkQueries.length; q++) {\n\tlet DQLResults = await dv.tryQuery(linkQueries[q]);\n\tDQLResults.values.forEach(link =&gt; {\n\t\tlet sRef = getNodeRefName(link[0]);\n\t\tlet sID = getLegalCharacters(sRef); \n\t\tlet dRef = getNodeRefName(link[1]); \n\t\tlet dID = getLegalCharacters(dRef); \n\t\tlet arrow = (link[2] == &quot;&quot; ? &quot; --&gt; &quot; : &quot; &quot; + link[2] + &quot; &quot;);\n\n\t\t//Find nodes if they were imported explicitly\n\t\tlet sIndex = nodesArray.ID.indexOf(sID);\n\t\tlet dIndex = nodesArray.ID.indexOf(dID);\n\n\t\t//Mark those nodes as linked if they were found\n\t\tif (sIndex &gt;= 0) {nodesArray[sIndex].linked = true;}\n\t\tif (dIndex &gt;= 0) {nodesArray[dIndex].linked = true;}\n\t\t\n\t\t//if both nodes exist OR we have permission to create them\n\t\tif ((sIndex &gt;= 0 || keepLinksWithoutSource) &amp;&amp; \n\t\t\t(dIndex &gt;= 0 || keepLinksWithoutDest)) {\n\n\t\t\tvar sString = &quot;    &quot;;\n\t\t\tif (sIndex &gt;= 0) { sString += sID; }\n\t\t\telse { sString += getNodeDef(sID, def.open, &quot;&quot;, \n\t\t\t\t\t\t\t\tcleanLabel(sRef), def.close, def.style); }\n\t\t\tvar dString = &quot;&quot;;\n\t\t\tif (dIndex &gt;= 0) { dString += dID; }\n\t\t\telse { dString += getNodeDef(dID, def.open, &quot;&quot;, \n\t\t\t\t\t\t\t\tcleanLabel(dRef), def.close, def.style); }\n\t\t\t\t\t\t\t\n\t\t\tmLinks += sString + arrow + dString + &quot;\\n&quot;;\n\t\t}\n\t});\n\tmLinks = mLinks + &quot;\\n&quot;;\n};\n\n//CREATE MERMAID STRING FOR THE NODES\nvar mNodes = &quot;%%---NODES---\\n&quot;;\nvar qID = -1;\nvar subGroup = &quot;&quot;;\nnodesArray.forEach(n =&gt; {\n\t//if node is linked, or I don&#039;t care about orphans\n\tif (n.linked || !removeOrphans) {\n\t\t//new subgroup\n\t\tif (qID != n.qID) {\n\t\t\tif (subGroup != &quot;&quot;) {mNodes += &quot;end\\n\\n&quot;} //close previous\n\t\t\tqID = n.qID;\n\t\t\tsubGroup = subGroups[qID] ?? &quot;&quot;;\n\t\t\tif (subGroup != &quot;&quot;) {\n\t\t\t\tmNodes += &quot;subgraph SG&quot; + qID + &quot; [&lt;B&gt;&quot; + subGroup + &quot;&lt;B&gt;]\\n&quot;;\n\t\t\t}\n\t\t}\n\t\tmNodes += &quot;    &quot;\n\t\tmNodes += getNodeDef(n.ID, n.open, n.link, n.display, n.close, n.style);\n\t}\n});\nif (subGroup != &quot;&quot;) {mNodes += &quot;end\\n\\n&quot;}\n\n//STYLE CLASSES\nvar styleClasses = &quot;%%---STYLES---\\n&quot;;\nfor (var s = 0; s &lt; styles.length; s++) {\n\tstyleClasses += styles[s] + &quot;\\n\\n&quot;\n}\n\n//OUTPUT \ndv.span(mFront + mNodes + mLinks + styleClasses + &quot;```&quot;);\n\n\n//--------------\n//HELPER FUNCTIONS\n//--------------\nfunction getNodeDef(ID, open, href, display, close, style) {\n\tvar def = &quot;&quot;;\n\tdef += ID + open + &quot;\\&quot;&lt;div style=&#039;padding:5px;&#039;&gt;&quot;;\n\tdef += (href != &quot;&quot; ? &quot;&lt;a class=&#039;internal-link&#039; href=&#039;&quot; + href + &quot;.md&#039;&gt;&quot; : &quot;&quot;);\n\tdef += display;\n\tdef += (href != &quot;&quot; ? &quot;&lt;/a&gt;&quot; : &quot;&quot;);\n\tdef += &quot;&lt;/div&gt;\\&quot;&quot; + close + &quot;:::&quot; + style + &quot;\\n&quot;;\n\treturn def\n}\n\nfunction getNodeRefName(node) {\n\t// return file name if valid page, otherwise return a string representation\n\tif (dv.page(node)) { \n\t\treturn dv.page(node).file.name;\n\t} else { \n\t\treturn cleanLinkyString(node);\n\t}\t\n}\n\nfunction getLegalCharacters(name) {\n\t//Remove characters not allowed in Mermaid IDs\n\treturn name.replace(/[^a-zA-Z0-9]/g, &#039;&#039;);\n}\n\nfunction cleanLinkyString(link) {\n\t//Remove [[ | ]] from linky strings\n\treturn String(link).split(&quot;|&quot;)[0].replace(/[\\[\\]]/g, &#039;&#039;);\n}\n\nfunction cleanLabel(node, display) {\n\t//Try to clean up the display name, but if empty - infer from node\n\tvar wrapped = &quot;&quot;;\n\tif (!display || display == &quot;&quot;) { \n\t\twrapped = getNodeRefName(node);\n\t} else {\n\t\twrapped = cleanLinkyString(String(display));\n\t}\n\twrapped = wrapped.replace(/(?![^\\n]{1,20}$)([^\\n]{1,20})\\s/g, &#039;$1\\&lt;br&gt;&#039;);\n\twrapped = wrapped.replace(/[❝❞]/g,&#039;\\\\&quot;&#039;);\n\treturn wrapped\n}"},"Debt":{"title":"Debt","links":[],"tags":["Economics/Finance"],"content":"Secured debt:= debt backed by collateral"},"Debter-in-posession":{"title":"Debter in posession","links":[],"tags":["Economics/Finance"],"content":""},"Decentralization":{"title":"Decentralization","links":["Institutional-Design","Centralized-Power","External/Michel-Foucault","horizontal-organization","Living-With-the-Internet","Spontaneous-Organization","Game-Theory"],"tags":["Economics/Game-Theory","Philosophy/Political-Philosophy","Computing"],"content":"Decentralization is a type of Institutional Design that breaks up central powers into smaller autonomous organizations.\n\nIt distributes Centralized Power into the microphysics of power\nIt allows for horizontal organization\nComputers and the internet allows these structures to naturally form\nSpontaneous Organization\n\nExample\n\nGit (decentralized version control &amp; devops)\nBlockchain (decentralized ledger)\nIPFS &amp; BitTorrent (decentralized file hosting)\nGame Theory Concepts\nResource Regeimes\n\n(DevonThink) 4-3. Ostrom, Collective action and the evolution of social norms\n\n\n"},"Decision-based-evidence-making":{"title":"Decision-based evidence-making","links":[],"tags":[],"content":""},"Deductive-Nomological-Model":{"title":"Deductive-Nomological Model","links":[],"tags":["Philosophy/Epistemology"],"content":""},"Delta-and-Gamma-Hedging":{"title":"Delta and Gamma Hedging","links":["Options-(Finance)","Black-Scholes-European-Option-Pricing-Formula","Stochastic-Calculus"],"tags":["Economics/Finance"],"content":"Motivation. Imagine you are an investment bank, selling (which is equivalent to short-selling) a call option. At the end, you have to pay to the holder CT​=max{ST​−K,0}. When the option is on the money at expiration time T, you have to pay ST​−K. You want to pay less, though; so you try to hedge against small changes in underlyer price.\nDelta Hedging for Call/Put Options §\nAt time t=0 (day 0) §\n\nSell the option −CtE​, and get the cash CtE​\nBorrow Lt​ amount of money to buy Δt​ units of stock. (this is the hedging part)\n\n&amp; this Δt​ is the same as the delta of the call option.\n\n\n\nVt​= short-sell −CtE​​​+ cash CtE​​​− loan Lt​​​+ underlyer Δt​St​​​\nIt is obvious that the value of this portfolio is Vt​=0, at time t=0, but to show that this portfolio at any time t always has value 0, we observe the fact that the last three terms (let Ctsyn​:=+CtE​−Lt​+Δt​St​) is a self-financing portflio that tracks the call price exactly.\nObserve from BSM derivation that a portfolio Wt​=nt​StC​+bt​Bt​ (we already used V) is self-financing and replicates when:\n\nnt​=Stc​St​​Δf​\nbt​=Bt​f(St​,t)−nt​StC​​\nIn our case, we have:\nnt​=StC​St​​Δt​\nbt​=Bt​CtE​−St​​\nAnd thus =bt​Bt​CtE​−Lt​​​+=nt​St​Δt​St​​​ by the above equation.\n\nAt time t=0+dt (day 1) §\n\nOption value changes to −Ct+dt​\nCash holding grows to CtE​er⋅dt\nWe must “Re-hedge” or “balance” the loan and underlier such that we maintain Δt+dt​ units of underlyer. We break it out into three cases:\n\nΔt+dt​=Δt​ (delta stays same): time value of money, and use dividend to pay back loan\n\nLt+dt​=Lt​ time value e(r−q)⋅dt​​\n\n\nΔt​&lt;Δt+dt​ (delta increases): Borrow more money to buy more stock:\n\nLt+dt​=Lt​e(r−q)dt+∣dΔt​∣St+dt​\n\n\nΔt+dt​&gt;Δt​ (delta decreases): Sell stock to reimburse loan.\n\nLt+dt​=Lt​e(r−q)dt−∣dΔt​∣St+dt​\n\n\nIn all three cases, we can say:\n\n\n\nLt+dt​=Lt​e(r−q)dt+(dΔt​)St+dt​\n\nWe then purchase or sell dΔt​ units of the security to get Δt+dt​St+dt​\nAs we show above, Ct+dtsyn​:=+Ct+dtE​−Lt​+Δt+dt​St+dt​ is self-financing. This means that total value of the portfolio is:\n\nVt+dt​​=−Ct+dtE​+CtE​er⋅dt−Lt+dt​=−Ct+dtE​+=Ct+dtE​CtE​er⋅dt−Lt​e(r−q)dt+(dΔt​)St+dt​​ loan ​+Δt+dt​St+dt​​​=0​​\nAt time t=T, (expiration date) §\nOn expiration and before the call is cashed out, the value of the portfolio is:\nVT​=0=−max{ST​−K,0}+C0E​erτ−LT​L0​e(r−q)τ+(dΔT−dt​)ST​​​+ΔT​ST​\nWe then consider our customer cashing out their loan:\n\nIf ST​&gt;K in the money,\n\n−CTE​ becomes −ST​+K\nΔT​=1\nThus VT​=−ST​+K+C0E​erτ−LT​+ST​=0 since synthetic call still holds\n\ni.e., liquidate the stock to pay ST​\n\n\nThus LT​=K+C0E​erτ\n\ni.e., pay back the loan with remaining K and cash invested\n\n\n\n\nIf ST​≤K out of the money:\n\n−CtE​ becomes 0\nThus VT​=+C0E​erτ−LT​+ΔT​ST​=0 since synthetic call still holds\nThus LT​=C0E​erτ+ΔT​ST​\n\ni.e. pay back the loan with cash invested and liquidating the stock\n\n\n\n\n\nGreek Neutral Portfolio of Derivatives §\nAny derivative we can modify to make delta-neutral. For a security modeled by geometric brownian motion (with no dividends, q=0):\ndSt​=mSt​dt+σSt​dBt​\n\n\n                  \n                  Notation \n                  \n                \nSince there are lots of notaions here, let’s outline them first:\n\nSt​ is the underlying security\nCtE​ is the call option\nV~ is the delta-neutral portfolio when parameter Ns​=ΔC​\nV^ is the gamma-neutral portflio when parameters Ns​=ΔC​, NC​=−ΓC​Γ~​\nV′ is the delta-gamma neutral portflio when parameters\n\n\nlet us create a portfolio of a derivative of this stock by having one call option and Ns​ units of the underlier. The value of this portflio is:\nV~(St​,t)=CtE​+Ns​St​\n\nCtE​ have greeks ΔC​,ΓC​,ΘC​\nwhich have greeks Δ~=ΔC​+NS​, Γ~=ΓC​, Θ~=ΘC​.\nUsing ito’s formula, we get:\n\ndV~​=​Θ~∂t∂V~​​​+mSt​Δ~∂St​∂V~​​​+21​σ2St2​Γ~∂St2​∂2V~​​​​dt+σSt​Δ~∂St​∂V~​​​dBt​=Γ~21​ See below σ2St2​dt​​+Θ~dt+Δ~(=dSt​mSt​dt+σSt​dBt​​​)=21​(dSt​)2Γ~+Θ~dt+Δ~dSt​​​\nWhere from Ito Isometries\n(dSt​)2​=m2St2​=0(dt)2​​+σ2St2​=dtdBt2​​​+2mσSt2​=0dtdBt​​​=σ2St2​dt​​\nNow, from the above formula we see that there are three components that cause changes in dV~\n\n∣Δ~∣&gt;0 i.e. non-zero delta\n∣Γ~∣&gt;0, i.e. non-zero gamma\n∣Θ~∣&gt;0, but considering Θ:=∂t∂S​ it’s impossible for this to be zero.\n\nDelta Hedging. §\nLets make Δ~=0. We can do this simply by setting Ns​≡ΔC​:\nV~∂St​∂V~​​=−CtE​+ΔS​St​=−∂St​∂V​+ΔC​=−ΔC​+ΔC​=0​​\nDelta-Gamma Hedging §\nAfter the delta hedging we still have a positive gamma. Lets make Γ~=0. We create a new portfolio of the delta-neutral portfolio, and NC​ units of the call:\nV^Γ^​=V~+NC​CtE​=∂St2​∂2V^​=Γ~+NC​ΓC​​​\nSetting NC​≡−ΓC​Γ~​ will make Γ^=0. But this incurs a new problem: Δ^ is non-zero again. We add Nmore​ more units of St​ to combat this:\nV′=V~+NC​CtE​+Nmore​"},"Demarcation-Problem":{"title":"Demarcation Problem","links":["Karl-Popper","Analytic-Philosophy"],"tags":["Philosophy/Epistemology"],"content":"Demarcation Problem. Problem in epistemology on how to distinguish science and non-science. Related to:\n\nFalsifiability Criterion\n\nKarl Popper is the modern analytic philosopher to discuss this\n\n\nEmpiricism\n"},"Depth-First-Search":{"title":"Depth First Search","links":["Shortest-Path","Directed-Acyclical-Graph"],"tags":["Computing/Algorithms"],"content":"alg. Depth First Search. DFS, but also record the preand post-times as it is convenient. From Class:\n\nTime Complexity: O(V+E)\nDFS is useful in most graph tasks except Shortest Path (that’s for BFS)\n\n\nthm. Identifying DAGs. If there is no back edge, the graph is a DAG.\nalg. DAG Topological Ordering. Perform DFS with postand pre-time. Check there are no back edges (i.e. no cycle) "},"Derivative-Rules":{"title":"Derivative Rules","links":["Integration-Rules"],"tags":["Math/Calculus"],"content":"\n\n                  \n                  Note \n                  \n                \n\n(DevonThink) IB Math HL Formula Booklet\nIntegration Rules\n\n\nDerivation Rules §\nChain Rule:\ny=g(u)⟹u=f(g(x))⟹dxdy​=dudy​⋅dxdu​\nf(g(x))=f′(g(x))⋅g′(x)\nProduct Rule:\ndxd​f(x)g(x)=f′(x)g(x)+f(x)g′(x)\nFilliping:\ndxdy​=(dydx​)−1\n\nSpecial Functions §\nExponentials §\n\nf(x)=ax then f′(x)=ln(a)⋅ax\nf(x)=ex then f′(x)=ex\nf(x)=ag(x) then f′(x)=ln(a)⋅ag(x)⋅g′(x)\nf(x)=eg(x) then f′(x)=eg(x)⋅g′(x)\n\nTrigonometric Functions §\nFrom (DevonThink) Mnemonic diagram for trigonometric and hyperbolic functions\n\n\nFunctions and cofunctions are on horizontal lines.\nDerivatives of functions on the right have a negative sign; those on the left do not.\nFunctions and reciprocals are on diagonal lines.\n\ne.g. sin(x)=csc(x)1​\n\n\nEach function is the ratio of the next two functions clockwise.\n\ne.g. tan(x)=cos(x)sin(x)​\n\n\nEach function the the product of its two neighbors.\nThe two functions at the top are bounded. The rest are unbounded.\nThe functions that are vertices of a triangle with a Roman numeral inside are related by Pythagorean identities.\n\ne.g. sin(x)2+cos(x)2=1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctionDerivativeIntegralsin(x)cos(x)−cos(x)+Ccos(x)−sin(x)sin(x)+Ctan(x)sec2(x)$-\\lncot(x)−csc2(x)$\\lnsec(x)sec(x)tan(x)$\\lncsc(x)−csc(x)cot(x)$\\ln\n\n\nFunction and cofunctions are on lines that make a 120° angle with the horizontal.\nDerivatives of functions on the right have a negative sign; those on the left do not.\nFunctions and reciprocals are on diagonal lines.\nEach function is the ratio of the next two functions clockwise.\nEach function the the product of its two neighbors.\nThe two functions at the top are bounded. The rest are unbounded.\nThe functions that are vertices of a triangle with a Roman numeral inside are related by Pythagorean identities.\n"},"Derivatives-(Finance)":{"title":"Derivatives (Finance)","links":["Options-(Finance)","Futures","Forwards","Swaps","Lifecycle-of-a-Trade"],"tags":["Economics/Finance"],"content":"Derivatives are tradable contracts.\nProperties of Derivatives §\nWhere is it traded?\n\nOver-the-Counter (OTC): the two parties arrage the terms by themselves. Harder to price.\nExchange-traded: everybody goes to the central exchange to trade. Higher liquidity.\n\nDo you need to execute?\n\nNon-contingent: No need to execute. Options (Finance)\nContingent: Must execute. Futures, Forwards, Swaps, etc.\n\nMargin §\nMargin fully mitigates couterparty risk. Clearinghouse requires both parties to post initial and variational margin as the underlyer fluctuates in value.\n\nInitial Margin: the margin you pay\nVariational Margin: you add onto margin every time\n"},"Desire-as-the-Object-of-Desire":{"title":"Desire as the Object of Desire","links":["Alexandre-Kojeve","(Book)-Phenomenology-of-Spirit","Jealousy"],"tags":["Philosophy"],"content":"…is Alexandre Kojeve’s interpretation of (Book) Phenomenology of Spirit.\n\nIt is what differentiates humans from others\nEssences of Jealousy and complex love.\n"},"Determinism":{"title":"Determinism","links":[],"tags":["Philosophy/Epistemology"],"content":""},"Development-Documentation-for-Writing-Helper":{"title":"Development Documentation for Writing-Helper","links":[],"tags":["Computing","Documentation"],"content":"PDF.js usage\nHow to Build a React PDF Viewer with PDF.js | PSPDFKit\nHow to Build a TypeScript PDF Viewer with PDF.js | PSPDFKitlog\n\nPDF.js uses workers to parse and render the PDF file. To keep things simple, we’ll avoid bundling the workers and instead copy them to our public folder (named public) using the following:\n\ncp ./node_modules/pdfjs-dist/build/pdf.worker.min.js ./public/\n\n{and then you can do:}\npdfJS.GlobalWorkerOptions.workerSrc =\n\t\t\t\twindow.location.origin + &#039;/pdf.worker.min.js&#039;;\nFeatures §\n\nList of past articles for the user\n\nIncludes what they learned, what they ignored\n\n\nWord order by frequency\n"},"Dialectical-Materialism":{"title":"Dialectical Materialism","links":[],"tags":["Philosophy/Political-Philosophy"],"content":"\nThe law of the unity and conflict of opposites\nThe law of the passage of quantitative changes into qualitative changes\nThe law of the negation of the negation\n"},"Difference-In-Difference":{"title":"Difference In Difference","links":["Fixed-Effects-Model-for-Panel-Data"],"tags":["Math/Statistics"],"content":"Fixed Effects Model for Panel Data\nDiD analysis is a way of observing and doing statistics on natural experiments. It’s called difference-in-difference because we’re comparing the differences in the change between the control and treatment groups. The model is:\nYit​=β0​+β1​isTreated+β2​isAfter+β3​isTreated×isAfter+ϵit​\nwhere isTreated and isAfter each is a dummy variable. The regression will look something like this:\n\n(a) illustrated when there is no difference-in-difference. (b) illustrates when there is a difference-in-difference. β3​, the coefficient that measures additional effect when there is treatment and is after treatment is the important coefficient."},"Difference-between-Art-and-Design":{"title":"Difference between Art and Design","links":["성민석"],"tags":["Art","Design"],"content":"Conversaion with 성민석\n\nArt is about the expression of human weaknesses (=“what is broken within us” =“art begins from what is broken”) and works within the limitations of the medium.\n\nRespect for the boundaries of the medium is important\n\ne.g. the fight between different styles of art\ne.g. literary freedom to break grammar\n\n\n\n\nDesign is about the creation of new things that are useful or pleasing to the senses as a means of direct expression.\n\nBreaking the boundaries of the medium is another form\n\n\n"},"Differential-Equation":{"title":"Differential Equation","links":[],"tags":["Math/Calculus"],"content":""},"Directed-Acyclical-Graph":{"title":"Directed Acyclical Graph","links":["Algorithm-Problem-Tips","Directed-Graph","Depth-First-Search","Shortest-Path"],"tags":["Computing/Data-Structures"],"content":"def. Directed Acyclical Graph (DAG). No shit.\n\nDirected Trees are (obviously) DAGs.\nIt also has a topological ordering\n\nAlgorithm Problem Tips. Detecting if a Directed Graph is Acyclic.\n\nIdea: DFS with tree. If there is no back edge, it is a DAG.\n\nalg. Topological Sort (DFS). DFS with preand post-times. Order by decreasing post-time.\n\nSee Also course\nTopological Sort may not be unique\n\nalg. Shortest Path for DAG.\nIdea: Topological sort of DAG. Then, linearly calculate minimum distance to that.\nslides"},"Directed-Graph":{"title":"Directed Graph","links":["Directed-Acyclical-Graph","Depth-First-Search"],"tags":["Computing/Data-Structures","Computing/Algorithms"],"content":"Terminology\n\nGT: Transpose of directed graph; G with all the directions of edges reversed\nSource vertex: outgoing edges only\nSync vertex: incoming edges only\nStrongly connected directed graph: all edges can reach all other edges\n\nSource Component: only outgoing edges to any vertex of component\nSync Component: only incoming edges to any vertex of component\n\n\nIf directed graph has no cycles, it is a DAG\n\nalg. Kosaraju’s Algorithm for Finding Strongly Connected Components.\n\nIdea: after performing a DFS on graph G, the\n\nlet v be the component with the maximum post-time\nReverse edges of G to get GT.\nlet VC​ be a sync component of GT\n**v must be an element of some sync component VC​\n\n\nSee also: course\n\n\nDFS on G\nDFS on GT but…\n\nwhen you run out of reachable components…\ncut off the DFS tree; that’s a strongly connected component; then\ngo in the next maximum unvisited post-time vertex\n\n\n\nthm. (no cycles implies no zero-degree vertex) Let G be a directed graph. If every vertex of G has one or more outgoing edges, the graph is cyclic.\nProof. graph theory - Having No Directed Cycles Guarantees a Vertex of Zero Outdegree - Mathematics Stack Exchange "},"Disenchantment-(Sociology)":{"title":"Disenchantment (Sociology)","links":[],"tags":["Sociology"],"content":""},"Disjoint-Set":{"title":"Disjoint Set","links":["Tree","Ackerman-Function"],"tags":["Computing/Data-Structures"],"content":"Data structure containing multiple sets.\n\nStored as array and Tree\nMake set: Initialize singleton (=single element) set for each element\nUnion set\n\nFind owner of both element\nMake tree with one as parent\n\n\nFind owner of elem\n\nPath compression: once you find something, directly attach all nodes of the path to that root\n⇒ with path compression, a sequence of n union-find operations areΘ(n⋅α(n)) where α(n) is the minimum t such that the Ackerman Function A(t,t)≥n"},"Distribution-(Math)":{"title":"Distribution (Math)","links":["Moment-(Probability)","Joint-Distributions","Conditional-Distribution","Uniform-Distribution","Bernouilli-Distribution","Gamma-Distribution","Exponential-Distribution","Normal-Distribution","Student's-T-Distribution","Poisson-Distribution","Binomial-Distribution","Multinomial-Distribution","Hypergeometric-Distribution"],"tags":["Math/Probability"],"content":"def. A distribution gives comprehensive information about an experiment. A distribution can be a table showing the probabilities of all outcomes, or a probability density function.\n\n\n                  \n                  Moment Generating Function\n                  \n                \n\ndef. let Ω be an outcome space. All functions P(x) satisfying the following criteria are probability distributions.\nCriteria:\n\nP(∅)=0 and P(Ω)=1\nFor all A⊆Ω, 0≤P(A)≤1\nIf A and B are disjoint then P(A∪B)=P(A)+P(B)\n\nThe 3rd condition can also be generalized for any distributions:\n\nIf A1​,…,An​ are pairwise disjoint, then P(A1​∪⋯∪An​)=P(A1​)+…+P(An​).\n\nRemark. The distribution P(A)=#Ω#A​ for countable, discrete outcome spaces follow the above axiom.\nFor a random variable X∼f(h) where h is the height of some population, the probability that a&lt;X&lt;b is the shaded area:\n\nCalculated by: P(a&lt;X&lt;b)=∫ab​f(h)dh\nProbability Mass Function §\ndef. Probability Mass Function. For a discrete random variable X, the probability mass function is the function that gies the probability of all values of X.\npmf(x)=P(X=x)\nthm. Addition Rule for Random Variables. For a discrete random variable X, the following is true:\nP(a≤X≤b)=k=a∑b​P(X=k)=k=a∑b​pmf(k)\nProbability Density Function §\ndef. Probility Density Function. fX​(x) is a probility density function of random variable X iff:\n\n∀x,fX​(x)≥0\n∀x,∫−∞∞​fX​(x)dx=1\nP(a≤X≤b)=∫ab​fX​(x)dx\n\ndef. Cumilitave Density Function. FX​(t) is the cumulative density function of random variable X:\nFX​(t):=∫−∞t​fX​(x)dx≡P(X≤t)\nif and only if:\n\n∀t,FX​(t)≥0\nFX​(t) is never decreasing over its domain\nlimt→∞​FX​(t)=1\n\n\n\n                  \n                  Info \n                  \n                \n\nRelationship between fX​(x) and FX​(x) is a derivate and antiderivative.\n\n\n&gt;fX​(x)derivativeanti-derivative​FX​(x)&gt;\n\nNote that when you get FX​(x) you will get a integration constant C. You can get rid of this by using the property limt→∞​FX​(t)=1.\n\n\nJoint Distributions\nConditional Distribution\n\nList of Common Distributions §\n\nUniform Distribution\nBernouilli Distribution\nGamma Distribution\nExponential Distribution\nNormal Distribution\nStudent’s T-Distribution\nPoisson Distribution\nBinomial Distribution\nMultinomial Distribution\nHypergeometric Distribution\n"},"Dividend-Discount-Model":{"title":"Dividend Discount Model","links":["Equity","Interest-Rate"],"tags":["Economics/Finance"],"content":"Dividend at the end of i-th period: Di​=(1+kg​)iD0​\n\nAssumption in this model is that dividend will grow at rate g per year\n\nPresent value of Equity\n\nr is the required rate\nSn​ is the stock price when you sell it (end of period n)\nR:=1+kr​1+kg​​\n\nS0​S∞​​=(1+kr​)D1​​+(1+kr​)2D2​​+⋯+yn​Sn​​=D1​(1+kr​)+⋯=i=1∑∞​(1+kr​)i(1+kg​)iD0​​=D0​i=1∑∞​Ri=D0​⋅1−RR​=D0​kr​−kg​1+kg​​=kr​−kg​D1​​​selling after period nholding stock foreverwhen r&gt;g⟹R&lt;1 (geometric sum) DDM​​"},"Document-Type-Definition":{"title":"Document Type Definition","links":["Extensible-Markup-Language"],"tags":["Computing/Data-Science"],"content":"A schema definition language for XML\nDeclaration\n\nElement &lt;&gt;\n\nElement Declaration §\n&lt;!ELEMENT book (title, author*, publisher?, year?, section*)&gt;\n\nthe element book must contain the following child elements, in order:\n\none title\nzero or more authors\nzero or one publisher\nzero or one year\nzero or more sections\n\n\n\n&lt;!ELEMENT p (#PCDATA | p | ul | dl | table | h1 | h2 | h3)* &gt;\n\nelement p may contain:\n\n{dtd}#PCDATA pure text only (no child elements)\np elements\nul elements\netc.\n* → and zero or more repetitions of them.\n\n\n\nAttribute Declaration §\n&lt;!ATTLIST img\n   src    CDATA          #REQUIRED\n   id     ID             #IMPLIED\n   sort   CDATA          #FIXED &quot;true&quot;\n   print  (yes | no)     &quot;yes&quot;\n&gt;\n\nType is…\n\nID: is the unique id of that element.\n\n! ID is globally unique, regardless of attribute name, element name, etc.\nSee xml - DTD - uniqueness of ID attributes - Stack Overflow\n\n\nIDREF: reference of some id\nCDATA: any string\n(val1|…): can only be either of these values.\n\n\nValue is…\n\n#REQUIRED: necessary\n#IMPLIED: optional\n#FIXED: constant\n⇒ “Final column &quot;default&quot;: default value of that attribute\n\n\n"},"Dominant-Strategy-Equilibrium":{"title":"Dominant Strategy Equilibrium","links":[],"tags":["Economics/Game-Theory"],"content":"Strategy A is dominant over strategy B iff all payoffs regardless of other’s actions.\n\nSometimes no strategy might be dominant\n"},"Dummy-Variables":{"title":"Dummy Variables","links":["Multivariate-Ordinary-Least-Squares-Regression"],"tags":["Math/Statistics"],"content":"Motivation. A treatment vs. Non-treatment group in medicine. A dummy variable in this case would be:\nTreatmenti​={10​if treated groupif control group​\nAnd regression:\nYi​=β0​+β1​Treatmenti​+ϵi​\n\nFitted value:\n\nY^i​=β^​0​ if Treatmenti​=0 ← “average of treatment group”\nY^i​=β^​0​+β^​1​ if Treatmenti​=1 ← “average of control group”\nThis is equivalent to performing a Difference of Means Test, which measures the difference of the mean of each group.\n\nThe difference of means is β^​1​ (=the treatment effect) \n\n\n\n\n\n\nMultiplicative Dummy Variables §\nMotivation. Gender pay gap is not an “additive” factor but a “multiplicative” factor, as it men are rewarded more for per unit experience than women. In this case we can model:\nSalaryi​=β0​+β1​Experiencei​+ Additive dummy β2​isMale​​+ Multiplicative dummy β3​isMalei​×Experience​​+ϵi​\nRegression Plot: \n\nβ1​^​ is the slope of non-treament group (=women)\nβ1​^​+β3​^​ is the slope of the treatment group (=men)\nTherefore Graph shape is:\n\nβ1​^​ determines the slope (upwards/downwards sloping)\nβ3​^​ will increase or decrease the magnitude of the slope in the treatment (men) group depending on its sign\n\n\n\n\n! When using multiplicative dummy regression, in order to test for statistical significance of isMale you must test additive and multiplicative term parameters together, i.e. using F-statistics to test H0​:β2​^​=β3​^​=0\n\nThe total effect of isMale is β2​+β3​, and the statistical significance is tested by H0​:β2​^​+β3​^​=0\nIn general, if just one of either β2​^​ or β3​^​ is individually significant the total effect is significant\n\n\n\nApplications of Dummy Variables §\nDummy Independent Variables in Multivariate OLS §\nModel:\nYi​=β0​+β1​Dummyi​+β2​Xi​+ϵi​\n→ β1​ will tell us the difference between the groups Dummy=1 and Dummy=0\nExample. How much does playing in home field create an advantage for the home team?\nGoal Differentiali​=β0​+β1​isHomei​+β2​Opponent Qualityi​+ϵ\n\nAdding Opponent Quality will control for opponent quality\nRegression Table \nRegression Plot \nAt Opponent Quality=0, the difference in means in β^​1​\nβ^​2​ is irrelevant for this difference of means test. But if you want, it’s the slope between opponent quality and goal differential\n\nCategorical to Dummy Variables. §\nMotivation. This is useful when you have categories like “1 indicates a person is from the Northeast, 2 from the Midwest, 3 from the South, and 4 from the West,” and you want a difference of means test for all of them combined. You can’t simply use the 1,2,3,4 as values because it “location” is not a quantity but a category. You use binary dummy variables instead:\nWagesi​=β0​+β1​isNE+β2​isMW+β3​isSouth+ϵ\n\nWe don’t need a isWest because false on all three dummy variables indicates obviously that that datapoint is from the west. (i.e. avoid the “dummy perfect multicollinearity trap” where e.g. isMale is perfectly negatively correlated with isFemale).\n\ni.e.g there cannot be any two categorical dummy variables which are both one.\n\n\nInterpretation, e.g. for β1​: the unit rise in wages by moving from reference (West) to NE.\nRegression Table: \n\nObserve that all columns are symmetric. The model we used above is column (a), but since there are many other ways to define categories as dummy variables, we are showing that they all have the same result.\n&amp; Each row represents “how better it is from the reference”; in (a), “how better is it from the west”.\n\n\n\nWe now have an example from the textbook that incorporates these.\nExample. Taxation vs Male suffrage, Year, War mobilization and location\nTax=β0​+β1​mSuffragei​+β2​Yeari​+β3​Wari​+ Location Dummies β4​isEuropei​+β5​isAsia+β6​isOceaniai​​​\n\nRegression Table: \n\n“Bivariate” column is the stupid model.\n(a) only adds the year. Year is significant endogenous factor\n(b) adds war. War is also a significant endogenous factor\n(c) includes location dummies, with North America as the reference (therefore model doesn’t include isNA).\n\n\n"},"Dynamic-Programming":{"title":"Dynamic Programming","links":["Knapsack-Problem","Path-Alignment","Matrix-Chain-Multiplication","Longest-Palindromic-Substring","Tree","Vertex-Cover-and-Minimum-Independent-Set"],"tags":["Computing/Algorithms"],"content":"\n\n                  \n                  The name &quot;Dynamic Programming&quot; is a misnomer. \n                  \n                \n\n\n\n                  \n                  Motto of Dynamic Programming \n                  \n                \n\n\nDynamic Programming (DP) is a algorithm design paradigm that includes the following features:\n\nAlgorithms are recursive\nThere are overlapping recursive subproblems\nMemoization can be used to reduce re-solving overlapping subproblems\nYou can also write an iterative solution which can be easier to analyze\n\n\n\n                  \n                  Think of DP as a &quot;filling in a table&quot; problem \n                  \n                \n\nTo devise a DP solution to a problem, you must\n\nUnderstand which subproblems overlap\n\nRecursion trees help in identifying overlap\n\n\nUnderstand the subproblem dependencies (which subproblems to solve first)\n\nHow to break ties (e.g. minimum or maximum of two dependencies)\n\n\nChoose which order to iterate\n\nIterate from i=0..n,j=0..n\n\nKnapsack Problem, Path Alignment\n\n\nIterate for increasing gap=j−i from gap=0..n\n\nMatrix Chain Multiplication, Longest Palindromic Substring\n\n\nIterate on all notes of a Tree, where the data is stored in the tree node itself.\n\nVertex Cover and Minimum Independent Set\n\n\n\n\n"},"E-Bike-Conversion":{"title":"E-Bike Conversion","links":[],"tags":[],"content":"Legal designation §\n\nClass 1: ~20mph pedal assist only\nClass 2: ~20mph throttle okay\nClass 3: ~28mph pedal assist only\n\nLocation of drivetrain §\n\nFront\nMid\nRear\n"},"Econ-201-Intermediate-Microeconomics-I":{"title":"Econ 201 Intermediate Microeconomics I","links":["Budget-Lines","Utility-Function","Lagrangian-Optimization","Income-Effect-(IE)","Substitution-Effect-(SE)","Types-of-Demand-Curves-(MicroEcon)","Production-Function","Profit-Maximization","Price-Controls","International-Trade","Game-Theory","Monopoly","Bertrand-Price-Competition","Monopolistic-Competition","Cartels-and-Collusion","Types-of-Goods-(Economics)"],"tags":["Courses"],"content":"\n\n                  \n                  Now, go and make the world a better place. \n                  \n                \n\nConsumer Theory §\n\nBudget Lines\nIndifference Curve\nLagrangian Optimization\nIncome Effect (IE)\nSubstitution Effect (SE)\nTypes of Demand Curves (MicroEcon)\n\nTheory of the Firm §\n\nProduction Function\nProfit Maximization\n\nMarket Distortion §\n\nPrice Controls\nInternational Trade\n\nMarket Power §\n\nGame Theory\nMonopoly\nBertrand Price Competition\nMonopolistic Competition\nCartels and Collusion\nPublic Good\n"},"Econ-204-Econometrics":{"title":"Econ 204 Econometrics","links":["Econometrics","Ordinary-Least-Squares-Regression","Confidence-Intervals-and-Hypothesis-Testing-in-OLS-Linear-Regression","Multivariate-Ordinary-Least-Squares-Regression","Omitted-Variables","Post-Treatment-Variable","Nonlinear-Models","Dummy-Variables","Fixed-Effects-Model-for-Panel-Data","Difference-In-Difference","Two-Stage-Lease-Squares-Regression","Simultaneous-Equation-Model","Controlled-Experiments","Natural-Experiments","Regression-Discontinuity"],"tags":["Courses"],"content":"\n\n                  \n                  Quote \n                  \n                \nCorrelation doesn’t imply Causation\n\n\nEconometrics\nOrdinary Least Squares Regression\n\nConfidence Intervals and Hypothesis Testing in OLS Linear Regression\nProblem: Autocorrelation? → Lagged Variables\nProblem: Heteroskedatisticy? → robust standard errors\n\n\n\nDifferent Models §\n\nMultivariate Ordinary Least Squares Regression\n\nProblem: Omitted Variables → Include the variables\nProblem: Post-Treatment Variable\n\nMediator Bias → Your choice on what you want\nConfounder Bias → Your choice on what you want\n\n\n\n\nNonlinear Models\n\n→ Choose the best one\n\n\n\nTechniques §\n\nDummy Variables\n\nMultiplicative Dummies\nCategories\nDifference of Means test\n\n\nFixed Effects Model for Panel Data\n\nProblem: Endogeneity due to time-dependent or panel (=group)-dependent factors\nLSDV approach\nDe-meaned approach\nDifference In Difference\n\n\nTwo-Stage Lease Squares Regression and Instrumental Variables\n\nSimultaneous Equation Model\n\n\n\nDesigning and Analyzing Experiments §\n\nControlled Experiments\n\nNatural Experiments\n\n\nRegression Discontinuity\n\nEndogeneity fight Summary:\n\nSoak-up\n\nOther causality? → Multivariate Ordinary Least Squares Regression\nMediator/Confounder? → Post-Treatment Variable\nTime/Group-dependent? → Fixed Effects Model for Panel Data\n\n\nIsolate/Create the exogenous variation\n\nIsolation? → Two-Stage Lease Squares Regression and IV\nRandomization? → Controlled Experiments\n\n\nRegression Discontinuity\n"},"Econ-205-Intermediate-Microeconomics-II":{"title":"Econ 205 Intermediate Microeconomics II","links":["Monotonic-Transformation","Lagrangian-Optimization","Rationality-(Economics)","Utility-Function","Budget-Lines","Utility-Maximization","Utility-Maximization-with-Endowments","Uncompensated-Demand-curve","Indirect-Utility-Function","Expenditure-Minimization","Marginal-Willingness-to-Pay","Expenditure-Function","Map-of-Microeconomic-Optimization","Profit-Maximization","Input-Demand-and-Output-Supply","Profit-Function","Cost-Minimization","Monopoly","Microeconomic-Market-Equilibrium","Edgeworth-Box","Pareto-Efficiency","Compensating-and-Equivalent-Variation","Game-Theory","Dominant-Strategy-Equilibrium","Equilibria-in-Game-Theory","Oligopoly","Bertrand-Price-Competition","Cornout-Quantity-Competition"],"tags":["Courses"],"content":"Background Knowledge §\n\nMonotonic Transformation\nLagrangian Optimization\nRational Taste\n\nConsumer Choice Theory §\n\nUtility Function\nBudget Constraints\nUtility Maximization\n\nUtility Maximization with Endowments\nOrdinary Demand\nIndirect Utility Function\n\n\nExpenditure Minimization\n\nHicksian Demand Curve\nExpenditure Function\n\n\n&amp; See: Map of Microeconomic Optimization\n\nTheory of the Firm §\n\nProfit Maximization\n\nInput Demand and Output Supply\nProfit Function\n\n\nCost Minimization\n\nConstrained Constrainted Input Demand\n\n\nMonopoly\n\nEquilibrium Theory §\n\nMicroeconomic Market Equilibrium\nTrade between individuals\n\nEdgeworth Box\nPareto Efficiency\nCompensating and Equivalent Variation\n\n\n\nGame Theory §\n\nGame Theory\n\nDominant Strategy Equilibrium\nEquilibria in Game Theory\nSubgame Perfect Nash Equilibirum\nRepeated Game\n\n\nOligopoly Games\n\nBertrand Price Competition\nCornout Quantity Competition\n\n\n"},"Econ-210-Macroeconomics":{"title":"Econ 210 Macroeconomics","links":["Efficient-Market-Hypothesis","Gross-Domestic-Product","Growth-Rate-Calculations","Unemployment","Inflation","Agents-of-the-Macro-Economy","Circular-Flow-of-Income","General-Equilibirum-Theory","Macroeconomic-General-Equilibrium-(One-period)","Household-Intertemporal-Consumption-Only-Optimization","Intertemporal-Consumption-Leisure-Optimization-(Full-General-Equilibrium)","Household-Social-Security-Consumption-Only-Optimization","Economic-Growth","Malthusian-Growth","Solow-Growth-Model","Human-Capital-Accumulation-&-Growth","Money-(Disambiguation)","Role-of-Money","Money-(Medium-of-Exchange)","Monetary-Policy","Real-Business-Cycle-Model","Coordination-Failure-Business-Cycle-Model","New-Keynesian-Business-Cycles-Model","Philips-Curve","Taxation","Laffer-Curve","Beveridge-Curve","Homogenous-Function"],"tags":["Courses"],"content":"Intuition. Macroeconomics studies Aggregate Pheonomena of: GDP / Employment / Investment / Inflation &amp; Money / LR growth / Investment, Govn’t Expenditure, Exports.\nHistorically, it began formally from the 20th century after the Great Depression; before that it ways just microeconomics. (Keynes &amp; Hayaek)\ndef. Static vs Dynamic models.\n\nStatic: comparing at the same point in time (=‘comparative static’) ¶Comparative statics, like an exogenous increase in demand causing QS​ to increase\nDynamic: comparing at different points in time\ndef. Economic Time of Short Run (SR), Medium Run (MR), and Long Run (LR) are distinguished by what the agents are allowed to do. ¶in SR, firms are allowed to change prices, but not leave; in the LR only firms are allowed to leave.\n\ndef. Real vs. Nominal.\n\nReal: real things as units (cars, hours, gallons), or base-year currency\nNominal: values in currency at a certain year\nThe real values are the actual use-value. Nominal takes into account exchange-value, which complicates things. (This is assuming the Efficient Market Hypothesis i.e. markets find the correct use-value)\n\nMacroeconomic Indicators §\n\nGDP &amp; Growth Rate Calculations\nUnemployment\nInflation\n\nEquilibirum &amp; Optimization §\n\nAgents of the Macro Economy\nCircular Flow of Income\nGeneral Equilibirum Theory\nMacroeconomic General Equilibrium (One-period)\n\nHousehold Intertemporal Consumption Only Optimization\nIntertemporal Consumption-Leisure Optimization (Full General Equilibrium)\n\n\nHousehold Social Security Consumption-Only Optimization\n\nGrowth §\n\nEconomic Growth\nMalthusian Growth ← pre-industrial\nSmitian Growth ← colonialism as division of labor\nSolow Growth Model ← industrial, technical innovation\nHuman Capital Accumulation &amp; Growth ← post-industrial\nSchumpeterian Growth ← tech bro growth (innovation, creative destruction)\n\nMoney §\n\nMoney (Disambiguation)\n\nRole of Money\nMoney (Medium of Exchange)\nMonetary Policy\n\n\n\nBusiness Cycles §\n\n! In this section, assume r≈R, i.e. i≈0\nReal Business Cycle Model\nCoordination Failure Business Cycle Model\nNew Keynesian Business Cycles Model\n\nEtc. §\n\nPhilips Curve\nTaxation\nLaffer Curve\nBeveridge Curve\nHomogenous Degrees\n"},"Econ-361-Distributive-Justice":{"title":"Econ 361 Distributive Justice","links":["assets/361-Midterm---Tit-for-Tat,-Footbinding.pdf","assets/361-Final---Game-Theory,-Financial-Markets,-Distributive-Justice.pdf","Robert-Axelrod","Robert-Frank","Milton-Friedman","assets/Week-1---Nash-Equilibria.pdf","assets/Week-2---Axelrod.pdf","assets/Week-3---Robert-Frank.pdf","assets/Week-4---Mackie-and-Footbinding.pdf","assets/Week-5---Hayek-and-Price-Mechanism.pdf","assets/Week-6---Monopoly.pdf","Elinor-Ostrom","assets/Week-7---Hampton-and-Public-Goods.pdf","assets/Week-8---Ostrom-and-Resource-Regimes.pdf","Michael-Munger","Joseph-Stiglitz","Thomas-Picketty","assets/Week-9---Munger-and-Crony-Capitalism.pdf","assets/Week-10---Stiglitz-and-Inequality-in-the-U.S..pdf","assets/Week-11---Efficiency-vs.-Growth.pdf","Amartya-Sen","John-Stuart-Mill","John-Rawls","Robert-Nozick","assets/Week-12---J.S.-Mill-and-Utilitarianism.pdf","assets/Week-13---Rawls-and-Maximin-Principle.pdf"],"tags":["Courses"],"content":"\n\n                  \n                  Remember, there are still people starving as we speak. \n                  \n                \n\nSubmissions §\n\n361 Midterm - Tit-for-Tat, Footbinding.pdf\n361 Final - Game Theory, Financial Markets, Distributive Justice.pdf\n\nApplying Game Theory §\n\nRobert Axelrod - The Evolution of Cooperation\nRobert Frank - Passions within Reason\nJ.L. Mackie - Ending Foot Binding and Infibulation: A Conventional Account\nFriedrich Hayek - The Social Uses of Information\nMichael Rosenberg - Pure Theory of Labor Market Monopsony\nMilton Friedman - Externality\nWriting Assignments\n\nWeek 1 - Nash Equilibria.pdf\nWeek 2 - Axelrod.pdf\nWeek 3 - Robert Frank.pdf\nWeek 4 - Mackie and Footbinding.pdf\nWeek 5 - Hayek and Price Mechanism.pdf\nWeek 6 - Monopoly.pdf\n\n\n\nMarket Failures §\n\nJonathan Anomaly - Public Goods and Government Action\nJean Hampton - Free Rider Problems in the Production of Collective Goods\nElinor Ostrom - Collective Action and the Evolution of Social Norms\nRobert Frank - Darwinian Economy, Chapters 1-3\nWriting Assignments\n\nWeek 7 - Hampton and Public Goods.pdf\nWeek 8 - Ostrom and Resource Regimes.pdf\n\n\n\nMeasuring Amount of Market Failure §\n\nMichael Munger and Mario Villarreal-Diaz - The Road to Crony Capitalism, Independent Review\nJoseph Stiglitz - The Price of Inequality, Chapter 2: Rent Seeking and Chapter 3\nThomas Picketty\n\nRobert D. Kirkby - Summary of Picketty\nPicketty Explained Blog - Summary of Capital in the Twenty-First Century\n\n\nWriting Assignments\n\nWeek 9 - Munger and Crony Capitalism.pdf\nWeek 10 - Stiglitz and Inequality in the U.S..pdf\nWeek 11 - Efficiency vs. Growth.pdf\n\n\n\nSolutions to Market Failure §\n\nAmartya Sen - Equality of What?\nJohn Stuart Mill - Utilitarianism, Chapter 1, Chapter 2 (pp. 4-11), Chapters 3, 4\nJohn Rawls - A Theory of Justice\nRobert Nozick - Anarchy, State, and Utopia\nDerek Parfit - Equality and Priority\nWriting Assignment\n\nWeek 12 - J.S. Mill and Utilitarianism.pdf\nWeek 13 - Rawls and Maximin Principle.pdf\n\n\n"},"Econ-371-Assets-&-Risk,-Finance":{"title":"Econ 371 Assets & Risk, Finance","links":["Present-Value-Calculations","Bonds-(Finance)","Equity","Valuing-a-Firm","Comparable-Company-(Comps)-Analysis","Free-Cashflow","Measuring-Security-Performance","Price-to-Earnings-Ratio","Portfolio-Theory-(Markowitz)","CAPM-Model","Futures","Short-selling","Trade-Through","Interest-Rate","Derivatives-(Finance)","Contagion-(Finance)"],"tags":["Courses"],"content":"\nPresent Value Calculations\nBonds (Finance)\nEquity\nValuing a Firm\n\nComparable Company (Comps) Analysis\nFree Cashflow Analysis (Discounted Cash Flow Analysis)\n\nMeasuring Security Performance\nPrice to Earnings Ratio\n\n\n\n\nPortfolio Theory (Markowitz)\n\nCAPM Model\n\n\nFutures\n\nGlossary\n\nShort-selling\nTrade-Through\nInterest Rate\nDerivatives (Finance)\nContagion (Finance)\n\n\n\n\n"},"Econometrics":{"title":"Econometrics","links":["Correlation"],"tags":["Economics"],"content":"\nEstablishing causality\n\ndoes “policy/behavior/program → outcome”?\nX→Y?\n\n\nChallenges\n\nIs it random noise?\n\nRemoving outliers\n\n\nAre there exogenous factors?\n\n\n\nX is endogenous if X correlates with ϵ (error)—non-causal, lurking variable\n\n\n\n\nX is exogenous if X doesn’t correlation with ϵ—likely causal\n\n\n\n\n\n\nRegression Analysis: Yi​=β0​+β1​Xi​+ϵi​\n\nβ0​: intercept. Less interesting\nβ1​: slope. More Interesting\nX: independent variable\nY: dependeng variable\n\n\n! be careful…\n\nCorrelation = Causation\nA higher slope doesn’t mean higher correlation; a higher Correlation coefficient (ρ=σX​σY​σXY​​) does mean higher correlation.\nStatistical Significance = Real-life Effects\n\n\n"},"Economic-Democracy":{"title":"Economic Democracy","links":["(Book)-The-Case-for-Economic-Democracy","(Economist)-Andrew-Cumbers"],"tags":["Economics","Philosophy/Political-Philosophy"],"content":"\n(Book) The Case for Economic Democracy\n(Economist) Andrew Cumbers\n(DevonThink) Economic Democracy and Labor Productivity\n"},"Economic-Growth":{"title":"Economic Growth","links":["Utility-Maximization","Cobb-Douglas-Utility","Unconstrained-Maximization"],"tags":["Economics/Macro-Economics"],"content":"\nAssumptions §\n\nass. Investments are equal to savings, i.e. all savings are invested. I=S\nass. Y=F(Kt​,Nt​,zt​)\n\nIntertemporal Utility Maximization §\nass. Households perform intertemporal utility maximization. Similar to Utility Maximization but according to the following general form of intertemporal utility maximization:\nc1​,c2​max​c1α​c21−α​,∀t Ct​+St​=wt​Nt​+rt​Kt​\nThe equality is the intertemporal budget constraint that holds from t=t0​,t1​,….\nLimit the time horizon to just two periods, t and t+1. Then:\n\nCapital stock is initially Kt​=0\nNo savings at second (=last) period St+1​=0\nThen the two constraints:\n\n\nCt​+St​=wt​Nt​\nCt+1​=wt+1​Nt+1​+(1+r)St​\nEquating for St​ we get:\n\n NPV (1+r)1​​​ Savings carried over (Ct+1​−wt+1​Nt+1​)​​= Save at t wt​Nt​−Ct​​​\nThis is the intertemporal budget constraint. Now solving for Ct+1​ the maximization problem for the above Cobb-Douglas Utility function to get Unconstrained Maximization:\nCt​max​Ctα​[ appreciated savings (1+r)wt​Nt​−(1+r)Ct​​​+ income wt+1​Nt+1​​​]1−α​ Ct+1​ ​\nwe get the first order condition (by optimizing the log of the utility function and getting first derivative):\nα(Yt​+1+rYt+1​​)=Ct​\nwhere Yτ​=wτ​Nτ​\nCapital Accumulation §\nass. Capital stock is acculated and depreciated via the following equation:\nKt1​​= Capital stock at t Kt​​​(1−δ depreciation )+ investment at t It​​​\nThen consider It​=St​=sYt​ where s is the savings rate. Then\nKt+1​Nt​Kt+1​​kt+1​Nt​Nt+1​​​=(1−δ)Kt​=(1−δ)Kt​+szNt​f(kt​)=(1−δ)kt​+szf(kt​)=(1−δ)kt​+szf(kt​)​+szF(Kt​,Nt​)​=Nt​(Nt​1​F(Kt​,Nt​))=Nt​(f(kt​,1))(1) CRS(2) Intensity​\nthm. Capital Accumulation in intensive form (=per person):\n future stock kt+1​​​(1+η)= leftover (1−δ)kt​​​+ inflow szf(k)​​\n\nkt​:=Nt​Kt​​ i.e. capital intensity, in (2) Intensity\nη:=Nt​Nt+1​−Nt​​ i.e. population growth rate\nf(kτ​) is the individual production function, i.e. per worker (Nτ​=1)\n(1) CRS works because of assumption of constant returns to scale: δF(Kt​,Nt​)=F(δKt​,δNt​)\n\nImplications §\n\nCapital accumulation cannot explain per capital GDP growth\n\n…due to diminishing MPL​,MPK​ in F(⋅)\n\n\nExplains why per capita GDP converges to a certain point (intersection point in right graph)\nExpalins why total GDP grows as population increases, but also that population growth does not cause per capital GDP growth\n\nSteady State Capital §\nWe consider the steady state point, the intersection point in the right graph. We assume:\n\nk^:=kt​kt+1​−kt​​ i.e. capital growth rate\nzf(kt​):=zkα i.e. Cobb-Douglas production function\n\nk^​:=kt​−δkt​+szf(kt​)​=−δ+sktα−1​​​\nAt steady state k^stst​=0. Thus:\nδkstst​​=skststα−1​=(szδ​)1/α−1​​"},"Economics-Course-Requirements-(4-or-3-left)":{"title":"Economics Course Requirements (4 or 3 left)","links":["Econ-205-Intermediate-Microeconomics-II"],"tags":["Courses"],"content":"B.S. in Economics, Finance Concentration\nFrequently Asked Questions: New Econometrics Sequence\nCourses\nMath Requirement :luc_check_circle: §\nCompletion of a higher-level math course (MATH 212 or higher) demonstrates proficiency in lower-level math courses; therefore, lower-level requirements may be waived for students who have successfully completed higher-level math courses.\n\nMATH 111L Laboratory Calculus I OR MATH 105L Laboratory Calculus and Functions I AND MATH 106L Laboratory Calculus Functions II\nMATH 122 Introductory Calculus II OR MATH 112L Laboratory Calculus II OR MATH 122L Introductory Calculus II with Applications\nMATH 202 Multivariable Calculus for Economics or MATH 212 Multivariable Calculus OR MATH 222 Advanced Multivariable Calculus OR any higher-level math course with MATH 212 as prerequisite.\n\nCore Economics Courses :luc_check_circle: §\n\nECON 101D Economics Principles\n\nSKIPPED, Grades transferred from AP\n\n\nEconometrics (see FAQs for more information about this new sequence)\n\nSKIPPED: ECON 104D Statistical Foundations of Econometrics and Data Science\nECON 204D Econometrics Data Science (must be taken before senior year)\nIll be taking STAT 230 → STAT 432 → Econ 204D\n\n\nMicroeconomics\n\nECON 201D Intermediate Microeconomics I\n==Econ 205 Intermediate Microeconomics IID== Intermediate Microeconomics II [2]\n\n\nMacroeconomics\n\nECON 210D Intermediate Macroeconomics\n\n\n\nElectives (2 or 3 left) §\nNo Concentration (1 left) §\n\nFive Economics General Electives at the 300or 400-level, of which one must be a 300level and one must be a 400-level\n\nECON 361 PPE Distributive Justice\nECON 372 Asset &amp; Risk\nECON 673 Mathematical Finance\nECON 565 Algorithmic Game Theory\nECON 386 PPE Capstone?\n(ECON 500-549 may only be counted toward the major with approval from the director of undergraduate studies))\n\n\n\nGeneral Restrictions §\n\nA maximum of two economics transfer credits will be accepted toward the major. This applied to courses taken in the United States and to study abroad courses. One exception if the London School of Economics full-year (fall and spring) program, from which a maximum of four courses may be counted toward the major.\n\nEffective for courses taken after the Spring 2018 semester, we will no longer accept transfer credits for the following courses: ECON 204, ECON 205, ECON 208, and ECON 210. Courses that are part of “Duke In …” programs count as Duke courses and not transfer courses (please note that a few “Duke In …” programs are hybrids in which some courses count as Duke courses but students may also take transfer courses at the foreign institution). Also, inter-institutional courses are not considered transfer courses, nor are pre-matriculation credits. If you have questions about whether a course taken away from Duke would be considered a transfer course, please consult the director of undergraduate studies or associate director of undergraduate studies before taking the course.\n\n\nDukeHub enforces prerequisites for many Economics courses.\n1 Students with credit for both AP Macro and AP Micro (4 or higher) may receive credit for ECON 101. To receive credit for ECON 101 using an international standardized exam, please visit the Trinity College policy for qualifying scores.\n2 Prerequisites are enforced for ECON 205D. They include ECON 201, and either MATH 202 or MATH 212 or MATH 222.\n3 Students are limited to counting a maximum of TWO Fintech courses toward the finance concentration requirements.\n"},"Edgeworth-Box":{"title":"Edgeworth Box","links":["Pareto-Efficiency","Uncompensated-Demand-curve","Budget-Lines"],"tags":["Economics/Game-Theory"],"content":"\nContract Curve is the set of pareto efficient points in the edgeworth box.\n\nExample. The black curve in (b) is the contract curve. \nThe gradient of the two curves must be equal in every point of the contract curve.\nThe Core is the set of Pareto efficient allocations that make both parties better off than their endowments. (The highlighted section inside the green is the core.) \n\n\nEquilibrium is when the exchange rate of goods (i.e. the price ratio) is set in such a way that trade occurs and there is market clearance (i.e. Qdemand​=Qsupply​).\n\nIt must be pareto efficient.\nExample of a Disequilibirum. Here. price is set to 1 orange=1 banana. Blue player will move E1​→A, and red E2​→B. But those two points do not meet.\nExample of Equilibirum. Here, price is set to 1.5 orange=1 banana. Blue moves E→C, and Pink moves E→C and therefore demand for bananas is equal to supply of bananas, and demand for oranges are equal to supply of oranges.\nSolving for the equilibirum:\n\nGet the Ordinary Demand for good x1​ from both parties.\n\nin this case, the Budget Constraint is I=p1​e1​+p2​e2​\n\n\nSet the equilibirum condition Qdemand​=Qsupply​\n\nx1A​+x1B​=e1A​+e1B​\nDemand by A+Demand by B=Endowment of A+Endowment of B\n\n\nSolving this for the price ratio p2​p1​​ will yield the relative price (=price that will lead to a trade that results in an equilibirum).\n\n\n\n\n"},"Edmund-Husserl":{"title":"Edmund Husserl","links":["Phenomenology"],"tags":["People","Philosophy"],"content":"See Phenomenology."},"Efficiency-(Statistics)":{"title":"Efficiency (Statistics)","links":["Cramer-Rao-Lower-Bound-(CRLB)"],"tags":["Math/Statistics"],"content":"Cramer-Rao Lower Bound (CRLB)"},"Efficient-Market-Hypothesis":{"title":"Efficient Market Hypothesis","links":[],"tags":["Economics/Finance"],"content":""},"Eigenvector-and-Eigenvalue":{"title":"Eigenvector and Eigenvalue","links":[],"tags":["Math/Linear-Algebra"],"content":"\nMotivation 1. Say a n×n matrix describes a 3-D rotation. Then what is the axis of rotation?\nMotivation 2. An n×n matrix describes a linear transformation, but looking at each column only shows you the action of the basis vectors. What is a better way to characterize this linear transformation?\n\ndef. Eigenvector and eigenvalue. For matrix A, if applying Av simply scales v into say, λ scalar v, then v is an eigenvector of matrix A. In other words:\nAv also eigenvector = eigenvalue λ​​ eigenvector v​​\nthm. For diagonal matrix D, all basis vectors are eigenvectors."},"Elasticity-of-Substitution":{"title":"Elasticity of Substitution","links":["Rationality-(Economics)","Marginal-Rate-of-Substitution-(MRS)","Utility-Function","Cobb-Douglas-Utility-(Two-Goods)"],"tags":["Economics/Micro-Economics"],"content":"\n\n                  \n                  tastes.)\n                  \n                \n\ndef. Elasticity of Substitution. The EoS of good x1​ of x2​ asks “how much % x2​ are you willing to give up to get one more % of x1​“?\nσ2,1​:=% change in x2% change in x1​=∣%ΔMRS1,2​%Δx1​x2​​​∣≡dln∣MRS1,2​∣dlnx1​x2​​​\n⇒ MRS is the Marginal Rate of Substitution (MRS) of good\nHigher elasticity of substitution (EoS) means the following things:\n\nUtility Function has less curvature\nx1​ and x2​ are better substitutes\n\nTaste and elasticity\n\nHomothetic tastes when MRS depends on x1​x2​​ alone:\n\ni.e. MRS(tx1​,tx2​)=tkMRS(x1​,x2​)\n\n\n\nElasticity &amp; Indifference Curve Shapes §\n\nCobb-Douglas EOS §\n\nu(x1​,x2​)=x1α​x21−α​\nMRS=−1−αα​⋅x1​x2​​\n\n⇒ Then we observe:\n​x1​x2​​=MRS1,2​α1−α​⟹dln∣MRS∣d​lnx2​x1​​=dln∣MRS∣d​(lnMRS+ln1−αα​)⟹dln∣MRS∣dlnx2​x1​​​=1 =σ2,1​​​\n⇒ We also can see that it is homogeneous and thus homothetic:"},"Emergent-Phenomena":{"title":"Emergent Phenomena","links":[],"tags":["Philosophy"],"content":"\n\n                  \n                  E pluribus unum\n                  \n                \n\nExample. Heat is Kinetic Energy Fundamentally heat is the average kinetic energy of the molecules. But this isn’t enough to understand the feeling of heat we experience at the macroscopic level. It also isn’t enough to describe the emotions that heat produces, the significance it had on human evolution, then culture, etc.\n\n\n                  \n                  A difference in quantity (at a certain point) makes a difference in quality. \n                  \n                \n"},"Endurance":{"title":"Endurance","links":[],"tags":[],"content":"Endurance is the capacity to be under stress, and"},"English-190FS-Renaissance-Literature":{"title":"English 190FS Renaissance Literature","links":["assets/190---Analysis-on-Doctor-Faustus.pdf"],"tags":["Courses"],"content":"Writing Assignments §\n\n190 - Analysis on Doctor Faustus.pdf\n\nBook Readings §\n\nShakespeare, The Merchant of Venice\nChristopher Marlowe, Doctor Faustus\nBen Jonson, The Alchemist\nMargaret Cavendish, The Blazing World\nGirolamo Cardano: The Book on Games of Chance\n\nAdditional Readings §\n\nColumbus, The Four Voyages\nJournals and Other Documents on the Life and Voyages of Christopher Columbus\nGeorge Sandy, Relation of a Journey (selections)\nGalileo Galilei, Dialogue Concerning the Two Chief World Systems (selections)\n"},"Entity-Relationship-Model":{"title":"Entity-Relationship Model","links":["Relational-Algebra"],"tags":["Computing/Data-Science"],"content":"\n\n                  \n                  Abstract \n                  \n                \nWe’re trying to model real-world data subject to real-world constraints. The entity relationship model is a simple method to do so.\nDesign goals:\n\nRedundancy is bad\nAttributes cannot be lists!\nTrade-off between capturing more constrainsts vs. simplicity\n\n→ Common Sense &gt; capturing all constraints\n\n\nDon’t introduce nonexistent constraints\n\n\nComponents of an E/R Model\n\nEntity—with attributes\n\nKey attributes are underlined\n⇒ This is NOT like database keys, in the case that weak entities require both its keys and its supporting entity’s keys to identify it uniquely.\n\n\nRelationship—with attributes\n\nRelationship attributes are not the same as entity attributes: they cannot be duplicated.\ne.g. StopsAt cannot have multiple times for each Train-Station pair: \n\n\n\nComplex Relationships §\n(DevonThink) 3. E/R Design\nMultiplicity §\n\nMany-to-Many: Most types of relations \nMany-to-One\n\nhalf-circle arrow: exactly one \nNormal arrow: one or zero\n\n\nOne-to-One\n\nRoles in a Relationship §\n\nif you’re taking two items from the same entity, the roles need to be clarified. \n\nN-ary Relationships §\n\none-to-one (=binary) relationships is not the norm.\nyou can break it up into many relationships…\n\n…but it’s hard and may require new relations, entities, etc.\n\n\ne.g. a isMemberOf relationship can have multiple members, but only (zero or) one initiator for a group—member pair\n\n\nWeak Entity &amp; Supporting Relationships (=Heirarchical relationships) §\n\nmore data is needed to uniquely identify the thing\ne.g. a room number in a building needs the building identification to uniquely identify it\n\n\nISA Relationship (Subclassing) §\n\nliterally “A is-a B” relationship\nInherits the attributes (with the key), relationships\n\n\nTranslating into Relational Model §\n(DevonThink) 4. E/R Translation\n\nEntity Set → Table. Train,LocalTrain, etc.\n\nAttribute → Column of Table. number, engineer\nKey attribute → Key number\n\n\nRelationship → Table of keys (on both entities) and attributes if necessary.\nWeak Entity Set → Table, but including all the keys up the hierarchy\n\nExpressTrainStopsAtStation includes keys of ExpressTrains\nYou need the keys of all supporting entities to uniquely identify it\n\n\nIsA Relationship → Table, but we can choose how to translate it:\n\nEntity-in-all-superclass-and-specific-class\n\nScattered List\n\n\nEntity-only-in-specific-class\n\nScattered List\n\n\nAll-entities-in-one-table\n\nGoogle BigTable’s approach\nNULL when a subclass’s attribute doesn’t apply\nResults in a sparse table (we have good ways to store this these days!)\n\n\n\n\n\n\n\n                  \n                  Full example\n                  \n                \n\n"},"Epistemic-Natural-Selection":{"title":"Epistemic Natural Selection","links":["Ultra-Complex-System","Social-Institution","Attention-is-Currency"],"tags":["Philosophy/Epistemology"],"content":"hyp. Natural selection (NS) is not just in biology but is present in every situation where:\n\nEntities (species) interact with an environment (ecosystem)\nEntities undergo random changes in design (genome)\nEnvironment selects for “fit” entities and kills the remainder\nThis applies also to ideas and knowledge:\ndef. Epistemic Natural Selection (ENS) is the occurrence of natural selection in the human pursuit of knowledge. This occurs in two layers:\nENS-1 Evolution of Ideas in Social Institutions of Knowledge (SIK)\n\nIdeas interact with SIKs—temple, church, university, etc.\nIdeas undergo random mutation\nSIKs selects for entities that aligns with their ideology/rules\n\n\nENS-2 Evolution of SIKs itself. Compatible with Imre Lakatos’s Hard Core paradigm of scienctific research\n\nSIKs generate research programs (RPs) ← defined by Lakatos\nSIKs undergo random mutation due to academics leaving, etc.\n(Instrumentalism) Usefulness selects for SIKs that generate more useful ideas\nhyp. Natural Selection is the only way we know to build proper Ultra Complex Systems. Complexity of systems will always increase, but our thought/brain capacity is limited by physics, chemistry and biology. Thus to build objects more complex than our brains we must rely on natural selection instead.\n\n\n\nhyp. Diversity in thought, isolation and Individualism helps with ENS. The more diverse the humans producing ideas are, the more isolated they are from each other the larger the breadth of the epistemic random search program.\n\n“Diversity is our power” is actually true. Different ways of thought, cultures, etc. will bring different ideas, some of which will survive. If we all thought the same, the culture may face an easy extinction-level event.\n\nApplications §\n\nhyp. Internet Selects for Attention (Attention is Currency). This Video Will Make You Angry - YouTube. On the internet (∈ SIK) ideas undergo ENS-1 with the internet selects for attention. Attention is chosen as the selective mechanism due to underlying capitalsm.\nNon-sciences also undergo ENS. Literature selects for works that are useful in the current social context, and ones that assist in perpetuating the current culture. Art selects for works that are “useful” in expression current human emotion and that resonates.\nTo use this fact to direct evolution, we need SIKs that will remain useful, and that will also produce positive externalities in the process.\ndir. Society must encourage epidemic diversity and SIK diversity to promote better natural selection of ideas.\n"},"Equilibria-in-Game-Theory":{"title":"Equilibria in Game Theory","links":["Strategy","Sperner's-Lemma","Brouwer's-Fixed-Point-Theorem","Linear-Programming","Integer-Linear-Programming","Signaling-Game"],"tags":["Economics/Game-Theory"],"content":"Intuition. Hierarchy of Games in game theory.\n\nPure Strategy §\ndef. Pure Strategy Nash Equilibrium is a set of strategies (one for each player) which is the best response strategy of each other’s move; i.e. you can’t deviate without destabilizing the equilibrium.\n∀i∈Players​, si​=argmins∈Si​​c1​(s,s−i​​)\nwhere:\n\ns−i​: strategies of all other players (excluding i-th player)\ns: strategy for i-th player\nS: strategy set for i-th player\nci​(s): cost function for i-th player\n\nHow to Find PNE §\n\nIn Simultaenous Games: Use the Corner method\nIn Sequential Games:\n\nMake the Decision Tree into a Payoff Matrix, and use the Corner method\nJust test out every combination of Strategy\n\n\n\nalg. Corner Method to find NE. Finding the Nash Equilibrium in a payoff matrix.\n\nFor each player:\nFor each other player’s move; highlight the line of the best response\n→ When both the left and top lines are highlighted that is NE.\n\nExample. Driving left or Right:\n\n\nSubgame Perfect Nash Equilibrium (for Pure Strategy) §\ndef. A Non-credible threat is when the follower in a sequential move game says: “If I can’t win, I’ll take you down no matter how much it costs to me.”\n\nIn the above game, (R,R) is an example of a non-credible threat because when Player 1 chooses L, Player 2’s best response is L—but Player 2 threatens to go R. This is because they want the best possible outcome for them, R,(R,R)\n\ndef. Subgame Perfect Nash Equilibria (SPNE) are NE where the follower will only choose strategies that are best for them, and can’t threaten the leader beforehand with non-credible threats.\nalg. Backwards Induction. To find the SPNE of a game, use Backwards Induction:\n\nDetermine the last player’s best strategy\nThe second-last player knows what the last player will do. Then determine what the second-last player will do.\nContinue solving until the first player.\n\n\nExample. Caterpillar game unraveling\nSPNE={(d,d),(d,d)}, payoff is (1,0) which is a lot worse off that global optimal.\n\nMixed Strategy Nash Equilibrium §\ndef. Mixed Strategy Nash Equilibrium\n\nEach player i chooses a distribution σi​ over strategy set Si​ ← σi​ is public knowledge\nAt runtime, each player chooses strategy si​∼σi​ independent of other players\n(s1​,…,sn​) is a Mixed Nash Equilibrium if:\n\n∀i​∀si​∈Si​,  Expected cost after switchingEs−i​∼σ−i​​[Ci​(si′​,s−i​​)]​​≥Expected cost of potential equal. strat.Es−i​∼σ−i​​[Ci​(si​,s−i​​)]​​\nthm. (Existence of MNE, John Nash) Any game with finite players and finite set of strategies for each player has an MNE.\nProof. The proof as three parts: the proof of Sperner’s Lemma, using it to prove Brouwer’s Fixed Point Theorem, and then reducing a mixed Nash Equillibrium to a fixed point in Brouwer’s Fixed Point theorem. We will only reveal the last part in this proof.\nConsider the following:\n\nAgent i has the strategy set Si​. There are k agents. Their payoff is πi​(Si​,S−i​).\nΔi​:=(p1​,p2​,…) is the mixed strategy set of agent i with each corresponding to the probability of each strategy in Si​.\n\nThis makes it a unit simplex in R∣Si​∣\n\n\nC:=Δ1​×⋯×Δk​ is all possible combinations of mixed strategy of all agents.\n\nC is a compact set, and we want to apply Brouwer’s theorem on this space\nLet function fi​: combination of mixed strat. C​​↦ i’s mixed strat. Δi​​​ be:\n\n\n\nfi​(xi​,x−i​​):= maximize, return mixed strategy x′ argx′∈Δi​max​​​ ...that maximizes expected payoff E(πi​(S))​​− regularizaer (don’t move fast) ∣∣xi′​−xi​∣∣22​​​\nThen, let function f:C↦C be defined f(x)=(f1​(x1​),f2​(x2​),…,fk​(xk​)). Then f is continuous, thus there exists a point in which f(x∗)=x∗. This means at x∗ there is no more expected payoff maximization that can be made by any individual, i.e. MNE.\n■\nalg. Computation of MNE using Linear Programming.\n\nThe payoff matrix denotes what the column player (=minimizing player) gives to row player (=maximizing player)\n\n\nRow player thinks that column player will minimize exchange (stackleberg solution)\nColumn player thinks that row player will maximize exchange (stackleberg solution)\nStrategy tuple that satisfies both conditions is an MNE.\n\nCorrelated Nash Equilibrium (CNE) §\ndef. Correlated Nash Equilibrium\n\n3rd party computes joint distribution of all parties S1​×S2​×⋯×Sn​, to joint distribution σ ← this joint distribution is public knowledge.\nAt runtime, 3rd party draws strategy vector s=(s1​,s2​,…,sn​)∼σ\n3rd party tells player i to play si​\nPlayer i computes the posterior distribution of what other players will do: σ−i​\n(s1​,…,sn​) is a Correlated Nash Equilibrium iff:\n\n∀i​∀si​∈Si​,  Expected cost after switchingEs−i​∼σ−i​​[Ci​(si′​,s−i​​)]​​≥Expected cost of potential equal. strat.Es−i​∼σ−i​​[Ci​(si​,s−i​​)]​​\nalg. CNE Computation Using Integer Linear Programming:\n\nExample of Correlated Nash Equilibirum: Traffic Lights game:\n\nImperfect Information §\nBaysian Nash Equilibrium §\nIn a\n\n\nmixed strategy (players each have probability of action)\n\n\none-shot game\ndef. Baysian Nash Equilibirum is a set of strategies that are mutual best responses given a certain probability of other’s actions\n\n\nFor a BNE to exist, each person’s strategy must assign probabilities such that the opponent is neutral to each of their options. (if not, it means there is a dominant strategy for the opponent)\n\n\ndef. Perfect Baysian Equilibirum. In a mixed strategy, finitely repeated game, with each player having a belief a Perfect Baysian Equilibirum is one that\n\nthe player’s beliefs are consistent with strategy (rational)\nis a BNE for each subgame\n→ Signaling Game is a good example\n"},"Equity":{"title":"Equity","links":["Private-Equity-Firms","Dividend-Discount-Model","Comparable-Company-(Comps)-Analysis"],"tags":["Economics/Finance"],"content":"Terminology §\n\nEquity. Equity is a partial ownership of an asset (mostly companies)\nEquity in a mortgage\n\nEquity=Net Market Value−Unpaid Balance\n\n\nEquity in a company.\n\nEquity index\n\n:= a weighted average of equities\ne.g. S&amp;P 500, Dow Jones Ind. Avg., Nasdaq, Rusells 2000 ← different indices with different weights\nownership rights (like voting in decisions) are relinquished\n\n\nPrivate Equity Firms\nEarnings Season\n\nFirms post earnings every quarter. Earnings ≡ Income ≡ Profit.\n\n\n\n\n\nYou buy an equity because you expect\n\nIt pays regular dividends\nYou expect its price to go up ← S&amp;P has always beaten any other asset class\n\n\n\nDividend Discount Model. Pricing an equity.\n\n\nPrincipal Shareholder: the majority shareholder who controls the firms fully\n\n\nMinority Interest (=non-controlling interest):= The interests of a non-controlling stockholder.\n\nThis interest is not interest rate. It’s about having a stake.\n\n\n\nFully Dilluted Shares:= total sum of outstanding shares.\n\n\nCommon Stock: may not pay dividend; but if it does, you expect it to grow\n\n\nPreferred Stock: No voting rights, but during bankruptcy you can liquidate first.\n\n\nComparable Company (Comps) Analysis. Which equity to buy?\n\n\nDividend Yield\n\n\nDividend Yield=Share priceDividend per share​"},"Essentialism":{"title":"Essentialism","links":[],"tags":["Philosophy"],"content":""},"Estimator":{"title":"Estimator","links":["Bias-(Statistics)","Variance","Efficiency-(Statistics)","Consistency","Mean-Squared-Error","Likelihood-(Statistics)"],"tags":["Math/Statistics"],"content":"def. Statistic. let X1​,…,Xn​ observable random variables [= data of an experiment]. then statistic T is:\nT=δ(X1​,...,Xn​)\n\nδ:{X1​,…,Xn}→R i.e. is a real-valued function\nδ cannot contain unknown variables\n\ndef. Estimator [= point estimate] is a statistic used to estimate the parameter of the model we think the data is showing. Note the following notation convention:\nθ^=δ(X1​,...,Xn​)\n\nAssume X as an r.v. of an experiment, whose model includes parameter θ.\nTo estimate ground truth parameter θ, we can use an estimator r.v. θ^(X)\nA specific estimate for a particular observed value x1​ is denoted θ^(x1​)\nAn estimator has to be a function of known variables &amp; data only.\nVar(θ^)=E[(θ^−Eθ^)2], NOT E[(θ^−θ)2] ← This is MSE\n\nHow Good is Your Estimator? §\n\nAccuracy is higher. Increased as Bias (Statistics) is decreased\nPrecision is higher. Increased as Variance V(θ^) is decreased\nEfficiency (Statistics) is higher. If estimators θ^1​,θ^2​ have the same accuracy, but V(θ^1​)&lt;V[θ^2​] then the former is more efficient than the latter.\nConsistency.\nMean Squared Error is lower.\nLikelihood (Statistics) is higher.\n→ In general, making sure to reduce bias of estimators is important. Note that:\n\n\nIf you can write down what the bias is mathematically [= characterize the bias], then you can make a new estimator that doesn’t have the bias.\nBias usually decreases as the data points increase\n\n\n\n                  \n                  Example \n                  \n                \nlet X1​,…,Xn​∼iidN(μ,σ2) and\nlet estimator θ^:=∑in​ai​Xi​ where\n\na1​,…,an​ are weights that sum to 1. [= weighted average]\nθ^ is estimating μ. σ is known.\n\n\n\n\nHow accurate is θ^? [=what is the bias?]\n\n\n\n&gt;&gt;E[θ^]&gt;&gt;&gt;&gt;B(θ^)​=E[∑ai​Xi​]=∑E[ai​Xi​]=∑ai​E[Xi​]=μ=0&gt;​&gt;&gt;&gt;\n\n\n\nHow precise is θ^? What are the best ai​,…,an​?\n\n\n\n&gt;&gt;V(θ^)&gt;&gt;&gt;&gt;&gt;​=V[∑ai​Xi​]=V[a1​X1​]+⋯V[an​Xn​]=a12​V[X1​]+⋯an2​V[Xn​]=a12​σ2+⋯an2​σ2=σ2∑ai2​&gt;&gt;​&gt;&gt;&gt;\n\n→ Thus V[θ^] is minimized when ai​=n1​.\n"},"Euclid's-Algorithm-for-Greatest-Common-Denominator-(GCD)":{"title":"Euclid's Algorithm for Greatest Common Denominator (GCD)","links":[],"tags":["Computing/Algorithms","Math"],"content":"Algorithm §\nEuclidean algorithm - Wikiwand\n\nThe Euclidean algorithm is based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number.\nFor example, 21 is the GCD of 252 and 105 (as 252 = 21 × 12 and 105 = 21 × 5), and the same number 21 is also the GCD of 105 and 252 − 105 = 147. Since this replacement reduces the larger of the two numbers, repeating this process gives successively smaller pairs of numbers until the two numbers become equal. When that occurs, they are the GCD of the original two numbers.\n\ndef gcd(a, b):\n\twhile b != 0:\n\t\ta, b = b, a % b\n\t\t# guaranteed a &gt; b, \n\t\t# because remainder is smaller than divisor\n\t\t# needs to be one-line because the new a and b musn&#039;t\n\t\t# interfere for the old a and b\n\treturn a\n\nIn this implementation, the algorithm iteratively calculates the remainder when dividing the larger number a by the smaller number b. It then updates a to be b and b to be the remainder. This process continues until b becomes zero, indicating that a is the GCD of the original two numbers.\n\n\nThe implementation has a time complexity of O(log(min(a, b))) as it iteratively reduces the numbers by taking their remainders.\n\nTime Complexity §\nStack Overflow\n\nSo the number of iterations is linear in the number of input digits. For numbers that fit into cpu registers, it’s reasonable to model the iterations as taking constant time and pretend that the total running time of the gcd is linear.\nOf course, if you’re dealing with big integers, you must account for the fact that the modulus operations within each iteration don’t have a constant cost. Roughly speaking, the total asymptotic runtime is going to be n^2 times a polylogarithmic factor. Something like n^2 lg(n) 2^O(log* n). The polylogarithmic factor can be avoided by instead using a binary gcd.\n"},"Everything-is-a-File":{"title":"Everything is a File","links":[],"tags":["Computing"],"content":"Everything is a File (Wikipedia)\n\nEverything is a file is an idea that Unix, and its derivatives, handle input/output to and from resources such as documents, hard-drives, modems, keyboards, printers and even some inter-process and network communications as simple streams of bytes exposed through the filesystem name space. […] The advantage of this approach is that the same set of tools, utilities and APIs can be used on a wide range of resources and a number of file types.\n"},"Evolutionary-Game-Theory":{"title":"Evolutionary Game Theory","links":["Social-Construct","Trust-Escrow-System","Prestige","Eastern-vs-Western-Work-Ethic","Social-Institution","Tribalism","Manners","Humanism","Compromise","Communication","Reciprocity","Cheat-Codes-of-Life","Capital-(Marxism)","Artificial-Needs","Society's-Scripts","정-(人情)","It's-hard-to-meet-good-people","한"],"tags":["Economics/Game-Theory"],"content":"Analysis of human behavior and Social Constructs using the tools of game theory.\n\nTrust Escrow System\nPrestige\nRice farming and sibling marriage ban to collectivism and individualism\n\nList of Evolved Emotional Principles §\n\nBoth\n\nTrust Escrow System\nEastern vs Western Work Ethic\nSocial Institution\nTribalism\nManners\n\n\nIndividualism\n\nHumanism\nCompromise\nCommunication\nReciprocity\nPrestige\n\nCheat Codes of Life\n\n\nCapital (Marxism)\n\nArtificial Needs\n\n\n\n\nCollectivism\n\nSociety’s Scripts\n정 (人情)\nIt’s hard to meet good people\n한\n\n\n"},"Executable-and-Linkable-Format":{"title":"Executable and Linkable Format","links":[],"tags":["Computing/Operating-System"],"content":"def. Executable and Linkable Format (ELF) is a standard for the format of a binary executable file in linux.\n\n\nSections of ELF §\n\n.text: code.\n.data: initialised data.\n.rodata: initialised read-only data.\n"},"Existentialism-Paper-1-Plans":{"title":"Existentialism Paper 1 Plans","links":["Existentialism-Paper-1"],"tags":["Courses"],"content":"Existentialism Paper 1\nQ. Is Hedda Gabler an “existential” play? Discuss the pros and cons.\n\nImpossibility of expression and subjectivity, Kierkegaard\nBoredom &amp; Alienation, via the Underground man\nDesire, Life-energy and Dionysus, Hedda the bird of prey\n\nAlienation as\n\nNot misunderstanding, but “talking past each other”\nAs using one as a means to an end\n"},"Existentialism-Paper-2-Plans":{"title":"Existentialism Paper 2 Plans","links":["Existentialism-Paper-2","Reciprocity"],"tags":["Philosophy/Existentialism"],"content":"Existentialism Paper 2\nBEAUVOIR–THE SECOND SEX\n“Woman is the Other.” Analyze the passage beginning “If the female function is not enough…” and ending with “… he is the Absolute, She is the Other” (pp. 4–6). Your analysis should bring out Beauvoir’s fundamental ideas about women, philosophy, and the nature of oppression.\n\nOppression as enforcing immanence unto a subject, i.e. make them the other\n1. Hegelian dialectics: master-slave, women does not enter\n2. Other: reciprocal other/absolute other\nReciprocity and why it’s lacking in men-women\n\nEternal Feminine as the archetype “other” women are made incident onto to maintain their immanence\n\nDiscuss modern archetypes, “manic pixie dream girl,” “cool girl,” ”,” etc.\nOne is to step outside oneself and look at yourself from a male polint of view (who said this?)\n\n\n\n\n"},"Expected-Value":{"title":"Expected Value","links":[],"tags":["Math/Statistics"],"content":"Expected Value §\ndef. Expected Value. For random variable X with countably many outcomes, its expected value E(X) is defined as:\nE(X)E(X)​:=∀x∈range(X)∑​x⋅P(X):=∫−∞∞​x⋅fX​(x)dx​for discretefor continuous​​\nProperties. The following identities hold for expected values, with constant k, random variables X,Y.\n\nE(k)=k\nE(X+Y)=E(X)+E(Y) Linearity\nE(k⋅X)=k⋅E(X)\nE(X⋅Y)=∑∀z​z⋅P(X⋅Y=z)\n\nIf X⊥Y then E(X⋅Y)⇒E(X)⋅E(Y) (reverse does not hold)\n\n\nlet g be a function over range(X). Then, E(g(x))=∑∀x​g(x)⋅P(X=x) (Law of Unconscious Statistician)\n\n! E(g(x))=g(E(x))\nE(Xk)=∑∀x​xk⋅P(X=x)\n\n\n\nthm. Tail Sum Formula. when X is a non-negative discrete random variable:\nE(X)=i=0∑∞​P(X≥i)\nRemark. The Tail Sum formula is useful when the random variable is defined as the minimum or maximum of a certain set of events (e.g. minimum of multiple dice rolls, etc.)\n\n\n                  \n                  Expectation Manipulation from class:\n                  \n                \n\n\nConditional Expected Value §\nE(g(X,Y))=∬R2​g(x,y)fX,Y​(x,y)dA\ndef. Conditional Expectation. let X,Y be jointly distributed. Then the conditional expected value is defined…\n\n…over an event: E(X∣A)=∑∀A​x⋅P(X=x∣A)\n…over an event on a random variable E(X∣Y=y)=∫−∞∞​x⋅fX∣Y=y​(x) dx\n…over a random variable: E(X∣Y):=E(X∣σ(Y))=∑∀x∀y​x⋅P(X=x∣Y=y)\n\n! While expectation conditioned on an event is a value, an expectation conditioned over a random variable is another random variable\nIntuition. Think of it as “given all information by Y, what’s the new random variable?”\n\n\n\n\nE(aX+bY∣A)=a⋅E(X∣A)+b⋅E(Y∣A) linearity\nE(X)=∑∀i​E(X∣Ai​)⋅P(Ai​) where A1​,…,An​ is a partition of Ω.weighted summation\nthm. Conditional Joint Expectation. X,Y and R∈[Range(X)×Range(Y)]. Then:\n\nE(X∣x,y∈R)where fX,Y∣x,y∈R​(x,y)​=∬R​x⋅fX,Y∣x,y∈R​(x,y) dA=P(x,y∈R)fX,Y​(x,y)​​​\nthm. Calculating Expected Value from Conditional Expected Value. (Identity 2 above) let X,Y be jointly distributed. Then the expected value of X is calculated:\nE(X)E(X)​=∀y∑​E(X∣Y=y)⋅P(Y=y)=E(E(X∣Y))=∫−∞∞​E(X∣Y=y)⋅ fY​(y)⋅dy=E(E(X∣Y))​​\n\nUseful for computing E(X) when X depends on Y.\nWorks regardless of whether X,Y are random or discrete, and when mixed.\n\n…for Stochastic Calculus §\nIntuition. Conditioning is done only by two objects: a sigma algebra, or an event. Each are defined:\nE[X∣A]E[X∣F]​=∫A​XdP=⎩⎨⎧​⋮E[X∣A] for every atom A∈F⋮​​A NumberA New Random Variable​​\nIn both cases, it’s useful to think of E as the best guess.\n\n\n                  \n                  \\mathbb{E}[X|Y] on an RV, it&#039;s shorthand for \\mathbb{E}[X|\\sigma(Y)].\n                  \n                \n\ndef. Measurability. R.V. X is F-measurable iff: \nE[X∣F]=X\nIntuition. Looking at the above definition, it simply means E[X∣F] also takes the same value for each atom of F as X.\nthm. For any R.V. and filtration X,G, E[X∣G], which is a random variable, is G-measurable.\ndef. Independence. R.V. X is independent from σ-alg G iff:\nE[X∣G]=E[X]\ni.e. giving the information on G has made zero difference\nProperties.\n\nIf X is independent of H then E(X∣H)=E(X) Taking out independent factors. \nif X is F-measurable, E(XY∣F)=XE(Y∣F) Taking out what’s known\n\ne.g. E(Xt+0.2​Xt​∣F)=Xt​E(Xt+0.2​)\n\n\nTower Property: E(E(X∣F))=E(X)\n"},"Expenditure-Function":{"title":"Expenditure Function","links":["Marginal-Willingness-to-Pay","Budget-Lines","Homogenous-Function","Shepard's-Lemma"],"tags":["Economics/Micro-Economics"],"content":"def. Expenditure Function. (≈Income Function) Relates prices and a certain utility to the required income to achieve that level of utility.\nE:(p1​,p2​,uˉ)↦I\nDerivation. Substitute Hicksian Demand functions into the Budget Constraint.\nProperties.\n\nHD1 in prices (E(tp1,tp2​,uˉ)=tE(p1​,p2​,uˉ))\nIncreasing in prices (∂p1​∂E​,∂p2​∂E​≥0)\n\nequal when you’re not buying that good.\nHigher prices require more income\n\n\nIncreasing in utility (∂u∂E​&gt;0)\n\nMore utility requires more expenditure.\n\n\nUse Shepard’s Lemma in order to get back to Hicksian Demand Curve\n"},"Expenditure-Minimization":{"title":"Expenditure Minimization","links":["Monotonic-Transformation","Utility-Maximization"],"tags":["Economics/Micro-Economics"],"content":"min I=p1​x1​+p2​x2​ such that uˉ=u(x1​,x2​)\n\nYou can do a Monotonic Transformation\nUse lagrangian\ncorner solutions, etc.\nSee Utility Maximization for the reverse casew\n"},"Exponential-Distribution":{"title":"Exponential Distribution","links":[],"tags":["Math/Common-Distributions"],"content":"def. Exponential Distribution. X has exponential distribution with intensity λ:\nX∼Exp(λ)fX​(x)={λe−λx0​x&gt;0else​FX​(t)=1−e−λtE(X)=λ1​           SD(X)=λ1​​\n\nThe exponential distribution is useful when modeling wait time X in a call center, with “business” of λ (e.g. “on average λ calls per hour”).\nBe careful that λ is the number of events in timespan, not the average time it takes between events.\n"},"Exponential-Family":{"title":"Exponential Family","links":["Normal-Distribution","Exponential-Distribution","Chi-Squared","Bernouilli-Distribution","Poisson-Distribution","Binomial-Distribution","Multinomial-Distribution","Hypergeometric-Distribution"],"tags":["Math/Probability"],"content":"def. Expoential Family of Distributions is the distributions whose pdfs are in the following form:\nfX​(x∣θ)=h(x)⋅exp[η(θ)T(x)−A(η)]\nIncludes:\n\nNormal Distribution\nExponential Distribution\nChi-Squared\nBeta\nDiriochelt\nBernouilli Distribution\nPoisson Distribution\n\nAs well as:\n\nBinomial Distribution (with fixed number of trials)\nMultinomial Distribution (with fixed number of trials),\nNegative binomial (with fixed number of failures)\nGeometric (Not Hypergeometric!)\n"},"Exponential-Function":{"title":"Exponential Function","links":[],"tags":["Math"],"content":"By definition… (Taylor Approximation)\nex=x→∞lim​(1+x1​)x=e\nOr by definition, the family of functions that satisfy:\nf(x+y)=f(x)⋅f(y)\n(See Euler’s formula with introductory group theory - YouTube)\nAnother Definition:\nex=n=0∑∞​n!xn​=1+x+2x2​+…"},"Extensible-Markup-Language":{"title":"Extensible Markup Language","links":["assets/Screenshot-2023-10-24-at-18.21.58.png","XPath-and-XQuery"],"tags":["Computing/Data-Science"],"content":"\nSemi-structured data (well-structured)\n\nDocument format (well-formed)\n\n\nSelf-describing\n\n{xml}&lt;book author=&quot;C. Darwin&quot;&gt;The Origin of Species&lt;/book&gt;\n\nTag: {xml}&lt;book&gt;,&lt;/book&gt;\nAttribute: {xml}author=&quot;C. Darwin&quot;\n\nID: id is a special attribute that is unique\n\n\nNamespace: definition of your schema\n\n&lt;myNS:book xmlns:myNS=&quot;http://.../mySchema&quot;&gt; \n\t&lt;myNS:title&gt;...&lt;/myCitationStyle:title&gt;\n\t&lt;myNS:author&gt;...&lt;/myCitationStyle:author&gt;...\n&lt;/book&gt;\n\nElements: The Origin of Species\n{xml}&lt;![CDATA[Tags: &lt;book&gt;,…]]&gt; means character data (escape not required)\nTree Representation\nUse XPath and XQuery to query it\n"},"Factor-of-Production":{"title":"Factor of Production","links":[],"tags":[],"content":""},"Fair-Value-Accounting":{"title":"Fair Value Accounting","links":["Black-Scholes-Merton-Derivative-Pricing-Formula","No-Arbitrage-Condition"],"tags":["Economics/Finance"],"content":"Motivation. Consider you are a company which just did an IPO. The amount of money you get from selling shares at the IPO price, is different from the market cap. Which one should you use for your balance sheets?\ndef. Fair Value of Asset is the estimate price if you liquidated it on the market right now. For liabiity assets (¶loan) it is the estimate price for selling that loan to another company (=exit value).\nBut, not all assets are traded actively on a high-volume market, and sometimes you never know what the price of an asset is until you’ve actually sold it. So we have three tiers of defining fair value, the best to worst:\n\nLevel I (Mark-to-market) quoted prices on active market\nLevel II (Mark-to-market with objective inputs): quoted prices on active market for similar instruments, or by valuation techniques (like Black-Scholes for derivatives) from the (inputs of) active price of underlyers.\n\nThe Risk-Neutral Measure for pricing applies this principle.\n\n\nLevel III (Mark-to-market with subjective inputs): valuation techniques where inputs are not all active market prices\n"},"Fairness-(Economics)":{"title":"Fairness (Economics)","links":["Pareto-Efficiency","Utility-Function","Rationality-(Economics)","Integer-Allocation","John-Rawls","Ordinal-Allocation","Matching-Problems","Fractional-Allocation"],"tags":["Economics/Game-Theory","Economics/Micro-Economics"],"content":"Motivation. In Game Theory we attempt to formalize our intuitive notion of fairness into rigorous mathematical statements, in order to be able to prove if our allocations are fair or not. There are many different was of doing this, and we will define a few:\n\nScale Invariance (You can’t compare utilities)\nEnvy-Free\nPareto Efficiency (see linked definition.)\nProportionality\nIn these definitions, we assume there are m divisible items, and n agents. We also assume that the Utility Function of the agents are\n\n\nContinuous\nStrictly increasing across a single item, for all items\nConcave (diminishing returns)\nIn other words, the agents have Rational Taste.\n\ndef. Scale Invariace. An Allocation is fair if it only depends on the ordinal preferences of the agents, not their cardinal value. i.e., if we scale any agent’s utility μi​ by αμi​, the allocation would not change.\ndef. Envy-Free (EF). An allocation is envy free if they like their allocation rather than anybody else’s; ∀i,∀j,xi​⪰i​xj​.\ndef. Proportionality (Prop). Allocation of n1​ of each item to each agent shouldn’t be a better solution than the current allocation for every agent. i.e.\n∀i, xi​​⪰i​ Proportional allocation (n1​x1​,n1​x2​,…,n1​xn​)​​\nOther Fairness Criteria §\n\nWhen EF cannot be achieved we may attempt approximate-envy freeness EF1\nMaximin Share is another benchmark based on Rawlsian maximin principle.\nIn Ordinal Allocation methods, in a probabilistic allocation setting, we may need to use baysian versions of the above criteria.\nIn Stable Matching Problems, stability is another fairness criteria.\n\nDirectory of Allocation Methods §\nCardinal Utility Functions §\n\nFractional Allocation\n\nFischer Market (=CEEI)–SI, EF, Prop, PO\n\n\nInteger Allocation\n\nSerial Dictatorship (=Round Robin)–EF1, but not anything else\nInteger Nash Welfare–EF1, MMS-bounded\n\n\n\nOrdinal Utility Functions §\n\nProbabilistic Allocation\n\nRandom Dictatorship–DSIC\nProportional Eating–BEF, BPO\n\n\nStable Allocation (Kideney)\n\nTop Trading Cycles (TTC)–DSIC, Stable\n\n\nStable Matching (Marriage)\n\nGale-Shapley—Proposer-optimal\n\n\nOnline\n\nDeterministic Ski Rental &lt;2 even in best\nRandomized Ski Rental &lt;e−1e​\nDeterministic Whole &lt;2\nFractional Greedy (&lt;2 in example bad case)\nFractional Water-filling (2ee+1​ in example bad case)\n\n\n"},"Fast-Exponentiation":{"title":"Fast Exponentiation","links":[],"tags":["Computing/Algorithms"],"content":"Observation:\nxn={x(x2)(n−1)/2,(x2)n/2,​\\mboxifn\\mboxisodd\\mboxifn\\mboxiseven​\nThus we can achieve O(logn) multiplications."},"Federal-Communication-Commission":{"title":"Federal Communication Commission","links":[],"tags":["Computing"],"content":"A U.S. Government agency that regulart"},"Federal-Reserve":{"title":"Federal Reserve","links":["Monetary-Policy","Inflation","Securities-and-Exchange-Commission","Too-Big-To-Fail"],"tags":["Economics"],"content":"The United States’ central bank independent of the government (or the Treasury) that has the following roles:\n\n\nSetting Interest Rates: The Federal Reserve has a significant influence over the interest rates in the U.S. economy. It can adjust the Monetary Policy to control Inflation and stabilize the economy.\n\n\nManaging the Money Supply: The Fed controls the amount of money circulating in the economy through measures such as open market operations, reserve requirements for banks, and setting the discount rate [&gt;=Monetary Policy].\n\n\nRegulating Financial Markets: It supervises and regulates banks to ensure they are safe and sound. It also monitors their impact on the financial system. (This is separate from the SEC)\n\n\nActing as a Lender of Last Resort: In times of financial distress or crisis, the Federal Reserve provides funds to financially strained banks to prevent bankruptcy and protect the economy. (This causes problems of Too Big To Fail)\n\n\nMaintaining Financial Stability: The Fed works to maintain the stability of the financial system and contain systemic risk that may arise in financial markets.\n\n\nProviding Banking Services to Depository Institutions: These services include supplying the economy with fiat money (U.S. dollar), managing those finances, and processing payments.\n\n\nConducting National Monetary Policy: The Fed works towards achieving maximum employment, stable prices, and moderate long-term interest rates in the U.S. economy.\n\n\nPreventing Banking Panics: The Fed was initially created to prevent widespread panics in the banking sector. Today, it continues to act as a stabilizing force in the financial system.\n\n\nPromoting a Safe and Effective Payment System: The Federal reserve system also ensures the reliability of payment methods including checks, cash, and electronic transactions.\n\n\nKeep in mind that these roles and responsibilities may evolve over time, and additional duties may be undertaken depending on economic and financial conditions."},"Federal-Trade-Commission":{"title":"Federal Trade Commission","links":["Monopoly"],"tags":["Economics"],"content":"Consumer protection agency in the United States.\n\nMost famously, it protects against mergers &amp; acquisitions that lead to Monopoly\n"},"Fetishization":{"title":"Fetishization","links":[],"tags":["Philosophy/Marxism"],"content":"Fetishization in general refers to giving more meaning to something that does not inherently have meaning, or does not deserve that degree of meaning.\n\nFetishization of social cues: Finding meaning in social cues is a type of fetish. The actions, expressions or words have less meaning than what we feel they do.\n\nFetishization, Social Nature §\nWe see, then, that everything our analysis of the value of commodities previously told us is repeated by the linen itself, as soon as it enters into association with another commodity, the coat. Only it reveals its thoughts in a language with which it alone is familiar, the language of commodities.\nIt is thus that this value first shows itself as being, in reality, a congealed quantity of undifferentiated human labour."},"Financial-Market":{"title":"Financial Market","links":["Role-of-Money","Sovereign-Welath-Fund","Asset-Manager-(Finance)","Escrow","Fractional-Banking","Investment-Bank"],"tags":["Economics/Finance"],"content":"A financial market is a market for money.\n\nSuppliers of Money include:\n\nHigh net worth individuals\nPension Funds\nAsset Managers\nDemanders of money include:\nGovernments\nCompanies\nIndividuals (for mortgage, etc.)\n\nBanks §\nBanks facilitate this exchange between suppliers and demanders (they exist as Escrows for supply/demand, i.e underwrites or guaratees the exchange). Different types of banks make money in different ways\n\nCentral banks don’t need to make money, and has a monopoly on increasing money supply (using Fractional Banking). Not really the system pictured above.\nCommercial Banks are the banks that take deposit from households. Most banks in day-to-day life. They make money with the difference in interest between buy side and issue side (Net Interest Margin, NIMs).\nInvestment Banks underwrite bigger loans from demanders like governments and companies. This is in the form of loans, bonds, or equity.\n\nCentral Banks §\nCommercial Banks §\nInvestment Banks §\nInvestment banks are corporations in itself, and make money in the following ways:\n\nPrimary Market, fundraising. The market for money; profit is also from NIM.\n\nLoans are simple interest-bearing loans\nBonds are tradable loans.\nEquities are loans that need not be paid back, because they offer ownership\nDerivatives are tradable contracts upon any securities\n\n\nIBs also take money from fees by facilitating the process of raising money, company valuations (e.g. during IPOs).\n\n\nSecondary Market, market-making. The exchange of securities and derivatives. Investment banks are market-makers that provide liquidity a securities exchange. They’re allowed to buy low, and sell high for providing this liquidity; the difference between them is the spread.\n"},"Finite-Automata":{"title":"Finite Automata","links":["Regular-Expressions"],"tags":["Computing/Formal-Languages"],"content":"Regular Expressions\nDefinitions §\ndef. Automata is an abstract model of a computer.\ndef. Regular Language is a language that can be expressed by a FSM\ndef. Trap state is a state in which any symbol input leads to the same state.\ndef. Closure of qi​ is simply the set of states reachable from qi​ with only λ.\nDeterministic Finite Automata (DFA) §\n[= Finite State Machine]\nDFA=(Q,Σ,δ,q0​,F)\n\nQ is the set of all states\nΣ is the set of all symbols\nδ:Q×Σ→Q is a function mapping from current state to the next state\n\nδ∗(qi​,λ)=qi​ [= empty strings lead to itself]\nδ∗(q,wa)=δ∗(δ(q,w),a) where a is a single symbol [= processes only one per tick]\n\n\nq0​ is the start state (entry point)\nF is the set of final states\n\n\n\n                  \n                  a,b.\n                  \n                \n\ndef. Language. A string is accepted by a DFA when:\n\nAfter processing the string, the DFA is in a final state\nThe string is in the language\n\nThe set of all aceepted strings by a DFA is the language of the DFA:\nL(M)={w∈Σ∗∣δ∗(q0​,w)∈F}\ni.e. all the strings which, after processing it thru δ∗, it lands on a final state\nNon-deterministic Finite Automata (NFA) §\ndef. Non-deterministic Finite Automata can have multiple edges with the same labels; i.e.\nδ:Q×Σ∪λ→2Q\ni.e. from the current state, you can go to multiple states.\n\n\n                  \n                  Example Non-deterministic FSM \n                  \n                \n\n\ncorr. “there exists a walk between qi​,qj​ whose labels concatenate to w” is equivalent to:\nqj​∈δ∗(qi​,w)\nthm. All NFA can be convered into a DFA which:\nDFA={ QΣδq0​FD​}​=2Q=QD​×Σ=QD​={Q∈QD​∣∃qi​∈Q where qi​∈FN​}​\nProving Regularity after Applying Properties §\ne.g. let L be a regular language. For all strings in L replace one a with b, and let this new language L′. Is this a regular language?\npf. let Mbe a DFA for L.\n\nMake a copy M1​,M2​ and enclose it in a new machine, M′\nFor all a arcs in M1​ write a b arc to the corresponding destination state in M2​.\nThe start state for M1​ is M′ start state.\n\n\nNow, let\n\nw=uav, and w′=ubv where w,u,v∈Σ∗\nδ1∗​(q0​,u)=qi​, δ′(qi​,b)=qj​, δ2∗​(qj​,v)∈F as we outlined above\n\n\nIf w∈L\nthen\n\n→ proofs often involve duplicating the machine in some way.\nDFA Minimization §\nExample:\n\n\n"},"Fish":{"title":"Fish","links":[],"tags":["Biology"],"content":"Typology §\nExternal Morphology §\n\nFins are the important part:\n\nCaudal fin determines sustained speed of fish.\n\nSplit &amp; Pointy: slow to start; fast sustained speeds\nRound &amp; Big: instant start; slower sustained speeds\n\n\nCan be asymmetric (=heteroceral) or symmetric (=homoceral)\n\n\nPelvic fins &amp; Pectoral Fins. Auxiliary steering. Sometimes radiation like lionfish for predation/mating\nDorsal Fin determines the size of the fish ← important for predation\nOther features\n\n\nMouths: Superior (upwards-facing), Inferior (downwards-facing) or Terminal (forward-facing). Determines what the fish eats. Eyes are always forward or upward facing to avoid prey.\nOverall body types\nDeep-Body\nLie-and-wait predators\nRover Predators\nBottom Feeders\nBottom Clingers\nEel-like\n\nFeeding §\n\nCannot eat fish that are bigger than their mouths\n\nSenses §"},"Fisher-Information":{"title":"Fisher Information","links":["Maximum-Likelihood-Estimator","Expected-Value","Likelihood-(Statistics)"],"tags":["Math/Statistics"],"content":"Fisher information helps us find better estimators.\n\nCramer-Rao Lower Bound shows what the best estimators can do with their precision\nReaching the CRLB means it’s finite-sample efficient\nThe Maximum Likelihood Estimator is also a very good estimator.\n\ndef. Fisher Information is the amount of information we have about the unknown parameter. It’s the Expected Value of score.\n\nGiven fX​(x;θ), if X has a high peak we may assume that X carries a lot of information about θ.\nIf X is spread out a lot, we may assume that X carries little information. Thus:\n\nI(θ)​:=Var[s]=E[s2]−E[s]2=E[s2]​​\n\n(2) → (3) as we know that E[s2∣θgt​]=0\n\nthm. Addition of fisher information. if X1​,…,Xn​∼iidfXi​​(x1​,…,xn​∣θ), then:\nIn​=n⋅I\nthm. Score and Fisher Information. if fX​(θ;x) is twice differentiable wrt θ, and under certain regularity conditions:\nI(θ)=−E[∂θ2∂2​lnfX​(X;θ)]\n\nKnowing this we also know that E[s’]=−E[s2].\n"},"Fixed-Effects-Model-for-Panel-Data":{"title":"Fixed Effects Model for Panel Data","links":["Dummy-Variables"],"tags":["Math/Statistics"],"content":"Panel data is a dataset that have multiple observation on a same timeframe. e.g.:\n\nGDP of OECD countries from 1900 to 2000\nAnnual crime rates of 20 U.S. Cities from 1980 to 2000\nPooled Model is when we mistakenly use all of the panel data to run the regression.\nExmaple. Robberies for large cities of California. Each city has its own reasons for having relatively larger or smaller robberies unrelated to policing. You can see that there are general clumps of datapoints corresponding to cities. \nInstead, we should separate the dataset w.r.t. cities and run regression on each of them separately: \n\nOne-way Fixed Effects Model §\nMotivation. In order to use a single regression run to capture the difference in cities, we introduce a fixed effect term (αi​), a addtitive term that is same for each city (i.e., all oakland datapoints have the same αi​, all SF datapoints have the same αi​, etc.). The model is this:\nYit​=β0​+β1​X1it​+ fixed effect αi​​​+ error νit​​​\n\ni indexes on each city\nt indexes on year (time)\nWe can’t run regression on this directly because we don’t know what we should set αi​ as. We instead use either Dummy Variables approach or a De-meaned apporach. Both result in the exact same regression result.\n\nLeast-Squares Dummy Variable Fixed Effects Model (LSDV) §\nYit​=β0​+β1​X1it​+β2​D1i​+⋯+βP​DP−1,i​+νit​\nWhere D1i​,…,DP−1,i​ are the binary dummy variables. We run regression on this to get the regression table:\n\nDe-Meaned Fixed Effects Model §\nYit​−Yˉi​=β1​(Xit​−Xˉi​)+ν~it​\nBy subtracting the mean of each group from the datapoints, we “normalize” the data so that they’re all comparable.\nDiscussion §\n\nFixed effects models cannot cause bias when used in non-panel data (i.e. all αi​=0)\nWhen αi​=0, and we use a pooled model, it only causes bias when fixed effect is correlated with independent variable (=αi​ and X1i​) are correlated\n\nTwo-way Fixed effects model §\nMotivation. We may want to control for time period, e.g. for 2008 during the financial crisis, crimes in all cities probabily went up. The two-way model controls for time-based fixed effects too in our Califorinan crimes example, with the following model:\nYit​=β0​+β1​X1i​+ city fixed eff. αi​​​+ time fixed eff. τt​​​+νit​\nWe can use double-LSDV, doubkle De-meaned, or LSDV-De-meaned approach. This is not discussed in the book.\nComparing the regression results for pooled, one-way, and two-way:\n"},"Floating-Point-Numerical-Stability":{"title":"Floating Point Numerical Stability","links":[],"tags":["Computing"],"content":"Floating point instabilities in numpy1\nnp.finfo(np.float64).max\n# 1.7976931348623157e+308, largest positive number\n \nnp.finfo(np.float64).tiny\n# 2.2250738585072014e-308, smallest positive number at full precision\n \nnp.finfo(np.float64).smallest_subnormal\n# 5e-324, smallest positive number\n \nWhen we use these extremes, we get instabilities:\nnp.finfo(np.float64).max * 2\n# inf, overflow error\n \nnp.inf - np.inf\n# nan, not a number error\n \nnp.finfo(np.float64).smallest_subnormal / 2\n# 0.0, underflow error\nFootnotes §\n\n\nNumerically Stable Softmax and Cross Entropy | Jay Mody ↩\n\n\n"},"Foreign-Direct-Investment":{"title":"Foreign Direct Investment","links":[],"tags":["Economics/Finance"],"content":"When companies directly invest into factories, workforce (=capital) in the country"},"Formal-Grammar":{"title":"Formal Grammar","links":["Greibach-Normal-Form","Chompsky-Normal-Form"],"tags":["Computing/Formal-Languages"],"content":"Grammar is formally defined as a tuple:\nG=(V,T,S,P)\n\nV are variable symbols to be used in the language (they can’t be in a string, because they’re placeholders)\nT are terminal symbols to be used in the language.\nS is the start variable\nP is the production rules\n\n\n\n                  \n                  Notation for production rules: \n                  \n                \n\n\nw⇒z: w derives z\nw∗​z: w derives z in zero or more steps\nw+​z: w derives z in one or more steps\n\nNormal Forms §\n\nGreibach Normal Form\nChompsky Normal Form\n"},"Formal-Languages":{"title":"Formal Languages","links":["Formal-Grammar"],"tags":["Computing/Formal-Languages"],"content":"Notations are borrowed from [[Set Theory]]\ndef. Σ is the set of all symbols\ndef. A string is a finite set of symbols\ndef. A language is a set of strings\nString Manipulation §\n\nλ is the empty string\nconcat(w,v)=w∘v=wv\n\n…naturally, wn=w⋯w\n\n\nreverse(w)=wR\nlen(w)=∣w∣\n\n\nWe can also define languages as containing strings. Some common ones:\n\nΣ∗ ← set of strings, which are concatenated symbols 0 or more times\nΣ+← set of strings, which are concatenated symbols 1 or more times\n\n\n\n                  \n                  \\Sigma=\\{a,b\\}. Then:\n                  \n                \n\n\nΣ∗={λ,a,b,aa,ab,ba,bb,...}\nΣ+={a,b,aa,ab,ba,bb,...}\n\nYou can also use set operations on languages\n\nL1​∘L2​={xy∣x∈L1​,y∈L2​}\n\n…naturally, L1​∘L1​=L12​\n\n\nL1​ˉ​=Σ∗−L1​\n\n\nA language given a grammar is defined as a set of terminal-only strings that are derivable from the starting strings. More formally:\nL(G)={w∈T∗∣S∗​w}"},"Forwards":{"title":"Forwards","links":["Derivatives-(Finance)","No-Arbitrage"],"tags":["Economics/Finance"],"content":"Forwards are a contract that promises to buy/sell an underlying asset at a certain strike price.\nExample. A farmer wants to sell their wheat, and a mill wants to buy some wheat. The current price of wheat is \\20$\n\nThe farmer is afraid of the price of wheat going down\nThe mill is afraid of the price of wheat going down\nTherefore, they draft up a contract: one year from now, they will transact the wheat at a price, \\22$ determined right now. One year from now, they will transact. This is a forward contract.\nUnderlying asset: wheat\nt: time of contract\nT: time of execution\nForward price (=Strike price): F_{T}(t)=K=\\22$\nSpot price: S(0)=\\20$\nContract size: how many units of wheat?\n! no money/assets changes hands now. Thus the value of a portfolio with a forwards contract is zero on the day they entered.\nNow, assume you’re neither a farmer nor a mill, and you just want to bet on the price of wheat.\nShort position: think wheat price will increase. At execution, you will buy wheat from the spot market at price S(T) and give it to the counterparty at price K according to the futures contract\n\nPayoff: K−S(T)\n\n\nLong position: think wheat price will decrease. At execution, you will get price the wheat for price K and then sell at the spot market for S(T)\n\nPayoff: S(T)−K\n\n\n\nthm. (The fair price of a forwards) Under assumption of No-Arbitrage, the fair strike price of a futures contract entered at time t and executed at future date T is:\nK=FT​(t)=S(t)e(r−q)(T−t)\n\nr is the risk-free rate\nq is the dividend rate. In many cases q=0. It only applies to underlying stocks.\nOne may prove this by contradiction, by assuming it doesn’t hold, and constructing an arbitrage portfolio:\n\nthm. (Value of ongoing forward contract) For a futures contract entered at t=0, executed at T, the value of this contract at intermediate time t is:\nFT​(t)=(FT​(t)−FT​(0))e−r(T−t)\nIntuition.\n\n! Value of a forward contract is not same as the fair price.\nFT​(t) is strike price of a hypothetical contract from t to T\nFT​(0) is strike price of a hypothetical contract from 0 to T\nThe difference of these two prices, discounted at risk-free rate.\n\nProof. Let portfolios:\n\nΠA​:\n\nlong forward, enter at t=0, execute t=T, with strike price FT​(0)\nShort forward, enter at t=t, execute t=T, with strike price FT​(t)\n\n\nΠB​: Deposit cash (FT​(t)−FT​(0))e−r(T−t) at risk-free r\nThen at time T:\nΠA​(T)= long forward S(T)−FT​(0)​​+ short forward FT​(t)−S(T)​​=FT​(t)−FT​(0)\nΠB​(T)=FT​(t)−FT​(0)\nSince ΠA​(T)=ΠB​(T)⟹ΠA​(t)=ΠB​(t) by Law of One Price. Then:\n\nΠA​(t)= value of long forward position FT​(t)​​=FT​(t)−FT​(0)=ΠB​(t)\n■\nForwards on Money or Bonds §\ndef. Forward Rate Agreement (FRA). A type of forwards where you lock in not the price of a good, but the interest rate.\nExample. You plan to borrow 1Min3monthstime,andreturnitby6monthstime.Butyou′reworriedaboutinterestratesrising.Soyoulockinacontractwiththebanksaying&quot;Iwilllendyou1M in 3m, return in 6m, at interest rate r.”\ndef. Bond Forward\nExample. You plan to buy a govn’t bond principal $100 in 3 months at a fixed price. but you’re worried the price might rise. So you lock in a contract with a counterparty"},"Fractional-Allocation":{"title":"Fractional Allocation","links":["Fairness-(Economics)","Pareto-Efficiency","Convex-Programming"],"tags":["Economics/Game-Theory","Economics/Micro-Economics"],"content":"Motivation. Given the Fairness (Economics) properties, we can start allocating things to people. Different allocation methods satisfy different fairness criteria. The most important allocation method is the CEEI Allocation, which is achieved by the Fischer market.\ndef. Fischer Market. The following describes a Fischer market. There are m items and n agents. There are qj​ units of item j. \n\nAll agents given \\1$\nSet prices at a certain point, p​ (ignore how we find this for now)\nAgents fine their demand set:\n\nxi​= Utility Maximization argmaxxi​​ μi​(xi​​)​​ s.t.  Budget Constraint p​⋅x≤1​​\nTo clarify the notation:\n\nq1(i)​ is the amount of item 1 allocated to agent j\nq1​:=∑∀i​q1(i)​ which is the total amount of item allocated to\n\ndef. Competitive Equilibrium with Equal Incomes (CEEI). In a Fischer market, if we set the prices p​ just right, we will get a solution where: \n\nAll agents spend all their money ∀i,pi​​⋅xi​​=1\nAll items are fully allocated (=market clears).\n\n(q1(i)​, each item quantity to i….,qj(i)​,…​​,qm(i)​)j∑​qj(i)​​:=xi​​=1​thenequivalently​​\n\nThe above two properties implies p​⋅∑i​xi​​=∑∀j​∑∀i​qj(i)​=n\nThen, this allocation is a CEEI.\n\nthm. CEEI always exists.\n(We won’t prove.)\nthm. CEEI satisfies Scale Invariance, EF, Prop, and Pareto Efficiency. Let x1​​,…xn​ be the CEEI allocation.\nProof of SI. In the Fischer market process agents will find their demand set. Demand set doesn’t change depending on the scale of the utility; only the ordinal preferences.\nProof of EF. By contradiction. Assume i envies j s.t. xj​​≻xi​​. But everybody has the same 1thusicouldhavejustdemanded\\vec{x_{j}}instead.Thusthereisnoenvy.∗ProofofPR∗.SinceallmoneyisspentbyCEEIdefinition,\\sum_{j} p_{j}q_{j}=nwhereq_{j}istheamountofunitsofitemj.Thendividebothbyntoget\\sum_{j}p_{j} \\frac{q_{j}}{n}=1whichmeansthattheproportionalallocation\\left(\\frac{q_{1}}{n},\\dots \\frac{q_{m}}{n} \\right)isfeasible.ButallagentsdemandedsomeothersetS_{i}thusit′satleastasgoodasproportionalallocationbundle.∗ProofofParetoOptimality.∗Bycontradiction.Assumethereisanalternativeallocation\\vec{y_{1}},\\dots,\\vec{y_{n}}suchthatitisaParetoimprovementsuchthatwinnerssetWdenotes&quot;winners&quot;agentswhoimproved:\\vec{y_{i}}\\succ_{\\forall i \\in W} \\vec{x_{i}}and&quot;indifferents&quot;setIdenotesagentswhohassameutility:\\forall i \\in I,~ \\mu_{i}(\\vec{y_{i}})=\\mu_{i}(\\vec{x_{i}}).Now,forthewinners,theydidn′tdemandset\\vec{y_{i}}inCEEIallocation\\vec{x_{i}},sotheyclearlycouldn′taffordit:\\vec{p} \\cdot \\vec{y_{j}}&gt;1.NowfortheIndifferents;if\\vec{p} \\cdot \\vec{y_{i}}&lt;1thentheycouldhaveinitiallybought\\vec{x_{i}}=\\vec{y_{i}}andafewmorestuffswithleftovermoney.Buttheydidn′tdemandanythingotherthan\\vec{x_{i}},thus\\vec{p} \\cdot \\vec{y_{i}}\\geq 1$ by contradiction.\nCombining the inequality for winners and losers:\ni∈W∑​p​⋅yi​​+i∈I∑​p​⋅yi​​∀i∑​p​⋅yi​​p​⋅∀i∑​yi​​∀i∑​yi​​&gt;∀i∑​xi​​​&gt;n&gt;n&gt; b/c CEEI clears marketn=p​=(q1​,…,qm​)∀i∑​xi​​​​​​​​\nBut we also know that CEEI allocation x1​​,…,xn​​ guarantees market clearing but y1​​,…,yn​​ does not, so:\n∀i∑​yi​​≤∀i∑​xi​​\nAnd this is a contradiction. ■\nNow, for the mystical p​, we can find that by using…\ndef. Eisenberg-Gale (EG) convex program is a Convex Programming for finding CEEI in additive utility. This maximizes:\nmax∀i∑​lnμi​\nw.r.t. the following constraints:\n\nμi​=∑∀j​μi,j​⋅xi,j​ ← utility is summation utility\n∑∀i​xi,j​≤1 ← nobody overspends\nxi,j​≥0 ← no under-allocation\n\nthm. the EG Convex program finds the CEEI.\nProof. ■"},"Free-Cashflow":{"title":"Free Cashflow","links":["Income-Statement"],"tags":["Economics/Finance"],"content":"def. Cashflow Statement. A table showing the firm’s cash in and out.\n\nD&amp;A addback: you’ve already payed for the factory (un-smearing the cost)\nCapEx &amp; Salvage: money you’re paying for the capital [=capital expenditure], and cash from selling the capital\nChange in Working Capital: Recieveables &amp; Payables\n\nNet Income=Revenue−(COGS, SG&amp;A)​EBITDA​−(D&amp;A+Interest)−Taxes\nFree Cash Flow=Net Income−CapEx+D&amp;A addback±Net Recievables\nExample. The process of valuing a firm using Free Cash Flows:\n\n\nCalculate Net Income (excl. interest, tax, etc.) from Income Statement\n\nFuture cash flows are just guesses\n\n\nCalculate Free Cashflow (w. CapEx &amp; Salvage)\nDiscount future cash flows to get Net Present Value = Firm Value\nDiscount parameter is usually obtained from investments with similar risk profiles, or the WACC formula.\n\n\n\n                  \n                  Internal Rate of Return (IRR). It’s another type of interest rate.\n                  \n                \n\n\nToo much cash isn’t a good thing, because it means they’re not getting high return on capital.\n"},"Fundamental-Review-of-the-Trading-Book":{"title":"Fundamental Review of the Trading Book","links":[],"tags":[],"content":""},"Future-Value-Calculations":{"title":"Future Value Calculations","links":["Role-of-Money","Present-Value-Calculations","Interest-Rate","Banker's-Rule"],"tags":["Economics/Finance"],"content":"\n\n                  \n                  A dollar now than a dollar tomorrow. \n                  \n                \n\nHow to calculate the future value of some cash.\n\nA dollar now is worth more than a dollar tomorrow.\nCan be denoted in basis points as well.\nReverse of Present Value Calculations\n\nDefinitions §\n\nk: periods per year\nτ: duration (in years)\nn: number of compounding periods.\n\nn=τ⋅k\n\n\nF0​: Present value = Principal = Amount lent/borrowed at time t0​\n\nFτ​: Future Value = Amount at time t0​+τ\n\n\nr: annual interest\nR: Interest Rate\n\nAlso:\n\nBanker’s Rule is a common way to simplify math calculations.\nDiscount Rate=1+τ⋅r1​.\n\ndef. Simple Interest\n⇒ Only principal is invested at the end of each year\nFτ​=(1+τ⋅r)F0​\ndef. Compound Interest\nDivide into k periods per year, and compound for n periods:\nFn periods​=(1+kr​)nF0​\nDivide into k periods per year, and compound for τ years (equivalent formula):\nFτ years​=(1+kr​)τkF0​\n⇒ Conceptual Demonstration of Compounding Interest:\n\ndef. Continuous Compounding\nCompound for τ years (equivalent formula):\nF(x)=eτ⋅r⋅F0​\n⇒ You can get this formula from compound interest and setting k→∞\ndef. Fractional Compounding\nFx​=(1+kr​)xF0​ where x∈R+\n⇒ You can do things like ”5.35 months of daily compounding.”\n\n\n                  \n                  Note \n                  \n                \nFor a fixed τ, Fk​(τ) is monotonic for k. ⇒ Proof in slides. (Using binomial expansion)\n"},"Futures":{"title":"Futures","links":["Forwards","tags/Short","tags/Long"],"tags":["Economics/Finance","Short","Long"],"content":"Futures contracts are same as Forwards but is exchange-traded. The pricing formula is exactly the same, but there is less counterparty risk because the exchange verifies the counterparty.\nA future is a contract that says “A will sell B a certain amount of resource R for $X at date N in the future.” This is because A wants to hedge against depreciation of R, and B appreciation of R.\n\n\nReduces risk for both parties\n\nFarmers have long position on wheat (=worry that wheat price↑)\nGeneral Mills have short position on wheat (=worry that wheat price↓)\n\n\n#Total Sellers=#Total Buyers i.e.#Short =#Long positions (you can’t subdivide a contract)\n\nUsed for liquid (=high trade volume) assets, usually highly demanded commondities like gold, oil.\nFutures Price Formula.\n\n\n\nFt​=S⋅exp[(r+q)⋅t]\n\n\nS is the spot [current] price of the commodity\n\n\nr is the riskless rate, continuous compounding at timespan (%)\n\n\nq is the carry rate, continuous compounding at timespan (%)\n\n\nt is the time to maturity (timespan)\n\n\nFutures Price &gt; Spot price (=Contango)\n\n…unless Backwardation: Futures Price &lt; Spot Price. ← This is abnormal.\n\n\n\nFutures Price converges to Spot price as it closes into maturity\n\n\n\n\n\n                  \n                  Futures Exchanges are large corporations themselves which manage these contracts. \n                  \n                \n…examples: Chicago Mechantile Exchange (CME), Tokyo Commodities Exchange (TOCOM), etc.\n…Transactors have escrow accounts to not incur fees and ensure safety of assets\n…Ensutre parties aren’t bankrupt and can carry out the exchange\n\nRolling the Contract. You don’t want to take delivery of the contract, but the contract is expiring soon. What you do? ⇒ You roll over the contract, i.e. sell your current contract and buy another one that matures later."},"G.-W.-F.-Hegel":{"title":"G. W. F. Hegel","links":["(Book)-Phenomenology-of-Spirit","G.-W.-F.-Hegel","Torii-Moi"],"tags":["People","Philosophy","Philosophy/Epistemology"],"content":"\nGeorg Wilhelm Friedrich Hegel (; German: [ˈɡeːɔʁk ˈvɪlhɛlm ˈfʁiːdʁɪç ˈheːɡl̩]; 27 August 1770–14 November 1831) was a German philosopher and one of the most influential figures of German idealism and 19th-century philosophy.\nBorn in 1770 in Stuttgart, Holy Roman Empire, during the transitional period between the Enlightenment and the Romantic movement in the Germanic regions of Europe, Hegel lived through and was influenced by the French Revolution and the Napoleonic wars. His fame rests chiefly upon (Book) Phenomenology of Spirit, The Science of Logic, his teleological account of history, and his lectures at the University of Berlin on topics from his Encyclopedia of the Philosophical Sciences.\nGuided by the Delphic imperative to “know thyself,” Hegel presents free self-determination as the essence of humankind–a conclusion from his 1806–07 Phenomenology that he claims is further verified by the systematic account of the interdependence of logic, nature, and spirit in his later Encyclopedia. He asserts that the Logic at once preserves and overcomes the dualisms of the material and the mental–that is, it accounts for both the continuity and difference marking of the domains of nature and culture–as a metaphysically necessary and coherent “identity of identity and non-identity.”\n[Wikipedia](https://en.wikipedia.org/wiki/GG. W. F. Hegellectical Synthesis|Dialectical Synthesis]]\n\ndef. Universal (Hegel). This is the realm of the subject. They can post a thesis, and an antithesis, and perform dialectical synthesis\ndef. Absolute (Hegel). This is the realm that the subject simply observes. This is the realm of god, material things, etc. This is where no dialectics can occur.\nHegelian Ethics (via Torii Moi) §\n\n\nMen meet in society and conflict, etc.\n\nThey form a dialectic, which synthesizes to ethical principles\n\n\nWomen live in the family.\n\nThey are “generic” in that they can’t form a dialectic, they have generic anonymity\nThe best “ethics” they can do is to serve and protect the family\n\n\n\nMaster-Slave Dialectic (via Torii Moi) §\n\n\nTwo individuals (subjects) meet\nIn the “life-death” struggle, one wins out. The winner becomes the master, the loser the slave\n\n\nBoth are subjects, in that they are acknowledged their consciousness\n\nby G. W. F. Hegel"},"GPT-2-Architecture":{"title":"GPT-2 Architecture","links":["Large-Language-Model","Softmax-and-Sigmoid"],"tags":["Computing/Maching-Learning"],"content":"def. GPT-2 is OpenAI’s first public Large Language Model\n\n\n                  \n                  Note \n                  \n                \n\nGPT in 60 Lines of NumPy | Jay Mody\nBut what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning - YouTube and Attention in transformers, visually explained | Chapter 6, Deep Learning - YouTube\n\n\n\nParameters §\nHyperparameters §\n\nContext Length: number of tokens allowed in input\n\nSequence length &lt; Context Length. Sequence is the actual length of text input\n\n\nVocabulary Size: the tokenizer’s size of vocabulary\nEmbedding Dimension: the number of dimensions in the word embedding\nHead Count: number of attention heads in each attention layer\n\nLearnable Parameters §\n\nwte (Vocab Size → Embedding Dimension): lookup table from token to embedding\nwpe (Context Length → Embedding Dimension): lookup table for positional embedding\nγ,β in the Normalization layer\nw,b (N → N), (N): Weights and biases in the linear (=linear) layers\nAttention\n\nQuery, Key, and Value matricies, each (Sequence Length → Embed. Dim.):\n\n\n\nWord Embeddings §\n\nGiven a string of text, a tokenizer will split this text into chunks (tokens)\nLearnable embedding matrix wte and wpe encodes this into an embedding\nThese vectors v1​,v2​, embedded in the embedding space will:\n\n\nBe similar in meaning if cosine similarity −1&lt;∣v1​∣∣v2​∣v1​⋅v2​​&lt;1 is larger\nDifference in vectors v2​−v1​ i.e. the direction embed meaning that is the “difference in meaning” between v1​ and v2​\n\nAttention §\n\nEach token in string is multiplied with a Query matrix Q1​=WQ​⋅v1​\n\nThe resulting vector encodes the question, e.g. “are there adjectives in front of me?”\nThe resulting vector\n\n\nEach token in string is multiplied with a Key matrix K1​=WK​⋅v1​\n\nThe resulting vector encodes the answer, e.g. “I’m an adjective!”\n\n\n\nQ=​———​Q1​Q2​⋮Qn​​———​​,K=​———​K1​K2​⋮Kn​​———​​,V=​———​V1​V2​⋮Vn​​———​​\n\nThe cosine similarity of a query-key pair encodes “how well the question is answered”\n\neach column of the result passes thru a Softmax and Sigmoid function\n\n\n\nsoftmax​ nice to compute dk​​​​QKT​ dot similarity ​​​V"},"GSF-386-Politics-of-Sexuality-1":{"title":"GSF 386 Politics of Sexuality 1","links":["GSF-Essay-1---Homosexuality-and-Action.pdf","GSF-Essay-3---Desire-Classes-and-Advertising.pdf","GSF-Final---Queer-in-Japan.pdf","GSF-Final-Planning-1","(Book)-The-Straight-State","(Book)-Not-Gay","(Book)-The-History-of-Sexuality","(Book)-Kids-on-the-Street","(Book)-The-Right-to-Sex","(Book)-Stagestruck","(Movie)-Stay-on-Board---The-Leo-Baker-Story-(2022)"],"tags":["Courses"],"content":"Submissions §\n\nGSF Essay 1 - Homosexuality and Action.pdf\nGSF Essay 2 - Queer Identity in Masculine Space\nGSF Essay 3 - Desire Classes and Advertising.pdf\nGSF Final - Queer in Japan.pdf\n\nGSF Final Planning 1\n\n\n\nUnit 1: Theoretical Frameworks §\n\nDefinitions\n\n(DevonThink) Somerville, “Queer” (p. 203-207)\n(DevonThink) Johnson, “Quare” studies, or (almost) everything I know about queer studies I learned from my grandmother\n(DevonThink) Peiss, Charity Girls and City Pleasures\nListen: Baldwin reads Baldwin - Joey Part 1&amp;2 from Giovanni’s Room Giovanni’s Room “Joey” - Part 1 | Spotify\n\n\nLaw\n\n(Book) The Straight State\n\n\nInventions\n\nSelections from (Book) Not Gay\n(DevonThink) Ambrosino, The invention of ‘heterosexuality’\n(Book) The History of Sexuality (Volume 1: An Introduction: We “Other Victorians”)\n\n\nQuare\n\nFebruary 9: Selections from Black Queer Studies: A Critical Anthology\n\n\n\nUnit 2: Politics and Poetics of Remembering §\n\nSpace\n\nAriana Vigil, “Heterosexualization and the State: The Poetry of Gloria Anzaldúa”\nAudrey Yue and Helen Hok-Sze Leung, “Notes towards the queer Asian City: Singapore and Hong Kong”\nRae Garringer, “Well, We’re Fabulous and We’re Appalachians, So We’re Fabulachians”\nBecki L. Ross, “Sex and (Evacuation from) the City: The Moral and Legal Regulation of Sex Workers in Vancouver’s West End, 1975—1985” 2010\n\n\nMemory\n\nRachel Gelfand, “Between Archives” Radical History Review\nHoracio N. Roque Ramírez, “Sharing Queer Authorities: Collaborating for Transgender Latina and Gay Latino Historical Meaning”\nElizabeth Lapovsky Kennedy, “Telling Tales: Oral History and the Construction of Pre-Stonewall Lesbian History\n\n\nKinship\n\n(Book) Kids on the Street\n\n\n\nUnit 3: Futures &amp; Pasts §\n\nPolitics &amp; Desire: (Book) The Right to Sex\nPerformance: (Book) Stagestruck\n(Movie) Stay on Board - The Leo Baker Story (2022)\nHistory\nProject Week!\n"},"GSF-386-Politics-of-Sexuality":{"title":"GSF 386 Politics of Sexuality","links":["GSF-Essay-1---Homosexuality-and-Action.pdf","GSF-Essay-3---Desire-Classes-and-Advertising.pdf","GSF-Final---Queer-in-Japan.pdf","GSF-Final-Planning-1","(Book)-The-Straight-State","(Book)-Not-Gay","(Book)-The-History-of-Sexuality","(Book)-Kids-on-the-Street","(Book)-The-Right-to-Sex","(Book)-Stagestruck","(Movie)-Stay-on-Board---The-Leo-Baker-Story-(2022)"],"tags":["Courses"],"content":"Submissions §\n\nGSF Essay 1 - Homosexuality and Action.pdf\nGSF Essay 2 - Queer Identity in Masculine Space\nGSF Essay 3 - Desire Classes and Advertising.pdf\nGSF Final - Queer in Japan.pdf\n\nGSF Final Planning 1\n\n\n\nUnit 1: Theoretical Frameworks §\n\nDefinitions\n\n(DevonThink) Somerville, “Queer” (p. 203-207)\n(DevonThink) Johnson, “Quare” studies, or (almost) everything I know about queer studies I learned from my grandmother\n(DevonThink) Peiss, Charity Girls and City Pleasures\nListen: Baldwin reads Baldwin - Joey Part 1&amp;2 from Giovanni’s Room Giovanni’s Room “Joey” - Part 1 | Spotify\n\n\nLaw\n\n(Book) The Straight State\n\n\nInventions\n\nSelections from (Book) Not Gay\n(DevonThink) Ambrosino, The invention of ‘heterosexuality’\n(Book) The History of Sexuality (Volume 1: An Introduction: We “Other Victorians”)\n\n\nQuare\n\nFebruary 9: Selections from Black Queer Studies: A Critical Anthology\n\n\n\nUnit 2: Politics and Poetics of Remembering §\n\nSpace\n\nAriana Vigil, “Heterosexualization and the State: The Poetry of Gloria Anzaldúa”\nAudrey Yue and Helen Hok-Sze Leung, “Notes towards the queer Asian City: Singapore and Hong Kong”\nRae Garringer, “Well, We’re Fabulous and We’re Appalachians, So We’re Fabulachians”\nBecki L. Ross, “Sex and (Evacuation from) the City: The Moral and Legal Regulation of Sex Workers in Vancouver’s West End, 1975—1985” 2010\n\n\nMemory\n\nRachel Gelfand, “Between Archives” Radical History Review\nHoracio N. Roque Ramírez, “Sharing Queer Authorities: Collaborating for Transgender Latina and Gay Latino Historical Meaning”\nElizabeth Lapovsky Kennedy, “Telling Tales: Oral History and the Construction of Pre-Stonewall Lesbian History\n\n\nKinship\n\n(Book) Kids on the Street\n\n\n\nUnit 3: Futures &amp; Pasts §\n\nPolitics &amp; Desire: (Book) The Right to Sex\nPerformance: (Book) Stagestruck\n(Movie) Stay on Board - The Leo Baker Story (2022)\nHistory\nProject Week!\n"},"Game-Theory":{"title":"Game Theory","links":["Rationality-(Economics)","Institutional-Design","Philosophy,-Political-Science,-Economics","Random-Variable","Equilibria-in-Game-Theory","Oligopoly","Cartels-and-Collusion","Bertrand-Price-Competition","Cornout-Quantity-Competition","Battle-of-the-Sexes","Prisoner's-Dillemma","Signaling-Game","Rock-Paper-Scissors","Traffic-Routing"],"tags":["Economics/Game-Theory","Economics/Micro-Economics"],"content":"A theory of interaction between rational agents.\n\nInstitutional Design is the field in public policy &amp; PPE that makes sure game theory &amp; agent incentives are taken into account\nIt’s part of economics because it’s about rational agents interacting\n\nGame §\ndef. Game. Assuming n players:\n\nSi​: Strategy space of player i\ns=(s1​,s2​,…,sn​): which strategy combination happened.\nc1​(s),c2​(s),…,cn​(s): associated cost of s happening for each player\n\nTypes of Games §\n\nTiming:\n\nStatic (=Simultaneous)\nDynamic (=Sequential Move)\n\n\nStrategy Formulation:\n\nPure Strategy: deterministic mapping from information set to action set \nMixed Strategy: probabilistic mapping that depends on a Random Variable\n\n\nInformation availability:\n\nComplete Information\nIncomplete Information\n\n\nRepetition:\n\nOne-off,\nFinite-Repetition,\nInfinite-Repetition\n\n\nPayoff structure:\n\nZero-sum: each strategy tuple sums to zero\nPositive/Negative Sum\n\n\n\nEquilibria Types §\n→ See Equilibria in Game Theory\n\nStatic, Pure, One-off→ Nash Equilibrium\nDynamic, Pure → Subgame Perfect Nash Equilibirum\nStatic, Incomplete, (Pure or Mixed) → Baysian Nash Equilbilibrium (BNE)\nDynamic, Incomplete, (Pure or Mixed) → Subgame Perfect Baysian Nash Equilibirum (PBE)\n\nGames Modeled by Game Theory §\n\n\n                  \n                  (DevonThink) Game Theory List of Games for many types of simultaneous game&#039;s payoff matricies.\n                  \n                \n\n\nOligopoly Games\n\nCartels and Collusion\nBertrand Price Competition\nCornout Quantity Competition\n\n\nBattle of the Sexes\nPrisoner’s Dillemma\n\nPublic Good Game\n\n\nSignaling Game\nStag Hunt\nAssurance Game\nChicken Game\nRock Paper Scissors\nTraffic Lights\nTraffic Routing\n\nNotation §\n\nStrategy Tuple of player A is denoted SA=(S1A​,S2A​)\nPayoff to player A given strategy SA by A, SB by player B, etc.\n⇒ is denoted πA(SA,SB…)\n\nAlternatively, Cost is given as: CA​(SA,…)\n\n\nNash Equilibria are Tuples: NE=(S1A​,S2B​)\n\nIn a simultaneous game, all player’s strategy should be specified\nIn a sequential game, the second player (=follower)’s strategy should include the response for all of the first player (=leader)’s moves.\n\nNotation: SPE=(S1A​;S1B​,S2B​)\n\n\n\n\n\nBranches of Game Theory §\n\nExperimental game theory\nEvolutionary game theory—using game theory to explain strategies that affect natural selection\nApplied game theory\n"},"Gamma-Distribution":{"title":"Gamma Distribution","links":["Central-Limit-Theorem"],"tags":["Math/Common-Distributions"],"content":"def. Gamma Distribution. In a Poisson Point Process with intensity λ, let Wi​ be the “wait times” between the i−1 -th and i-th event. let X be the total “wait time” for r events; i.e. X=W1​+⋯+Wr​. Then X is distribued over Gamma:\nX∼Γ(r,λ)fX​(x)={λe−λx(r−1)!(λx)r−1​1​x&gt;0else​FX​(t)=1−k=0∑r−1​e−λtk!(λt)k​E(X)=λr​               SD(X)=λr​​​\n\n\n                  \n                  Tip \n                  \n                \n\nGamma Distribution also follows the Central Limit Theorem. Thus limr→∞​X∼Normal"},"Garbage-In,-Garbage-Out":{"title":"Garbage In, Garbage Out","links":["Eat"],"tags":["Computing"],"content":"\nIn computing: bad data leads to bad results regardless of how good the process is\nIn health: You are what you Eat.\n"},"Gaussian-Splatting":{"title":"Gaussian Splatting","links":["NERF"],"tags":["Computing/Graphics"],"content":"Intuitive Understanding:\n\nhttps://www.youtube.com/watch?v=VkIJbpdTujE\n\n\nImprovement over NERFs (pure neural network)\n\n3-D Gaussians (3d bell curve) is fulled\n\nStart with simple gaussian, use gradient descent to fit the scene better\nColor harmonics, blending to achieve realism\n\n\n\nRasterization is so much faster because traditional triangle rendering has so much more triangles, but GS uses much less gaussians\n\n"},"General-Equilibirum-Theory":{"title":"General Equilibirum Theory","links":["Macroeconomic-General-Equilibrium-(One-period)","Household-Intertemporal-Consumption-Only-Optimization","Intertemporal-Consumption-Leisure-Optimization-(Full-General-Equilibrium)","Real-Business-Cycle-Model","Money-(Medium-of-Exchange)","Production-Function"],"tags":["Economics/Macro-Economics"],"content":"\nMacroeconomic General Equilibrium (One-period)\nHousehold Intertemporal Consumption Only Optimization\nIntertemporal Consumption-Leisure Optimization (Full General Equilibrium)\nReal Business Cycle Model\nMoney (Medium of Exchange)\nVariables\nDynamic, τ=t0​,t1​:\n\nReal Prices: real wage w, real interest r, risk premium x\nNominal price P, nominal wage W, nominal interest R\nHousehold: labor N, utility function u(⋅), leisure ℓ—consumption C, lifetime wealth W\nFirm: Capital K, labor N, Production Function F(⋅), investment I\nGovernment G=T\nMonetary: M, Liquidity function L(r,Y)\n\n\nStatic: lifetime wealth W\nMarket Functions Dynamic, τ=t0​,t1​\nNs: w0+​,r0+​,We−\nNd: z0+​,K0+​\nYd: positive among\n\nCd: r0−​,Y0+​,We+\nId: r0−​,K−,z1+​\nT=G: but consumption less responsive than tax, so tax multiplier higher\n\n\nYs: Ns+ but not when r, Nd+ F(⋅)+,z0+​\nL: r0−​,Y0+​ (clockwise)\n"},"Generalized-Likelihood-Ratio-Test":{"title":"Generalized Likelihood Ratio Test","links":[],"tags":["Math/Statistics"],"content":"thm. Generalized Likelihood Ratio Testing (GLT). Comparing Hypotheses:\n\n\nH0​∈Θ0​\n\n\nH1​∈Θ1​\nThe Generalized Likelhood Ratio is:\n\n\nΛ~=supθ∈Θ0​​L(θ)supθ∈Θ0​∪Θ1​​L(θ)​=usuallyL(θ^MLE​)L(θ0​)​\nThe Generalized Likelihood Ratio Test (GLT) is:\nδ:{H0​H1​​else if Λ~&gt;c​\n\n\n                  \n                  \\tilde \\Lambda is often very hard to manipulate. Use Wilk’s phenomenon in order to approximate the cutoff regions.\n                  \n                \n\n\n\nthm. Wilk’s Phenomenon. For Hypotheses\n\n\nH0​∈Θ0​ ← r0​ dimentions\n\n\nH1​∈Θ1​ ← r1​ dimensions\ni.e. H0​⊂H1​ then:\n\n\n\n\n2lnΛd⟶n→∞​​χr1​−r0​2​"},"Generative-Adversarial-Network":{"title":"Generative Adversarial Network","links":[],"tags":["Computing/Maching-Learning"],"content":"def. Generative Adversarial Network (GAN). Consider Discrimminator\n\nObjective:\nV(D,G)​=Ex∼preal​(x)​[lnDθd​​(x)]A​ Good at recog. real img? ​=∫x​preal​(x)lnDθd​​(x)dx=…=∫x​preal​(x)lnDθd​​(x)​+Ez∼N(0,1)​[ln1−Dθd​​(Gθg​​(z))]​ Good at recog. fake img? ​+ change of variables ∫z​ϕ(z)ln1−Dθd​​(Gθg​​(z))dz​​+∫x​p^​G​ln1−Dθd​​(x)dx+p^​G​(x)ln1−Dθd​​(x)dx​​\nwhere\n\nDiscrimminator D and parameters θd​\n\nGenerator G and parameters θg​\n\n\nreal distribution is x∼preal​(x)\nlatent is simply N(0,1) but G transforms it into implicit distribution x∼p^​G​(x)\n\nThere’s a lot.1\nFootnotes §\n\n\n\nGitHub - hindupuravinash/the-gan-zoo: A list of all named GANs!\n\n↩\n\n\n"},"Gini-Coefficient":{"title":"Gini Coefficient","links":[],"tags":["Economics/Macro-Economics"],"content":"measures inequality."},"Git-for-Every-Minimal-Incremental-Feature":{"title":"Git for Every Minimal Incremental Feature","links":["Atomic"],"tags":["Computing"],"content":"Atomic git commits. Commit every time you implement the smallest chunked chage. You should have many commits in a single day."},"Git":{"title":"Git","links":["Atomic"],"tags":["Computing"],"content":"\nHistory should keep a record, not be pretty.\n\nWhen pulling changes from main → feature, merge instead of rebase since it leaves a history \n\n\nAtomic git commits. Commit every time you implement the smallest chunked chage. You should have many commits in a single day.\n.gitignore should be used widely\n"},"Global-Industry-Classification-Standard":{"title":"Global Industry Classification Standard","links":["Commodities","Real-Estate"],"tags":["Economics"],"content":"classified industries into:\n\nEnergy\nMaterials\nIndustrials\nConsumer Discresionary\nConsumer Staples\nHealthcare\nFinancials\nInformation Technology\nCommunication Services\nUtilities\nReal Estate\n"},"Graph":{"title":"Graph","links":["Tree","Directed-Graph","Directed-Acyclical-Graph","assets/Screenshot-2023-10-25-at-21.15.21.png","assets/Screenshot-2023-10-25-at-21.14.48.png"],"tags":["Math","Computing"],"content":"def. Graph. A Graph is defined as:\n\nOrdered pair (V,E) where\n\nV is the set of all vertices\nE={x,y∣x,y∈V and x=y}\n\n\n\nTypes of Graphs §\n\nVariations on:\n\nDirected or Undirected\nConnected or Not Connected\n\nIn directed graph, strongly connected means following direction; weakly connected means ignoring direction.\n\n\nCyclical or Acyclical\n\n\nCommon types:\n\nTree is a connected undirected acyclical graph\n\nForest is a set of trees\n\n\nDirected Graph\n\nDirected Acyclical Graph\n\n\n\n\n\nProperties of Graphs §\n\nDegree of a vertex v∈V: number of edges that connect to v\nLoop: an edge that connects a vertex to itself\nCycle: a path that starts and ends with the same vertex\n\nRepresentation of Graphs in Memory §\n\nAdjacency Matrix\n\n2D table of all nodes: Store a 1 if the edge between two nodes exists, 0 otherwise. Example\n\n\nAdjacency List\n\nArray of all vertices, which are also linked list that list all reachable neighbors. Example\n\n\n\n\nComplexities for the two types of representations: \n\nPaths §\n\nv1​→v2​→⋯→vk​\nA Simple Path is one that does not repeat vertices\n\nSpecial Variants §\n\nA Metric weighted graph is where the edge weights satisfy the triangle inequality; i.e. the vertices lie on a surface, and the edges are Euler distances of the verteces\n"},"Greedy-Algorithm":{"title":"Greedy Algorithm","links":["Proof-by-contradiction","Interval-Scheduling","Proof-by-Contradiction","Scheduling-Problem","Minimal-Spanning-Tree-Problem"],"tags":["Computing/Algorithms"],"content":"\n\n                  \n                  Be Skeptical of Greedy Algorithms \n                  \n                \n\n\n\nOften for optimization problems (minimize/maximize)\nHard to argue for correctness\n\nUse Proof by contradiction, or the exchange argument\n\n\nWorks well for approximating optimal solutions\n\nWhen the correct algorithm is intractable (O(2n)), then often the greedy solution that is O(nk) is useful (=tractable)\n\n\nGradient descent is a greedy optimization algorithm.\n\nLinear optimization algorithms are “analytical” and correct, but take lots of time.\nGradient descent algorithms (all of ML) is greedy, but not the global optimum solution. They’re “good enough” and “tractable”\n\n\n\nProof of Correctness §\n\nFeasibility: there is an algorithm that gives a solution that obeys the constraints of the problem\nOptimality: the algorithm’s solution is the best possible. Use either:\n\nProof by Contradiction\n\nLet solution by greedy algorithm solution A.\nAssume there is a more optimal solution O∗\nThen derive a contradiction\n\n\nExchange Argument\n\nLet solution by greedy algorithm solution A\nAssume there is a optimal solution O∗\nBy exchanging individual elements which don’t reduce optimality, slowly show A is at least as good as O∗\n\n\nStaying Ahead\n\nAt every stage of the greedy algorithm, show that it is at least as good as the optimal solution\n\n\n\n\n\nExamples\n\nScheduling Problem\nMinimal Spanning Tree Problem\n"},"Greeks-(Option)":{"title":"Greeks (Option)","links":["Black-Scholes-European-Option-Pricing-Formula","Normal-Distribution"],"tags":["Economics/Finance"],"content":"\nNotation is equivalent to the BSM formula.\nDerivative of CDF of Normal Distribution\n\n\nDelta §\ni.e. sensitivity to either underlier price or strike price\nΔC,S​ΔC,K​​:=∂S∂C​:=∂x∂C​∣x=St​​:=∂K∂C​​=e−q(T−t)N(d1​)=e−r(T−t)N(d2​)​​\nTable of Deltas for Call/Put options as execution time approaches.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nt→T or τ→0​At the Money CallSt​−K=0In the Money CallSt​−K&gt;0Out of the Money CallSt​−K&lt;0d1​0∞−∞N(d1​)21​10ΔC​21​10ΔP​−21​0−1For putsAt the money PutK−St​=0Out of the Money PutK−St​&lt;0In the Money PutK−St​&gt;0\nGamma §\ni.e. second derivative sensitivity to underlier price\nΓC​:=∂S2∂2V​=ΓP​=St​στ​e−qτN′(d+​)​\nTheta §\ni.e. sensitivity to time (=time decay)\nΘC​:=∂τ∂V​ΘP​:=∂τ∂V​​=−St​e−qτσN′(d+​)=−St​e−qτσN′(d+​)​+qSt​e−qτN(d+​)−qSt​e−qτN(−d+​)​−rKe−rτN(d−​)+rKe−rτN(−d−​)​​\nVega §\ni.e. sensitivity to volatility\nVC​:=∂σ∂C​\nRho §\ni.e. sensitivity to risk-free rate\nρ=∂r∂C​"},"Greibach-Normal-Form":{"title":"Greibach Normal Form","links":[],"tags":["Computing/Formal-Languages"],"content":""},"Grice's-Maxims":{"title":"Grice's Maxims","links":["Natural-Transitions","Distance-&-Expectation"],"tags":["Sociology"],"content":"Cooperative principle - Wikiwand\nWhen conversation breaks down, or it feels awkward, one of these maxims are often not upheld:\n\nMaxim of Quantity (Length &amp; Depth)\n\nContent should be informative\nContext should be no longer than necessary\n\n\nMaxim of Quality (Truth)\n\nOne should tell the truth\n\n\nMaxim of Relation (Relevance)\n\nContent should be immediately relevant information\nNatural Transitions are important\n\n\nMaxim of Manner (Clarity)\n\nOne should avoid ambiguity\nContent should be ordered\n\n\n\n\n\n                  \n                  Example \n                  \n                \nA (to passer by): I am out of gas.\nB: There is a gas station round the corner.\n\nQuantity: “around the corner” is the minimum necessary information\nQuality: there probably is a gas station\nRelation: implies that you can get gas at gas station\nManner: Nothing unambiguous about this answer\n\n\n\nPresumptions of conversation\nThe benefits of assuming cooperation\n\n\nWhy are segues important in conversations?\nBecause of System 1’s priming tendencies or cognitive ease. It seems unnecessary but has unconscious benefits."},"Gross-Domestic-Product":{"title":"Gross Domestic Product","links":["Factor-of-Production"],"tags":["Economics/Macro-Economics"],"content":"def. Gross Domestic Product (GDP). The total value of goods and services produced in the economy.\nGDP=Y​=f(l,k)=Il​+Ik​+π+tax=C+I+Gdeficit​+NE​​\n\n(1) uses the production function f(l,k) against the whole economy. ℓ,k are each labor and capital, which are Factor of Productions\n\nGDP increases theoretically by the increase in either quantity or productivity of labor or capital. Historically labor has been the driver.\n\n\n(2) uses the Income method\n(3) uses the Output method. Generally, C&gt;G&gt;I&gt;NE.\n\nDuring recessions, consumption is stable because you need to eat; Investment suffers\nGovernment spending composes around 1/3 in US, 1/2 is EU\nNE:=Export−Imports and thus Net Exports can be negative.\n\n\n&amp; (2) and (3) are same because the macroeconomic circular flow is a closed loop.\nSome economic activity is not in GDP:\n\nInformal market (big in some countries; e.g. 1/3 in Brazil)\nHome production; i.e. homemaking\n\n\n\nChanges in GDP §\n\nHistorically, the LR trend of GDP has been growing exponentially since the industrial revolution…\n\n…GDP per capita is growing too…\n…Consumption per capita is growing too!\n\nGDP per capita is production per person. But consumption per capita is more important to measure quality of life. ¶Early soviet union: high GDP, but income is all reinvested; the people suffer.\n\n\nThis growth seems to be coming from growth in labor income, while capital income is stagnant.\n\n\nShort-run fluctuations in GDP are due to the business cycle.\n\nCalculation Methods §\n\n\nOutput [= production] method\nY​=VA1​+VA2​+⋯=(P1​)+(P2​−P1)+⋯​\n\n\nExpenditure method:\nY=C+I+G+NX\n→ C is only for final sales, not for intermediate goods. Final sales are always to the household.\n\n\nIncome method\nY=IK​+IN​+π+t\n→ Note that firms pass on their taxes to the consumer (HH), or corporate taxes are paid by entrepreneurs who are at the end of the day HHs.\n\n\nGNI [= GNP] §\n\nGNI method\nYD=IKD​+IND​+π+t\n…t if both incomes are pre-tax incomes\nGNP method\nYD=C+I+G+NX+NI\n…where NI is the net domestic income (inflow - outflow)\n\nDeflator &amp; CPI §\n\nDeflator is the reduction of nominal GDP in years after the base year due to inflation.\n\nDeflator=YRealYNominal​⋅100\n…thus the real gdp for year i is calculated YiR​=Deflatori​YiN​​⋅100\n\nDeflator is 100 in the base year\nDeflator ∝ inflation rate\n\n\nCPI is another index to reduce the nominal GDP. Assume the current year is i, base year b\n\nCPIcurrent​=∑pbaseqbase∑pcurrentqbase​⋅100=Yof basein current dollars​Yof basein current dollars​​⋅100​\n…thus the real gdp for year i is calculated YiReal​=CPIi​YiNominal​​⋅100"},"Ground-Truth":{"title":"Ground Truth","links":[],"tags":["Math/Statistics"],"content":"the actual true value, or the current idealized value of a r.v."},"Growth-Rate-Calculations":{"title":"Growth Rate Calculations","links":["Malthusian-Growth","Gross-Domestic-Product","Industrial-Revolution"],"tags":["Economics/Macro-Economics"],"content":"\nGrowth of GDP is exponential since the Industrial Revolution\nGrowth Rate is always quoted as the exponential growth rate eg\nGrowth rate itself is mostly constant\n\nYt​=Yt−1​⋅egeg≈Yt−1​Yt​​≈Yt−n−1​Yt−n​​\n\nPredicting growth into the future: Yt−n​⋅en⋅g=Yt​\nAdding growth rates: gx​=gy​+gz​\ne.g. Ynominal=Price Level⋅Yreal\n⇒ gYnominal=gprice level+gYreal\nDividing growth rates: ga​=gb​−gc​\ne.g. GDP per Capita=populationY​\n⇒ gGDP per Cap.=gY−gpopulation\nfunction of growth rate:\nlet A=f(B) then ga​=∂B∂A​⋅f(B)B​⋅gb​\ne.g.2. Y=KαN1−α ⇒ elasticity gY=αgα​+(1−α)gN​\n"},"Hacking-Flash-Apps":{"title":"Hacking Flash Apps","links":[],"tags":["Computing"],"content":"https://github.com/jindrapetrik/jpexs-decompiler is a flash decompiler and ide. Edit flash source &amp; view assets, etc.\nhttps://github.com/ruffle-rs/ruffle is a emulator for flash written in rust. It doesn’t support all of the API yet. If you look at the FAQ there’s a link to a webarchive’s archive of adobe’s flash debugger tools. It’s in google drive.\nSwiftchan is an archive of .swf files (mainly flash games)."},"Halo-Effect":{"title":"Halo Effect","links":[],"tags":["Psychology"],"content":""},"Hash-Table":{"title":"Hash Table","links":[],"tags":["Computing/Data-Structures"],"content":"Hash table - Wikiwand\n\n\n                  \n                  Tip \n                  \n                \nThe nice thing about hash tables is that most algorithms are O(1) unless hash collisions occur.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithmAverageWorst caseSpaceΘ(n)O(n)SearchΘ(1)O(n)InsertΘ(1)O(n)DeleteΘ(1)O(n)"},"Hashing-Algorithms":{"title":"Hashing Algorithms","links":[],"tags":["Computing/Algorithms"],"content":"def. Hash Function. A function that maps data to a hash table.\n\nData is denoted S⊂U (=Universe)\n\nUniverse can be continuous or discrete\n\n\nHash table T has m slots, thus T=[m]\n\n[m]:={0,…,m−1}, [m]+:={1,…,m−1}\nAccess time is O(1)\n\n\nh(x) is the hash function that takes in data point x\n\nUniform Hash Function §\ndef. Uniformity. A hash function is uniform iff:\n∀i∈[m], P[h(x)=i]=m1​\ne.g. Modular Hash Function.\nh(x)=x mod m\n\nIf x is random, P[h(x)=i]=m1​\n\nUniversal Hash Function §\nalg. Universal Hashing. A hash function is universal iff:\n∀x=y P[h(x)=h(y)]≤m1​\n\nInitialization: Choose a random h from family H\nUse that h(x) for all future hashing needs for that dataset\n⇒ probability of collision is m1​\n\ne.g. Universal Modular Hash Function.\ndef. Linear Congruence Hashing (integer key)\n\nChoose a very large prime number p (bigger than the number of things you need to hash=∣U∣)\nConstruct a hash table of size m\nConstruct a family of hash functions H={ha,b​(x)=(ax+b mod m) mod m}∣a,b&lt;p\n⇒ H is a universal family\n\ndef. Multiply-Shift Binary Hashing (integer key) (SotA)\n\nCollision §\ndef. Collision Probability.\nAlternative Techniques §\nDouble Hashing §\n\n\nConstruction: S→hT→hi​Ti​\nTi​: the secondary hash table\nSi​: set that denotes all the elements hashed to Ti​. Depends on what S, the data, actually is\n\nE[Si​]≈mn​\nFor some slack, we usually make the secondary hash table ∣Ti​∣=O(∣Si​∣2)\n\n\n\nBloom Filters §\n\nFrom a family of hash functions H choose k different functions h1​…hk​\nInitialize boolean array T which has size m\n\n\nInsert(x)\n\ncalculate h1​(x),…hk​(x) and store them into T[h1​(x)]…T[hk​(x)].\nIf there’s alreay a 1 in the table, keep it that way\n\n\nSearch(x)\n\ncalculate h1​(x),…hk​(x)\nIf all of T[h1​(x)]…T[hk​(x)] returns 1, it’s highly likely that it is present.\n\n\n(false positive rate=) Probability that search(x) returns True, even if x∈/S: ≈(1−e−kn/m)k……(1)\n\n\n\n\n(1)……n is not really known. m is limited by memory. So choose k as big as possible\n\nk=ln2⋅nm​ is good ⇒ false positive search(x) is ≈0​.\n\n\nProbability that bit B[j]=0 is (1−m1​)k∣S∣\n"},"Hedonism":{"title":"Hedonism","links":["Drugs-Catalogue","Desire","Functional-Daily-Life"],"tags":["Philosophy"],"content":"\nDrugs Catalogue\nDesire\n\nHedonism is not bad if you can live a normal life.\n\n\n                  \n                  If it&#039;s not life-ruining drugs… \n                  \n                \n"},"Hedonistic-Utilitarianism":{"title":"Hedonistic Utilitarianism","links":[],"tags":["Philosophy/Ethics"],"content":"Morality is Simply a Discussion of Suffering §\nAll of ethics and morality is a discussion of pain and suffering, and who has it."},"Historical-Materialism":{"title":"Historical Materialism","links":["Proletatriat-(Marxism)"],"tags":["Philosophy/Marxism"],"content":"Productive Forces, RoP, Historical Progression §\n\n[Capitalism] has pitilessly torn asunder the motley feudal ties that bound man to his “natural superiors”, and has left remaining no other nexus between man and man than naked selfinterest, than callous “cash payment”. […] It has resolved personal worth into exchange value, and in place of the numberless indefeasible chartered freedoms, has set up that single, unconscionable freedom—Free Trade.\n\n\nIt has converted the physician, the lawyer, the priest, the poet, the man of science, into its paid wage labourers.\n\n\nProductive forces are the steady march of technology which drive economic growth through “good ideas” and efficiency.\nRelations of production are the social institutions that facilitate (or hinder) production—the context within which production occurs\n→ Think: patent systems (transforming ideas into property), protection of proptery, chattle slavery\n\n\nDirection of History §\n\nOur epoch, the epoch of the bourgeoisie, possesses, however, this distinct feature: it has simplified class antagonisms. Society as a whole is more and more splitting up into two great hostile camps, into two great classes directly facing each other—Bourgeoisie and Proletatriat (Marxism).\n\nFeudalistic society i.e. guilds + birth-determined rank → Capitalist Society i.e. private property, wage-labor\n→ Productive forces has outgrown capitalism—”specture haunting Europe”\n⇒ Constant change in ideas, innovation, striving for growth, and societal thought, idealology changed to serve its mode of production\n\nAll fixed, fast-frozen relations, with their train of ancient and venerable prejudices and opinions, are swept away, all newformed ones become antiquated before they can ossify. All that is solid melts into air, all that is holy is profaned, and man is at last compelled to face with sober senses his real conditions of life, and his relations with his kind.\n\n⇒ BZ RoP destroys national boundaries, cosmopolitan nature of production as it brings together global supply of goods into global consumer demand (ultimately, the “epidemic of overproduction”)\n\nThe bourgeoisie has through its exploitation of the world market given a cosmopolitan character to production and consumption in every country.\n\n\nIt compels all nations, on pain of extinction, to adopt the bourgeois mode of production; it compels them to introduce what it calls civilisation into their midst, i.e., to become bourgeois themselves. In one word, it creates a world after its own image.\n\n⇒ Ultimately consuming the whole world economy into its RoP\n\nCommunism is the necessary form and the dynamic principle of the immediate future, but communism is not as such the goal of human develop­ meant - the form of human society.9\n\nand communism is thus inevitable:\n\nenough. In order to supersede private property as it actually exists, real communist activity is necessary. History will give rise to such activity, and the movement which we already know in thought to be a self-superseding move­ meant will in reality undergo a very difficult and protracted process.\n\nbecause the brotherhood of man is real\n\nThe brotherhood of man is not a hollow phrase, it is a reality, and the nobility of man shines forth upon us from their work-worn figures.\n\n\n\n[…] in one word, the feudal relations of property became no longer compatible with the already developed productive forces; they became so many fetters. They had to be burst asunder; they were burst asunder.\n\n\na society that has conjured up such gigantic means of production and of exchange, is like the sorcerer who is no longer able to control the powers of the nether world whom he has called up by his spells.\n\n⇒ When PF outgrows RoP, naturally PF wins; i.e. communism is inevitable. Feudalism to Capitalism, Capitalism to Communism\nCurrently we observe this as the epidemic of overproduction:\n\n[…] the epidemic of over-production. Society suddenly finds itself put back into a state of momentary barbarism […] And how does the bourgeoisie get over these crises? On the one hand by enforced destruction of a mass of productive forces; on the other, by the conquest of new markets, and by the more thorough exploitation of the old ones.\n"},"Homogeneous-within-subcultures":{"title":"Homogeneous within subcultures","links":[],"tags":["Sociology"],"content":""},"Homogenous-Function":{"title":"Homogenous Function","links":["Monotonic-Transformation"],"tags":["Economics","Math"],"content":"def. Homogenous production function\nf(xz,xK,xN)=xλf(z,K,N)\n\n\nz: Productivity\n\n\nK: Capital\n\n\nN: Labor\ni.e. if the inputs are multiplied by x, the output is multiplied by xλ.\n⇒ We say: ”f is a homogenous function of degree λ”\n\n\nThis implies we have increasing returns to scale\n\n\nMonotonic Transformation of a homogenous function is also homogenous.\n\n"},"Hotelling's-Lemma":{"title":"Hotelling's Lemma","links":[],"tags":["Economics/Micro-Economics"],"content":"∂p∂π(w,r,p)​∂w∂π(w,r,p)​∂r∂π(w,r,p)​​=x(w,r,p)=−l(w,r,p)=−k(w,r,p)​output supplylabor demandcapital demand​​"},"Household-Intertemporal-Consumption-Only-Optimization":{"title":"Household Intertemporal Consumption Only Optimization","links":["Financial-Market","Utility-Function","Bernouilli-Distribution"],"tags":["Economics/Macro-Economics"],"content":"In Perfect Credit Markets §\nass. Financial Markets are perfectly competitive, i.e.:\n\nthey have no market power, and no profits\ninterest rates are unaffected by household optimization\nQ. Intertemporal household optimization problem. We start with the two-period situation with the following budget constraints:\n\nCt​+St​Ct+1​​=YtD​=Yt+1D​+St​(1+r)​​\nwhere:\n\nYtD​:=Yt​−Tt​: disposable income\nr: interest rate\nAnd solving for St​ we get:\nlem. Two-period intertemporal household lifetime budget constraint:\n\nCt​+1+rCt+1​​Ct+1​​=YtD​+1+rYt+1D​​​ NPV Lifetime Wealth ​=Yt+1D​+(1+r)(YtD​−Ct​)​at τ=tequivalently at τ=t+1​​\nthus we get the optimization problem:\nCt​,Ct+1​,ℓt​,ℓt+1​max​u(⋅) s.t. Ct+1​=Yt+1D​+(1+r)(YtD​−Ct​)\nwe can make this into an unconstrained optimization problem by substituting Ct+1​ in the utility function with the budget constraint. Rearrange the NPV lifetime wealth, which we term Wt​:=YtD​+1+rYt+1D​​:\nWt​=Ct​+1+rCt+1​​Ct+1​=Wt​(1+r)−(1+r)Ct​​​\nWhich is the budget constraint plotted below:\n\nwhere:\n\nE: consumption endowment. We always start here\nuˉ: maximum utility without borrowing/lending (=w/o financial markets)\nuˉ: maximum utility with borrowing/lending\nSlope of the budget constraint is −(1+r) as derived from the lifetime budget constraint; simply because this is the rate at which you can trade current and future consumption\nIntuition. Financial markets allow a household to shift consumption forwards/backwards in time. this is called consumption smoothing.\nRemark. The household still has convex utility, meaning it has a taste for variety. This means often that households will try to balance current and future consumption\nRemark. MRSCt​,Ct+1​​=Slope of Indifference Curve thus =1+r at optimal\n\nTaxation Effects §\nTiming of Tt​,Tt+1​ does not distort interest rates, thus does not affect optimization\nIn Imperfect Credit Markets §\nMotivation. In reality credit markets do not follow the assumptions stated above. In reality, we consider risk.\nthm. Profit function of Banks\nπ=(1+rborrow)L−(1+rlend)D\nwhere:\n\nrborrow is the interest rate for (businesses/HHs) borrowing investment for consumption\nL is the total amount loaned out\nrlent is the interest rate for consumers “lending” to banks\nD is the total amount deposited\nRemark. rb−rℓ is also known as the credit spread.\nass. From now on, we assume L=D thus:\n\nπ=(1+rb)L−(1+rℓ)L\nModelling risk §\nA loan amount L is a Bernouilli trial with:\n\nprobability a∈[0,1] of successful repayment → repay L(1+rb)\nprobability 1−a of default → repay 0\nExpected bank profit for loan L is E(π)=[a(1+rb)−(1+rℓ)]L\nIn perfect competition, we have π=0 thus:\n\n1+rℓ=a(1+rb)\nThus when a&lt;1 we have rb&gt;rℓ.\nImperfect Credit Household Optimization §\nThe intertemporal budget constraint is broken into two parts:\nCt​={YtD​+1+rbYt+1D​​−1+rbCt+1​​YtD​+1+rℓYt+1D​​−1+rℓCt+1​​​when borrowingwhen saving​\nThus:\nCt+1​={Wt​(1+r)−(1+rb)Ct​Wt​(1+r)−(1+rℓ)Ct​​if net borrowif net save​​​\nDrawing only relevant parts from the endowment point E:\n\nwhere:\n\nrℓ&lt;rb due to imperfect markets\nA∗ is the potential optimal point if Financial Markets were perfect\n\nEffects of… §\nTaxation §\nIntuition. Government tax cuts are always good, and always better right now, because the government essentially borrows for you at the lower rate rℓ:\n\nRecession/Market Risks §\nIntuition. Changes in a i.e. default risk will simply change the borrowing rate:\n\nCounterparty Risk and Collateral §\nMotivation. Banks don’t know if a borrower is credible or not, due to counterparty risk a&lt;1 (=Asymmetric Information), banks require a collateral (usually a house) for consumers to borrow. We limit the borrwing to the total future value of the collateral, pt+1​H:\n−S≤1+rpt+1​H​\nWe know that savings is Ct​−YtD​ and thus:\nCt​−YtD​≤1+rpt+1​H​Ct​≤ total usable cash at t YtD​​ income at t ​+1+rpt+1​H​​ PV of max. loan ​​​​​\nIntuition. The possession of the house is simply the far-limit to which one can borrow, i.e.\nWt​=YtD​+1+rYt+1D​+pt+1​H​ collateral ​​\nExample. To illustrate this, consider how the budget constraint might change when the housing market collapses:\n\nPublic Pension Systems §"},"Household-Intertemporal-Optimization":{"title":"Household Intertemporal Optimization","links":["Financial-Market","Utility-Function","Bernouilli-Distribution"],"tags":["Economics/Macro-Economics"],"content":"In Perfect Credit Markets §\nass. Financial Markets are perfectly competitive, i.e.:\n\nthey have no market power, and no profits\ninterest rates are unaffected by household optimization\nQ. Intertemporal household optimization problem. We start with the two-period situation with the following budget constraints:\n\nCt​+St​Ct+1​​=YtD​=Yt+1D​+St​(1+r)​​\nwhere:\n\nYtD​:=Yt​−Tt​: disposable income\nr: interest rate\nAnd solving for St​ we get:\nlem. Two-period intertemporal household lifetime budget constraint:\n\nCt​+1+rCt+1​​Ct+1​​=YtD​+1+rYt+1D​​​ NPV Lifetime Wealth ​=Yt+1D​+(1+r)(YtD​−Ct​)​at τ=tequivalently at τ=t+1​​\nthus we get the optimization problem:\nCt​,Ct+1​,ℓt​,ℓt+1​max​u(⋅) s.t. Ct+1​=Yt+1D​+(1+r)(YtD​−Ct​)\nwe can make this into an unconstrained optimization problem by substituting Ct+1​ in the utility function with the budget constraint. Rearrange the NPV lifetime wealth, which we term Wt​:=YtD​+1+rYt+1D​​:\nWt​=Ct​+1+rCt+1​​Ct+1​=Wt​(1+r)−(1+r)Ct​​​\nWhich is the budget constraint plotted below:\n\nwhere:\n\nE: consumption endowment. We always start here\nuˉ: maximum utility without borrowing/lending (=w/o financial markets)\nuˉ: maximum utility with borrowing/lending\nSlope of the budget constraint is −(1+r) as derived from the lifetime budget constraint; simply because this is the rate at which you can trade current and future consumption\nIntuition. Financial markets allow a household to shift consumption forwards/backwards in time. this is called consumption smoothing.\nRemark. The household still has convex utility, meaning it has a taste for variety. This means often that households will try to balance current and future consumption\nRemark. MRSCt​,Ct+1​​=Slope of Indifference Curve thus =1+r at optimal\n\nTaxation Effects §\nTiming of Tt​,Tt+1​ does not distort interest rates, thus does not affect optimization\nIn Imperfect Credit Markets §\nMotivation. In reality credit markets do not follow the assumptions stated above. In reality, we consider risk.\nthm. Profit function of Banks\nπ=(1+rborrow)L−(1+rlend)D\nwhere:\n\nrborrow is the interest rate for (businesses/HHs) borrowing investment for consumption\nL is the total amount loaned out\nrlent is the interest rate for consumers “lending” to banks\nD is the total amount deposited\nRemark. rb−rℓ is also known as the credit spread.\nass. From now on, we assume L=D thus:\n\nπ=(1+rb)L−(1+rℓ)L\nModelling risk §\nA loan amount L is a Bernouilli trial with:\n\nprobability a∈[0,1] of successful repayment → repay L(1+rb)\nprobability 1−a of default → repay 0\nExpected bank profit for loan L is E(π)=[a(1+rb)−(1+rℓ)]L\nIn perfect competition, we have π=0 thus:\n\n1+rℓ=a(1+rb)\nThus when a&lt;1 we have rb&gt;rℓ.\nImperfect Credit Household Optimization §\nThe intertemporal budget constraint is broken into two parts:\nCt​={YtD​+1+rbYt+1D​​−1+rbCt+1​​YtD​+1+rℓYt+1D​​−1+rℓCt+1​​​when borrowingwhen saving​\nThus:\nCt+1​={Wt​(1+r)−(1+rb)Ct​Wt​(1+r)−(1+rℓ)Ct​​if net borrowif net save​​​\nDrawing only relevant parts from the endowment point E:\n\nwhere:\n\nrℓ&lt;rb due to imperfect markets\nA∗ is the potential optimal point if Financial Markets were perfect\n\nEffects of… §\nTaxation §\nIntuition. Government tax cuts are always good, and always better right now, because the government essentially borrows for you at the lower rate rℓ:\n\nRecession/Market Risks §\nIntuition. Changes in a i.e. default risk will simply change the borrowing rate:\n\nCounterparty Risk and Collateral §\nMotivation. Banks don’t know if a borrower is credible or not, due to counterparty risk a&lt;1 (=Asymmetric Information), banks require a collateral (usually a house) for consumers to borrow. The budget constraint changes thusly:\n\nPublic Pension Systems §"},"Household-Social-Security-Consumption-Only-Optimization":{"title":"Household Social Security Consumption-Only Optimization","links":["Household-Intertemporal-Consumption-Only-Optimization","Financial-Market"],"tags":["Economics/Macro-Economics"],"content":"Motivation. We pay for social security by taking money from the young and giving it to the old. The payment framework for a three-period social security is:\n\nlet\n\npopulation Nτ​ grows by rate n\nyoung will pay lump sum t per capita\nold will receive lump sum b per capita\nthen consider period τ=t:\n\nNt−1​bt​​ distribution to old ​t​=Nt​tt​​ payment by young ​=1+nb​​​\nHousehold Intertemporal Pension Optimization §\nVariation of Household Intertemporal Consumption Only Optimization. We have\n\nyτ​: income at time τ\nr: interest rate. ass. perfect Financial Markets.\nWτ​: lifetime wealth, value at time τ\nThen we have budget constraint for a houshold young at time t:\n\nWt​​=yt​−tt​+1+ryt+1​+bt+1​​=yt​−1+nb​+1+ryt+1​​+1+rbt+1​​=NPV gross income yt​+1+ryt+1​​​​+ NPV retirement income (1+n)(1+r)bt+1​(n−r)​​​​​\nThe last term, NPV of retirement income is positive when n−r&gt;0, i.e. when population growth outpaces interest rates. if not, then instead of social security one should invest in the capital markets (i.e. 401(k) policies, etc.) to use r to grow their money."},"How-Possibly-vs.-How-Actually":{"title":"How-Possibly vs. How-Actually","links":["Quantifier-(Math)"],"tags":["Philosophy/Epistemology"],"content":"Similar to Quantifier (Math) in that:\n\nHow-possibly is an existence demonstration\nHow-actually is a for-all proof\n"},"Huffman-Text-Compression-Algorithm":{"title":"Huffman Text Compression Algorithm","links":[],"tags":["Computing/Algorithms"],"content":"The Huffman algorithm is the most efficient single-text compression for text.\nHow Computers Compress Text: Huffman Coding and Huffman Trees - YouTube\n\nProven to be most efficient for single-character compression\nUses a complete binary tree to store data\nIs a optimization problem.\n\nExample of Huffman Tree.\n\nNumber indicates summed frequency\n0: left, 1: right\n\n0010 encodes n\n111 encodes (blank)\n\n\n⇒ The more frequent the letter, the shorter the encoding is\nNo encoding is a prefix of any other tree (each letter is encoded by different number of bits)\n\n"},"Human-Capital-Accumulation-&-Growth":{"title":"Human Capital Accumulation & Growth","links":[],"tags":["Economics/Macro-Economics"],"content":"Assumption §\n\nass. Factors of production is only human capital H\nass. Human capital H does not have diminishing marginal returns. The production function is simply:\n\nY=zuH\nwhere u is the portion of time spent training, i.e. generating human capital. Namedly:\nh= H.C. Generation (1−u)​​+ Working u​​\n\nass. Human capital is generated by Ht+1​=b(1−u)Ht​ where b is the learning efficiency, i.e. is there a university, apprehenticship, etc.\n\nProfit Maximization §\nProduction will occur via profit maximization:\numax​π(z−w)∴z​=Yt​− wages wuHt​​​=(z−w)uHt​=set0=w​FOC​​\nObserve that firms will provide wage that is simply equivalent to productivity z, regardless of how many hours worked. The labor market has perfectly elastic demand.\n\nHuman Capital Law of Motion §\nHt​Ht+1​​=Ht​b(1−u)Ht​​=b(1−u)​​\nObserve:\n\nIf b(1−u)&gt;1, or b&gt;1−u1​ human capital will grow\nNo declining MPH​. explosive growth.\nConvergence will not happen\n)\n"},"Human-Development-Index":{"title":"Human Development Index","links":["United-Nations"],"tags":["Economics"],"content":"Published by the United Nations"},"Humanism":{"title":"Humanism","links":[],"tags":["Philosophy"],"content":"\nHumanism is a philosophical stance that emphasizes the individual and social potential, and agency of human beings, whom it considers the starting point for serious moral and philosophical inquiry.\nThe meaning of the term “humanism” has changed according to successive intellectual movements that have identified with it. During the Italian Renaissance, ancient works inspired Italian scholars, giving rise to the Renaissance humanism movement. During the Age of Enlightenment, humanistic values were re-enforced by advances in science and technology, giving confidence to humans in their exploration of the world. By the early 20th century, organizations dedicated to humanism flourished in Europe and the United States, and have since expanded worldwide. In the early 21st century, the term generally denotes a focus on human well-being and advocates for human freedom, autonomy, and progress. It views humanity as responsible for the promotion and development of individuals, espouses the equal and inherent dignity of all human beings, and emphasizes a concern for humans in relation to the world.\nStarting in the 20th century, humanist movements are typically non-religious and aligned with secularism. Most frequently, humanism refers to a non-theistic view centered on human agency, and a reliance on science and reason rather than revelation from a supernatural source to understand the world. Humanists tend to advocate for human rights, free speech, progressive policies, and democracy. People with a humanist worldview maintain religion is not a precondition of morality, and object to excessive religious entanglement with education and the state.\nContemporary humanist organizations work under the umbrella of Humanists International. Well-known humanist associations are Humanists UK and the American Humanist Association.\nWikipedia\n\nThe principle of inherent life value\n직업에 귀천은 없다\nx"},"Hyper-Reality":{"title":"Hyper-Reality","links":["Social-Construct","Humanism","Reliable,-Replicable-and-Scalable","Living-in-Hyper-Reality"],"tags":["Sociology"],"content":"Hyperreality is a society in which everything is a Social Construct and there is little or no ties to the underlying reality.\n\nHumanism\nComputer Science and Economics is the study of truths and construction of the constructed world.\nDeconstruction Fallacy: Assuming something is not Reliable, Replicable and Scalable because it builds on the constructed world\n\ne.g. “Using this medicine that I have to use forever is not reliable because it doesn’t work if I can’t afford it or live in a place where it’s inaccessible”\ne.g. “This program is unreliable because it connects to the internet”\n⇒ It is a fallacy because human society is always built; the “natural” thing doesn’t exist. We’re all Living in Hyper-Reality.\n\n\n"},"Hypergeometric-Distribution":{"title":"Hypergeometric Distribution","links":[],"tags":["Math/Common-Distributions"],"content":"def. Hypergeometric Distribution. Describes probabilities with successive draws without replacement. In a population of N with K members containing the feature, among n trials the probability of drawing k with the feature is:\nXP(X=k)​∼Hypergeometric[N,K,n]=(kn​)Nn​Kk​(N−K)n−k​​=(nN​)(kK​)(n−kN−K​)​​\ndef. Geometric Distribution. let X s.t. Range(X)=N, where X is the number of trials until the first success, and success probability is p. X is Geometrically p-Distributed:\nX∼Geom(p)P(X=k)=p(1−p)k−1\n\nwhere X is the number of total trials and p is the probability of an event—normally, geometric distributions are used to model the number of successful events before a failure.\ne.g. the number of basketball free throws until a failure\nE(X)=p1​\nSD(X)=p1−p​​\n\n\n\n                  \n                  Info \n                  \n                \n\nY=X−1 is also a geometric distribution; the number of failures before a success.\n\n\n&gt;Y∼Geom(p)&gt;P(Y=k)=p(1−p)k&gt;\ndef. Negative Binomial Distribution. let X describe the number of successes with probability p before r failures. X is a Negative Binomal Distribution where:\nX∼NegBinom(r,p)P(X=k)=(kk+r−1​)(1−p)rpk\n\nE(X)=1−ppr​\nSD(X)=1−ppr​​\n"},"Hypothesis-Testing":{"title":"Hypothesis Testing","links":["Estimator","Likelihood-Ratio-Test","Generalized-Likelihood-Ratio-Test","Student's-t-test","Wilcoxon-Signed-Rank-Test","Wilcoxon-Rank-Sum-Test","Permutation-Test","Applications-of-Discrete-Markov-Chains"],"tags":["Math/Statistics"],"content":"def. A Hypothesis test δ is a criteria to determine between two statements about an unknown parameter of a distribution:\nδ:{H0​H1​​if criteriaif alternative​\nwhere:\n\nH0​ is the Null hypothesis\nH1​ is the Alternative hypothesis\n\ndef. Type I and II Errors, as well as size and power are defined as follows:\n\nType I Error is the probability of a false positive:\nP(Assume H1​ ∣ H0​ is true) → the Size of the test = Level of the test\nType II Error is the probability of a false negative:\nP(Assume H0​ ∣ H1​ is true) → the Power of the test\n\n⇒ Constructing a γ=1−α -level test is to construct one that has a true negative rate of γ [= a false positive rate of α]\nP-Values §\ndef. let X1​,…,Xn​∼iidF(). then the p-value is the minimum α [= false positive rate = size] at which you would adopt H1​.\n\nCommon Hypothesis Tests §\n\nLikelihood Ratio Test\nGeneralized Likelihood Ratio Test\n\nMultiple Hypothesis Testing §\nMotivation. Assume 20 sets of sample data. Then the false positive rate of one sample:\nP(at least one significant result ∣ all is null)=1−P(all is null ∣ all is null)=1−(1−0.05)20≈0.64\nWhat Is the Bonferroni Correction?\nA good explanation of the bonferroni correction.\n⇒ This is too high to be acceptable. This is p-hacking. Thus:\ndef. Bonferroni Correction. In m-tests on a single dataset X1​,…,Xn​, level must be changed to mα​ in order to make a reasonable test.\nMore Types of Tests §\n\nStudent’s t-test\nWilcoxon Signed Rank Test\nWilcoxon Rank Sum Test\nPermutation Test\nGamber’s Ruin^Gambler’s Ruin Application (Hypothesis Test)\n"},"Ideology":{"title":"Ideology","links":["CGP-Grey","Social-Construct","Normative-Scripts","Heteronormativity","Necessary-Lies-of-Civilization","Cheat-Codes-of-Life","Capital-(Marxism)","Artificial-Needs","Eastern-vs-Western-Work-Ethic","Microphysics-of-Power","Social-Institution"],"tags":["Sociology"],"content":"“Necessary Lies of Civilization” is coined by CGP Grey in Hello Internet. Alternatively Benno Rice defined ideology as “something you don’t know that you know.”\n\nSocial Construct\n\nNormative Scripts and Heteronormativity\n\n\nNecessary Lies of Civilization\nCheat Codes of Life\nCapital (Marxism)\n\nArtificial Needs\nEastern vs Western Work Ethic\n\n\nMicrophysics of Power\n\nSocial Institution\n\n\n“The Important Things Money Can’t Buy”\n“Violence is not the solution”\nHope, as the concept and belief\n"},"Income-Effect-(IE)":{"title":"Income Effect (IE)","links":[],"tags":["Economics/Micro-Economics"],"content":"Assume you consume only pasta and steak. As exogenous income rises, you consume more steak, and less pasta. In this case, steak is a normal good, while pasta is an inferior good.\ndef. The Income effect is a change in consumption only due to income\nAbsolute changes in consumption define normal/quasi-linear/inferior goods\n\n\n                  \n                  ↑ — Normal Good\n                  \n                \nIncome↑…consumption same — Quasi-linear Good\nIncome↑…consumption↓ — Inferior Good\n\n\nOn the other hand, relative changes in consumption define luxury/homothetic/necessary goods.\n+% Income &gt; +% consumption — **Luxury Good\n+%** Income = +% consumption — **Homothetic Good\n+%** Income &lt; +% consumption — **Necessary Good**\n\n\nIncome Elasticity of Demand §\n"},"Income-Statement":{"title":"Income Statement","links":[],"tags":["Economics/Finance"],"content":"a Income Statement is a table showing the firm’s income [≈ profit]\nExample Income Statement: \nwhere\n\nCoGS: variable cost\nSG&amp;A: admin cost\nEBITDA = Profits before financial costs\nDepreciation &amp; Amortization: Capital depreciation\nEBIT = Profits after capital depreciation\n"},"Independence-(Math)":{"title":"Independence (Math)","links":["Stat-230"],"tags":["Math/Probability"],"content":"Independence of Events §\nthm. For partition B1​,…,Bn​ of Ω, for all A⊆Ω:\nP(A)=P(AB1​)+P(AB2​)+⋯+P(ABn​)=P(A∣B1​)⋅P(B1​)+⋯+P(A∣Bn​)⋅P(Bn​)\n\ni.e. P(A) is the weighted average of conditional probabilities P(A∣Bi​) with weights P(Bi​).\n\ndef. Independence of Two Events. Two events A, B are independent if\nP(A∣B)=P(A∣BC)⇔P(A∣B)=P(A)\nthm. Necessary and sufficient for independence of two events:\nA,B are independent⇔P(AB)=P(A)⋅P(B)⇔P(AB)=P(B)⋅P(A∣B)\n\n\n                  \n                  Warning \n                  \n                \n\nEvents are independent not when they are related, but instead whether one event influences the probabilities of another.\ndef. Independence of Three Events. Three events A, B, C are independent if both:\n\nThey are pairwise independent\nP(ABC)=P(A)⋅P(B)⋅P(C)\n\nthm. Necessary and sufficient for independence of two events; if both:\n\nP(AB)=P(A)⋅P(B)\nP(C∣AB)=P(C∣ACB)=P(C∣ABC)=P(C∣ACBC) ← all of them have to be equal.\n\nthm. Multiplication Rule for Three Independent Events.\nP(ABC)=P(A)⋅P(B)⋅P(C)\nIndependence of Random Variable §\ndef. Independence. Two Stat 230s X,Y are independent IFF for all pairs of (x,y)\n∀x,y​P(X=x,Y=y)=P(X=x)⋅P(Y=y)⇕∀x,y​P(X=x∣Y=y)=P(X=x)\n\ni.e. the same as it is for events.\n\ndef. For n random variables X1​,…,Xn​ are mutually Independent IFF for all (k1​,…,kn​):\n∀x,y​P(X1​=k1​,...Xn​=kn​)=P(X1​=k1​)⋅⋯⋅P(Xn​=kn​)"},"Independent-Component-Analysis":{"title":"Independent Component Analysis","links":[],"tags":["Math/Statistics","Computing/Maching-Learning"],"content":"Motivation. Consider the problem where q speakers are playing separate songs, and d mics are picking them up. Just from the d mic’s signals, can you separate the songs playing at each speaker? (=Blind Source Separation problem)\ndef. Independent Component Analysis (ICA) Model. We have observed data {xn​∈Rd}n=1N​ and we want to extract ‘sources’ {hn​∈Rq}n=1N​.\nxn​=Ahn​\nwhere\n\nA is the hypothetical, unobserved ‘mixing matrix’.\nwe assume any source components hi​,hj​ are independent of each other\nWe want to find how to unmixing matrix W which gets h^=Wxn​.\nIntiution. For this, we take advantage of the central limit theorem—the (weighted) summation of any distribution will tend to a normal distribution. In this case, the mixing matrix does a weighted summation of each source component hi​, thus xn​ will the ‘more normal’ than multivariate hn​.\nMethod 1. Maximization of Negentropy. Gaussian distribution has the largest entropy. Thus we can see how far the distribution of xn​ is from gaussian and make it farther.\n\nWmax​N1​n=1∑N​H[N(0,⋅)]−H[xn​]\nMethod 2. Maximization of Kurtosis."},"Indirect-Utility-Function":{"title":"Indirect Utility Function","links":["Utility","Lagrangian-Optimization","Uncompensated-Demand-curve","Homogenous-Function","Roy's-Identity"],"tags":["Economics/Micro-Economics"],"content":"The indirect utility function relates (Prices, Income) directly to Utility, assuming the person is utility-maximizing\nUtility Function: Indirect Utility Function: ​u:(x1​,x2​)↦utilityv:(p1​,p2​,I)↦utility​​\nDerivation Process:\n\nUse Lagrangian Optimization to derive Demand Functions x1​(p1​,p2​,I), x1​(p1​,p2​,I)\nSubstitute these demand functions into the original utility function u(x1​,x2​) to get the indirect utility function.\n\nProperties\n\nHD0 in (Prices,Income) → Check after derivation!\n\nIn other words, inflation in prices and income doesn’t change utility\n\n\nDecreasing in prices (∂p1​∂v​,∂p2​∂v​&lt;0), Increasing in income (∂I∂v​&gt;0)\n\n→ Check after derivation!\n\n\nUse Roy’s Identity to get back to the Marshallian Demand.\n"},"Industrial-Revolution":{"title":"Industrial Revolution","links":[],"tags":["Humanities"],"content":""},"Inflation-vs.-Recession":{"title":"Inflation vs. Recession","links":["Inflation"],"tags":["Economics/Macro-Economics"],"content":"Paul A. Volcker was the fed chair who brought down high Inflation at the cost of a recession in the 1980’s.\n⇒ Generally, central banks will try to bring down inflation first, at the cost of a recession."},"Inflation":{"title":"Inflation","links":[],"tags":["Economics/Macro-Economics"],"content":"Inflation:= change in the price of the basket of goods [= a representative sample of GDP]\nI.R.=Pt−1​Pt​−Pt−1​​\n\n\nInterest rate ∝ inflation rate [← explained later]\n\n\nHistorically, price of capital equipment (relative to consumption goods) have gone down\n…but the price of housing have gone up\n→ thus must know inflation of what?\n\nTotal Inflation: CPI measured using the representative basket of goods\nCore Inflation: represents the long run trend in the price level.\n\nIn measuring long run inflation, transitory price changes should be excluded.\n⇒ Exclude items frequently subject to volatile prices, like food and energy\ni.e. the change in Core CPI\n\n\n\n\n\nInflation data is often shown as Year-over-Year (YoY) which means you can’t compound\n\n"},"Information-Theory":{"title":"Information Theory","links":["Rigor-Ambiguity-Axis","Deep-Learning"],"tags":["Computing/Information-Theory"],"content":"A Short Introduction to Entropy, Cross-Entropy and KL-Divergence - YouTube\nMotivation. Suppose a weather station is sending you information about the current weather. There is 50% chance of sun, and 50% chance of rain. Then the weather station can send you just one bit of information to sum this information: 1 if sunny, 0 if rainy.\ndef. Shannon Information.1 Given a random variable X, the information given in a particular realization x of X is: \nIX​(x):=−logb​(p(x))\nwhere b, the base, determines the units (either bits when b=2 or nats when b=e)\nIntuition. Information is the reduction of ambiguity/uncertainty.\ndef. Shannon Entropy.2 Given a random variable X, the entropy of this random variable is:\nH(X)​:=−∀x∑​p(x)IX​(x)=EX​[IX​(x)]​​\nIntuition. Entropy is the average amount of information transmitted in total.\nExample. In our weather station example, X is the random variable:\nX={10​if sunny with p=0.5if rainy with p=0.5​\nWhen 1:sunny, transmitted is IX​(1)=−log2​0.5=1 bit of information; same when 0:rainy is transmitted. Entropy of X is:\nH(X)=−(0.5log2​(0.5)+0.5log2​(0.5))=1 bit\nwhich matches our intution that entropy is the average amount of information transmitted.\ndef. Cross Entropy. The cross-entropy of the distribution q relative to distribution p is:^bic1ka \nH(p,q)=− expectation Ep​​​[logq]\nIntution. Cross-entropy is a measure of a “distance” between two distributions.\nRemark. Often used in Deep Learning but it doesn’t have anything to do with information theory, but simply that coincidentally they had to calculate the “distance between two distributions.”\ndef. Kullback-Leibler Divergence (KL-Divergence). Another measure of “distance” between two distributions\nDKL​(p∥q)​:=x∈X∑​p(x)ln(q(x)p(x)​)=Ex∼p(x)​[lnp(x)−lnq(x)]​​\nExample. KL Diverge between two multivariate gaussians p(x)∼N(μp​,Σp​) and q(x)∼N(μq​,Σq​) both of dimension k can be derived3 to be:\nDKL​(p∥q)=21​(ln∣Σp​∣∣Σq​∣​−k+(μp​−μq​)⊤Σq−1​(μp​−μq​)+tr{Σq−1​Σp​})\nFootnotes §\n\n\nInformation content - Wikipedia ↩\n\n\nEntropy (information theory) - Wikipedia ↩\n\n\nKL Divergence between 2 Gaussian Distributions ↩\n\n\n"},"Initial-Public-Offering":{"title":"Initial Public Offering","links":["Initial-Public-Offering","Security-(Finance)","Investment-Bank","Valuing-a-Firm"],"tags":["Economics/Finance"],"content":"def. Initial Public Offering. A private company chooses to raise money by selling ownership (=Securitys). The following is the process:\n\nPitch. Company looks for Investment Banks that will help with their IPO (i.e. find the bankrunner or underwriter). Often a syndicate of banks.\n\nEach banks pitch\n\n\nRoadshow/Marketing. Announce to the public that you’re doing an IPO.\nBookbuild. Gauge Investor demand. Due Diligence on the books.\nPricing. Value the Company and divide by the number of shares exist.\n\n\nQ. How many exchanges to they list on?\nA. Shares that are traded are same at any exchange. They only need to list at one, the benefit of listing at multiple is that people can trade at multiple exchanges for their shares so may be more liquid. But these days no need to since internet.\nQ. How many shares do you sell?\nA. When you found a company you declare on the document (legally) that your company will have n number of shares, probably all held by you. In an IPO, you can decide out of n how many you want to sell to the public.\n"},"Inner-Product":{"title":"Inner Product","links":[],"tags":["Math/Linear-Algebra"],"content":"\nReal and Complex numbers R,C: Arithmetic Muliplication\nReal space Rn: Dot Product\n→ You generalize up to Hilbert Spaces\n\nNOT a\n\nMatrix Product\n\n"},"Input-Demand-and-Output-Supply":{"title":"Input Demand and Output Supply","links":["Uncompensated-Demand-curve","Profit-Function","Utility-Maximization","Cost-Minimization","Input-Demand-and-Output-Supply","Profit-Maximization-for-Perfect-Competition","Production-Function"],"tags":["Economics/Micro-Economics"],"content":"Long Run Input Demand §\ndef. Ordinary Input Demand. Ordinary Demand for Inputs (=labor l, capital k)\n\nTo derive: Profit Function\nProperties:\n\nHD0 in (w,r,p)\nDecreasing in own-price ←regardless of anything! (Unlike Utility Maximization)\n\n\n\ndef. Conditional Input Demand. Demand of input (=labor l, capital k) in order to produce a certain level out output xˉ\n\nHOWTO get: Cost Minimization\nProperties\n\nHD0 in input prices w,r\nDecreasing in own-price\n\n\n\nShort Run Input Demand §\nShort run (own-price) labor demand: lkˉ​(w,p∣kˉ)\n\nkˉ is a parameter. Set it as the long-run input demand k(w,r,p).\nIn the short run, a change in w or p will only operate within lkˉ​ with no change in kˉ possible.\nIn the long run, we simply calculate the long-run input demand l(w,r,p).\nRelationship between long-run input demand, visually: \n\nLong Run Output Supply §\ndef. Ordinary Supply. Relates the prices of inputs and output with the quantity of output produced\nx:(w,r,p)↦x\nProperties\n\nHD0 in prices w,r,p\nincreasing in output price x\ndecreasing in input price w,r,\n\n(HowTo) Derive Supply Function §\nMethod 1:\n\nGet Input Demand and Output Supply from Profit Maximization\nSubstitute into the Production Function x=f(l(w,r,p),k(w,r,p))\nSimplify to get x(w,r,p).\nMethod 2:\nGet cost function from Cost Minimization\n\nShort Run Output Supply §\nShort run (own-price) output supply: xkˉ​(w,p∣kˉ)\n\nkˉ is a parameter. Set it to the long-run input demand k(w,r,p)\nUse the Production Function x=f(l,k) but with kˉ fixed.\nIn the short run, a change in w or p will only operate within xkˉ​ with no change in kˉ possible.\nIn the long run, we simply calculate the long-run output demand l(w,r,p).\n\n"},"Institutional-Design":{"title":"Institutional Design","links":["horizontal-organization","Centralized-Power","Microphysics-of-Power","Utility","Game-Theory"],"tags":["Economics/Game-Theory","Philosophy/Political-Philosophy"],"content":"\nHorizontal vs Vertical organization structure\nCentralized vs Decentralized (Microphysics of-) Microphysics of Power\nIncentives &amp; Payoff structures in Game Theory\n"},"Instruction-Set":{"title":"Instruction Set","links":[],"tags":["Computing/Computer-Architecture"],"content":"Instruction Set: a set of instructions—the “vocabulary”—that a process of certain architecture understands (x86, MIPS, etc.)\nTypes of Instruction Set: ARMv7, ARMv8 (64-bit), MIPS, x86 (32, 64-bit). The concept of the instruction set supports the stored-program concept (idea that instructions and data are both stored in main memory, with a memory heirarchy.)"},"Instructions-(Computer-Science)":{"title":"Instructions (Computer Science)","links":["Instruction-Set","Branching-(Computer-Science)"],"tags":["Computing/Computer-Architecture"],"content":"2.1. Introduction §\nInstruction Set language of the hardware. Vocabulary understood by a given architecture.\n2.2. Operations of the Computer Hardware §\n\nRegister: Places to store variables\n\nAn instruction in MIPS looks like this:\nadd $s1, $s2, $s3 which means\ns1 ← s2 + s3\nNotice that one instruction has three operands. In MIPS, there are 32 registers and 230memory addresses which can be used as operands.\n\nSimplicity favors regularity. It’s easier to design hardware for a fixed number of operands and registers.\n\n2.3. Oper_and_s Of the Computer Hardware §\n\nWord: size of a register (32-bits in a 32-bit architecture)\ni.e. 1 word = 4 bytes = 32 bits &gt; Smaller is Faster. Having only 32 registers is simple; having 3 operands is simple. More registers will lead to slower clock cycles. &gt;\nAlignment Restriction: data in memory must be aligned to multiples of 4.\ni.e. since each memory address refers to one byte, valid memory addresses are 0x04, 0x08, … (each are 1 word in size).\nEndian-ness: big endian refers the leftmost 0x04 referring to 0x04~0x07 and so on. little endian refers to the rightmost 0x04 referring to 0x01~0x04 and so on. Picture: big▶️little\n\nData Transfer Instruction: instruction to move data between RAM and registers. Supply the memory address stored in a register\n// Load Word Example\nlw $t1, 8($s1)\n// access *address* stored in $s1 in RAM\n// move down 8 bytes (offset)\n// and store that in register $t1\n \n// Store Word Example\nsw $t1, 8($s1)\n// the same thing, but storing $t1 into memory addr $s1 offset by *2 words*\nNote that the offset is in bytes, not words. → Thus to access arr[1] you should offset 4($s1). Offset addressing is natural for arrays and structs (from C).\nImmediate Operations: instructions to add a constant to a register value. Immediate operations are very fast, since they don’t need to load data from memory.\naddi $s1, $s2, 4 // &quot;add immediate&quot;\n// store in s1 the sum of value in s2 and 4\n// s1 = s2 + 4;\n2.4. Signed and Unsigned Numbers §\n0000 0000 0000 0000 0000 0000 0000 0000 // 32 bit number = 1 word\n// most significant bit            // least significant bit\nTwo’s Compliment Representation: think of it like this:\n−231−1⋅⋅⋅−1,0,1⋅⋅⋅231\n1000 0000 ... 0000 = -2^31 - 1\n...\n1111 1111 ... 1111 = -1\n// this is where it resets...\n0000 0000 ... 0000 = 0\n0000 0000 ... 0001 = 1\n...\n0111 1111 ... 1111 = 2^31\n// and overflow back to 1000 0000 ... 0000 = -2^31 - 1\nThis representation is the default since 1965.\nAdvantages of Two’s Compliment Representation:\n\nBinary to Decimal: most significant bit is -2^32 and the rest is normal binary\nNegation: invert bits and add one—this works both ways!\nSign Extension(=Precision Extension) (16→32 bits, etc.): extend the sign bit leftward (most significant bit)\ni.e. positive number: extend the zeros; negative number: extend the one.\n\n2.5. Representing Instructions in the Computer §\nRegister Representation:\n#8  ~ #15: $t0 ~ $t7\n#16 ~ #23: $s0 ~ $s7\nInstruction in Machine Code: instructions can be R-type or I-type. Each have different sized-fields (but is 32 bits in total).\n// R type instruction example\nadd $t1, $s1, $s2\n// divided into fields:\nop   | rs  | rt  | rd  |shamt| funct | // field names\n0    | 17  | 18  | 8   |  0  | 32    | // machine code in decimal\n6b   | 5b  |  5b |  5b |  5b | 6b    | // bit size of each field\nEach part of the instruction is called a field. List of Field Names:\n\nop: opcode, the operation\nrs: register source\nrt: register source 2\nrd: register destination (result of the operation)\nshamt: shift amount\nfunct: function code, select a specific variant of the opcode.\n\nLine label. loop in the following instruction is a line label. It’s just there for humans; like comments in code.\nloop: lw [...]\n\t\t\tadd [...]\n\t\t\tj loop\n\n2.6. Logical Operations §\n| Operation | command | example | explanation | comments |\n| ----------- | --------- | ----------------- | ------------------------------ | ----------------------------------------------------------------------------------------- | ------------------------------------------------ |\n| Shift Left | sll | sll t2,s0, 4 | s0to left 4 bits, store in t2 | equivalent to multiply by 2^n |\n| Shift Right | srl | srl t2,s0, 4 | s0to right 4 bits, store in t2 | equivalent to divide by 2^n |\n| AND | and, andi | and t0,t1, $t2 | t0 = t1 &amp; t2 | andi has a constant in place of a second register. |\n| OR | or, ori | or t0,t1, $t2 | t0 = t1 | t2 | orihas a constant in place of a second register. |\n| NOT | nor | nor t0,t1, $t2 | | NOR(A, B) = NOT (A OR B) thus if one operator is 0, then it becomes a simple NOT command. |\n2.7. Instructions for Making Decisions §\n\nBasic block: a block of assembly code without branches\n\nTwo types of jump operations: conditional branches and unconditional jump.\nConditional Branches:\nbeq $t0, $t1, L1 // branch if equal\n// if t0 == t1, goto label L1\nbne $t0, $t1, L1 // branch not equal\n// if t0 != t1, goto label L1\nUnconditional Jump:\nj Exit // go to label Exit\nLess Than (branch on less than is not included since it takes clock cycles)\n/* SIGNED */\nslt $t0, $s1, $s2 // set on less than\n// if s1 &lt; s2, t0 = 1 ; else t0 = 0\nslti $t0, $s1, 10 // set on less than immediate\n// second argument is a constant\n \n/* UNSIGNED */\nsltu $t0, $s1, $s2 // slt, but compare as if two integers are unsigned\nsltiu $t0, $s1, $s2 // slti, but compare as if two integers are unsigned\n\n\n                  \n                  Treating signed numbers as unsigned is useful in checking 0 \\leq x \\lt y. &gt; sltui $t0, $s1, 10 ← if s1 is negative, treating it as an unsigned number the 2^{31}th place has a 1 which makes it a *very big signed number—*and thus t0 is set to 0. If s1is positive then its value is same signed or unsigned; thus s1 &lt; 10 is evaluated normally.\n                  \n                \n\n2.8. Supporting Procedures in Computer Hardware §\nProcedure: =function call in assembly.\n\nLeaf Procedure. A procedure that doesn’t call any other procedures. (think: the end of a branch)\n\nTo execute a procedure you need to:\n\nPut parameters in a place expected by the procedure\nTransfer control to the procedure\nPerform task\nPut result value in expected place (by the caller)\nreturn control to caller\n\nRegisters (usually) used for procedure calling:\n\na0 ~ a3: pass arguments\nv0 ~ v1: return values\nra: return address (return here after procedure is done)\nProgram Counter (PC): holds the memory address of the instruction currently executing \n\nInstruction specialized for procedure calling:\njal Procedure // jump and link\n// an unconditional jump + saves current calling address to $ra\n// used when transferring control to the procedure\njr $ra // jump register\n// an unconditional jump but instead of a Label, jump to the address in register $ra\n// used when returing control to the caller from a procedure\n\nSpilling Registers into the Stack\n\nStack. When a0 ~ a3 isn’t enough for a procedure, s0 ~ s7 should be saved into a memory portion call thed stack. (t0 ~ t9 is not saved)\n\n$sp points to the end of the stack (where spilled data should be stored next)\nplacing and removing data is called push / pop\nBy convention stack grows from higher memory address to lower memory address, e.g. from 0x08 to 0x04, and by one word each.\n\n\nProcedure Frame (=Activation Record). Space on stack reserved for storing the procedure’s local variables and arrays that don’t fit into registers. The frame is located below the saved registers at the little end of the stack.\n\n$fp (frame pointer) points to the little end of the procedure frame. This is useful because stack pointer can change during a procedure.\n\n\n\nCalling a leaf procedure:\n\n\nadjust stack pointer by number of registers that requires saving:\naddi $sp, $sp, -12 // stack grows down\nsw $s1, 8($sp) // push first variable\nsw $s2, 4($sp)\nsw $s3, 0($sp)\n\n\nexecute procedure body\n\n\nsave return value to registers v0 ~ v1\n\n\nrestore register values to registers\nlw $s3, 0($sp) // pop last variable\nlw $s2, 4($sp)\nlw $s1, 8($sp)\naddi $sp, $sp, 12 // stack deletes up\n\n\nData preserved/not preserved across procedure calls:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreservedNot Preserveds0 ~ s7t0 ~ t9sp stack pointera0 ~ a3 argument registerra return addressv0 ~ v1 return value registerstack above stack pointer (bigger end)stack below stack pointer (littler end)\n(Not storing the temporary registers t0~t9 reduces memory store/load.)\n\nAllocating Data on the Heap\nStructure of the executable, from big end to little end:\n\nThe Stack grows down (higher address → lower address). sp → 7fff fffc\nThe Heap grows up (lower addr → higher addr).\nText Segment (instructions)\nReserved\n\n2.9. Communicating with People §\nASCII Table on p.106. Notice that:\n\none ascii character is 1 byte (=8 bits)\n0b0 is NULL\nupper and lower letters differ by 32=25\n\nTo store strings, either:\n\nfirst position in string stores length\naccompanying variable holds string length or\nthe last position marks the end of the string\n\nSinced people use ASCII a lot, and ASCII is 1 byte, there are data transfer instructions for bytes:\nlb $t0, 0($sp) // load byte (and sign extend)\nsb $t0, 4($sp) // store byte and sign extend\nlbu $t0, 0($sp) // load byte unsigned\nsbu $t0, 4($sp) // store byte unsigned\n\nUnicode is used by Java and other modern languages. Notice that:\n\nUTF-16 is default, using 16-bits\nUTF-8 is ASCII-compatible, and is variable-length\nUTF-32 uses 32 bits\n\nMIPS also accommodates UTF-16 by providing data transfer instructions for half-words (16-bits):\nlh $t0, 0($sp) // load halfword and sign extend\nsh $t0, 4($sp) // store halfword and sign extend\nlhu $t0, 0($sp) // load halfword unsigned\nshu $t0, 4($sp) // store halfword unsigned\n2.10. MIPS Addressing for 32-bit Immediates and Addresses §\nWhen you need to load a 32-bit constant into register, you need two instructions (a new one, lui):\n/* loading 0000 0000 0011 1101 0000 1001 0000 0000 */\nlui $s0, 61 // load upper immediate;\n// 61   = 0000 0000 0011 1101 (upper half of constant)\nori $s0, $s0, 2304 // logical OR immediate;\n// 2304 = 0000 1001 0000 0000 (lower half of constant)\n// s0 has the full value.\nAddressing in Jumps and Branches\njump instruction:\nj 10000\n|  2  |             10000                 |\n|  6b |              26b                  |   // can use 26 bits for addressing\n \nbranch instruction:\nbne $s0, $s1, Exit\n|  5   |  16   |  17   |        Exit      |\n| 6b   |  5b   |  5b   |        16b       |   // can only use 16 bits for addressing"},"Instrumentalism":{"title":"Instrumentalism","links":["Ends-Disregards-the-Means"],"tags":["Philosophy/Epistemology"],"content":"Ends Disregards the Means"},"Integer-Allocation":{"title":"Integer Allocation","links":["Fractional-Allocation","Fairness-(Economics)","Utility-Function"],"tags":["Economics/Game-Theory","Economics/Micro-Economics"],"content":"Motivation. As opposed to Fractional Allocation where an item can be infinitely divided (and thus much fairness criteria are satisfied), in a situation where you cannot divide the items it is much harder to find a “fair” allocation. We will try nevertheless; here are a few attempts:\n\nConverting fraction allocation to integer allocation\nSerial Dictatorship\nNash Welfare Objective\n\n\n\n                  \n                  \\mu_{i} is an additive utility function.\n                  \n                \n\nFractional to Integer §\nFirst use fractional, then convert to integer allocation.\nSerial Dictatorship §\ndef. Serial Dictatorship. (Round Robin). The following process of allocation is a SD allocation:\n\nRound i: Each agent picks favorite item\nRepeat until all items allocated\nProperties.\n\n\nSD is EF1 (defined and proved below)\nSD is not PO\nComparable to MMS (bounded; see below)\ndef. Envy-Free without one item (EF1). An allocation is envy-free without one item if:\n\n∀i,j ∃item g s.t.  i’s alloc.μi​(Si​)​​≥μi​( some other’s alloc without g Sj​−{g}​​)\nIntuition. If i envies j’s basket Sj​ (=traditional Envy) then take out one item g, and then i no longer envies Sj​−{g}. \nthm. SD is EF1.\nProof. For all agent i, let’s say i envies j.\n\nDue to SD game order, i’s pick in the k-th round will always be preferred to j’s pick in the k+1-st round. Therefore, i will: i’s picks from round 1 to finish≻i​j’s picks from round 2 to finish\nThe issue arises because j may pick before i in round 1. If we simply remove that item, i will not envy j anymore.\nTherefore, in any pair of agents i envying j, removing one item that was picked by j before i in the first round is enough to make i no longer envy j. ■\nExample.\n\n\n\n2 may envy 1 because 2 prefers a to d, its first pick.\nBut 2 does not envy any other of 1’s pick, because it picked it.\nRemoving a from 1’s set makes 2 no longer envy 1.\n\nComparison to MMS §\ndef. Maximin Share (MMS). Imagine first an allocation process where \n\ni splits the items first\nAll other agents chooses which item to have\ni gets the remaining item.\nIn this case, it is in i’s best interest to maximize the minimum utility bundle (for itself)‘s utility. Thus we have the definition of MMS as the utility of this maximin bundle:\n\nμi​(MMSi​):= maximize the... maxS​​​ minimum utility bundle min{μi​(Si​)}​​\nExample. The blue groups are what each agent splits the items into to maximize their minimum utility. The red circles are an example allocation that give more utility than MMS for every agent. \nlem. (MMS utility worse than proportional utility). If we allow a fractional allocation on items T, a proportional allocation will give utility nμi​(T)​ for agent i. On the other hand, if you use (integer) MMS allocation:\nμi​(MMSi​)​=allocation Smax​ Si​∈Smin​μi​(Si​)≤Smax​ avg. utility of each good n∑Si​∈S​μi​(Si​)​​​=Smax​nμi​(T)​=nμi​(T)​​min. &lt; avg.max doesn’t apply​​\nTherefore we μi​(MMSi​)≤nμi​(T)​ ■\nthm. (Serial Dictatorship vs. Proportional vs. MMS) Let Si​ be a serial dictatorship allocation. For agent i, let μimax​ be a single item that i values the most. Then:\n SD μi​(Si​)​​+μimax​≥ prop nμi​(T)​​​≥μi​(MMSi​)\nProof. Model the round robin with n agents and items from set T, s.t. we call round 1 starting with agent i. All items allocated before i in round 1, let T0​. These are ignored for now.\n\nLet the items allocated to agents in round k be Tk​. Let μi(k)​ be the utility to agent i by the single item allocated to i on round k. For agent i in every round:\nμi(k)​∀k​∑​μk(k)​​≥t∈Tk​max​μi​(t)≥nμi​(Tk​)​≥∀k∑​nμi​(Tk​)​​max≥avg.​​\nNow, consider the ignored items T0​. We can easily establish that:\nμi​(T0​)​=∣T0​∣ avg. ∣T0​∣μi​(T0​)​​​≤∣T0​∣t∈T0​max​μi​(t)≤n⋅t∈T0​max​μi​(t)≤n⋅t∈Tmax​μi​(t)​avg.≤max​​\nContinuing on from above:\n∀k​∑​μk(k)​μk​( SD alloc Si​​​)​≥n∑∀k​μi​(Tk​)​≥nμi​(T∖T0​)​=nμi​(T)−μi​(T0​)​≥nμ(T)−nmaxt∈T​μi​(t)​=nμi​(T)​−t∈Tmax​μi​(t)​​\nThus we have the final inequality w.r.t proportional allocation and thus MMS (due to theorem before).\nμi​(Si​)+t∈Tmax​μi​(t)≥nμi​(T)​≥μi​(MMSi​)\n■\nthm. (Alternative SD vs MMS) Serial Dictatorship allocation Si​ satisfies for each agent that:\nμi​(Si​)≥21​MMSi​\nProof.\n■\nInteger Nash Welfare §\ndef. Integer Nash Welfare. For integer allocation x, Nash welfare is:\nNash Welfare:= product of everybody i∏​​​ Sum a person’s utility j∑​μij​xij​​​\n\nNP hard to compute\n…but satisfies EF1\n\nthm. (Integer Nash Welfare objective satisfies EF1).\nProof. Suppose in the allocation S, i envies j even remove one item. Now, choose g∗ as the most bang-for-buck item to move from Sj​ to Si​:\ng∗:=argming∈Sj​​μi​(g)μj​(g)​\nAnd we also note the inequality (1):\nμi​(g∗)μj​(g∗)​≤μi​(Sj​)μj​(Sj​)​\nNow per our assumption, i still envies Sj​∖{g∗}:\nμi​(Si​)μi​(Si​)+μi​(g∗)μi​(g∗)μj​(g∗)​(μi​(Si​)+μi​(g∗))μi​(g∗)μj​(g∗)​μi​(Sj​)​&lt;μi​(Sj​)−μi​(g∗)&lt;μi​(Sj​)&lt; by inequality (1) μi​(g∗)μj​(g∗)​μi​(Sj​)≤μi​(Sj​)μj​(Sj​)​μi​(Sj​)​​=μj​(Sj​)≤μi​(Sj​)​multiply by μi​(g∗)μj​(g∗)​inequality (2)​​\nNow, on the other hand, since Si​,Sj​,… are NW allocations, it is the maximum product:\nμi​(Si​)μj​(Sj​)μj​(g∗)(μi​(Si​+μi​(g∗)))μi​(g∗)μj​(g∗)​(≤μi​(Sj​)since I μi​(Si​)+μi​(g∗)​​)​≥ give to i (μi​(Si​)+μi​(g∗))​​ remove from j (μj​(Sj​)−μj​(g∗))​​≥μj​(Sj​)μi​(g∗)≥μj​(Sj​)​expand and simplify​​\n■"},"Integer-Linear-Programming":{"title":"Integer Linear Programming","links":[],"tags":["Computing/Algorithms"],"content":"Example of a {0,1} ILP problem.\n\n\n{0,1} ILP is NP-complete\nGeneral ILP is also NP-complete\n"},"Integer-Multiplication":{"title":"Integer Multiplication","links":[],"tags":["Computing/Algorithms"],"content":"alg. Long Multiplication. (grade school) algorithm\n\nComplexity: Time O(n2).\n\nalg. Karatsuba Multiplication. Recursive algorithm to compute\n\nIdea: To get x×y…\n\nLet a,b,c,d be digits of x=abˉ,y=cdˉ, and observe:\n\nx=10n/2a+b\ny=10n/2c+d\n\n\nx×y=10nac+10n/2(ad+bc)+bd\n\nRecurse Compute a×c …(1)\nRecurse Compute b×d …(2)\nTo compute the middle term, instead of computing a×d,b×c we do:\n\nRecursively compute (a+b)(c+d)=ac+ad+bc+bd …(3)\n(3) subtract (1) and (2) to get middle term ad+bc\n\n\n\n\n\n\nSee Karatsuba Multiplication in 13 min - YouTube\n\n\nComplexity\n\nTime: O(nlog2​3≈n1.58)\nSpan: T∞​(n)=O(n) (by parallelzing all 3 recursive calls)\n\n\n\nalg. Lattice Multiplication. Developed for longer integer hand-calculation."},"Integration-Rules":{"title":"Integration Rules","links":[],"tags":["Math/Calculus"],"content":""},"Interest-Rate-Arbitrage":{"title":"Interest Rate Arbitrage","links":["No-Arbitrage"],"tags":["Economics/Finance"],"content":"def. Interest Rate Arbitrage. If a foreign country has a highe risk-free rate:\n\n\nBorrow money (USD) from domestic bank with interest iD​\nExchange to foreign currency at spot rate S USD=1 EUR\nLend money (EUR) to domestic bank with interest iF​\nGet money back, convert it back at future rate F or E(St+1​)\nTotal profit for investing \\x$:\n\nProfit=​Sx​​ conversion ​ foreign lend (1+iF​)​​​ conversion back F​​− domestic borrow x(1+iD​)​​\nYou can hedge (=cover) or not hedge for future fluctuations:\n\nCovered: Enter a futures contract at t0​ for a rate F at time t+1\nUncovered: Just take whatever future spot price E[St+1​] (also called currency carry trade)\n\nNo-Arbitrage §\nMotivation. Since there should be no arbitrage in the economy the profit from this risk-free trate should be zero. This holds either for the covered or the uncovered. Simply from setting Proft=0:\n1+iD​1+iD​​=SF​(1+iF​)=SE(St+1​)​(1+iF​)​CoveredUncovered​​"},"Interest-Rate":{"title":"Interest Rate","links":["Present-Value-Calculations","Monetary-Policy"],"tags":["Economics/Finance"],"content":"Rates in General §\ndef. Rate is a simple abstraction of interest rate so that we can calculate things easily.\nR​=F(0)F(τ)−F(0)​=τ⋅r=(1+kr​)τk−1​ (Definition) (...for simple interest) (...for compound interest)​​\ne.g. when banks say they have an APR (Annual Percentage Rate) of 10.99%, their actual rate [=Annual Percentage Yield] is:\nR=(1+36510.99%​)365−1≈11.61%\ndef. Required Rate of Return (RRR) (=Discount Rate).\n\nRRR of asset: Minimum amount of profit that the investor will accept\nRRR in general: Average rate of similar-risk investments in the market\n\ndef. Internal Rate of Return (IRR).\n\nAnnual rate of growth of an investment.\nSolution of the equation NPV(rIRR​)=set0\n\nSee NPV calculations\n\n\n\nInterest Rate §\nEquivalently:\n\nDiscount rate\nPrice of Money\n\nTypes of interest rates:\n\nYield: interest rate on bonds\nMortgage Prime rates\nLIBOR (inter-bank interest rates)\nMonetary Policy\n"},"Interest-rate-swap":{"title":"Interest rate swap","links":[],"tags":["Economics/Finance"],"content":"def. Interest rate swap is a financial instrument that can transform a floating rate to a fixed rate, and v.v."},"Intermittent-Reinforcement":{"title":"Intermittent Reinforcement","links":["Building-a-Friendship","Parental-Abuse"],"tags":["Psychology"],"content":"\nIn reinforcement theory, it is argued that human behavior is a result of “contingent consequences” to human actions. […]\nIntermittent reinforcement schedules\nMuch behavior is not reinforced every time it is emitted, and the pattern of intermittent reinforcement strongly affects how fast an operant response is learned, what its rate is at any given time, and how long it continues when reinforcement ceases. The simplest rules controlling reinforcement are continuous reinforcement, where every response is reinforced, and extinction, where no response is reinforced. Between these extremes, more complex “schedules of reinforcement” specify the rules that determine how and when a response will be followed by a reinforcer.\nWikipedia\n\nThis works for meeint somebody new, training parents, and living with relationship partners"},"International-Baccalaureate-(IB)":{"title":"International Baccalaureate (IB)","links":["assets/IB-Econ-HL---(Macroeconomic-Investigation)-Japan-vs-US.pdf","assets/IB-Econ-HL---(IA)-Analysis-on-Solar-Tariffs.pdf","assets/IB-Econ-HL---(IA)-Analysis-on-BoE-Monetary-Policy.pdf","assets/IB-Econ-HL---(IA)-Chinese-Currency-Devaluation.pdf","assets/IB-Econ-HL---Elasticities-and-Taxes.pdf","assets/IB-Econ-HL---Basic-Concepts.pdf","assets/IB-Econ-HL---Theory-of-the-Firm.pdf","assets/IB-Econ-HL---Market-Failure.pdf","assets/IB-Physics-HL---(Internal-Assessment-Report)-Lagrangian-of-Atwood-Machine.pdf","assets/IB-Physics-HL---(Lab-Report)-Specific-Heat-Capacity.pdf","assets/IB-Physics-HL---(Lab-Report)-Circular-Motion.pdf","assets/IB-Physics-HL---(Lab-Report)-Internal-Resistance.pdf","assets/IB-Physics-HL---(Lab-Report)-Free-Fall.pdf","assets/IB-Physics-HL---(Lab-Report)-Lorentz-Force.pdf","assets/IB-Literature---(IA)-The-Assult.pdf","assets/IB-Literature---(Comparative)-Equus-vs.-Streetcar-Named-Desire.pdf","assets/IB-Literature---(Presentation)-1984.pdf","assets/IB-Literature---(Commentary)-Acceptance-Speech.pdf","assets/IB-Literature---(Commentary)-Blaze.pdf","assets/IB-Literature---(Commentary)-My-Rival's-House.pdf","assets/IB-Literature---(Commentary)-Elephant-Riding.pdf","assets/IB-Literature---(Commentary)-Nighttime-Fires.pdf","assets/IB-Literature---(Commentary)-A-Fraction-of-the-Whole.pdf","assets/IB-Literature---(Commentary)-A-Sense-of-an-Ending.pdf","assets/IB-Literature---(Commentary)-The-Thing-Around-Your-Neck.pdf","assets/IB-Japanese-A---(Creative-Writing)-「羅生門」.pdf","assets/IB-Japanese-A---Written-Task-1-「日本語と社会構造」.pdf","assets/IB-Japanese-A---Written-Task-2-「文学批判」.pdf","assets/IB-Japanese-A---Written-Task-3-「マスコミュニケーション」.pdf","assets/IB-Japanese-A---Written-Task-4-「文学批評」.pdf","assets/IB-Japanese-A---(Presentation)-甘えの構造・第二章の二.pdf","assets/IB-Japanese-A---(Presentation)-日本語とアニメ「うさぎドロップ」.pdf","assets/IB-Japanese-A---(Presentation)-Apple-広告批評.pdf","assets/IB-Japanese-A---(Commentary)-「日本語」.pdf","assets/IB-Japanese-A---(Commentary)-「こころ」.pdf","assets/IB-Japanese-A---(Commentary)-「野呂松人形」.pdf","assets/IB-Japanese-A---(Commentary)-「羅生門」.pdf","assets/IB-Extended-Essay---Evaluation-of-Optimization-Algorithms-for-Machine-Learning.pdf","assets/IB-ToK-Essay---Epistemiology.pdf","assets/IB-ToK-Presentation---Consciousness.pdf"],"tags":["Courses"],"content":"Economics HL §\n\nIB Econ HL - (Macroeconomic Investigation) Japan vs US.pdf\nInternal Assessments\n\nIB Econ HL - (IA) Analysis on Solar Tariffs.pdf\nIB Econ HL - (IA) Analysis on BoE Monetary Policy.pdf\nIB Econ HL - (IA) Chinese Currency Devaluation.pdf\n\n\nAnalysis Essays\n\nIB Econ HL - Elasticities and Taxes.pdf\nIB Econ HL - Basic Concepts.pdf\nIB Econ HL - Theory of the Firm.pdf\nIB Econ HL - Market Failure.pdf\n\n\n\nPhysics HL §\n\nIB Physics HL - (Internal Assessment Report) Lagrangian of Atwood Machine.pdf\nLab Reports\n\nIB Physics HL - (Lab Report) Specific Heat Capacity.pdf\nIB Physics HL - (Lab Report) Circular Motion.pdf\nIB Physics HL - (Lab Report) Internal Resistance.pdf\nIB Physics HL - (Lab Report) Free Fall.pdf\nIB Physics HL - (Lab Report) Lorentz Force.pdf\n\n\n\nLiterature SL §\n\nIB Literature - (IA) The Assult.pdf\nIB Literature - (Comparative) Equus vs. Streetcar Named Desire.pdf\nIB Literature - (Presentation) 1984.pdf\nPoetry Commentary\n\nIB Literature - (Commentary) Acceptance Speech.pdf\nIB Literature - (Commentary) Blaze.pdf\nIB Literature - (Commentary) My Rival’s House.pdf\nIB Literature - (Commentary) Elephant Riding.pdf\nIB Literature - (Commentary) Nighttime Fires.pdf\n\n\nProse Commentary\n\nIB Literature - (Commentary) A Fraction of the Whole.pdf\nIB Literature - (Commentary) A Sense of an Ending.pdf\nIB Literature - (Commentary) The Thing Around Your Neck.pdf\n\n\n\nJapanese A SL §\n\nIB Japanese A - (Creative Writing) 「羅生門」.pdf\nWritten Tasks\n\nIB Japanese A - Written Task 1 「日本語と社会構造」.pdf\nIB Japanese A - Written Task 2 「文学批判」.pdf\nIB Japanese A - Written Task 3 「マスコミュニケーション」.pdf\nIB Japanese A - Written Task 4 「文学批評」.pdf\n\n\nPresentations\n\nIB Japanese A - (Presentation)　甘えの構造・第二章の二.pdf\nIB Japanese A - (Presentation) 日本語とアニメ「うさぎドロップ」.pdf\nIB Japanese A - (Presentation) Apple 広告批評.pdf\n\n\nCommentary\n\nIB Japanese A - (Commentary) 「日本語」.pdf\nIB Japanese A - (Commentary) 「こころ」.pdf\nIB Japanese A - (Commentary) 「野呂松人形」.pdf\nIB Japanese A - (Commentary)　「羅生門」.pdf\n\n\n\nCore §\nExtended Essay §\nIB Extended Essay - Evaluation of Optimization Algorithms for Machine Learning.pdf\nTheory of Knowledge §\n\nIB ToK Essay - Epistemiology.pdf\nIB ToK Presentation - Consciousness.pdf\n"},"International-Trade":{"title":"International Trade","links":["Balance-of-Payments"],"tags":["Economics/Macro-Economics"],"content":"Balance of Payments"},"Intertemporal-Consumption-Leisure-Optimization-(Full-General-Equilibrium)":{"title":"Intertemporal Consumption-Leisure Optimization (Full General Equilibrium)","links":["Constrained-Optimization","Cobb-Douglas-Utility","Macroeconomic-General-Equilibrium-(One-period)","assets/image-2.png"],"tags":["Economics/Macro-Economics"],"content":"Household Intertemporal §\nCt​,ℓt​,Ct+1​,ℓt+1​max​ s.t. ​u(⋅)Ct​+StP​Ct+1​​​=wt​(h−ℓt​)+πt​−Tt​=wt+1​(h−ℓt+1​)+πt+1​−Tt+1​+StP​(1+r)​\nEquating the savings StP​ for the budget constraint we have:\n NPV consumption Ct​+1+rCt+1​​​​= NPV labor income wt​(h−ℓt​)+1+rwt+1​(h−ℓt+1​)​​​+ NPV capital income πt​+1+rπt+1​​​​− NPV taxes (Tt​+1+rTt+1​​)​​​ Lifetime Wealth W ​\nUsing Lagrangian Method we have optimality conditions:\n\nIntratemporal Constraint: ∂u/∂Ct​∂u/∂ℓt​​=MRSCt​,ℓt​​=wt​\nIntertemporal Leisure Constraint: ∂u/∂ℓt​∂u/∂ℓt+1​​⋅=MRSℓt​,ℓt+1​​=wt​(1+r)wt+1​​\nIntertemporal Consumption Constraint: ∂u/∂Ct+1​∂u/∂Ct​​=:MRSCt​,Ct+1​​=1+r\nWe observe that assuming a Cobb-Douglas Utility, labor supply and consumption depends on r as well as w. Consider:\n\n\n\n                  \n                  \\tau=t\n                  \n                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNts​w↑ (Ct​ vs. ℓt​)r↑ (ℓt​ vs ℓt+1​)S.E.⊕ ∵leisure expensive⊕ ∵ work todayI.E.⊖ ∵lifetime wealth up⊖ ∵ easy moneyTotal⊕⊕\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCtd​w↑ (Ct​ vs. ℓt​)r↑ (ℓt​ vs ℓt+1​)S.E.⊕, leisure expensive⊖ ∵ consume tomorrowI.E.⊕, lifetime wealth up⊕ ∵ consume more totalTotal⊕⊖\n\nThus we have (1) Labor supply and (2) Goods demand\nFirm Intertemporal §\nNtd​,Nt+1d​,Imax​s.t. where ​V=πt​+1+rπt+1​​Kt+1​=(1−d)Kt​+Iπt​:=zF(Kt​,Nt​)−wt​Nt​− investment I​​πt+1​:=zF(Kt+1​,Nt+1​)−wt+1​Nt+1​+ sell leftover capital (1−d)Kt+1​​​​​\nV is the NPV of the firm.\nOptimal labor supply. Ns vs Wage w. Recall from 2. Household Optimization that the optimal point is MPN​=∂N∂Y​=wt​. Assuming the Cobb-Douglas Utility, labor supply is proportional to wage. ⊕\nGoods Supply Ysvs Interest rate r.\n\nlabor demand Nd being proportional to interest rate r as seen above\nGiven the optimal labor supply, observe the equilibirum labor quantity N∗ is proportional to interest rate r (See graph)\nThe goods supply Ys is proportional to quantity of labor supplied in Cobb-Douglas Utility.\nThus goods supply is proportaionl to interest rate. ⊕\n\nInvestment Decision §\nWe haven’t considered the firm decision to invest, I. First observe that the return from the firm considers the marginal cost of investment…\nMC(I)=∂I∂I​=1\n…against the marginal benefit (PV) of investment using the capital law of motion constraint…\nMB(I):=∂I∂V​= PV 1+r​​MPKt+1​​+1−d​\nThe firm’s optimal investment decision is when MB(I)=MC(I):\n1+rMPKt+1​​+1−d​MPKt+1​​−d​=1=r​​\nIntuition. This has a nice interpretation. The firm will invest exactly as much capital such that the (marginal product of capital) less (the depreciation) will beat the risk-free rate r.\nWe also see, more importantly, in the Cobb-Douglas production function that the optimal investment is inversely proportional to r. ⊖\nGeneral Intertemporal Equilibirum §\nGoods demanded is a combination of consumption and investment. Thus we have:\nYd=Cd⊖r+I⊖r where both Cd and I are inversely proportional to r making Yd​ also so.\nFor time τ=t, we have the following general equilibrium in both the labor and goods markets. We list\n\nLabor market\n\nNs(w⊕,r⊕,W⊖)\nNd(w⊖,z⊕)\n\n\nGoods market\n\nYs=z⊕F(K,N⋆⊕(w,r⊕,W))\nYd=Cd(r⊖,…)+I(r⊖,…)+G\n\n\n\n\nExogenous Changes §\n"},"Intertia-(Psychology)":{"title":"Intertia (Psychology)","links":["Get-Lost-in-the-Weeds","Depressive-Episodes"],"tags":["Psychology"],"content":"\n\n                  \n                  You must do uncomfortable things. One of the most uncomfortable things in life is practicing something you’re bad at. \n                  \n                \n\n시작이 반이다 suggests one of the biggest hurdle is starting the project. You’re breaking open a new field, and by doing so you get used to doing the project. It’s the establishment of a new identity habit and routine.\nFollowing through with the effort of keeping the habit maintained and the porject open is the role of Habits and Routines. But groundbreaking comes before maintenance. So don’t hesitate to break open a field when there are available resources (i.e. time and willpower). Starting a project earlier will bring the finale that much closer. The regret is usually, “I should’ve begun earlier.”\nRemember polymath: to be good at art you need to be smiling at your own garbage until you get better.\n\nMomentum is a closely-related theory that explains how it’s exponentially easier to continue even a difficult task than to break ground (e.g. dull physics report calculations can be done with little relative distress, but starting the work with those calculations is a big burden.)\nAlso see how Get Lost in the Weeds and compounding interest with momentum allows for great efficiencies and achievements.\n\nWhen rebuilding momentum due to a distraction, an environment transition, or Depressive Episodes, you should not time track. This distracts from actually focussing on the task. It removes the “case-study” mindset and frames the mind in a productivity oriented, quantity based unhelpful mindset.\n\nIt gets easier and easier as you start rolling the ball."},"Investment-Bank":{"title":"Investment Bank","links":["Initial-Public-Offering","Valuing-a-Firm","Balance-Sheet","Options-(Finance)","Interest-rate-swap"],"tags":["Economics/Finance"],"content":"Investment banks:\n\nDon’t take deposits\nUnderwrite a company’s stock (become a Market Maker)\nPocket the difference between buy and sell (Underwriting Spread)\n\nOperations §\n\nPrimary Market (=“corporate finance”)\n\nHelping firms borrow for the first time\nLow volume, large transactions\nIPO advising with Valuation\n\n\nSecondary Market (=“global markets”)\n\nTrading already-issued securities in the market\nHigh volue, small transactions\n\n\n\nOrganization of a Bank §\nBooks §\nBanks keep track of their assets in mainly two books:\n\nBanking book. Assets to keep long-term, often held to maturity. ¶mortgages, loans\n\nRecorded as book value in Balance Sheet \n\n\nTrading book. Assets to trade to make money. ¶stocks, Options.\n\nRecorded as Mark-to-market in Balance Sheet \n\n\n\nDivisions and Desks §\n\nFixed Income Division (FICC)\n\nRates Desk\n\nGovernment bonds\nInterest rate swaps\n? Futures, swaptions\n\n\nCredit Desk\n\nCorporate bonds\nCredit default swaps (CDS)\n? Credit indices (CDX, iTraxx)\n\n\nFX Desk\n\nSpot FX\nFX Forwards\nFX options\n\n\nCommodities Desk\n\nOil, gas, metals\nCommodity futures and options\n? Emerging Markets Desk\n\n\n? EM sovereign bonds\n\nEM FX\nEM CDS\n\n\nSecuritized Products Desk\n\nMortgage-backed securities (MBS)\nAsset-backed securities (ABS)\n? Commercial MBS (CMBS)\n\n\n\n\nEquities Division\n\nCash Equities Desk\n\nSingle stocks\nETFs\nEquity Derivatives Desk\nOptions\nEquity swaps\n? Volatility products\n\n\n? Delta One Desk\n\nIndex swaps\n? Custom baskets\nETF arbitrage\n\n\nPrime Brokerage\n\nClient financing\n? Securities lending\n? Trade clearing\n\n\n\n\n? Structured Products Division\n\n? Structured Notes Desk\n\n? Autocallables\nCapital-protected products\n\n\n? Hybrid Derivatives Desk\n\nCross-asset derivatives (e.g., equity + FX)\n\n\n\n\nOther Groups (Support &amp; Infrastructure)\n\nQuant/Strats\n\nPricing models\nRisk analytics\n\n\n\n\nRisk Management\n\nValue-at-Risk (VaR) controls\nLimit monitoring\n\n\nTreasury\n\nLiquidity and balance sheet management\n\n\n"},"Jean-Baudrillard":{"title":"Jean Baudrillard","links":["Postmodernism","Living-in-Hyper-Reality"],"tags":["People","Philosophy"],"content":"The postmodern condition (Postmodernism)\ndef. Simulation\n\nbtw. illusion problem!= simulation problem\nThere is no way out of the simulation\npostmodern age: no longer a distinction between illusion and reality ⇒ this situation is a simulation\n“stranded in a world without referents”\nThe desert of the real (everything scary about postmodernism!)\n“tribute to individuals which is property and function of the simulation”\n“the analyst of simulation, therefore, is subject to the very rule he or she analyzes”\nSimulation attempts to\n\nhypermarket\ndef. Referent\n\na signpost into reality\n\ndef. Simulacra\ndef. Hyperreality. See Living in Hyper-Reality"},"Jean-Paul-Satre":{"title":"Jean-Paul Satre","links":["Subject-Object-Dialectic","Martin-Heidegger","Lit-285-Existentialism","Lord-Chandos-Letter","Storytelling-in-Life","Justification-for-Existence","Contingency-(Sociability)","Desire","The-Female-Family","Authenticity","Torii-Moi","Walt-Whiteman","Michel-Foucault","G.-W.-F.-Hegel"],"tags":["People","Philosophy/Existentialism"],"content":"def. For-itself and In-itself \n\nFor-itself is the subject, self-conscious being that can perceive itself\nIn-itself is the object, things that for-itself percieves, and just exists in the world.\nSimpler and more Subject-Object Dialectic focused than Heideggar’s Sein-Dasein. Focus is on Lit 285 Existentialism and not metaphysics.\n\nConstructing the Existential Moment §\nhyp. Existence is superfluous. The existence of a being for-itself.\nTheme. Nausea is the emotional description of the Existentialist moment when you realize existence is superfluous\n\nObjects becoming meaningless because…\nWords are things that distinguish boundaries between things. That is falling apart. (See also Lord Chandos Letter for an example of a deconstruction of language)\nViscosity of existence\n\nScene. Roquentin picks up a chestnut tree root.\n\n\nUltimately he concludes he himself is the Nausea, i.e. existence itself is nausea\n\nhyp. Forcibly Injecting Essence into Existence (Storytelling)\nScene. Roquentin considers how any Storytelling in Life is an post-hoc situation. Existence itself is moment-to-moment, and no story is toldForcing an essence into existence is done post-hoc, ¶Telling an adventure play.\nMotif. He studies the life of a guy called Marquis de Rollebon, but sees that there’s conflicting second-hand accounts of his personality, his ideas, and what kind of person he is. ⇒ The idea we have of a single person, is still Storytelling, a post-hoc forcing of essence into existence\nhyp. You can achieve a Justification for Existence via:\n\nAbove, focibly injecting essence (=telling a story). This is a failure.\nContingency (Sociability): via being needed, desired by other people.\n\nScene. “When the [black woman] sings” Some of These Days. He wants to be Desired by others like the black woman desires her lover in the song.\nScene. The art gallery scene. “The father of my father…” These people have Contingency because they have Family, history, and connections to others\n\n\nConnecting with in-itselves: Roquentin decided to write a book instead. Leaving behind something intentional that is material evidence of one’s Subjectivity.\n\nAuthenticity is also key, because the material existence needs to authentically represent you and your existence to serve as justification\n\n\nIf you fail, commit suicide (failure)\n\nInterpreting Nausea (via Moi) §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExistenceEssenceMoment-to-momentStorytelling, “Play of life (Walt Whiteman)”Post-structuralism (Foucault)Modernist three-act play\nhyp. Nausea is a story with conflict not with a character, but with the hollowness and emptiness of existence.\n\nThe resolution is to find a justification for existence, or commit suicide\n\nViews on Oppression (via Moi) §\n\n\nBecoming aware of subjectivity\nBecoming aware of role in history\nBecoming an anti-thesis\nvoila! Dialectical Synthesis\n\n"},"John-Rawls":{"title":"John Rawls","links":[],"tags":["People","Economics/Game-Theory"],"content":"\nJustice as Fairness\nMaximin Principle\nBasic Rights\n"},"John-Stuart-Mill":{"title":"John Stuart Mill","links":[],"tags":["Economics","Philosophy","People"],"content":""},"Joint-Distributions":{"title":"Joint Distributions","links":["Marginal-Distribution","Expected-Value"],"tags":["Math/Probability"],"content":"Discrete Joint Distribution §\ndef. Joint Distributions of two discrete random variables X,Y encode the probabilities for every pair of (x,y) for (X,Y). Following is an example of a joint distribution where X is the result of rolling a first dice and Y the result of a second roll.\n\nREMARK. Joint Distributions are distributions too, which means it has to follow all the rules of distributions (e.g. ∑=1)\nContinuous Joint Distribution §\ndef. Joint Probability Density. Let X,Y be two independent continous random variables. Then the joint probability density function is defined as the derivative of the cumulative density function:\nfX,Y​(x,y):=∂x∂y∂2FX,Y​(x,y)​\nfX,Y​(x,y)ΔxΔy=P((x,y)∈R)∴fX,Y​(x,y)=Δx,Δy→0lim​ΔxΔyP(x,y)∈R​\nAnd thus the following holds:\n\nP((x,y)∈R)=∬R​f(x,y) dA where R is an event\n∬R2​fdA=1\nX⊥Y⇔fX,Y​=fX​⋅fY​\n\n\nThe blue volume in the picture is the probability of the event X and Y are in R.\n\nSee Joint Marginal Distribution\nSee Joint Expected Value\n\nFull Visual Example §\n\n\nBlue is the probability density function, fX,Y​(x,y)={π1​0​(x−1)2+y≤1else​\nRed is the marginal probability density of Y,\n\nf∗Y(y)={∫∗x=1−1−y2​1+1+y2​f_X,Y(x,y)dx0​y∈[−1,1]else​\nMinimum and Maximum Joint Dist §\nthm. Let X1​,…Xn​ be i.i.d.; let P=min(X1​,…,Xn​),Q=max(X1​,…,Xn​). Then:\nfP,Q​(p,q)={n(n−1)⋅fXi​​(p)⋅fXi​​(q)⋅[∫pq​fXi​​(x)dx]n−20​y&lt;zelse​\n\nExamples of Joint Distributions §\n\n\n                  \n                  Tip \n                  \n                \n\nRecall that joint distributions are also distributions [=encapsulate fully the information of an experiement].\nUniform Joint §\nthm. If X,Y are both uniformly distributed over [x1​,x2​],[y1​,y2​]≡Ω, then…\n\nheight of the distribution is ∣Ω∣1​ (where ∣Ω∣ denotes the area of the outcome space.)\nP((x,y)∈R)=∣Ω∣∣R∣​\n\n\nNormal Joint (Linear Combination) §\nthm. Linear Combination of Normal Distributions. If X1​∼N(μ1​,σ1​) and X2​∼N(μ2​,σ2​) then:\n∀a,b∈R,aX1​+bX2​∼N(aμ1​+bμ2​,a2σ12​+b2σ22​)\nNormal Joint (Product) §\nthm. if X∼N(μ1​,σ2) and Y∼N(μ2​,σ2) (i.e. std. dev. is the same) then\n\nfX,Y​=fX​⋅fY​\nVolume of sector θ from (μ1​,μ2​) is 2πθ​\n\n\nRayleigh Distribution §\n[=Squared &amp; Rooted Joint Normal]\ndef. Rayleigh Distribution. let X,Y∼Normal(0,σ) and W=X2+Y2​, then:\nWfR​(r)FR​(r)E(X)=σ2π​​   ​∼Rayleigh(σ)R∼RL(σ)=σ2r​e2⋅σ2−r2​ for r&gt;0=1−e2⋅σ2−r2​ for r&gt;0      SD(X)=σ24−π​​​\n\nWhere σ is the “scaling factor” (standard dev. must be same for X,Y)\nIf σ=1 then W is a Standard Rayleigh distribution:\n\nthm. Standardizing Rayleigh Distributions. If W∼RL(σ):\nσW​∼RL(1)"},"Joseph-Stiglitz":{"title":"Joseph Stiglitz","links":[],"tags":["Economics/Game-Theory","Economics/Finance","People"],"content":"Occupy Wall Street Movement"},"Jupyter-Notebook-T&T":{"title":"Jupyter Notebook T&T","links":[],"tags":["Computing/Linguistics/Python"],"content":"Auto Reload Classes and Modules §\n%load_ext autoreload\n%autoreload 2\n\nWill automatically reload functions and clasdses imported without you having to restart the kernel.\npyprogress And for Jupyter ipyprogress §\nThese are nicely made GUIs for showing progress. Use them when possible."},"Jupyter-Notebook-Tips-&-Tricks":{"title":"Jupyter Notebook Tips & Tricks","links":[],"tags":["Computing/Linguistics/Python"],"content":"Auto Reload Classes and Modules §\n%load_ext autoreload\n%autoreload 2\n\nWill automatically reload functions and clasdses imported without you having to restart the kernel.\npyprogress And for Jupyter ipyprogress §\nThese are nicely made GUIs for showing progress. Use them when possible."},"K-Clustering-Problem":{"title":"K-Clustering Problem","links":[],"tags":["Computing/Algorithms"],"content":"Notation §\n\np: a point\nϕ:p→c: which center is assigned to point p\nc1​…ck​: centers\n\nk: number of centers we want\n\n\n\nK-Max §\nQ. K-Max. K-Clustering with cost as maximum distance from center\nalg. K-Max approximation.\n\nChoose any point as one center\nChoose the point most distant from any chosen center points\nRepeat until you have any many centers as you wish\n\n\nTight 2-approximation\n\nUse triangle inequality\n\n\n\nK-Means §\nQ. K-Means. K-Clustering with cost total squared distance (equiv. to cost mean squared distance):\nC=∑∣∣p−ϕ(p)∣∣2\nalg. Lloyd’s Approximation Algorithm. (=K-Means Approximation)\n\nInitialize k centers arbitrarily\nDivide into clusters\nRecompute new centers as the mean point of each cluster\nRepeat as many times as you want\nAnalysis\n\n\nGuaranteed that every iteration will only decrease cost (=will converge)\nConverged centers are not guaranteed to be global optimum\n\nalg. K-Means++ Approximation Algorithm.\n\nInitialize k centers by…\n\nChoose any first center c1​\nChoose randomly another point as the next center, but the probabiliy of choosing point p is proportional to distance between ci​ and p\nRepeat until we have all centers c1​…ck​\n\n\nRun Llyod’s algorithm\nRe-initialize, re-run Llyod for T trials\nAnalysis\n\n\nE(ALG[T=1])=O(5lnk+10)OPT\nALG[T=O(logϵ1​)]=O(lnk)OPT with probability 1−ϵ\n\ni.e. after T=O(logϵ1​) trials…\nthe best trial run will very likely have cost O(lnk)OPT\n\n\n"},"Karl-Marx":{"title":"Karl Marx","links":[],"tags":["People","Philosophy/Political-Philosophy","Philosophy","Economics"],"content":""},"Knapsack-Problem":{"title":"Knapsack Problem","links":["Utility","Subset-Sum"],"tags":["Computing/Algorithms"],"content":"def. Knapsack Problem. You have items 0…n−1 with costs c[0…n−1] and Utility v[0…n−1] with a budget B. What is the maximum utility you can achieve? Course Description\nIdea: Simple iteration DP\n\nthm. Knapsack is NP-complete\n\nIdea: Reduce from Partition Problem.\n"},"Knuth–Morris–Pratt-Substring-Matching":{"title":"Knuth–Morris–Pratt Substring Matching","links":[],"tags":["Computing/Algorithms"],"content":"You can match text of length m with pattern of length n in O(m+n). (Brute force matching takes O(mn)).\n\nLeetcode Problem: Leetcode 28. Find the Index of the First Occurrence in a String\nExplanation Video: Knuth–Morris–Pratt(KMP) Pattern Matching\n\nGiven two strings needle and haystack, return the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack.\n1. Build Prefix-suffix Table §\ne.g.1:\npattern: A B A B C  \ntable:   0 0 1 2 0\nmeaning: e.g.  |--there is a prefix of length 2\n\ne.g.2:\nindex:   0 1 2 3 4 5 6 7 8 \npattern: a a b a a b a a a\ntable:   0 1 0 1 2 3 4 5 2\nmeaning e.g.         |--there is a prefix of length 4\n\n2. Compare Text with Pattern §\ntext:    a b x a b c a b c a b y\n---------------**********^---------\n\t\t\t\t\t\t |- y != c, search from index 2 \npattern: a b c a b y\nindex    0 1 2 3 4 5\ntable:   0 0 0 1 2 0\n\nTime to build table: O(n)\nTime to search text: O(m)\nTotal Time: O(m+n)"},"LRU-algorithm":{"title":"LRU algorithm","links":["Cache"],"tags":["Logistics","Computing/Algorithms"],"content":"Least Recently Used (LRU) is a cache replacement algorithm\n\nIn computing, cache replacement policies (also frequently called cache replacement algorithms or cache algorithms) are optimizing instructions, or algorithms, that a computer program or a hardware-maintained structure can utilize in order to manage a cache of information stored on the computer. Caching improves performance by keeping recent or often-used data items in memory locations that are faster or computationally cheaper to access than normal memory stores. When the cache is full, the algorithm must choose which items to discard to make room for the new ones.\nWikipedia\n\n(DevonThink) How Should You Organize Your Closet? Exactly Like a Computer Organizes Its Memory | WIRED"},"Laffer-Curve":{"title":"Laffer Curve","links":[],"tags":["Economics/Macro-Economics"],"content":"\nGovernment Revenue:\nG=wNSt=w(h−l)t\nObserve:\n∂t∂G​=w(h−l)−tw∂t∂l​\n\nIn economics, the Laffer Curve illustrates a theoretical relationship between rates of taxation and the resulting levels of the government’s tax revenue. The\nThe shape of the curve is a function of taxable income elasticity—i.e., taxable income changes in response to changes in the rate of taxation. As popularized by supply-side economist Arthur Laffer, the curve is typically represented as a graph that starts at 0% tax with zero revenue, rises to a maximum rate of revenue at an intermediate rate of taxation, and then falls again to zero revenue at a 100% tax rate. However, the shape of the curve is uncertain and disputed among economists.One implication of the Laffer curve is that increasing tax rates beyond a certain point is counter-productive for raising further tax revenue. Particularly in the United States, conservatives have used the Laffer curve to argue that lower taxes may increase tax revenue. However, the hypothetical maximum revenue point of the Laffer curve for any given market cannot be observed directly and can only be estimated—such estimates are often controversial. According to The New Palgrave Dictionary of Economics, estimates of revenue-maximizing income tax rates have varied widely, with a mid-range of around 70%.The Laffer curve was popularized in the United States with policymakers following an afternoon meeting with Ford Administration officials Dick Cheney and Donald Rumsfeld in 1974, in which Arthur Laffer reportedly sketched the curve on a napkin to illustrate his argument. The term “Laffer curve” was coined by Jude Wanniski, who was also present at the meeting. The basic concept was not new; Laffer himself notes antecedents in the writings of the 14th-century social philosopher Ibn Khaldun and others.\nWikipedia\n"},"Lagrangian-Optimization":{"title":"Lagrangian Optimization","links":["Budget-Lines","Monotonic-Transformation","Utility-Function","Utility-Maximization"],"tags":["Economics/Micro-Economics","Math/Calculus"],"content":"Alg. Lagrangian Optimization.\nLet:\n\nf(x) the target function to optimize\ng(x)=c is the constraint function\nλ is the Lagrange multiplier.\n\n\n(when optimizing for budget constraint) Do a Monotonic Transformation on the Utility Function to make the function easier to manipulate\nThe Lagrangian function is constructed to find the maximum or minimum of a target function subject to constraints:\n\nThe Lagrangian: L(x,λ)=f(x)−λ(g(x)−c)\nλ is an unknown constant\n\n\nThe first-order necessary conditions (also known as KKT conditions) are found by taking the derivative of the Lagrangian with respect to all variables and the Lagrange multipliers, and setting them equal to zero:\n\nFor all i, ∂xi​∂L​=0\n∂λ∂L​=0\n\n\nFeasibility condition:\n\nIs it in the feasible region: g(x)=c?\n\n\nSolve for x,λ ← this is the optimal point\n\n\n\n                  \n                  Example \n                  \n                \nWorked Example\n![[Pasted image 20230905153050.png|Worked Example|625]]\nUtility Maximization\n"},"Langchain-&-RAG":{"title":"Langchain & RAG","links":[],"tags":["Computing/Maching-Learning"],"content":"\nRunnables are what you compose the models out of; the models themselves are also runnables. They are run by calling .invoke().\nUse Document s instead of just text for RAG. This includes metadata which improves retrieval quality.\n"},"Large-Language-Model":{"title":"Large Language Model","links":["GPT-2-Architecture","Low-Rank-Adaptation"],"tags":["Computing/Maching-Learning"],"content":"Mathematical Formalization §\nGPT-2 Architecture outlines the standard transformer architecture with multi-head, self-attention. The attention mechanism is at the core of an LLM’s capabilities\nUse Cases §\nLLMs have a different kind of intelligence that you would expect humans. Because of this, using an LLM effectively requires that you understand what it is good at, and what it is not. In general, they are good at:\n\nSummarization: often better than humans\nTranslation: near-human translation in literal meaning. Laggs behind when cultural or technical context is required\nThings that are in the training data\n\nXML is better than JSON because HTML is often present in training data\nPython and Javascript is better than Fortran or Haskell, because more python/javascript in github\nEnglish is better than Japanese or Korean because most of the internet is in English\n\n\nReplicating examples (few-shot tasks)\nThey are bad at:\nPrecise reasoning. The kind in math or science\nUnderstanding of the physical world. They are good at linguistic descriptions of the physical world, but not in actually understanding how 3D space operates\nThings that require cross-correlating large amounts of information that don’t fit into the context window. Admittiedly humans are bad at this too, but we can spend time learning. Since often LLMs cannot be re-trained due to cost constraints, it must\n\nImproving Performance §\n\nPrompt engineering is changing how you state the question to better improve response.\n\nSystem vs User prompts. This is done during training to improve following the system prompt. Also solves prompt-injection attacks\n\n\nChain-of-Thought (CoT) Reasoning. Make it think through it’s thought process before it reaches a final answer\n\nAgent &amp; Tool-use\n\nRetrieval Augmented Generation (RAG): Get documents related to the question and stuff into context\nUse of precise tools (Wolfram Alpha, python shell, etc.) to improve performance in precise computation\n\n\n\n\nFine-Turing\n\nLow-Rank Adaptation\n\n\n\nSpeculation §\nThe limitations of LLMs arise because:\n\nIt cannot constantly learn. Learning from conversation is hard or impossible because:\n\nA human needs to rate the LLM’s response every time for training to occur\nComputationally expensive to train\n\n\nIt cannot get input while getting output. Humans listen while talking, see while writing, etc. LLMs have input and output in separate times.\n\nMore generally though, LLMs don’t have a model of time other than the positional encoding of tokens. They’re good with order, not with cardial time.\n\n\n"},"Late-stage-Capitalism-1":{"title":"Late-stage Capitalism 1","links":[],"tags":["Economics"],"content":""},"Leverage":{"title":"Leverage","links":[],"tags":["Economics/Finance"],"content":""},"Lifecycle-of-a-Trade":{"title":"Lifecycle of a Trade","links":["Investment-Bank","Escrow"],"tags":["Economics/Finance"],"content":"Motivation. A trade involving millions of dollars probably should be executed properly and not by some web app. Thus a trade is made not simply by a market order on an exchange. Also, a random person can’t go up to an exchange and trade a security. Thus a trade must be made:\n\nThrough an Investment Bank who has a seat at the table at an exchange\nGo through multiple confirmations by both parties to ensure accuracy\nSettled via an escrow eventually\ndef. The Lifecycle of a Trade is the process by which a trade is negotiated, executed, confirmed, and settled. Example. Imagine a fund manager wanted to buy $1M worth of AAPL shares.\n\n\n\nNegotiation. Fund manager approaches the sales team of a bank, and negotiates the price, volume, etc.\n\nNormally it’s any institutional investor like a fund, or High Net-worth Individual (HNI).\n\n\nValidation &amp; Confirmation. Both fund manager and sales team tell their trader/dealing desk about the agreement. The two communicate with each other to execute the trade.\n\nBoth do due diligence (“Know Your Client”, risk analysis)\n\n\nBoth the fund and bank have systems in place to make sure the trade isn’t suspicious, is at the correct price, etc.\nSettlement. The money and securities are transferred in a central escrow called the Clearinghouse.\ndef. Clearinghouse/Central Securities Depository (CSD). An entity that:\nKeeps track of which banks/brokerages/funds have which securities\n\nIn case of individuals, brokerages keep track of how many each investor has\n\n\nActs as Escrow for trades\n"},"Lifetime-(Programming-Language)":{"title":"Lifetime (Programming Language)","links":[],"tags":["Computing"],"content":"Rust has generic lifetime annotations.\nValidating References with Lifetimes - The Rust Programming Language\nfn main() {\n    let r;                // ---------+-- &#039;a\n                          //          |\n    {                     //          |\n        let x = 5;        // -+-- &#039;b  |\n        r = &amp;x;           //  |       |\n    }                     // -+       |\n                          //          |\n    println!(&quot;r: {}&quot;, r); //          |\n}                         // ---------+\n…this returns a compiler error, because variable x does not live long enough, even though r is a valid value."},"Likelihood-(Statistics)":{"title":"Likelihood (Statistics)","links":["Estimator"],"tags":["Math/Statistics"],"content":"Likelihood:\n\ninstead of asking “given parameter θ what is the probability of r.v. X=x”…\n…ask: “given data about X, what is the likelihood of parameter θ being within an interval?”\nThis is the best way to evaluate an estimator.\n\nLikelihood Function §\ndef. The likelihood function for X1​,…,Xn​∼iidfXi​​(x1​,…,xn​∣θ) is the likelihood that given the data, the liklihood of parameter to be that value:\nL(θ)=fX​(θ;x)Ln​(θ)=fn​(θ;x1​,...,xn​)=iidi=1∏n​fXk​univar​(θ;xk​)\nwhere for the liklihood function, the variable is θ and the parameters are x1​,…,xn​.\n\nThe domain of the likelihood function is the parameter space.\n\n\n\n                  \n                  Info \n                  \n                \n\nAlternatively, understanding L to be the (mythical) value of the pdf is another way to think of it.\n확률(Probability) vs 가능도(Likelihood)\ndef. the Log liklihood is simply the natural log of the likelihood function. It exists because it’s just easy to manipulate.\nl(θ)=log[L(θ)]=log[fXi​​(θ;x)]ln​(θ)=log[Ln​(θ)]=log[i=1∏n​fXk​univar​(θ;xk​)]\nScore §\ndef. the Score is the derivative of the log likelihood. It measures how close the estimator θ^ is to the actual value of θ.\ns(θ)=∂θ∂logL(θ)​\n\nScore is best when 0, and the absolute value measures how far away θ^ is from actual θ. Signed for direction.\nE[s∣θgt​]=0 ← under regularity conditions. Obviously, if we know real θ, then the score is perfect.\n"},"Likelihood-Ratio-Test":{"title":"Likelihood Ratio Test","links":["Likelihood-(Statistics)"],"tags":["Math/Statistics"],"content":"def. let X1​,…,Xn​∼f(;θ). To compare hypotheses:\n\n\nH0​:θ=θ0​\n\n\nH1​:θ=θ1​\nWe define the Likelihood (Statistics) Ratio Λ as:\n\n\nΛ(X1​,...,Xn​)=L(θ0​;X1​,...,Xn​)L(θ1​;X1​,...,Xn​)​\nThen devise the Likelihood Ratio Test (LRT) for cutoff c:\nδ:{H0​H1​​else if Λ&gt;c​\nThe Significance of the LRT is in that it is the most powerful test:\nthm. Neymann—Pearson Lemma. Given level α [=false positive rate = type I error rate], then the LRT is the Uniformly Most Powerful (UMP) test.\nThere is also another significant result for certain likelihood ratio. But first define:\ndef. Monotone Likelihood Ratio (MLR) is when the likelihood ratio Λ is never decreasing for some variable x.\nThen we can determine the UMP of such Λ:\nthm. Karlin—Rubin Theorem. In a distribution with Λ that is MLR for region T(x) [statistic] for a sampling X1​,…,Xn​ in this distribution the UMP is:\nδ:{H0​H1​​else if T&gt;c​"},"Limit-Laws":{"title":"Limit Laws","links":[],"tags":["Math/Calculus"],"content":"L’Hôpital’s Rule §\nx→clim​g(x)f(x)​=x→clim​g′(x)f′(x)​\nInapplicable Scenarios\nSure, here are the limit laws in markdown table format:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLawsDescriptionConstant Lawlimx→a​c=cIdentity Lawlimx→a​x=aSum/Difference Lawlimx→a​[f(x)±g(x)]=limx→a​f(x)±limx→a​g(x)Product Lawlimx→a​[f(x)⋅g(x)]=limx→a​f(x)⋅limx→a​g(x)Quotient Lawlimx→a​g(x)f(x)​=limx→a​g(x)limx→a​f(x)​ (if limx→a​g(x)=0)Exponentiation Lawlimx→a​[f(x)]n=[limx→a​f(x)]nComposition Lawlimx→a​[f(g(x))]=f(limx→a​g(x))Squeeze TheoremIf f(x)≤g(x)≤h(x) for all x in some interval containing a, and limx→a​f(x)=L=limx→a​h(x), then limx→a​g(x)=L.Intermediate Value TheoremIf f(x) is a continuous function on the closed interval [a,b] and C is any number between f(a) and f(b), then there exists at least one number c in the interval (a,b) such that f(c)=C."},"Limits-of-Math-and-Computing":{"title":"Limits of Math and Computing","links":["Algorithm","Turing-Machine","(Book)-Godel-Escher-Bach"],"tags":["Philosophy/Analytic","Math","Computing/Formal-Languages"],"content":"\n\n                  \n                  Church-Turing Thesis \n                  \n                \nA function can be calculated by an Effective Method if and only if it is computable by a Turing Machine.\n\ni.e.\n\nAlgorithms and [[Turing.\nTuring Machines are the mostsible automation.\n\n\n\n                  \n                  Warning \n                  \n                \nIt cannot be proven until we know exactly what an ”Effective Method” is. Can humans be described by an effective method? Nobody knows.\n\nSee (Book) Godel Escher Bach for a deep dive."},"Linear-Algebra":{"title":"Linear Algebra","links":["main-diagonal","Matrix-Chain-Multiplication"],"tags":["Math/Linear-Algebra"],"content":"Vector Algebra\n\naTa=∣a∣2 is a scalar. aaT is an n×n matrix.\n∣a∣=a12​+⋯+an2​​\n\nMatrix can be\n\n\nSymmetric\n\nPositive-definite\nPositive-semi-definitie\nNegative-definite\nNegative-semi-definite\n\n\n\nDiagonal: entries outside the main diagonal are all zero\n\nDiagonalizable: A is diagonalizable iff exists invertible matrix P where PAP−1 is a diagonal matrix\n\nInverse of a diagonalizable matrix is also diagonalizable\nOrthogonally diagonalizable: A is orthogonally diagonizable iff exists orthogonal matrix P where PAPT=PAP−1=I\n\n\n\n\n\nInvertible vs Singular\n\nInvertible: There exists an inverse = determinant is non-zero ∣A∣=0\n\nThe inverse of a symmetric matrix is also symmetric\n\n\nSingular: There does not exist an inverse\nAA−1=I\n\n\n\nOrthogonal:A is orthogonal iff A−1=AT\n\n\nRank:= dimension of the vector space generated by its columns\n\n\nIdentity Matrix I\n\n\nDeterminant: scalar value that determines if the matrix has a determinant\n\n\nMatrix Algebra Identities\n\nNon-commutative\n\nCommutative only with scalars\n\n\nDistributive (w.r.t. matrix addition)\nAssociative\n\nComputational complexity depends on which you multipliy first: Matrix Chain Multiplication\n\n\nTranspose-distribution (AbC)T=CTbAT\nInverse-distribution (AB)−1=B−1A−1\nTranspose-Inverse (AT)−1=(A−1)T\nIf Σ=diag{σ1​,…,σn​} is a diagonal matrix:\ntr{Σ}=σ1​+σ2​+⋯+σn​\n∣Σ∣=σ1​×σ2​×⋯×σn​\n"},"Linear-Discriminant-Model":{"title":"Linear Discriminant Model","links":["Ordinary-Least-Squares-Regression"],"tags":[],"content":"Motivation. In the input space, draw a line to classify into two categories. This is achievable using linear regression\nDiscriminant Function §\ndef. Discriminant Function takes an input vector x and assigns into one of K classes, Ck​.\n\ndef. Decision Surface is the boundary that splits the classes in input space.\ndef. Decision Region is the regions generated by the decision surfaces that correspond to a single class.\nWhen there is two classes, C1​,C2​ then:\n\ny(x)= weight=paramsw⊤​​x+ bias w0​​​\nAnd assign:\nx∈{C1​C2​​if y(x)≥0if y(x)&lt;0​\n\nw is orthogonal to the decision surface hyperplane (via linalg; green in image)\nw0​ determines the distace from the origin of the decision surface\nWhen there are K&gt;2 classes then we cant just split into K regions naively because:\n\nTherefore we instead introduce a discriminant function for every pair of classes, in total 2K(K−1)​ DFs for each k∈{0…K}:\n\nyk​(x)=wk⊤​x+wk,0​\nand then assign to: Cargmaxk​{yk​(x)}​ i.e. the maximum of all discriminant functions. The pairwise decision surface for pair j,k is:\n⊤(wk​−wj​)​​x+ distance from origin (wk,0​−wj,0​)​​=0\nthm. Such decision regions are singly connected and convex.1\nLinear Discriminnant Models §\ndef. Perceptron Algorithm. (only works for K=2, but is illustrative.)\ny(x)=f(w⊤ϕ(x))\nwhere:\n\na fixed (=non-trained) function ϕ(x). Includes bias component.\nan activation function f(x) where:\n\nf(x)={10​if x≥0if x&lt;0​\nOptimizing the Perceptron Algorithm §\ndef. Perceptron Error Function.\nEP​(w)=−n∈M∑​ always &gt;0 w⊤⋅ϕn​(x)⋅tn​​​\nwhere\n\nSums over M, all the misclassified x’s\ntn​∈{−1,+1} indicates which class it is in, C1​,C2​.\nIntuition. In this error function:\n\n\nCorrect classification has error 0\nIncorrect classification has error w⊤⋅ϕn​(x)⋅tn​\nOptimizing this globally using ∇EP​=0 is too computationally intensive. We instead use:\n\nw(τ+1)​=w(τ)−η∇EP​(w)=w(τ)+η⋅ϕn​tn​​​\n\nτ is simply the learning step index\nη is the learning rate. Adjustable\nVisualization. Following are two steps of a perceptron optimization.\n\n\nTop Left: decision boundary (=black line) is initialized, defined by the orthogonal vector (=black arrow). Green point is misclassified with error w⊤⋅ϕn​(x)⋅tn​ (=red arrow).\nTop Right: error is added to the parameters to obtain a new decision boundary and new orthogonal vector.\nBottom Left: Green point is misclassified with error (=red arrow).\nBottom Right: error is added to the parameters to obtain a new decision boundary and new orthogonal vector.\nMotivation. We are updating for every single new data x that comes in. Doesn’t this mean that the error for other data may go up? No; the math says otherwise:\nthm. Perceptron Convergence Theorem. If the training data is linearly separable, the perceptron learning algorithm is guaranteeed to find a “exact solution” in finite steps.2\n\nFootnotes §\n\n\nproof ↩\n\n\nproof ↩\n\n\n"},"Linear-Factor-Models":{"title":"Linear Factor Models","links":["Probabilistic-Principle-Component-Analysis","Independent-Component-Analysis","Slow-Feature-Analysis","Sparse-Coding-and-Dictionary-Learning"],"tags":["Computing/Maching-Learning"],"content":"Linear Factor Models is when we have data {x(n)}n=1N​, we extract the latent representation h(n)(x(n)) and then we can reconstruct the original data in a linear map W:\nxn​=Whn​+[…]\n\nIt is used for unsupervised learning.\nh is an (unobserved) abstraction, i.e. latent features of the data.\nDifferent LFMs are used to learn different types of datasets:\n\n\nProbabilistic Principle Component Analysis\n\nUnderlying factors hn​ are distributed normally\nSource components hi​s are independent of each other\n\n\nIndependent Component Analysis\n\nUnderlying factors hn​ are not distributed normally\nSource components hi​s are independent of each other\n\n\nSlow Feature Analysis\n\nUnderlying data is in time series x(t)\nThe abstract features are “slow-moving”\n\n\nSparse Coding and Dictionary Learning\n\nUnderlying data has a certain ‘vocabulary’ or dictionary D\nData can be reconstructed by combinding just a few of the ‘vocabulary’.\n\n\n"},"Linear-Programming":{"title":"Linear Programming","links":[],"tags":["Computing/Algorithms"],"content":"Linear Programing §\nInteger Linear Programming §\nLinear programming but only for integer solutions. Example of a {0,1} ILP problem.\n\n\n{0,1} ILP is NP-complete\nGeneral ILP is also NP-complete\n"},"Linking-(Computing)":{"title":"Linking (Computing)","links":[],"tags":["Computing/Linguistics"],"content":"\nStatic Linking: at compile time\nDynamic Linking: at runtime, operating system looks for .so (linux) .dll (win) .dylib (mac) extensions\n\nDynamic Linking §\nWhere Are They? §\nOS looks for dynamic libraries in:\n\nMac\n\nPaths baked into the binary (via @rpath, @executable_path, @loader_path)\nDYLD_LIBRARY_PATH (if not restricted by SIP).\nDefault system locations:\n\n/usr/lib/\n/usr/local/lib/\n\n\n\n\nLinux\n\nRPATH, RUNPATH in the binary (ldd shows them).\nLD_LIBRARY_PATH environment variable.\nConfigured dirs in /etc/ld.so.conf and /etc/ld.so.conf.d/.\nStandard locations:\n\n/lib\n/usr/lib\n/usr/local/lib\n\n\n\n\nWindows\n\nThe directory containing the executable.\nSystem directories (C:\\Windows\\System32, C:\\Windows\\SysWOW64).\nCurrent working directory.\nDirectories in the PATH environment variable.\n\n\n\nAt Runtime §\nDynamic Linker (ld.so (linux), dyld on mac, Windows loader on win)matches all undefined symbols in the symbol table in the program with exported symbols by the dynamic libraries"},"List-of-Elasticities-and-Rates-of-Substitution":{"title":"List of Elasticities and Rates of Substitution","links":["Price-Elasticity-of-Demand","Elasticity-of-Substitution","Technical-Rate-of-Substitution"],"tags":["Economics"],"content":"\nElasticity of Demand\nElasticity of Substitution\nTechnical Rate of Substitution\n"},"List-of-Machine-Learning-Datasets":{"title":"List of Machine Learning Datasets","links":["CS-675-Deep-Learning"],"tags":["Computing/Maching-Learning"],"content":"Datasets mentioned by professor in CS 675 Deep Learning\nImage Datasets (Vision) §\nObject Classification §\n\nMNIST\nFashion-MNIST\nThe Quick, Draw!\nCIFAR-10 and CIFAR-100\nThe Street View House Numbers (SVHN) Dataset\nImageNet\n\nObject Detection, Segmentation, and Captioning §\n\nCommon Objects in Context (COCO)\n\nFace Recognition and Attributes §\n\nLarge-scale CelebFaces Attributes (CelebA)\n\nText Datasets (Natural Language Processing) §\nText Classification and Sentiment Analysis §\n\nReuters-21578 Text Categorization Collection\nReuters Corpora (RCV1, RCV2, TRC2)\nLarge Movie Review Dataset\nNews Group Movie Review\n\nLanguage Modeling §\n\nBrown University Standard Corpus of Present-Day American English\nProject Gutenberg\n\nImage Captioning §\n\nThe PASCAL Object Recognition Database Collection\n\nMachine Translation §\n\nAligned Hansards of the 36th Parliament of Canada\n\nQuestion Answering §\n\nStanford Question Answering Dataset (SQuAD)\nDeepmind Question Answering Corpus\n\nSpeech Datasets §\nSpeech Recognition §\n\nTIMIT Acoustic-Phonetic Continuous Speech Corpus\nLibriSpeech ASR corpus\n\nTime Series Datasets §\nUnivariate Time Series Prediction §\n\nMonthly Sunspot Dataset\nDaily Female Births Dataset\n\nMultivariate Time Series Classification §\n\nEEG Eye State Data Set\nOzone Level Detection Dataset\n"},"Lit-285-Existentialism":{"title":"Lit 285 Existentialism","links":["Existentialism-Paper-1-Plans","Existentialism-Paper-1","Existentialism-Paper-2-Plans","Existentialism-Paper-2","Simone-de-Beauvoir","Soren-Kierkegaard","G.-W.-F.-Hegel","Fyodor-Dostoeyvsky","Friedrich-Nietzsche","Hedda-Gabbler","Lord-Chandos-Letter","Martin-Heidegger","Jean-Paul-Satre","Albert-Camus","Leslie-Feinberg"],"tags":["Philosophy"],"content":"\n\n                  \n                  Put pressure on the text, and see if it yields. \n                  \n                \n\n\nExistentialism Paper 1 Plans &amp; Existentialism Paper 1 ← Hedda Gabler and Alienation\nExistentialism Paper 2 Plans &amp; Existentialism Paper 2 ← Simone de Beauvoir and Othering of Women\n\n19C §\n\nSoren Kierkegaard\n\nEscaping Hegelian Ethics, Religion\n\n\nFyodor Dostoeyvsky\n\nSpite\nAlienation\n\n\nFriedrich Nietzsche\n\nBird-of-Prey\nSlave Morality\n\n\nHedda Gabbler\n\nLord Chandos Letter\nKierkega\n\n\n\n20C §\n\nMartin Heidegger\n\nAnxiety and Boredom as methods\n\n\nJean-Paul Satre\n\nIn-itself, For-itself\n\n\nAlbert Camus\n\nEmbodied Existence §\n\nSimone de Beauvoir\n\nEmbodiment &amp; Situation\nReciprocal Other, Absolute Other\nTranscendence &amp; Immanence\n\n\nLeslie Feinberg\n\nEmbodiment\n\n\n\nAftermaths §"},"Lit-380-Marxism":{"title":"Lit 380 Marxism","links":["External/Productive-Forces,-Relations-of-Production,-and-Historial-Materialism","Capital-(Marxism)","Proletatriat-(Marxism)","Communism","External/Private-Property","Sensuous-Activity","Alienation-(Marx)","Species-being","Artificial-Needs","Base-&-Superstructure","Valorization,-Surplus-Value","External/Worker-vs-Machine","Commodities","Use-value,-Exchange-value"],"tags":["Courses"],"content":"\nProductive Forces, Relations of Production, and Historial Materialism\nCapital (Marxism) and Proletatriat (Marxism)\nCommunism, Private Property, Sensuous Activity\nAlienation (Marx) and Species-being\nArtificial Needs\nBase &amp; Superstructure\nValorization, Surplus Value\nWorker vs Machine\nCommodities - Use value, Exchange value\n"},"Living-With-the-Internet":{"title":"Living With the Internet","links":["Personal-Computing","Manners","CGP-Grey","Negative-Emotions-are-Not-Helpful","Attention-is-Currency","Highly-Sensitive-Person-(HSP)","Stream-of-Content","Nudging","Action-Queue","Importance-of-Inputs","Personal-Culture"],"tags":["Computing/Internet"],"content":"\n\n                  \n                  Abstract \n                  \n                \n\nThe internet is a world you can access with the terminal of your personal computer.\nThe internet was, is, and always will be uncharted territory.\n\n\nProblems §\n\n\n                  \n                  Digital Minimalism \n                  \n                \n\n\nMinimize online disagreements. Kurzgesagt—Internet While traditionally disagreement and debate is viewed positively, this is not true online. ^1840c9\n\nOffline, there are ways to ameliorate hard feelings through Manners and ettiquete, such as sports players bowing after a game. Online, there is no way of connecting with the other person after disagreement.\nAlgorithms exacerbate this by optimizing for engagement, and outrage is particularly effective in this. Though Germs by CGP Grey \nNegative Emotions are Not Helpful.\n\n\nAttention is Currency\n\nModern society, wrote Simon, faces a challenge: to learn to “allocate attention efficiently among the overabundance of sources that might consume it.”\n\nAttention is the currency of the modern internet businesses.\nCorporate internet tries to entice you with your attention. (there is a distinction between corporate internet and grassroots internet.)\n\n\nBeware Overstimulation. Give attention to what matters only.\n\nUse the Stream of Content—pick out only what you find nutritious, and ignore the rest.\n\n\nCGP Grey: Thinking about Attention by CGP Grey\n\n\n\nSolutions §\n\nNudging. Built-in friction to reduce the chances of you giving into attention-sucking content or outrage content. The first line of defense.\nPrioritise the small internet. This means engaging in communities where you share the same interests, opinions. Often, they come with rules and guidelines of behavior.\nUse Action Queues to browse the internet. This encourages you to think before you consume (also helps w.r.t. Importance of Inputs).\nUse the sources from Personal Culture. This means using human curated sources (either by yourself or others) as the starting point of online life. Do not use algorithmically curated sources.\n"},"Living-in-Hyper-Reality":{"title":"Living in Hyper-Reality","links":["Jean-Baudrillard","Emergent-Phenomena","Hyper-Reality","Normative-Scripts","Ask-vs.-Guess-Culture","Social-Media-Strategy","Living-With-the-Internet","Personal-Computing","Tools-and-Structures-Define-Your-Capacity","Trans-Pride","Gender-is-Performative"],"tags":["Philosophy/Epistemology"],"content":"We no longer live in a world that is understandable through the natural way of things. (Jean Baudrillard)\n\nThe Emergent Phenomena our societies have constructed, from Gender Roles to Ask vs. Guess Culture, Social Media Strategy is far removed from our nature as human beings or our nature of what it means to be a social creature.\n⇒ It’s past the tipping point of being connected to the “real”, thus Baudrillard’s Hyperreality\nThe naturalism argument no longer holds\n“It’s the way it’s always been” no longer holds ground\n\nInstead of invoking hyperreality as something to be afraid of, embrace it.\n\nLiving With the Internet\nPersonal Computing\nTools and Structures Define Your Capacity\nTrans Pride, and Gender is Performative\n"},"Loans":{"title":"Loans","links":[],"tags":["Economics/Finance"],"content":"def. Loans are the simplest way to borrow money for investment.\n\nCan be collateralized by another asset\nHighest seniority usually in the capital structure\ndef. Loan To Value (LTV) is the loan principle vs collateral value:\n\nLTV:=collateral amountloan amount​\n¶A mortgage for a house worth 1M,with200k down payment and 800kmortgageloanhas80AlowerLTVisbetterforthebankbecausethecollateralcanbesoldmoreeasilytoregaintheloanamount.Intheaboveexample,whenmortgagedefaults,auctioningthe1M house will probably get you $800k."},"Log-Rules-and-Exponent-Rules":{"title":"Log Rules and Exponent Rules","links":[],"tags":["Math"],"content":"\nProduct Rule:\n\nlogb​(xy)=logb​x+logb​y\n\n\nQuotient Rule:\n\nlogb​(x/y)=logb​x−logb​y\n\n\nPower Rule:\n\nlogb​xn=n⋅logb​x\n\n\nChange of Base Rule:\n\nlogb​a=logc​blogc​a​\n\n\nIdentity Rule:\n\nlogb​b=1 and logb​1=0\n\n\n"},"Log-Rules":{"title":"Log Rules","links":[],"tags":["Math"],"content":"\nProduct Rule:\n\nlogb​(xy)=logb​x+logb​y\n\n\nQuotient Rule:\n\nlogb​(x/y)=logb​x−logb​y\n\n\nPower Rule:\n\nlogb​xn=n⋅logb​x\n\n\nChange of Base Rule:\n\nlogb​a=logc​blogc​a​\n\n\nIdentity Rule:\n\nlogb​b=1 and logb​1=0\n\n\n"},"Logistic-Model":{"title":"Logistic Model","links":["Softmax-and-Sigmoid","Information-Theory","Math-is-an-Abuse-of-Notation"],"tags":[],"content":"Motivation. Suppose there was a transformation from the input space into a “feature space”, where all datapoints where nicely linearly dividable, like the following:\n\nThen, let’s run a classifier using a logistic model (=Softmax and Sigmoid) We will: \n\nConstruct a likelihood of a single datapoint being classified correctly\nConstruct a likelihood of all datapoints classified correctly\nUse the negative of log-likelihood as a error function to minimize\nCalculate the gradient of that error function\nThe input data is mapped into a output classification like this:\n\nRinput spaceD​⟼ϕ​Rfeature spaceF​⟼w1​,…,wk​​ ’activations’ a1​…aK​​​⟼softmax​p(C1​∣ϕ)=:y1​…yK​\n\nD is the dimension of the input space\nF the feature space (how many features there are).\nK is the number of classes.\n\nNote that the output y​=(y1​,…,yK​) is a one-hot encoding.\nWe have a weight vector wk​ for each feature k.\n\n\nWe are given N datapoints {(xn​,tn​)}\n\ntn​ is in this case also a vector because the target is one-hot encoded as well\nVisualization. The following visualization of matrix-vector multiplication may clarify this process:\n\n\n\n\nConstructing the Likelihood Function §\nWe start with p(Ckclass​​∣ϕndata​​(xndata​​)) which is the density given one data point, ndata​ for a single class, kclass​. We omit the xndata​​ for clarity. Let’s omnit subscript labels too.\nThen consider the density that a single data point, n has the correct classification (one-hot encoding):\np(tndata​​∣ϕndata​​;w1​,…,wK​)=∀k∈[K]∏​p(Ck​∣ϕn​)tn,k​\nRaising to the power simply has the effect of only obtaining p(⋅) for the class where t=1. The rest of the elements of the vector are ignored.\nAnd finally that all datapoints have the correct classification:\np(T∣ϕn​;w1​…wK​)=∀n∈[N]∏​∀n∈[K]∏​p(Ck​∣ϕn​)tn,k​\nError Function §\nThe error function is the negative-log of the likelihood. Simply: \nE(w1​,…,wK​)​=−ln∀n∈[N]∏​∀k∈[K]∏​ =:yn,k​ p(Ck​∣ϕn​)​​tn,k​=−∀n∈[N]∑​∀k∈[K]∑​tn,k​lnyn,k​​​\nCoincidentally this error function is the cross-entropy between “distributions” tn,k​ and yn,k​. This is just a coincidence and abuse of notation to call it cross-entropy.\nMoving on; now, consider optimizing for a single weight class, wj​. We have:\ndwj​dE​=−∀n∈[N]∑​∀k∈[K]∑​yn,k​tn,k​​⋅dwj​dyn,k​​\nrequires the chain rule. Observing the mapping from the input data to the output probabilities, let’s consider the gradient for just one of the weights, wk​ for class k∈[K]\ndwk​dE​=dyk​dE​⋅dadyk​​⋅dwk​da​\nWe calculate all three of these:\n-1. \ndyk​dE​​=​​"},"Lognormal-Distribution":{"title":"Lognormal Distribution","links":["Normal-Distribution"],"tags":["Math/Common-Distributions"],"content":"def. Lognormal Distribution. A random variable X is a lognormal random variable iff\n\nX=ln(N(μ,σ2)) i.e. log of the Normal Distribution random variable\nPDF:\n\nfX​(x)=xσ2π​1​exp[−2σ2(lnx−μ)2​]\n\nE(X)=eμ+2σ2​\nVar(X)=(eσ2−1)e2μ+σ2\nCDF:\n\nFX​(x)=Φ(σlnx−μ​)\n- Where $\\Phi$ is the Standard [[Normal Distribution]] CDF.\n"},"Longest-Common-Sequence":{"title":"Longest Common Sequence","links":[],"tags":["Computing/Algorithms"],"content":""},"Longest-Palindrome-Algorithm":{"title":"Longest Palindrome Algorithm","links":[],"tags":["Computing/Algorithms"],"content":""},"Longest-Palindromic-Substring":{"title":"Longest Palindromic Substring","links":[],"tags":["Computing/Algorithms"],"content":"def. Longest Palindromic Substring.\ndef. Smallest Palindromic Decomposition. Homework Link"},"Low-Rank-Adaptation":{"title":"Low-Rank Adaptation","links":[],"tags":["Computing/Maching-Learning"],"content":""},"Machine-Learning":{"title":"Machine Learning","links":["CS-675-Deep-Learning"],"tags":["Computing"],"content":"\nCS 675 Deep Learning\n"},"Macroeconomic-General-Equilibrium-(One-period)":{"title":"Macroeconomic General Equilibrium (One-period)","links":[],"tags":["Economics/Macro-Economics"],"content":"General Equilibrium §\n\nMarket Equilibrium is a point that is both optimal and feasible:\n\nOptimality: HHs, firms, govn’t are maximally happy; π, u maximized, government plan is in place\nFeasibility: Budget/Resource Constraints are in action\n\nFeasibility §\n\nLabor Market: NS=ND\nGoods Market: YS=C+G ←“Resource Constraint”\nHH income: C+T=wN+π ←”Consumer Budget Constraint”\nGovernment G=T ←”Government Budget Constraint”\n\n⇒ If the two of three budget constraints are satisfied, the other one is automatically satisfied\n\nDeriving the Equilibrium §\n1. Production Possibility Frontier §\n\n\nPPF indicates current level of technology, levels of govn’t spending\nGradient of the PPF is the Marginal Rate of Transformation (MRT) of the economy; i.e. opportunity cost of transforming between consumption and leisure. This is equivalent to wage.\nGovernment speding is indicated by a downward shift of the PPF\n\n2. Household Optimization §\n\n\nConsider: even if households do not work, they will get π+Net Transfers amount of income\n\n1+2: Market Equilibrium §\n\nAt Point A∗(C∗,L∗):\n\nFirms produce maximally, HHs maximize utility\nIs feasible [=inside PPF, inside HH budget constraint]\nMRS=MRT=−w\n\n\n\n                  \n                  Examples of Disequilibrium \n                  \n                \n→ The left case: MRS=MRT but is infeasible ∵ household indifference curve is outside\n→ The right case: feasible, but household utility is not maximized\n\n\nChanges in Equilibrium §\n\nChanges in Government Spending [= G], normally due to war, or infrastructure projects\n\n\n\n\n                  \n                  Government can crowd out private consumption, because it reduces HH income (income effect): \n                  \n                \n\nTaxes↑ ⇒ HH income ↓ ⇒ YD↓ ⇒ NS↓ ⇒ Y likely increases ∵ Y↑=C↓+G⇑"},"Macroeconomic-Market-Equilibrium":{"title":"Macroeconomic Market Equilibrium","links":[],"tags":["Economics/Macro-Economics"],"content":"General Equilibrium §\n\nMarket Equilibrium is a point that is both optimal and feasible:\n\nOptimality: HHs, firms, govn’t are maximally happy; π, u maximized, government plan is in place\nFeasibility: Budget/Resource Constraints are in action\n\nFeasibility §\n\nLabor Market: NS=ND\nGoods Market: YS=C+G ←“Resource Constraint”\nHH income: C+T=wN+π ←”Consumer Budget Constraint”\nGovernment G=T ←”Government Budget Constraint”\n\n⇒ If the two of three budget constraints are satisfied, the other one is automatically satisfied\n\nDeriving the Equilibrium §\n1. Production Possibility Frontier §\n\n\nPPF indicates current level of technology, levels of govn’t spending\nGradient of the PPF is the Marginal Rate of Transformation (MRT) of the economy; i.e. opportunity cost of transforming between consumption and leisure. This is equivalent to wage.\nGovernment speding is indicated by a downward shift of the PPF\n\n2. Household Optimization §\n\n\nConsider: even if households do not work, they will get π+Net Transfers amount of income\n\n1+2: Market Equilibrium §\n\nAt Point A∗(C∗,L∗):\n\nFirms produce maximally, HHs maximize utility\nIs feasible [=inside PPF, inside HH budget constraint]\nMRS=MRT=−w\n\n\n\n                  \n                  Examples of Disequilibrium \n                  \n                \n→ The left case: MRS=MRT but is infeasible ∵ household indifference curve is outside\n→ The right case: feasible, but household utility is not maximized\n\n\nChanges in Equilibrium §\n\nChanges in Government Spending [= G], normally due to war, or infrastructure projects\n\n\n\n\n                  \n                  Government can crowd out private consumption, because it reduces HH income (income effect): \n                  \n                \n\nTaxes↑ ⇒ HH income ↓ ⇒ YD↓ ⇒ NS↓ ⇒ Y likely increases ∵ Y↑=C↓+G⇑"},"Macroeconomic-Variable-Relationships":{"title":"Macroeconomic Variable Relationships","links":["Philips-Curve","Beveridge-Curve","Laffer-Curve"],"tags":["Economics/Macro-Economics"],"content":"\nPhilips Curve: Unemployment vs Interest Rate\nBeveridge Curve: Unemployment vs Job Vacancies\nLaffer Curve: Government Revenue vs Tax Rate\n"},"Macroeconomics":{"title":"Macroeconomics","links":[],"tags":["Economics/Macro-Economics"],"content":""},"Majority-Voting-Algorithm":{"title":"Majority Voting Algorithm","links":["CS330-HW03"],"tags":["Computing/Algorithms"],"content":"From CS330 HW03\nProblem. Call an array good if more than half of its elements are the same. The domain from which the elements are taken is not necessarily ordered like integers so one cannot make comparisons like “Is A[i] &gt; A[j]?” or sort the array, but one can check whether two elements are the same in O(1) time.\nIdea: a majority element must be majority in either of the two halves of an array.\nAlgorithm:\n# array to check\nA[1..n]\n \nfunction Count(l, r, elem)\n\t# base case\n\tif l = r\n\t\treturn 1\n \n\t# divide array into two, and count there\n\tm = ceiling((l+r)*0.5)\n\tl_count = PCount(l, m, elem)\n\tr_count = PCount(m+1, r, elem)\n \n\t# return the sum of left and right counts\n\treturn l_count + r_count\n \nfunction MajorityElem(l,r)\n\t# base case\n\tif l = r\n\t\treturn A[l] \n \n\t# check left, right half majority\n\tm = ceiling((l+r)*0.5)\n\tleft_majority = MajorityElem(l, m)\n\tright_majority = MajorityElem(m+1, r)\n \n\t## check consensus\n \n\t# check no consensus first; if one is null return the non-null\n\tif left_majority == NULL and right_majority != NULL:\n\t\treturn right_majority\n\tif left_majority != NULL and right_majority == NULL:\n\t\treturn left_majority\n \n\t# there cannot be a both no consensus case\n \n\t# consensus agrees\n\tif left_majority == right_majority:\n\t\treturn left_majority\n\t\n\t# consensus disagrees\n\telse if left_majority != right_majority:\n \n\t\t# count the majority\n\t\tleft_count = Count(l, r, left_majority)\n\t\tright_count = Count(l, r, right_majority)\n\t\t\n\t\t# if tied, no consensus\n\t\tif left_count == right_count\n\t\t\treturn NULL\n\t\t\n\t\t# winner exists, return the winner\n\t\treturn left_count &gt; right_count ? left_majority : right_majority\n\t\t\n\nCount function: T(n)≤2T(2n​)+O(1)=O(n)\nMajorityElement function: T(n)≤2T(2n​)+2⋅O(n)\n\nFirst term: recursive calls\nSecond term: calls Count\nSolution: T(n)=O(nlogn) (same recurrence relation as mergesort)\n\n\nCorrectness (Brief argument):\n\nBase case: in n=1 the element is majority element\nIH: Assume MajorityElement(k/2) is correct.\nIS:\n\nIf one of them has no majority but the other ones does, the one with majority must be the true majority because of an element exists in more than half of the total array it must be that in one of the halves it will be the majority.\nIf both halves have a majority, then the one that has more counts in the total array wins\nThere cannot be a both halves no-majority case as explained in (1)\n\n\n\n\n"},"Malthusian-Growth":{"title":"Malthusian Growth","links":[],"tags":["Economics/Macro-Economics"],"content":"Assumptions §\n\nass. Factor of production is just labor N and land L; there is no capital K\nass. Nobody saves. There is no way to save, thus there is no investments. All output results in consumption. i.e. Y=C\nass. Law of Motion (Malthusian Population). Population growth is positively correlated to consumption per capita:\n\nNt​Nt+1​​=g​ consumption per cap Nt​Ct​​​​​\nwhere\n\ng(⋅) is the population growth function wrt consumption per capita\nNote that Nt​Ct​​=Nt​Yt​​\nThus we have the population growth formula:\n\nNt+1​=Nt​g(z⋅f(ℓ))\n\nwhere\n\nf(⋅) is the production function in intensive form. Note here there is no capital, because capital is not a factor of production\nObserve.\nSteady state (red point) is achieved when Nt+1​=Nt​, thus from the LoM (Pop.) we get g(zf(ℓ))=1, the intersection with the 45∘ line\nIncrease in z, causes:\n\nIncrease in production per capita zf(ℓ)\nIncrease in consumption per capital Ct​\nIncrease in population Nt​\nThus consumption per capita normalizes\n\n\nVisually this is the economy moving to the left\n\n\n\nMalthusian Trap §\ndef. Malthusian Trap. this fact that even when population grows there is not an increase in consumption or production per capita. The ways to escape this trap is:\n\nInstead of increasing quantity of children, increase the quality of children, via apprehenticeships, universities, etc.\nEmergence of capital, i.e. factories, tools, that can accumulate\nrmk. Charging interest in a Malthusian society is stealing, because nobody can invest, and thus nobody’s capital will grow\n"},"Map-of-Microeconomic-Optimization":{"title":"Map of Microeconomic Optimization","links":["Utility-Maximization","Uncompensated-Demand-curve","Indirect-Utility-Function","Roy's-Identity","Expenditure-Minimization","Marginal-Willingness-to-Pay","Expenditure-Function","Shepard's-Lemma","Profit-Maximization","Input-Demand-and-Output-Supply","HowTo---Profit-Maximization-for-Perfect-Competition-for-Perfect-Competition","Hotelling's-Lemma","Cost-Minimization","Cost-Function"],"tags":["Economics/Micro-Economics"],"content":"Utility Maximization (Household) §\n\n\nUtility Maximization\n\nUncompensated Demand Function (p1​,p2​,I)↦x1​\nIndirect Utility Function (p1​,p2​,I)↦u\nRoy’s Identity\n\n\nExpenditure Minimization\n\nCompensated Demand Function (p1​,p2​,u)↦x1​\nExpenditure Function (p1​,p2​,u)↦I\nShepard’s Lemma\n\n\nInvert between Indirect Utility Function and Expenditure Function by solving for I or u depending on what you want.\n\nProfit Maximization (Firm) §\n\n\nProfit Maximization maxl,k​ π=px−wl−rk  such that x=f(l,k)\n\nOrdinary Input Demand and Output Supply (w,r,p)↦l,k\nOutput Input Demand and Output Supply (w,r,p)↦x\nProfit Function (w,r,p)↦π\n\nHotelling’s Lemma back to input demand\n\n\n\n\nCost Minimization minl,k​ c=wl+rk such that x=f(l,k)\n\nConditional Input Demand and Output Supply (w,r,xˉ)↦l,k\nConditional Cost Function (w,r,xˉ)↦C\n\nShepard’s Lemma back to conditional input demand\n\n\n\n\nmaxx​px−C(w,r,xˉ) for cost function to output supply\nsubstitute Input Demand and Output Supply into Conditional Input Demand to get Ordinary Input Demand and Output Supply\n"},"MarSci-202-Marine-Animals":{"title":"MarSci 202 Marine Animals","links":["Taxonomic-Rank","Marine-Mammals","Fish","Sponges","MarSci-202-Marine-Animals","MarSci-202-PSet-1","MarSci-202-PSet-2","Marsci-202-PSet-3","Marsci-Exam-1-Prep","Marsci-Exam-2-Prep"],"tags":["Courses"],"content":"Taxonomic Rank\nMarine Vertebrates §\n\nMarine Mammals\nSea Turtles\nSea Birds\nFish\n\nMarine Invertebrates §\n\nSponges\nCorals\nWorms\nMolluscs\nMarSci 202 Marine Animals\nMarSci 202 PSet 1\nMarSci 202 PSet 2\nMarsci 202 PSet 3\nMarsci Exam 1 Prep\nMarsci Exam 2 Prep\n\n\n\n                  \n                  Q \n                  \n                \nHow does the presence of colonies of seals on haul-out sites affect local coastal ecosystems, and how do ecosystems coastal transition zones influence seal population dynamics and behavior?\n\nEnvironmental Effects §\n\nUpwelling\n\nIn continental slope/shelf e.g. SoCal\nUpwelling fertilizes coastal water\n&amp; Concentrates fauna &amp; ecosystem interactions\n\n\nSeasonal Stratification\n\nWhen depths are different temperatures, water body is still, waters don’t mix. (not rivers or most seas, but often in lakes and some seas)\n\nsummer: upper water is hot, fall/winter stratification breaks &amp; mixes\ne.g. cheaspeak bay, florida bay\n\n\nNutrients are not going to break\nCoral reefs: waters never come up ← no stratification\n\n\nHeat stress\n\nDifferent heat capacity, inter-organism &amp; inter-species variations\nSubstrate (sea floor &amp; water) temperature, solar radiation\nDifferent color—some organisms change color per season to adapt to radiation, etc.\n\n\nWarming &amp; Acidification\n\nEspecially affects organisms that are already on the verge of stress\n\ne.g. coral bleach (=kicked out symbiotic algae).\n\n30degC: burst cytoplasm.\n\n\n\n\nSea naturally has buffer capacity for protons (by organism carbonates)\nExoskeleton creation (e.g. in corals) emits \\ceCO2​\n\n\nFlow speed\nDensity &amp; Geology: Salt wedge\n\nDifferences in salinity in layers (salinity stratification) in bays where river (=freshwater) flows in\nOften in cheaspeak bay, SF bay used to (← now humans take all freshwater)\nfreshwater/saltwater all\n\n\nViscosity\n\nAffects more small organisms\nboundary layer of water that doesn’t move near hard structure (due to drag)\n\n\nTides\n"},"MarSci-202-PSet-1":{"title":"MarSci 202 PSet 1","links":[],"tags":[],"content":"Following is code used for regression:\n### KELP DATA ###\n\n# Kelp data\nLength_kelp &lt;- c(0.12, 0.56, 0.56, 0.99, 1.20, 20.90, 33.10, 12.50, 8.70, 3.40, 2.90, 16.40, 21.20, 8.90, 12.60)\nBiomass_kelp &lt;- c(21, 32, 27, 34, 45, 1456, 3752, 950, 501, 198, 167, 1275, 1987, 645, 876)\ndata_kelp &lt;- data.frame(Length = Length_kelp, Biomass = Biomass_kelp)\n\n# Linear Model for Kelp\nmodel_kelp &lt;- lm(Biomass ~ Length, data = data_kelp)\n\n# Plot for Kelp with custom y-axis limits to include negative values\nplot(data_kelp$Length, data_kelp$Biomass,\n     main = &quot;Biomass vs Length (Kelp)&quot;,\n     xlab = &quot;Length (m)&quot;,\n     ylab = &quot;Biomass (g)&quot;,\n     pch = 19, col = &quot;blue&quot;,\n     ylim = c(-500, max(data_kelp$Biomass)))  # Set y-axis limits to include negative values\n\n# Add regression line for Kelp model\nabline(model_kelp, col = &quot;red&quot;, lwd = 2)\n\n# New Length values for prediction (Kelp)\nnew_lengths_kelp &lt;- data.frame(Length = c(34.2, 19.8, 15.4, 0.21, 1.5))\n\n# Predict Biomass using the kelp model\npredicted_biomass_kelp &lt;- predict(model_kelp, newdata = new_lengths_kelp)\n\n# Combine Lengths and Predicted Biomass into a data frame\nkelp_predictions &lt;- data.frame(\n  Length = new_lengths_kelp$Length,\n  Predicted_Biomass = predicted_biomass_kelp\n)\n\n# Add the new predicted points to the plot\npoints(kelp_predictions$Length, kelp_predictions$Predicted_Biomass,\n       pch = 17, col = &quot;darkgreen&quot;, cex = 1.5)\n\n# Add a legend to the plot\nlegend(&quot;topleft&quot;, legend = c(&quot;Original Data&quot;, &quot;Regression Line&quot;, &quot;Predicted Points&quot;),\n       col = c(&quot;blue&quot;, &quot;red&quot;, &quot;darkgreen&quot;), pch = c(19, NA, 17), lty = c(NA, 1, NA))\n\n\n### WHALE DATA ###\n\n# Whale data\nLength_whale &lt;- c(3.3, 9.3, 13.2, 4.1, 11.7, 4.2, 4.5, 8.4, 8.1, 6.2, 9.6, 9.7, 3.9, 7.3, 12.2)\nBiomass_whale &lt;- c(600, 9100, 8000, 800, 9800, 2000, 1800, 7500, 4100, 5000, 9800, 9200, 700, 4000, 11800)\ndata_whale &lt;- data.frame(Length = Length_whale, Biomass = Biomass_whale)\n\n# Linear Model for Whale\nmodel_whale &lt;- lm(Biomass ~ Length, data = data_whale)\n\n# Plot for Whale with custom y-axis limits to include negative values\nplot(data_whale$Length, data_whale$Biomass,\n     main = &quot;Biomass vs Length (Whale)&quot;,\n     xlab = &quot;Length (m)&quot;,\n     ylab = &quot;Biomass (kg)&quot;,\n     pch = 19, col = &quot;green&quot;,\n     ylim = c(-10000, max(data_whale$Biomass)))  # Set y-axis limits to include negative values\n\n# Add regression line for Whale model\nabline(model_whale, col = &quot;purple&quot;, lwd = 2)\n\n# New Length values for prediction (Whale)\nnew_lengths_whale &lt;- data.frame(Length = c(14.1, 2.9, 2.1, 13.8, 6.2))\n\n# Predict Biomass using the whale model\npredicted_biomass_whale &lt;- predict(model_whale, newdata = new_lengths_whale)\n\n# Combine Lengths and Predicted Biomass into a data frame\nwhale_predictions &lt;- data.frame(\n  Length = new_lengths_whale$Length,\n  Predicted_Biomass = predicted_biomass_whale\n)\n\n# Add the new predicted points to the plot\npoints(whale_predictions$Length, whale_predictions$Predicted_Biomass,\n       pch = 17, col = &quot;orange&quot;, cex = 1.5)\n\n# Add a legend to the plot\nlegend(&quot;topleft&quot;, legend = c(&quot;Original Data&quot;, &quot;Regression Line&quot;, &quot;Predicted Points&quot;),\n       col = c(&quot;green&quot;, &quot;purple&quot;, &quot;orange&quot;), pch = c(19, NA, 17), lty = c(NA, 1, NA))\nFollowing are figures generated by the code:\n\nFigure 1: Relationship between Biomass and Length in Kelp\nThe scatter plot illustrates the relationship between Length (in meters) and Biomass (in grams) for Kelp. The blue dots represent the original dataset, with Biomass values corresponding to various Lengths. The red line is the linear regression line fitted to the data, indicating the estimated relationship between Length and Biomass. Green triangles show the predicted Biomass values for new Length inputs based on the regression model. The x-axis represents Length (m), and the y-axis represents Biomass (g).\n\nFigure 1: Relationship between Biomass and Length in Whales\nThis scatter plot illustrates the relationship between Length (m) and Biomass (kg) for Whales. The green dots represent the original dataset, with Biomass values corresponding to various Lengths. The purple line is the linear regression line fitted to the data, showing the estimated relationship between Length and Biomass. Orange triangles show the predicted Biomass values for new Length inputs, based on the regression model. The x-axis represents Length (m), and the y-axis represents Biomass (kg).\n&gt; summary(model_kelp)\n\nCall:\nlm(formula = Biomass ~ Length, data = data_kelp)\n\nResiduals:\n   Min     1Q Median     3Q    Max\n-479.6 -172.8   24.9  121.8  587.7\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -169.328     92.746  -1.826   0.0909 .\nLength       100.715      6.858  14.687 1.79e-09 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 253 on 13 degrees of freedom\nMultiple R-squared:  0.9432,\tAdjusted R-squared:  0.9388\nF-statistic: 215.7 on 1 and 13 DF,  p-value: 1.791e-09\n\n&gt; summary(model_whale)\n\nCall:\nlm(formula = Biomass ~ Length, data = data_whale)\n\nResiduals:\n    Min      1Q  Median      3Q     Max\n-3638.8  -785.3  -166.6  1196.0  2114.7\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    -2858       1092  -2.618   0.0213 *\nLength          1098        131   8.382 1.34e-06 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1599 on 13 degrees of freedom\nMultiple R-squared:  0.8439,\tAdjusted R-squared:  0.8319\nF-statistic: 70.26 on 1 and 13 DF,  p-value: 1.337e-06\n\n1. Scatterplots with Regression Lines, Equations, R², and P-values §\nFor the Kelp regression, the equation is:\nBiomass=−169.33+100.72×Length\n\nR² value: 0.9432\np-value: 1.79e-09 (for length parameter estim.)\nFor the Whale regression, the equation is:\n\nBiomass=−2858+1098×Length\n\nR² value: 0.8439\np-value: 1.34e-06 (for length parameter estim.)\n\n2. Predict the Biomass for the New Samples §\nKelp\n  Length Predicted_Biomass\n1  34.20         3275.1112\n2  19.80         1824.8209\n3  15.40         1381.6766\n4   0.21         -148.1782\n5   1.50          -18.2564\nWhales\n  Length Predicted_Biomass\n1   14.1        12627.1889\n2    2.9          327.3169\n3    2.1         -551.2454\n4   13.8        12297.7281\n5    6.2         3951.3863\n3. Interpretation of R² and P-values §\n\nR² Value:\nThe R² value, also known as the coefficient of determination, indicates the proportion of the variance in the dependent variable (Biomass) that is predictable from the independent variable (Length). For example, in the Kelp regression, an R² of 0.9432 means that approximately 94.32% of the variation in Kelp biomass is explained by its length. A higher R² value suggests a better fit of the model to the data.\np-value:\nThe p-value assesses whether the relationship observed between Length and Biomass is statistically significant. A small p-value (typically less than 0.05) indicates strong evidence against the null hypothesis (which posits no relationship between Length and Biomass), meaning that the observed relationship is unlikely to have occurred by chance. In both the Kelp and Whale models, the p-values are very small (1.79e-09 and 1.34e-06), suggesting that Length is a statistically significant predictor of Biomass for both organisms.\n\n4. Predicting with Regression Equations vs. Using the Mean §\n\nRegression vs. Mean:\nThe regression equations provide a more precise and tailored prediction for Biomass based on the specific length of each organism, whereas using the mean Biomass only provides a general estimate for all organisms, irrespective of their length.\nWhy Regression is Better:\nThe regression models account for the relationship between Length and Biomass, which is especially important because Biomass tends to increase with Length in a non-linear fashion. Using the mean would ignore this relationship, leading to less accurate predictions. The high R² values in both models (94% for Kelp and 84% for Whale) indicate that Length is a strong predictor of Biomass, making regression the superior method for making predictions in this case. While the negative predicted biomasses doesn’t make sense, it can be safely ignored in this case, using other (possibly non-linear) models in order for a realistic prediction.\n"},"MarSci-202-PSet-2":{"title":"MarSci 202 PSet 2","links":[],"tags":["Courses"],"content":"Problem 1 §\nA. §\nThe major factors leading to the decline of sea turtles over the past 500 years include over-exploitation, habitat loss, and bycatch. Historically, sea turtles were heavily hunted for their meat, eggs, and shells, which severely reduced their populations. In modern times, incidental capture in fisheries, coastal development affecting nesting sites, and climate change have compounded these pressures. These factors, coupled with pollution and illegal poaching, have caused significant declines in sea turtle numbers (Houtan &amp; Bass, 2007).\nB. §\nThe loggerhead sea turtle (Caretta caretta) population has seen declines of around 43% in some regions, particularly in Florida, over the 18-year period from 1989 to 2006 (Witherington et al., 2009). The green sea turtle (Chelonia mydas) populations in certain regions, such as Peninsular Malaysia, have declined by more than 80% since the 1950s, largely due to egg harvesting and other human activities (Tibbetts, 2009).\nC. §\nFive current threats to sea turtles include:\n\nBycatch in fisheries\nHabitat destruction due to coastal development\nClimate change affecting nesting sites\nIllegal poaching of eggs and adults\nPollution, particularly plastic waste and marine debris\n(Fiedler et al., 2012)\n\nThe most critical threat to address is bycatch, as this causes significant mortality in both adult and juvenile sea turtles, leading to population declines. Reducing bycatch through the implementation of turtle-excluder devices (TEDs) and improved fishing practices can have an immediate and measurable impact on sea turtle conservation (Heppell et al., 1996)\nProblem 2 §\nA §\n\nx: age\nNx​: population at age x\ndx​: deaths during age x\nlx​: percentage alive wrt x=0\nqx​: percentage dead during age x\n\nB &amp; C §\n\nD §\nConservation biologists should focus their efforts on the life stages corresponding to ages 8 and 9, where the highest mortality rates are observed (q8​=0.5090 and q9​=0.3664). These ages likely represent critical developmental stages, such as when turtles transition to new habitats or face increased threats from predators and human activities like bycatch in fisheries. While deaths right after hatching do contribute a noticeable amount of deaths, it is overshadowed by the dropoff in pupulation between ages 8 and 9, implying that money spent on saving sea turtle nests is not the most efficient use of resources.\nProblem 3 §\nA &amp; B §\n\nC §\nAs opposed to the general survivorship of green sea turtles, the population in Bahamas do experience a steep dropoff just after aging (q0​=0.3965) implying that spending money on this early stage i.e. protecting hatchlings would have the biggest impact.\nBibliography §\nHoutan, K., &amp; Bass, O. (2007). Stormy oceans are associated with declines in sea turtle hatching. Current Biology, 17, R590-R591. https://doi.org/10.1016/j.cub.2007.06.021.\nFiedler, F., Sales, G., Giffoni, B., Monteiro‐Filho, E., Secchi, E., &amp; Bugoni, L. (2012). Driftnet fishery threats sea turtles in the Atlantic Ocean. Biodiversity and Conservation, 21, 915 - 931. https://doi.org/10.1007/s10531-012-0227-0.\nWitherington, B., Kubilis, P., Brost, B., &amp; Meylan, A. (2009). Decreasing annual nest counts in a globally important loggerhead sea turtle population.. Ecological applications: a publication of the Ecological Society of America, 19 1, 30-54. https://doi.org/10.1890/08-0434.1.\nTibbetts, J. (2009). Dangerous Delicacy: Contaminated Sea Turtle Eggs Pose a Potential Health Threat. Environmental Health Perspectives, 117, A407 - A407. https://doi.org/10.1289/EHP.117-A407B."},"Marginal-Distribution":{"title":"Marginal Distribution","links":["Joint-Distributions"],"tags":["Math/Probability"],"content":"Discrete Marginal Distribution §\ndef. Marginal Distributions are used when you only have the joint distribution available, and you want to get the distribution of one of the variables. They’re called that because you can write them in the margins.\n\nJoint Marginal Distribution §\ndef. Marginal Distribution of Continuous Joint Distributions. Let X,Y and given fX,Y​. Then PDF of X is:\nfX​(xˉ)=∫y=−∞∞​fX,Y​(xˉ,y)dy\n…and vice versa for fY​.\n\nThe red area is the marginal probability density at x."},"Marginal-Rate-of-Substitution-(MRS)":{"title":"Marginal Rate of Substitution (MRS)","links":["Utility-Function","Rationality-(Economics)"],"tags":["Economics/Micro-Economics"],"content":"def. Marginal Rate of Substitution. MRS1,2​ asks “How much of x2​ can you give me for one unit of x1​?”\n\nMRS1,2​ means MRS of good x1​ for good x2​, i.e. “one unit of x1​ for how many units of x2​?&quot;&quot;\n\nMRS1,2​:=−dx1​dx2​​=−∂x2​∂u​∂x1​∂u​​\n\n\n                  \n                  x_{1},x_{2} changes spots in the nominator/denominator in the final formula.\n                  \n                \n\n\n\nThe tangent of Utility Function at a certain point is the MRS at that specific point (more accurately, its absolute value of the tangent)\nConvexity Assumption causes MRS to decrease (=Diminishing MRS) rightward (the more of good x1​ we consume, the more we would like x2).\nWhen two people have different MRS, then they can trade for both of their utility.\n\ne.g. MRSA​=−3, MRSB​=−21​\n⇒ they can trade at any rate where MRSA​&lt;rate&lt;MRSB​ to both of their benefit.\n\n\n\nDeriving the MRS Formula §\nWe’re looking for a place on the scalar field provided by u(x1​,x2​) where the gradient is zero.\n⇒ Thus we need to find a point where the following is true:\n∇u(x1​,x2​)=δx1​δu​dx1​+δx2​δu​dx2​=0\nBy the definition of MRS it’s the gradient of a indifference curve, i.e. the gradient of a level curve in the field:\nMRS=−dx1​dx2​​=−∂u/∂x2​∂u/∂x1​​\nChanges in MRS §\n\nMRS can change due to exogenous factors; in this case the whole indifference curve will change accordingly too.\ne.g. After 9.11., air travel is perceived by consumers to be less safe.\n\n\n\nObserve that air travel becoming less safe, causes the MRS to decrease, as one is willing to trade less money for flying by air.\nYou can think of this from a different perspective by instead of thinking of air safety as an exogenous factor, actually quantifying air safety as a measure as well, and graphing it together.\nGraphing air safety as the third axis shows that we can in fact visualize the decrease in MRS of air miles and $‘s of other consumption.\n\n"},"Marginal-Willingness-to-Pay":{"title":"Marginal Willingness to Pay","links":["CGP-Grey","Consumer-Surplus","Expenditure-Minimization","Utility-Function","HD"],"tags":["Economics/Micro-Economics"],"content":"\n\n                  \n                  Abstract \n                  \n                \nBasically, “In a vacuum, how much do you want it?”\n\nThink CGP Grey’s “I’d pay anything for Apple to make X.”\nUsed to calculate Consumer Surplus\nDoesn’t really show up in actual reality. A theoretical construct to help calculate some things\n\n\nMarginal Willingness to Pay (MWTP) Curve is derived from a single Indifference curve’s MRS along different consumption quantities—when the y-axis is measured in $ of other goods.\nDerivation. Expenditure Minimization of the Utility Function\nProperties.\n\nDecreasing in own price\nHD0 in prices (h1​(tp1​,tp2​,uˉ)=h1​(p1​,p2​,uˉ))\n\nIn other words, if the ratio of prices are the same, your behavior won’t change (but you may need more income).\n\n\n\n⇒ The area under the MWTP curve is closely related to Consumer Surplus.\n→ Think of it as the amount a consumer is willing to pay per unit of good—when they want it more, they’ll obviously willing to pay less for even more of it, so the MWTP slopes down.\n\nThis is sometimes called the Compensated Demand Curve, since you can think of it in terms of:\n\nA change in price (blue to red)\nCompensating your budget by a certain amount so you’re not worse off than before (red to green)\nGraphing the optimal points along quantity consumed and price\n\nMWTP curves can be same or different to own-price demand curves (OPDC). This is because\n\n\nMWTP accounts only for the substitution effect, while OPDC accounts for both the substitution effect and the income effect\nMWTP is along one indifference curve—a fixed utility—, while OPDC doesn’t care about utility.\n\n"},"Marine-Mammals":{"title":"Marine Mammals","links":[],"tags":["Biology"],"content":"Pinnipeds §\nclade. Pinnipedia (Pinnepeds) seals, sea lions, fur seals and walrus (Carnivores)\nCetaceans §\ninfraorder. Cetacea (Cetaceans) whales, dolphins and porpoises. Two main types:\n\nOdontocetes. small toothed ones (odontocetes)\n\nType.\n\nporpoise (spade teeth)\ndolphin (conical teeth)\nsperm whale (biggest), eats gian squid\nmonodonts (narwhal &amp; beluga)\n\n\nFeeding. on medium-sized sea animals\nSenses. uses ecolocation\n\n\nMysticetes (=baleen whales). big non-toothed ones\n\nTypes. blue whale, humpback whale, etc.\nFeeding. Small animals by “filtering” them out from a giant gulp of water\n\n\n\nOthers §\n\nSirenians. manatees and dugongs\nMustelidae. Otters\nUrsi. Polar bear\n"},"Market-Beta":{"title":"Market Beta","links":["CAPM-Model"],"tags":["Economics/Finance"],"content":"Market Beta (β)\n\nIntuition. Market Beta measure the correlation between security i with market M. It therefore is proportional to market risk (if β is big, market volatility triggers security volatility).\nConsider:\n\nSecurity i with (Ri,​,σi​,μi​)\nMarket (RM​,σM​,μM​)\n&amp; Market Beta measures the degree to which the security price correlates with the market price\n\nMarket beta is a coefficient of regression, derived from past data\n\n\nMarket’s market beta is 1.\n\n\nDefinition: βi​:=σM2​Cov(Ri​,RM​)​=ρi,M​⋅σM​σi​​\nβi​=μM​−μf​μi​−μf​​ or equivalently μi​−μf​=β(μM​−μf​) ← ”CAPM Model”\n\nUnder assumptions V is positive definite, μ​,e are linearly independent and 0≤μf​&lt;max(μ​)\nβ&lt;0: security return is opposite of market\nβ=0: security is uncorrelated to market\n0&lt;β&lt;1: security return moves same as market, but fluctuates less (defensive)\n1&lt;β: security return moves same as market, but fluctuates more (aggressive)\n\n\n\n\n\n                  \n                  Security Market Line \n                  \n                \nPlotting μi​−μf​=β(μM​−μf​) with return vs market beta:\n\n"},"Market-Power":{"title":"Market Power","links":["Monopolistic-Competition"],"tags":["Economics/Micro-Economics"],"content":"Monopolistic Competition"},"Market-Risk":{"title":"Market Risk","links":["External/Fundamental-Review-of-the-Trading-Book","Portfolio-Theory-(Risk)","Taylor-Approximation"],"tags":["Economics/Finance"],"content":"Market risk considers the following set of risks to a portfolio:\n\nEquity risk\nInterest rate risk\ncurrency risk\nCommodity risk\nmargining risk\n\nFundamental Review of the Trading Book is the regulation that require banks to have certain about of capital (for cushion in case of statistics)\nTerminology §\n\nValue at Risk (VaR): investment\n\n\nContinuing from Portfolio Theory (Risk)\ndef. Time series. Some of the time series like risk changes can be continuous, but we want to model discrete time-step changes. So we turn the continuous time t into:\nτt​:= index t​​⋅ quanta of time Δt​​\n\nΔt is usually 3651​ or 2501​(trading days per year), i.e. timestep of one day.\ndef. Loss Operator. Instead of mapping g from risk factor to portfolio value (as in a portfolio), we’re interested in how risk factor change maps to loss. Thus we define the Loss Operator l:\n\nl[t]​(x):=−(g(τt+1​,zt​+x)−g(τt​,zt​))\n\ng:(τt​,Zt​)→Vt​ but as opposed to f is a discrete time-step function\nzt​ is the risk factor vector\nx is the risk factor change at time t\n\nDelta Approximation §\nWe can approximate the loss operator with a first-order Taylor Approximation:\ndef. First-Order (=Delta) Approximation of loss operator:"},"Markov-Chain":{"title":"Markov Chain","links":["Stochastic-Process","(Proof)-Markov-Recurrence-by-Sum-Divergence"],"tags":["Math/Probability"],"content":"def. Markov Chain. A Stochastic Process {Xt​}t=0,1,2,…​ where:\n\nXt​ has finite or infinite states =0,1,…\nL(Xn+1​) only depends on L(Xn+1​), i.e. is memoryless\nP(Xn+1​=j∣Xn​=i)=Pij​ where P is a matrix that contains all transition probabilities, the transition matrix:\n\nP=​P00​P10​⋮Pi0​⋮​P01​P11​⋮Pi1​⋮​P02​P12​⋮Pi2​⋮​⋯⋯⋱⋯⋱​​\n\nRow sums are always 1, since total probability exiting a state should add to 1\nAbsorbing States\n\nthm. Chapman-Kolmogorov Eq. Given Markov chain {Xt​}t=0,1,2,…​ and transition probability matrix P, the transition probability from state i to j in n steps is denoted Pijn​ and for all i,j in matrix Pn, the n-step transition matrix. This is calculated as:\nPijn+m​=∀k∈states∑​Pikn​Pkjm​\nAbsorbing States §\ndef. Absorbing State is a state i where once entered, it is never left.\nMotivation. Calculation of the probability that, from an initial state, the process will ever enter a certain state is a common question yet difficult to calculate. The following formalizes this type of problem and offers a concise solution.\nConsider a Markov chain {Xn​}n=0,1,…​, initial state i, transition matrix P. let A⊂states and we want to know if X ever enters any states A within m transition steps. This is:\n​P(∃k≤m,Xk​∈A∣X0​=i)=P(k=1⋃m​{Xk​∈A}∣X0​=i)=P(k=1⋃m​[j=1⋂k−1​{Xk​∈A}]∣X0​=i)​​\nwhich is a pain in the ass to calculate.\nInstead, we construct a new Markov chain {Wn​}n=0,1,…​, transition matrix Q with states AC∪{A∗} which A∗ is an absorbing state. Q is constructed accordingly to model this absorption, in the following way:\n\nQij​=Pij​ for all i,j∈A\nQiA∗​=∑j∈A​Pij​, obviously only for i∈A\nQAA​=1 since it’s an absorbing state\nThen the original probability is\n\nP(∃k≤m,Xk​∈A∣X0​=i)=P(Wm​=A∗∣W0​=i)=QiA∗m​​​\nClassification of States §\ndef. Accessible. From state i state j is accessible iff:\n∃n≥0, Pijn​&gt;0\ndef. Communication &amp; Class. State i and j communicate iff i accesses j and vice versa. Properties:\n\nIdentity. i communicates with itself\nCommutative. if i comm. with j, and j with k, then i comms. with k. Proof:\n\nPikn+m​:=∀r∈states∑​Pirn​Prkm​≥Pijn​Pjkm​&gt;0​ since i,j and j,k communicates ​\ndef. Irreducible Markov Chain. A markov chain that contains only one class. We also see below, irreducible markov chain has one recurrent class.\ndef. Recurrence and Transience. Initial state i. Let fi​=P(⋃k=1∞​{Xk​=i}∣X0​=i), i.e. that we will ever visit state i again (indefinitely) State i is:\n\nrecurrent if fi​=1, will visit again\ntransient if fi​&lt;1, will probably never visit again\nthm. Determiniation of Recurrence. State i is:\nrecurrent if ∑n=1∞​Piin​&lt;∞ Converges\ntransient if ∑n=1∞​Piin​=∞ Diverges\nProof. See (Proof) Markov Recurrence by Sum Divergence.\nThis leads to following corollaries that are pretty self-evident.\n\n\nthm. Finite state markov chains always contain a recurrent state.\n\n…but infinite state markov chains may have no recurrent states\n\n\nthm. Recurrence/transience is a class property. If state i,j comm., and i is recurrent, so is j.\nthm. Irreducible markov chains have one recurrent class that contains all states.\ndef. Positive/Null Recurrent. let state i, and T=min{XT​=i∣X0​=i}, the expected time of return. Then i:\n\nE[T]={&lt;∞=∞​Positive-RecurrentNull-Recurrent​\nIntuition. Null recurrence is an odd variety. It means the process will definitely eventually return to i, but you have to wait infinitely long(!).\n\nthm. A Markov Chain does not have a stationary distribution iff it is null recurrent\nthm. In a finite-state markov chain, all recurrent states are positive recurrent.\ndef. Periodic/Aperiodic. A state i is periodic with period d&gt;1 iff:\n\n\nPiin​=0 for all n not a multiple of d\nd=max{d:d divides n}, the largest of such numbers\n\n\nIf d=1, the state is aperiodic.\nthm. Periodicity is a class property.\n\nStationary Distribution §\nMotivation. In certain Markov chains, as we run the process for a large number of steps, the fraction of time spent in every state is approaches some enumber. This is equally the transition matrix at infinite number of transitions.\nn→∞lim​Pn\n\ndef. Ergodic state. A state that is positive-recurrent, aperiodic.\ndef. Ergodic Markov Chain is a markov chain with only ergodic states.\nSince ergodicity is naturally also a class property, we consider irreducible ergodic MCs, i.e. chains that “go in cycles”, will return in a finite amount of time, and has only one “cycle group”.\nthm. Stationary Distribution. For an ergodic markov chain there exists the limit:\n\nn→∞lim​Pn=​π1​⋮π1​​π2​⋮π2​​⋯⋱⋯​πn​⋮πn​​​=[π1​,π2​,…,πn​​]\nNotice first that each column has just one limiting value. Thus we index π with just one index, the destination. πj​ for each column is the unique non-negative solution to:\nπj​1​=i=0∑∞​πi​Pij​,=j=0∑∞​πj​​j≥0is a distribution​​\n\nthm. If MC is (just) irreducible then there always exists such a solution iff MC is positive recurrent\n\nIf solution exists i.e. MC is pos-rec, solution will be unique\nIf MC is aperiodic as well, πj​ is then the limiting probability.\n\n\n"},"Markov-Inequality":{"title":"Markov Inequality","links":[],"tags":["Math/Statistics"],"content":"thm. Markov Inequality. For a non-negative random variable X, the following hols for all positive constant a:\nP(X≥a)≤aE(X)​"},"Marsci-202-PSet-3":{"title":"Marsci 202 PSet 3","links":[],"tags":["Courses"],"content":"Homework 3 §\nQ1 §\n\nSea Fans belong to the Phylum Cnidaria and Class Anthozoa because of:\n\nPresence of Cnidocytes: Specialized stinging cells used for prey capture and defense, characteristic of all Cnidarians.\nPolyp-Only Life Cycle: As Anthozoans, they exist solely in the polyp form and do not have a medusa stage.\n\n\nBlue Crabs belong to the Phylum Arthropoda and Class Malacostraca because of:\n\nExoskeleton and Jointed Appendages: Arthropods possess a chitinous exoskeleton and segmented bodies with jointed limbs.\nBody Segmentation into Tagmata: Malacostracans have bodies divided into head, thorax, and abdomen, with a carapace covering the thorax.\n\n\n\nQ2 §\nSimilarities:\n\nExternal Appendages for Prey Capture. Sea Fans use their polyps and tentacles to trap plankton, while Blue Crabs use their claws to grab and manipulate larger prey like clams.\nFeeding Influenced by Water Flow. Their feeding rates are affected by the hydrodynamic conditions.\nHeterotrophic/Carnivorous Feeding. Both consume other organisms for nutrition.\n\nDifferences:\n\nFeeding Mechanism: Sea Fans are passive suspension feeders using tentacles to capture plankton, while Blue Crabs are active predators/scavengers using claws to capture prey.\nMobility: Sea Fans are sessile organisms attached to substrates, whereas Blue Crabs are mobile and can move to locate food.\nPrey Type: Sea Fans feed on microscopic plankton, while Blue Crabs consume larger organisms like clams and other benthic fauna.\n\nQ3 &amp; Q4 §\nCode\n \n# Data for Sea Fans and Blue Crabs\nsea_fans_data = {\n    &quot;Replicate&quot;: [1, 2, 3, 4, 5, 6, 7, 8, \n                  1, 2, 3, 4, 5, 6, 7, 8],\n    &quot;Treatment&quot;: [&quot;Low Flow (2cm/s)&quot;]*8 + [&quot;High Flow (20cm/s)&quot;]*8,\n    &quot;# copepods eaten&quot;: [21, 32, 27, 34, 45, 52, 29, 30, \n                1020, 987, 675, 1156, 721, 447, 823, 678]\n}\n \nblue_crabs_data = {\n    &quot;Replicate&quot;: [1, 2, 3, 4, 5, 6, 7, 8, \n                  1, 2, 3, 4, 5, 6, 7, 8],\n    &quot;Treatment&quot;: [&quot;Low Flow (2cm/s)&quot;]*8 + [&quot;High Flow (20cm/s)&quot;]*8,\n    &quot;# clams eaten&quot;: [9, 6, 11, 8, 7, 6, 12, 11, \n                      2, 1, 3, 0, 2, 1, 2, 1]\n}\n \nsea_fans_df = pd.DataFrame(sea_fans_data)\nblue_crabs_df = pd.DataFrame(blue_crabs_data)\n \n# Calculate mean and standard deviation\nsea_fans_stats = sea_fans_df\n    .groupby(&quot;Treatment&quot;)[&quot;# copepods eaten&quot;]\n    .agg([np.mean, np.std])\nblue_crabs_stats = blue_crabs_df\n    .groupby(&quot;Treatment&quot;)[&quot;# clams eaten&quot;]\n    .agg([np.mean, np.std])\n \n# dataframe manipulate for t-tests between high flow and low flow\nsea_fans_low_flow = sea_fans_df[\n    sea_fans_df[&quot;Treatment&quot;] == &quot;Low Flow (2cm/s)&quot;\n    ][&quot;# copepods eaten&quot;]\nsea_fans_high_flow = sea_fans_df[\n    sea_fans_df[&quot;Treatment&quot;] == &quot;High Flow (20cm/s)&quot;\n    ][&quot;# copepods eaten&quot;]\n \nblue_crabs_low_flow = blue_crabs_df[\n    blue_crabs_df[&quot;Treatment&quot;] == &quot;Low Flow (2cm/s)&quot;\n    ][&quot;# clams eaten&quot;]\nblue_crabs_high_flow = blue_crabs_df[\n    blue_crabs_df[&quot;Treatment&quot;] == &quot;High Flow (20cm/s)&quot;\n    ][&quot;# clams eaten&quot;]\n \nsea_fans_ttest = stats.ttest_ind(\n    sea_fans_low_flow, sea_fans_high_flow\n    )\nblue_crabs_ttest = stats.ttest_ind(\n    blue_crabs_low_flow, blue_crabs_high_flow\n    )\n \n### Show the stats &amp; p-val\nsea_fans_stats, blue_crabs_stats, \nsea_fans_ttest.pvalue, blue_crabs_ttest.pvalue\nOutput\n                       mean         std\n Treatment                              \n High Flow (20cm/s)  813.375  230.081313\n Low Flow (2cm/s)     33.750   10.053429,\n                     mean      std\n Treatment                        \n High Flow (20cm/s)  1.50  0.92582\n Low Flow (2cm/s)    8.75  2.37547,\n 1.595498936667517e-07,\n 1.2862551269462247e-06\n\n\nCode\n# Plotting the bar graph\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\n \n# Sea Fans Plot\nax[0].bar(\n    sea_fans_stats.index, \n    sea_fans_stats[&quot;mean&quot;], \n    yerr=sea_fans_stats[&quot;std&quot;], \n    capsize=5)\nax[0].set_xlabel(&quot;Flow Treatment&quot;)\nax[0].set_ylabel(&quot;Mean Copepods Eaten&quot;)\nax[0].text(\n    0.5, \n    max(sea_fans_stats[&quot;mean&quot;]), \n    f&quot;P-value = {sea_fans_ttest.pvalue:.4f}&quot;,\n    ha=&#039;center&#039;)\nax[0].set_title(&quot;Sea Fans&quot;)\n \n# Blue Crabs Plot\nax[1].bar(\n    blue_crabs_stats.index, \n    blue_crabs_stats[&quot;mean&quot;],\n    yerr=blue_crabs_stats[&quot;std&quot;],\n    capsize=5)\nax[1].set_xlabel(&quot;Flow Treatment&quot;)\nax[1].set_ylabel(&quot;Mean Clams Eaten&quot;)\nax[1].text(\n    0.5, \n    max(blue_crabs_stats[&quot;mean&quot;]), \n    f&quot;P-value = {blue_crabs_ttest.pvalue:.4f}&quot;, \n    ha=&#039;center&#039;)\nax[1].set_title(&quot;Blue Crabs&quot;)\n \nplt.tight_layout()\nplt.show()\nOutput\n\nFigure: The effect of flow rate (2 cm/s and 20 cm/s) on the feeding rates of sea fans and blue crabs. Bars represent the mean number of prey (copepods or clams) consumed under low and high flow conditions. Error bars indicate standard deviation. Significant differences were observed between flow treatments for both species, with sea fans showing a higher feeding rate under high flow conditions and blue crabs showing a lower feeding rate. P-values for t-tests are presented within each.\nQ5 §\nEffects of Water Flow on Feeding Rates of Sea Fans and Blue Crabs §\nAbstract §\nThis study investigates the impact of water flow on the feeding rates of Sea Fans (Phylum Cnidaria, Class Anthozoa) and Blue Crabs (Phylum Arthropoda, Class Malacostraca). Feeding rates were measured under low (2 cm/s) and high (20 cm/s) flow conditions. Results indicate that high flow significantly increased feeding rates in Sea Fans while decreasing feeding rates in Blue Crabs. These findings highlight species-specific responses to water flow and underscore the importance of hydrodynamic conditions in marine ecosystems.\nResearch Questions §\n\nHow does water flow rate affect the feeding rates of Sea Fans?\nHow does water flow rate affect the feeding rates of Blue Crabs?\nAre the effects of water flow on feeding rates similar or different between these two species?\n\nResults §\n\nFigure: The effect of flow rate (2 cm/s and 20 cm/s) on the feeding rates of sea fans and blue crabs. Bars represent the mean number of prey (copepods or clams) consumed under low and high flow conditions. Error bars indicate standard deviation. Significant differences were observed between flow treatments for both species, with sea fans showing a higher feeding rate under high flow conditions and blue crabs showing a lower feeding rate. P-values for t-tests are presented within each.\nIn Sea Fans, the mean number of copepods consumed under low flow was 3.75±10.05, whereas under high flow, it increased to 813.38±230.08. A t-test confirmed that this difference is statistically significant (p≈1.60×10−7). Conversely, Blue Crabs showed a decrease in feeding rates from 8.75±2.38 clams under low flow to 1.50±0.93 clams under high flow, with the difference also being statistically significant (p≈1.29×10−6). These results suggest that high water flow enhances feeding in Sea Fans but inhibits feeding in Blue Crabs.\nDiscussion §\nThe study demonstrates a stark contrast in how water flow affects the feeding strategies of Sea Fans and Blue Crabs. Sea Fans exhibited significantly higher feeding rates under high flow conditions. This can be attributed to their passive suspension feeding mechanism, where increased water flow delivers more planktonic prey to their extended polyps. The structural morphology of Sea Fans, with their fan-like branches, is adapted to capture food particles efficiently in flowing water.\nIn contrast, Blue Crabs showed reduced feeding rates under high flow. As active predators and scavengers, Blue Crabs rely on their ability to detect and capture prey, which can be hindered by strong currents. High flow conditions may disrupt their sensory cues or make it energetically unfavorable to pursue prey."},"Marsci-Exam-1-Prep":{"title":"Marsci Exam 1 Prep","links":[],"tags":[],"content":"Select Terminology §\n\nSessile lifestyle\nbenthic environment vs. pelagic\nradioles vs radula\nSubstrate\nNematocyst\nRadula\nScyphozoa\nAnthozoa\nChoanocytes\nArcheocytes\nColloblasts\nMadreporite\nPelagic\nCountershading\nTropicalization\nFacilitation\n\nQuestions §\n\nDefine science, ecology, driving agent, and response variable.  What are the 5 different approaches in the scientific method? Provide a one-sentence definition of each approach. List a strength and weakness for each method?\nWhat is a trophic cascade? Describe the trophic cascade that regulates plants in southern U.S. salt marshes. How is drought thought to interact with this indirect species interaction?\nWhat are the main climate change drivers affecting marine foundation species, and how do these drivers interact with other anthropogenic stressors?\n\nInvertebrates §\n\nWhat is a foundation species? Name 4 invertebrate taxa that can act as foundation species.  What characteristics do these taxa have in common that allow them to rise to such prominence in marine systems?\n\nFoundation species: Species that create or modify habitats, benefiting other organisms and enhancing ecosystem stability.\nExamples of invertebrate foundation species:\n\nCorals (Cnidaria)\nOysters (Bivalvia)\nKelp-associated amphipods (Crustacea)\nTube worms (Polychaeta)\n\n\nCommon characteristics:\n\nAbility to modify the environment (reef-building, bioturbation)\nSupport complex food webs\nResilience to environmental stress\nCreate habitats used by many other species. \n\n\n\n\nCompare feeding in sea stars and sea cucumbers. Describe 2 things they have in common and 2 that are different.\n\nCommonalities:\n\nBoth belong to the phylum Echinodermata.\nBoth use external parts for feeding (e.g., tube feet or tentacles).\n\n\nDifferences:\n\nSea stars: Carnivorous, evert stomach to digest prey externally.\nSea cucumbers: Deposit or suspension feeders, use tentacles to collect particles.\n\n\n\n\n⭐ How is it that we are more closely related to blob-like sea squirts than highly complex insects? \n\nReason: Both sea squirts and humans are part of the phylum Chordata.\nCommon chordate traits:\n\nNotochord (at least in embryonic stages).\nDorsal nerve cord.\nPharyngeal slits.\n\n\nInsects belong to the phylum Arthropoda, which is more distantly related.\n\n\n\nMollusca &amp; Annelids: Clams, Octopus, Segmented Worms §\n\nWhat are the 4 major classes in the phylum Mollusca? Choose 3 of these classes and compare and contrast 3 adaptations each has evolved from a common body plan for either a life in the benthic or pelagic environment.\n\nClasses:\n\nGastropoda (snails, slugs)\nBivalvia (clams, mussels)\nCephalopoda (octopuses, squids)\nPolyplacophora (chitons)\n\n\nAdaptations (Gastropoda):\n\nShell for protection.\nRadula for scraping food.\nAdaptation to terrestrial environments.\n\n\nAdaptations (Bivalvia):\n\nTwo-part shell for defence.\nFilter feeding with siphons.\nSessile lifestyle in benthic environments.\n\n\nAdaptations (Cephalopoda):\n\nTentacles for prey capture.\nAdvanced nervous system.\nJet propulsion for swimming.\n\n\nAdaptations in Polyplacophora (chitons) Adaptations for Benthic Life:\n\nMultiple shell plates: Flexible, allows chitons to conform to uneven surfaces like rocks.\nStrong foot for adhesion: Helps resist wave action and predators in intertidal zones.\nRadula with magnetite-enriched teeth: Scrapes algae off hard surfaces more effectively.\n\n\n\n\nWhat are 3 characteristics that distinguish annelids from mollusks and 3 characteristics they have in common?  \n\nDifferences:\n\nAnnelids: Segmented bodies. Closed circulatory system.\nMollusks: Often have shells. Usually open circulatory system.\n\n\nCommonalities:\n\nBilateral symmetry.\nPresence of a coelom.\nTrochophore larvae in some species.\nprotostome clade, in which the mouth develops before the anus during embryonic development.\n\n\n\n\nDescribe 3 unique adaptations that annelids have evolved for tube dwelling versus errant lifestyles.\n\nTube-dwelling annelids:\n\nBuild protective tubes (calcareous, mucous, or sandy).\nFilter feeding using radioles.\nReduced locomotion.\n\n\nErrant annelids:\n\nParapodia for active movement.\nWell-developed sensory organs.\nPredatory or scavenging lifestyle.\n\n\n\n\nWhat is a radula, and which Phylum is it unique to?  Describe 4 different ways it has evolved for specialized use.\n\nRadula: A ribbon-like structure with teeth used for feeding.\nUnique to: Phylum Mollusca.\nSpecialized uses:\n\nScraping algae (Gastropoda).\nDrilling into shells (some snails).\nCutting prey (Cephalopoda).\nFiltering plankton (some bivalves).\n\n\n\n\n\nCnidaria, Porifera: Corals, Jellyfish, Sponges §\n\nCorals are successful benthic animals that can monopolize space. Discuss 3 reasons that might explain why corals instead of sponges are the dominant space-holding invertebrates on most tropical, shallow water reefs.  Why do sponges tend to dominate reefs greater than 100m in depth? Hint: sponges also dominate deep underneath ledges on coral reefs. \n\nCorals:\n\nPhotosynthetic symbionts (zooxanthellae) aid in energy production.\nRapid skeleton growth to monopolizes space.\nStrong competition for light in shallow waters.\n\n\nSponges:\n\nEfficient filter feeders, adapted to low-light conditions.\nCan survive in nutrient-poor environments.\nOften dominate deeper reefs or under ledges.\n\n\n\n\nCompare and contrast the feeding strategies used for prey capture and subjugation used by a hard coral, sponge, and sea walnut (ctenophore), with specific mechanisms.\n\nHard Coral (Scleractinia):\n\nFeeding Strategy: Suspension feeder using tentacles and nematocysts.\nPrey Capture Mechanism: Uses tentacles to capture plankton and small particles from the water. Specialized stinging cells called nematocysts on the tentacles release toxins to immobilize prey.\nSubjugation Mechanism: Once prey is immobilized, the tentacles bring the food to the mouth, where it is digested in the gastrovascular cavity.\n\n\nSponge (Porifera):\n\nFeeding Strategy: Filter feeder using choanocytes (collar cells).\nPrey Capture Mechanism: Water is drawn into the sponge through tiny pores (ostia) and filtered through choanocytes, which have flagella that create water current. Microorganisms and organic particles are trapped in the mucus on the choanocytes’ collars.\nSubjugation Mechanism: The trapped particles are engulfed by the choanocytes via phagocytosis, and nutrients are distributed throughout the sponge by amoeboid cells.\n\n\nSea Walnut (Ctenophore):\n\nFeeding Strategy: Predatory suspension feeder using colloblasts.\nPrey Capture Mechanism: Uses specialized sticky cells called colloblasts on its tentacles to capture zooplankton and small prey. Tentacles extend and sweep through the water, ensnaring prey with the sticky colloblasts.\nSubjugation Mechanism: Once captured, the tentacles retract, and the prey is moved toward the mouth, where it is swallowed and digested.\nComparison and Contrast:\n\n\n\n\n\n\nSimilarities:\n\nAll three organisms are suspension feeders, relying on waterborne prey or particles.\nEach has specialized cells (nematocysts, choanocytes, colloblasts) adapted to capture prey.\n\n\nDifferences:\n\nCoral uses toxins and stinging cells (nematocysts) for active prey immobilization.\nSponges rely on passive filtering through choanocytes, with no active predation or immobilization.\nSea walnuts use sticky colloblasts to trap prey without stinging or toxins.\nHard corals and sea walnuts are more active in prey capture, while sponges passively filter water for nutrients.\n\n\n\n\nCompare and contrast alternation of generation and polymorphism in the phylum Cnidaria. Give specific examples of each using specific species.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectAlternation of GenerationsPolymorphismWhat it involvesAlternating between polyp (asexual) and medusa (sexual) stages.Different specialized forms (zooids) in a colony.Occurs inSpecies like jellyfish and some hydrozoans.Mostly in colonial cnidarians (e.g., siphonophores).Reproductive roleInvolves both asexual and sexual reproduction.Different zooids may specialize in reproduction, feeding, or defense.PurposeFacilitates life cycle completion across habitats (benthic/pelagic).Allows efficient division of labour within a colony.Example speciesMoon jelly (Aurelia aurita).Portuguese man o’ war (Physalia physalis).\n\nAlternation of generations enables individual cnidarians to switch between sessile and free-swimming forms, maximizing reproductive success and habitat utilization.\nPolymorphism enhances the efficiency of colonial life by assigning specific tasks (feeding, defense, reproduction) to different zooids within the colony.\nTogether, these strategies exemplify the diverse life history adaptations that cnidarians have evolved to thrive in marine environments.\n\n\nDefine bipartite life cycle.  Describe the bipartite life cycle in 3 different species each occurring in a different phylum.  Also, describe at least 3 alternative hypotheses that potentially led to the evolution of the bipartite life history strategy in many marine organisms.\n\nBipartite life cycle: Life stages split between planktonic larval stage and benthic or adult stage.\nExamples:\n\nCorals (Cnidaria): Planula larvae and adult polyp.\nSea urchins (Echinodermata): Pluteus larvae and adult.\nAnnelid worms: Trochophore larvae and benthic adult.\n\n\nHypotheses for evolution:\n\nReduced intraspecific competition between life stages.\nIncreased dispersal opportunities for larvae.\nAdaptation to different environmental conditions.\n\n\n\n\nCompare and contrast the feeding behavior and skeletons of Porifera, Echinodermata, and Cnidaria.\n\nPorifera (sponge):\n\nFeeding: Filter feeders, capture particles via choanocytes.\nSkeleton: Spicules or spongin fibers.\n\n\nEchinodermata (stars, urchins, cucumbers):\n\nFeeding: Varies (e.g., predation, grazing, deposit feeding).\nSkeleton: Calcareous endoskeleton with ossicles.\n\n\nCnidaria:\n\nFeeding: Use nematocysts to capture prey.\nSkeleton: Calcium carbonate skeleton (in corals).\n\n\n\n\n\nCrustaceans §\n\nDescribe the process of molting in blue crabs. What are 3 advantages and 3 disadvantages of having an exoskeleton on top of your epidermis.\n\nMolting Process in Blue Crabs:\n\nPremolt (preparation): The crab absorbs calcium from the old shell and stores it internally. A new, soft exoskeleton forms underneath.\nMolting (ecdysis): The old shell splits, and the crab crawls out. The new exoskeleton is soft and pliable.\nPostmolt (expansion): The crab absorbs water to expand its size, allowing space for future growth.\nHardening: The new shell hardens and calcifies over several days.\n\n\nAdvantages of an Exoskeleton:\n\nProvides protection from predators and environmental damage.\nActs as a barrier to water loss, crucial in aquatic and intertidal environments.\nOffers structural support for muscle attachment, aiding movement.\n\n\nDisadvantages of an Exoskeleton:\n\nLimits growth, requiring the crab to molt frequently.\nDuring molting, the crab is vulnerable to predation.\nExoskeleton production is energetically expensive.\n\n\n\n\nDo you think crustaceans or insects have been more successful over earth’s history?  Defend your answer in 4-5 sentences.\n\nInsects are generally more successful than crustaceans.\n\nInsects dominate terrestrial environments with vast species diversity and adaptations like flight and metamorphosis.\nCrustaceans thrive in aquatic habitats, but insects have spread across more ecological niches.\nInsects have evolved highly complex social systems (e.g., ants, bees), which have contributed to their success.\nBoth groups are highly successful, but insects’ global distribution and diversity make them more influential overall.\n\n\n\n\n\nVertebrates §\n\nName three sensory capabilities that marine vertebrates have that we don’t (or barely have) and explain how each one works. Use drawings if that is helpful in your explanation.\n\nEcolocation\n\nSpeak: Phonic lips create sound → Melon amplifies noise\nListen: Lower jaw is earbone → brain interprets sound \n\n\nElectroreception (sharks, rays):\n\nDetects electric fields generated by other animals’ muscle movements.\nWorks through ampullae of Lorenzini—gel-filled pores that sense electrical signals.\nIn the ampullae there are receptor cells with potassium-calcum pumps \n\n\nLateral Line System (fish, amphibians):\n\nDetects water movement and vibrations around the animal.\nConsists of sensory hair cells embedded in canals along the body which is connected to the outside seawater\n\n\n\n\nDescribe thermoregulation in sea otters, great white sharks, walruses, blue whales, green turtles, and Nassau groupers. Which of these species can tolerate the greatest range of temperatures, and why is that the case?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeciesThermoregulation StrategySea OttersDense fur traps air for insulation.Great White SharksCountercurrent heat exchange to retain warmth.WalrusesBlubber for insulation in cold waters.Blue WhalesBlubber maintains body temperature in cold oceans.Green TurtlesEctothermic; bask to regulate body temperature.Nassau GroupersEctothermic; prefer warm waters.\n\nMost temperature-tolerant: Great white sharks, due to their endothermic abilities and countercurrent heat exchange.\n\nTurtles §\n\nGive the common and scientific names of all the known sea turtle species.  Compare and contrast feeding behavior in green turtles vs. loggerheads, giving 3 examples of how they are similar and 3 examples of how they are different.  How is the digestive track of each adapted to their specific diets?\n\n\nSpecies:\n\nLoggerhead (Caretta caretta)\nGreen turtle (Chelonia mydas)\nHawksbill (Eretmochelys imbricata)\nLeatherback (Dermochelys coriacea)\nKemp’s ridley (Lepidochelys kempii)\nOlive ridley (Lepidochelys olivacea)\nFlatback (Natator depressus)\n\n\nFeeding behavior\n\nSimilarities:\n\nBoth are herbivorous or omnivorous as juveniles.\nBoth use beak-like jaws adapted for feeding.\nBoth play essential roles in maintaining ecosystem balance (e.g., controlling seagrass and jellyfish populations).\n\n\nDifferences:\n\nGreen turtles: Primarily herbivorous, feed on seagrass and algae as adults.\nLoggerheads: Carnivorous, feed on crustaceans, mollusks, and jellyfish.\nDigestive tract: Green turtles have longer intestines for breaking down plant material, while loggerheads have powerful jaws for crushing hard-shelled prey.\n\n\n\n\n\n\nIn no more than 6-7 sentences, describe the migratory journey of a female loggerhead turtle from the time it was born on the east coast of Florida until the time she returns to a beach to lay her eggs. \n\nHatching: Loggerhead hatchlings emerge from sandy nests on Florida’s beaches and make their way to the ocean.\nJuvenile Drift: The young turtles drift in the North Atlantic Gyre, feeding on floating debris and small prey.\nAdolescence: After several years, they settle in coastal feeding grounds in the Mediterranean or the Gulf of Mexico.\nMaturation: Upon reaching adulthood (20-30 years), females return to feeding areas near Florida.\nMigration to Nesting Site: When ready to lay eggs, the female makes a long migration to the beach where she was born (natal homing).\nNesting: She lays multiple clutches of eggs during the nesting season and returns to the ocean, beginning the cycle anew.\n\n\n\nPinipeds §\n\nCompare and contrast swimming and land movement in seals and sea lions.\n\nSwimming:\n\nSeals: Hind flippers for propulsion. They move them side to side in a motion similar to a fish’s tail. Their foreflippers are smaller and mainly help with steering.\nSea Lions: Rely on their large foreflippers to propel themselves through the water using a wing-like movement. Their hind flippers help with steering and can rotate forward, allowing easier movement on land compared to seals.\n\n\nTerrestrial Movement:\n\nSeals: Limited mobility on land, wriggling on their bellies.\nMore agile on land, using their flippers to walk.\n\n\n\n\n\nWhales §\n\nCompare and contrast the feeding behavior of Orcas that feed on baleen whales vs. those that feed on salmonid fish.  Describe 3 things they have in common and 3 that are different.\n\nSimilarities:\n\nCooperative hunting: Both ecotypes (mammal-eating and fish-eating orcas) use teamwork. Whether attacking large whales or herding fish, orcas rely on coordinated strategies.\nCommunication: Both types use vocalizations to communicate during hunts, although the sounds vary depending on prey type and group dynamics.\nSkill transmission: Knowledge about hunting techniques is passed through generations, suggesting cultural learning in both groups.\n\n\nDifferences:\n\nHunting techniques: Baleen-whale hunters (transient orcas) target massive prey using ambush and exhaustion tactics. In contrast, fish-eating orcas (resident orcas) use high-speed chases and tail slaps to corral salmon.\nPrey type and frequency: Transients hunt infrequently since large prey offers more calories, while residents feed frequently because salmon provide smaller energy payoffs.\nVocalization during hunts: Transient orcas are silent when hunting to avoid alerting prey, while resident orcas are vocal during hunts, possibly to coordinate movements and communicate within their pod.\n\n\n\n\nCompare and contrast feeding behavior in Right Whales and Humpback Whales. Describe 3 things they have in common and 3 that are different.\n\nSimilarities:\n\nFilter feeding: Both species are filter feeders, relying on baleen plates to trap small prey like krill or plankton.\nSeasonal feeding: Both migrate to specific regions during feeding seasons, such as cooler polar waters where prey is abundant.\nBulk consumption: Both consume large amounts of prey to build up fat reserves for migration and non-feeding seasons.\n\n\nDifferences:\n\nFeeding strategies: Right whales skim-feed at the surface, slowly moving with their mouths open to filter plankton. Humpback whales engage in bubble-net feeding, encircling prey with bubbles to trap it before lunging through the center.\nPrey type: Right whales primarily feed on zooplankton and copepods, whereas humpbacks target small fish (like herring) and krill.\nSocial feeding: Humpbacks sometimes engage in cooperative feeding with other individuals, whereas right whales tend to feed alone.\n\n\n\n\nExplain in detail how echolocation works. Which group of whales uses this superpower? For what functions?  How does the other group of whales that don’t have this superpower perform the same functions?\n\nSee above question for mechanism.\nWhales that Use Echolocation:\n\nOdontocetes: This group includes dolphins, porpoises, and sperm whales.\n\n\nFunctions: Odontocetes use eThcholocation for navigation, hunting in murky water or at great depths, and communication with pod members.\nHow Baleen Whales (Mysticetes) Perform Similar Functions Without Echolocation:\n\nRely on other senses: Mysticetes depend on visual cues, environmental cues (such as temperature gradients), and olfaction to locate food and navigate.\nLong-distance communication: They produce low-frequency sounds (songs) that can travel over vast distances, possibly for navigation or coordinating with other whales.\n\n\n\n\n\nFish §\n\nCompare and contrast the feeding strategies of a bar jack, a Nassau grouper, and a banded butterfly fish.  What specific morphological, coloring, and behavior adaptations have each fish evolved that increases the success of its respective feeding strategies?\n\nBar Jack:\n\nFeeding Strategy: Pursuit predator.\nMorphological Adaptations: Streamlined body for fast swimming.\nColoring Adaptations: Silvery coloration helps with camouflage in open water.\nBehavioral Adaptations: Hunts in schools or alongside larger predators, benefiting from distracted prey.\n\nNassau Grouper:\n\nFeeding Strategy: Ambush predator.\nMorphological Adaptations: Large mouth and powerful suction for engulfing prey.\nColoring Adaptations: Camouflage pattern with stripes and spots for blending into reef environments.\nBehavioral Adaptations: Lurks motionless near the reef, striking quickly when prey approaches.\n\n\n\nBanded Butterfly Fish:\n\nFeeding Strategy: Coral polyp and invertebrate picker.\nMorphological Adaptations: Small, narrow mouth adapted for picking at corals.\nColoring Adaptations: Bold patterns with eye spots to confuse predators.\nBehavioral Adaptations: Uses maneuverability to pick food from coral reefs and avoid predation.\n\n\n\n\n\n\n\nName the 5 different types of fish tails.  Draw each tail and place each at the proper location along a gradient of increasing initial thrust force so that its relative capability at generating a large, initial thrust force is conveyed.  Now, draw each tail again and draw squiggly lines emanating anywhere from the tail that turbulence will be generated when that fish is moving at sustained high speeds. Which of these tails generates the least turbulence and why?  Hint: the answer lies in understanding drag.\n\nRounded Tail: High maneuverability, low thrust.\nTruncate Tail: Moderate thrust, moderate maneuverability.\nEmarginate Tail: Higher thrust, less maneuverability.\nForked Tail: High thrust, good sustained swimming.\nLunate Tail: Maximum thrust, least maneuverability.\n\n\nCompare and contrast how bony fishes and sharks ventilate their gills. \n\nBony Fishes:\n\nUse a buccal pump mechanism.\nOperculum (gill cover) helps create water flow over the gills.\nContinuous water flow is achieved by mouth movements and opercular pumping.\nSharks:\n\nUse ram ventilation (swimming with mouth open) or spiracles to pump water over their gills.\nLack an operculum, which results in less controlled water flow compared to bony fishes.\n\n\n\n\n\n\nWhat is the scientific family name for parrot fish, groupers, butterfly fish, damselfish, snappers, and barracuda? Compare and contrast the coloration patterns in the barracuda and grouper families.  Name one way they are different and one way they are similar. What same selective force could be driving both similarity and contrast in their coloration\n\nGroupers: Epinephelidae.\nButterfly fish: Chaetodontidae.\nDamselfish: Pomacentridae.\nSnappers: Lutjanidae.\nBarracuda: Sphyraenidae.\nParrotfish: Scaridae.\nColoration patterns:\n\nDifference: Barracudas have silver, streamlined bodies for open water camouflage, while groupers often have more complex, camouflaging patterns suited to reefs.\nSimilarity: Both have coloration that aids in ambush or stealth.\nSelective force: Predator avoidance and hunting stealth are selective pressures driving these coloration patterns.\n\n\n\n\nDescribe 5 different functional roles that the dorsal fin in fish can play and how the morphology of that fin is linked to a particular function.\n\nStability in swimming: Tall, stiff dorsal fin helps stabilize fish in fast, straight-line swimming.\nManeuverability: Flexible dorsal fins enhance quick turns and complex movements.\nDefense: Some dorsal fins have sharp spines used to deter predators.\nSocial signaling: Brightly colored or elaborate dorsal fins are used in courtship displays or territorial disputes.\nHydrodynamic efficiency: Shorter dorsal fins may reduce drag in fast swimmers, enhancing speed efficiency.\n\n\n\nPapers §\n\nIn the paper, “Why whales are big but not bigger: Physiological drivers and ecological limits in the age of ocean giants,” what is the main driver limiting rorqual whale size? List and describe two of the methods scientists used to determine this.\n\n\nMain driver limiting rorqual whale size: PREY AVAILABILITY based on space and time\n\n\n\nWhale-borne tag data and foraging performance measurement\n\n\nused multisensor tags placed on whales to record their foraging behavior and dive patterns\n\n\nProvides data on the number of lunge-feeding events during each dive\n\n\nAllows researchers to quantify energy intake from each lunge &amp; compare efficiency among differently sized roquals\n\n\nPrey density and distribution mapping\n\n\nMeasured biomass, density, and distribution of krill in rorqual foraging hotspots\n\n\nHelped estimate the E available to whales during each feeding event + how efficiently larger rorquals can exploit denser krill patches\n\n\nIn the paper “Are the ghosts of nature’s past haunting ecology today?”, what were the three explanations used to examine expansion of large consumers into non-typical habitats?\n\n\nRecolonization of Formerly Occupied habitats:\n\n\nMany large consumers: X expanding into new habitats but rather recolonizing ecosystems that they once inhabited before human-driven exclusion\n\n\nThese animals previously lived in these ecosystems but were driven out\n\n\nClimate Change Reducing Physical Stress:\n\n\nClimate change may suppress the physical stress that limited the range of large consumers -&gt; allows them to expand into ecosystems where they were previously absent\n\n\nReduction of environmental constraints -&gt; helps explain the presence of large consumers in habitats thought to be outside their thermal tolerance\n\n\nCompetitive Release:\n\n\nReduction/elimination of native competitors in the ecosystems (mostly due to huaman activities) -&gt; large consumers expanding into non-typical habitats\n\n\nFewer competitors -&gt; large consumers can exploit new resources\n\n\nIn the paper “Surviving in a Marine Desert: The Sponge Loop Retains Resources Within Coral Reefs,” explain the key factors that contribute to the sponge loop and its role in maintaining resource retention within coral reef ecosystem.\n\n\nDissolved Organic Matter Uptake by Sponges\n\n\nSponges take up dissolved organic matter (DOM)\n\n\nDOM produced by reef primary producers (corals/algae), but difficult for most other organisms to directly utilize\n\n\nSponges efficiently absorb DOM -&gt; serves as vital energy source within the reef\n\n\nTransformation of DOM into Particulate Organic Matter\n\n\nSponges convert absorbed DOM into particulate organic matter (POM) through rapid cell turnover (rapid cell cycling)\n\n\nChoanocytes (sponge cells) are shed into the surrounding water as detritus (POM) -&gt; becomes available for consumption by other organisms in the reef\n\n\nNecessary to maintain energy flow within the reef\n\n\nTransfer of sponge-derived detritus to higher trophic levels\n\n\nSponge loop: Detritus produced by sponges is consumed by detritivores (small crustaceans/polychaetes) -&gt; then preyed upon larger animals higher in the food web\n\n\nEnsure that energy and nutrients are retained within the coral reef rather than being lost in the open ocean -&gt; sustains a highly productive ecosystem despite the nutrient-poor environment of the surrounding waters\n\n\nIn the paper “Do Alternate Stable Community States Exist in the Gulf of Maine Rocky Intertidal Zone,” describe what results support the conclusion that the occurrence of mussel beds and seaweed canopies is highly deterministic.\n\n\nHydrodynamic Conditions\n\n\nMussel beds dominate habitats with high water flow\n\n\nVice versa for seaweed canopies (low water flow)\n\n\nThis division was consistent across multiple sites -&gt; suggests that the type of community is driven by predictable physical conditions rather than stochastic events\n\n\nRapid Return to Original State\n\n\nWhen disturbances (clearings) were created in mussel beds and seaweed canopies, both types of communities returned to their original state regardless of the size of the disturbance.\n\n\nrecovery was rapid and happened independently of disturbance size, but only in the absence of consumers (such as crabs and snails).\n\n\nreestablishment of these communities is not random but governed by strong ecological drivers.\n\n\nCompare the hypothesized ecological effects of large sharks like tiger sharks and hammerhead sharks vs smaller sharks like the grey reef sharks in “The Ecological Role of Sharks on Coral Reefs”. What scientific approach was used to get this hypothesis? Were the authors’ able to find empirical evidence to support this hypothesis?\n\n\nLarge Sharks\n\n\nhypothesized to be true apex predators, exerting strong direct and indirect effects on their prey.\n\n\nregulates populations of other large marine animals, including other sharks, marine mammals, and turtles\n\n\nSmaller Sharks\n\n\nHypothesized to influence prey populations but are also subject to predation by larger sharks\n\n\nEffects on the reef ecosystem are more limited, as they occupy an intermediate trophic position\n\n\nScientific Approach\n\n\nHypothesis was formed using a combination of stable isotope analysis, dietary studies, and ecological modeling to examine the trophic roles of sharks.\n\n\nmethods provided insights into the feeding behavior, trophic levels, and potential ecological roles of different shark species.\n\n\nEmpirical Evidence\n\n\nVery limited\n\n\nEvidence for strong top-down control by sharks was weak, particularly on reefs with high biodiversity and trophic redundancy, which tend to resist dramatic changes in community structure due to predator loss\n\n\nReferring to the paper “The topicalization of temperate marine ecosystems: climate-mediated changes in herbivory and community phase shifts”, describe two of the impacts climate change will have on the interaction between temperate macroalgae and tropical herbivorous fish?\n\n\nIncreased Herbivory Due to Range Expansion of Tropical Herbivorous Fish: \n\n\nAs ocean temperatures rise, tropical herbivorous fish are expanding their range into temperate regions\n\n\nincreased herbivory leads to significant overgrazing of temperate macroalgae, causing a decline in kelp forests and other macroalgal habitats.\n\n\nHas already been observed in Japan and the Mediterranean\n\n\nAltered Recovery Dynamics of Macroalgae\n\n\nClimate change hampers the recovery of macroalgal communities after disturbances\n\n\nTropical fish such as grazers, scrapers, and excavators prevent the regrowth of macroalgal species by continuously grazing on new growth or recruits -&gt; reinforces phase shift from macroalgal dominance to barren or coral-dominated habitats\n\n\nIn the paper, “Why whales are big but not bigger: Physiological drivers and ecological limits in the age of ocean giants,” what is the main driver limiting rorqual whale size? List and describe two of the methods scientists used to determine this.\n\n\nIn the paper “Are the ghosts of nature’s past haunting ecology today?”, what were the three explanations used to examine expansion of large consumers into non-typical habitats?\n\n\nIn the paper “Surviving in a Marine Desert: The Sponge Loop Retains Resources Within Coral Reefs,” explain the key factors that contribute to the sponge loop and its role in maintaining resource retention within coral reef ecosystem.\n\n\nIn the paper “Do Alternate Stable Community States Exist in the Gulf of Maine Rocky Intertidal Zone,” describe what results support the conclusion that the occurrence of mussel beds and seaweed canopies is highly deterministic.\n\n\nCompare the hypothesized ecological effects of large sharks like tiger sharks and hammerhead sharks vs smaller sharks like the grey reef sharks in “The Ecological Role of Sharks on Coral Reefs”. What scientific approach was used to get this hypothesis? Were the authors’ able to find empirical evidence to support this hypothesis?\n\n\nReferring to the paper “The topicalization of temperate marine ecosystems: climate-mediated changes in herbivory and community phase shifts”, describe two of the impacts climate change will have on the interaction between temperate macroalgae and tropical herbivorous fish?\n\n\nBonus Questions §\n\nI will ask the scientific name of one of the sea turtles. (2pts)\nI will ask you the family name of one of the fish families I covered. (3pts)\nI will ask the Genus species of one of the baleen whales (5pts)\n"},"Marsci-Exam-2-Prep":{"title":"Marsci Exam 2 Prep","links":[],"tags":["Courses"],"content":"Terms §\n\n\nStress gradient hypothesis\n\n\nHysteresis\n\n\nAlternative Stable States\n\n\nTrophic Cascade\n\n\nParadox of the plankton\n\n\nRiftia\n\n\nChemosynthesis\n\n\nFringing vs. barrier reef\n\n\nDisturbance\n\n\nIntermediate Disturbance hypothesis\n\n\nFacilitation cascade\n\n\nFoundation species\n\n\nKeystone species\n\n\nSpartina\n\n\nCrassostrea\n\n\nZostera\n\n\nAcropora\n\n\nMacrocystis\n\n\nRhizophora\n\n\nGreen vs. brown food webs\n\n\nAerenchyma\n\n\nLacuna\n\n\nCustom terms\n\nTurbid\nblue water paradox of coral reefs\n\n\n\n\nWhat are the 5 most important biological lessons you learned during the course and why?\nExplain convergent and divergent evolution and give three examples of traits that you’ve learned so far for each type of evolution.\n\nConvergent Evolution: Different species evolving similar traits independently due to similar environmental pressures.\n\nStreamlined body shapes in dolphins and sharks\nFilter feeding structures in baleen whales and whale sharks\nBioluminescence in different deep-sea organisms\n\n\nDivergent Evolution: Related species evolving different traits as they adapt to different environments.\n\nDifferent beak shapes in Darwin’s finches\nVarying body forms in coral reef fish from a common ancestor\nDifferent feeding strategies in sea stars from the same family\n\n\n\n\n\nStressors §\n\nUsing graphs and words describe alternative stable states and define hysteresis. Draw and describe 2 graphs of urchin (as stressor) and kelp abundance showing (1) no hysteresis and then (2) large hysteresis in kelp decline-recovery dynamics.\n\nGraph 1 (No Hysteresis): Shows a simple negative relationship between urchin density and kelp abundance. The system responds predictably to changes in urchin density.\nGraph 2 (Large Hysteresis): Shows how the same kelp system can exist in two alternative stable states at the same urchin density. The blue line shows kelp decline as urchin density increases, while the dashed red line shows the recovery pathway. The vertical gray line shows where two stable states can exist at the same urchin density.\n\n\nDefend or refute this comment with data/stats and with at least three examples: Sewage pollution is isolated in coastal marine systems and has minimal impacts on marine ecosystems.\n\nincorrect for several reasons:\n\nSewage contains nutrients that cause eutrophication, affecting entire food webs\nStudies show sewage-derived nitrogen can be traced kilometers offshore\nExamples of widespread impacts:\n\nDead zones in coastal areas due to oxygen depletion\nHarmful algal blooms affecting entire regions\nCoral reef degradation from nutrient loading\n\n\n\n\n\n\nDefine density-dependence and density-independence. Using qualitative graphs and words, compare and contrast the density-dependent impacts of hurricanes, predation pressure, and foundation species on mortality for oysters and infaunal clams.\n\nHurricanes (Density-Independent Impact):\n\nDescriptionon: Hurricanes can cause widespread destruction through strong winds, storm surges, and altered salinity levels.\nImpact on Mortality: Mortality rates for oysters and infaunal clams increase significantly during hurricanes, regardless of their population densities. The physical forces and environmental changes affect individuals equally, leading to density-independent mortality.\n\n\nPredation Pressure (Density-Dependent Impact):\n\nDescription: Predators such as blue crabs feed on oysters and infaunal clams.\nImpact on Mortality: At higher prey densities, predators can locate and consume more individuals, leading to increased mortality rates. This results in a positive correlation between prey density and mortality due to predation.\n\n\nFoundation Species (Density-Dependent Impact):\n\nDescription: Foundation species like seagrasses and oysters create habitats that provide shelter and resources for other organisms.\nImpact on Mortality: The presence and density of foundation species can reduce mortality rates for oysters and infaunal clams by offering protection from predators and stabilizing the environment. As the density of foundation species increases, the protective benefits also increase, leading to lower mortality rates.\n\n\n\n\nDescribe 4 processes and the mechanism(s) underlying them that determine effectiveness of MPAs.\n\nSize and Spacing:\n\nMechanism: Larger MPAs support viable populations and provide spillover benefits\nOptimal spacing allows larval connectivity between protected areas\n\n\nHabitat Quality:\n\nMechanism: Higher quality habitats support greater biodiversity\nIncludes factors like structural complexity and habitat diversity\n\n\nEnforcement Level:\n\nMechanism: Effective enforcement prevents illegal fishing and habitat destruction\nRegular monitoring and penalties maintain protection\n\n\nConnectivity:\n\nMechanism: Networks of MPAs allow species movement and genetic exchange\nConsiders both physical oceanographic connections and species’ life histories\n\n\n\n\n\nSalt Marsh &amp; Mangrove §\n\nWhy do salt marshes dominate the intertidal zone on the East Coast of the US, while rocky shores dominate the West Coast? How does this impact the distribution of fiddler crabs?\n\nEast Coast:\n\nGentle slope, sediment accumulation from glacial deposits\nEnables marsh plant establishment\nSuitable habitat for fiddler crabs\n\n\nWest Coast:\n\nSteep, rocky terrain from tectonic activity\nLimited sediment accumulation\nNo habitat for fiddler crabs\n\n\n\n\nIn salt marsh plants and mangroves, why are the upper elevation range limits of plants most often set by competition and their lower elevation range limits set by physical forces, while the opposite pattern occurs for rocky intertidal organisms?\n\nReason:\n\nSalt marsh and mangrove plants can engineer their environment through root systems and canopy development\n\n\nRocky intertidal organisms are largely at the mercy of existing physical conditions\nLimited ability to modify environment\n\n\nSalt marsh/mangroves:\n\nLower limit: Flooding stress/salinity/submergence thus lower oxygenation\nUpper limit: Competition from terrestrial plants\nPlants can modify environment to reduce stress\n\n\nRocky intertidal:\n\nLower limit: Predation pressure\nUpper limit: Desiccation stress\n\n\n\n\nDescribe how incorporation of positive interactions into restoration design of mangrove or salt marsh plants could increase yield of those conservation projects. Give 3 examples for each foundation species.\n\nSalt marsh:\n\nClumped planting for mutual shading (Reduces soil salinity through shared root oxygen, increases sediment binding)\nAdult nurse plants protecting seedlings (Reduced soil temperature, increased humidity, wave protection)\nMixed species assemblages for facilitation (Different root depths reduce competition, increase soil oxygenation)\n\n\nMangroves:\n\nDensity optimization for root stability (Shared prop root structure increases stability)\nPioneer species establishing substrate, i.e. facillitation succession (Plant A. germinans 2 years before L. racemosa)\nPlant in patches (Increases seed capture, natural recruitment, sediment trapping, natural expansion)\n\n\n\n\n\nFood Chain §\n\nDefine trophic cascade and food chain length.\n\nTrophic cascade: Effects of predator changes propagating through food chain\nFood chain length: Number of feeding links from primary producers to top predators\n\n\nHow can changes in food chain length affect the outcome of a trophic cascade?\n\nLonger chains dilute top-down effects\nIntermediate consumers buffer impacts\nTime lag effects\nIn gneeral, longer chains are:\n\nLess stable demographically\nMore vulnerable to perturbation\nHigher extinction risk\n\n\n\n\nCompare and contrast the mechanisms and end results of the trophic cascade in rocky shores as described by Bob Paine and in salt marshes as described by Silliman and Bertness.\nWhat is the range of food chain links typically found in natural food webs? List three factors that can limit the total number of food chain links in a food web and discuss why they can limit food chain links.\n\nNaturally 3-5\n\nEnergy loss between levels (10% energy transfer rule between levels)\nPopulation stability requirements (Each level needs minimum viable population)\nBody size constraints (Size increases up chain by ~10x)\n\n\n\n\nConstruct a biomass pyramid reflecting relative biomass at each trophic level for the food web of the Serengeti Plains. The opposite/inverse pattern of that Serengeti Plain biomass pyramid is true for open ocean systems. Draw this open ocean biomass pyramid as well. Formulate a hypothesis as to why these biomass pyramids are the inverse of each other.\n\nSerengeti: Slow plant turnover, large standing biomass\nOcean: Rapid phytoplankton turnover, low standing biomass but high productivity supports larger consumer populations\n\n\n\nEcological Interaction &amp; Environment §\n\nCompare and contrast how whales facilitate primary production in phytoplankton communities and grunts facilitate primary production on coral reefs.\n\nWhales: release nutritious fecal plumes after consuming in high-productivity feeding areas and arriving in low-productivity calving areas, will cause phytoplankton blooms.\nGrunts: Go to other seagrass beds and sand plains to feed on microorganisms, return to coral reef, shit near coral polyps with nitrogen and phosphorus shit, causes algae blooms near juvenile polyps that suffocate them\n\n\nWhat is a competitive dominant? Describe 3 forces that can maintain species co-existence in the presence of a competitive dominant. Why do these forces have such effects on increasing diversity?\n\nCompetitive dominant: Organism that is best adapted to the environment, one that will presumably outcompete all other organisms in the habitat\n3 forces:\n\nResource partition: with resource limitations, the competitive dominant species will hit density dependent factors such as lack of resources much faster and be less dominant\nAdopting different life cycle (one organism should be more resource efficient in larval stage and one should be less resource efficient in adult stage. These stages must not clash so that they aren’t really competing for resources at any stage.),\nPredation on the competitive dominant (mussels and starfish, or keystone species, on rocky shores)\n\n\n\n\nDefine fundamental and realized niche. What role does competition play in the development of these terms? Now use schematic diagrams and words to show how positive interactions should modify the original theory behind the fundamental and realized niches.\n\nFundamental niche: niche the organism would occupy without any limiting factors\nRealized niche: niche the organism actually occupies\nEx. Cthalamus and Balanus interactions, cthalamus should occupy everything, but bc of competition Balanus occupies half of intertidal zone \nNiche theory: Competition traditionally reduces realized niche. Positive interactions can expand realized niche beyond fundamental through stress amelioration/resource enhancement.\n\n\nCompare and contrast the terms ecosystem services and ecosystem function. Describe 3 ecosystem functions that coral reefs, oyster reefs and salt marshes have in common. For each function mentioned, list a corresponding service.\n\nEcosystem function: “the ecological processes that control the fluxes of energy, nutrients and organic matter through an environment”\nEcosystem service: “the conditions and processes through which natural ecosystems, and the species that make them up, sustain and fulﬁll human life.” Basically the ecosystem functions that are useful to humans\nBoth are active elements of the ecosystem that regulate its biotic factors. The difference lies in the fact that ecosystem services are a subset of ecosystem functions that act on human existence.\nCommon Functions &amp; Services:\n\nWave Attenuation\n\nFunction: Energy dissipation\nService: Coastal protection\n\n\nNutrient Cycling\n\nFunction: Element transformation/storage\nService: Water quality improvement\n\n\nHabitat Provision\n\nFunction: Physical structure creation\nService: Fishery enhancement\n\n\n\n\n\n\n\nOceanographic Process/Environment §\n\na) What is the Coriolis effect? b) How is it generated? c) How would the Coriolis effect impact a giant swarm of krill floating in body of oceanic water moving 500 miles from East to West vs. one moving 500 miles South to North in the Northern Hemisphere?\n\nThe Coriolis effect is an apparent deflection of moving objects due to Earth’s rotation. It occurs because Earth rotates underneath moving objects, causing rightward deflection in Northern Hemisphere and leftward in Southern Hemisphere.\nFor krill:\n\nEast-West movement: Less Coriolis impact since movement is parallel to Earth’s rotation\nSouth-North movement: Stronger rightward deflection in Northern Hemisphere\n\n\n\n\nDescribe an Ekman spiral and the processes that created it. Why is the Ekman spiral important for the ability of zooplankton to control their movements?\n\nEkman spiral: Wind-driven surface water moves 45° right of wind direction, with each deeper layer moving progressively further right and slower, creating a spiral pattern.\nCreated by balance between Coriolis force and friction.\nImportant for zooplankton as they can move between layers to control their transport direction/distance.\n\n\nDescribe and name 3 oceanographic processes that drive massive anchovy fisheries off the west coast of the Americas in the temperate zone. Describe and name the oceanographic process that can shut those processes down and lead to fishery collapse.\n\nKey processes driving anchovy fisheries:\n\nCoastal upwelling: Wind drives surface water offshore, bringing nutrient-rich deep water up\nThermocline shoaling: Brings nutrient-rich water closer to surface\nTidal mixing: Vertical mixing of nutrients in coastal areas\n\n\n\n\nEl Niño can shut these down by:\n\nDeepening thermocline\nWeakening upwelling\nReducing nutrient availability\n\n\n\n\nWhat is the average ocean salinity? What are the eight most abundant salts in the ocean? Why is the ocean salty, while lakes are fresh?\n\nAverage ocean salinity: 35 PSU (parts per thousand)\nMost abundant salts:\n\nSodium chloride (NaCl)\nMagnesium chloride (MgCl₂)\nMagnesium sulfate (MgSO₄)\nCalcium sulfate (CaSO₄)\nPotassium sulfate (K₂SO₄)\nCalcium carbonate (CaCO₃)\nMagnesium bromide (MgBr₂)\nPotassium chloride (KCl)\n\n\nOceans are salty due to accumulated mineral weathering from land over millions of years, while lakes flush minerals through outlets.\n\n\nWhat is pH and what is the scale on which is measured? How many more H+ are in solution of a pH 4 vs. pH of 7? What is the average pH of the ocean and why is it higher than 7? Use words and chemical equations to describe how increasing C02 in the atmosphere is lowering the pH of the ocean and how that negatively affects skeleton deposition in corals.\n\npH measures hydrogen ion concentration on 0-14 scale. pH 4 has 1000x more H+ than pH 7 (3 pH units = 1000x difference).\nOcean average pH ≈ 8.1, higher than 7 due to carbonate buffering system.\nCO₂ impact:\n\nCO₂ + H₂O → H₂CO₃ (carbonic acid)\nH₂CO₃ → H+ + HCO₃- (bicarbonate)\nMore CO₂ = more H+, lowering pH. This affects coral calcification:\nCa²+ + CO₃²- → CaCO₃ (coral skeleton)\nHigher H+ reduces available CO₃²-, making skeleton formation harder.\n\n\n\n\n\nEcological Succession §\n\nWhat is ecological succession? How does this contrast with zonation?\n\nEcological succession: Basically, the series of progressive changes in the composition of an environment over time (e.g. after wildfire)\nZonation: Gradual change of the distribution of a species across an environmental gradient over space (e.g. shoreline)\n\n\nDefine and contrast the three models of succession as defined by Connell and Slayter and describe an example of each.\n\nFacilitation: Early species improve conditions for later species; e.g. Lichens break down rock, creating soil for mosses\nTolerance: Early species neither help nor harm later species; e.g. Different species of sponges colonizing a reef wall, each establishing independently based on available space and water flow conditions\nInhibition: Early species prevent establishment of later species; e.g. Dense mussel beds preventing barnacle settlement through space occupation and filtering of larvae\n\n\nDesign a manipulative experiment to examine the impacts of pioneering plant species on later plant colonizers during succession in mangrove forests. What are your independent variables? What is your response or dependent variables?\n\nIndependent Variables: Presence/absence of pioneer species (treatment plots) &amp; Different pioneer species combinations\nDependent Variables: Soil characteristics (salinity, organic matter); Survival rates of later colonizers\nExperimental Design: Replicated plots with/without pioneer removal; Control for environmental gradients; Measure physical and biological parameters\n\n\nDefine the three types of ecological succession. Which one typifies salt marsh succession and systems represented by alternative stable states?\n\nPrimary Succession: Succession that happens in lifeless areas, e.g. earth’s creation. Rocks eroded to soil by microorganisms, soil can now sustain plant life\nSecondary Succession: When primary succession community gets wiped out. E.g. stuff after wildfire. Recolonizing of previously life-containing area\nCyclic succession: Constant changing of structure of ecosystem, associated with seasonal changes. Some plants are alive at some time, some aren’t. some animals hibernating, some active. Like cicada life cycle\n\n\nCyclic succession by erosion disturbance. Alternative stable states endpoints develop\n\n\n\nCorals §\n\nCompare and contrast how acidification, increased nutrient loading, and overfishing might impact coral reefs.\n\nAcidification: Reduces calcium carbonate availability → problems with coral skeleton fomation → weakens &amp; susceptible to physical damage\nNutrient Loading: excessive algal growth → outcompete corals for space; increased turbidity (→ reduced light penetration); increased pathogen growth; distrupts nutrient cycling\nOverfishing: Removes key herbivores that control algal growth → phase shift from coral-dom to algal-dom system; trophic cascade\n\n\nDescribe in detail what happens when corals bleach in response to temperature stress, and how they could recover.\n\nThe bleaching process:\n\nHigh temperatures cause coral polyps to become stressed\nThe symbiotic zooxanthellae (algae) begin producing excessive reactive oxygen species (=ROS)\nCorals expel their zooxanthellae as a stress response\nWithout zooxanthellae, corals lose their color and primary energy source\nCoral tissue becomes transparent, revealing white skeleton underneath\n\n\nRecovery pathway:\n\nIf conditions improve quickly enough, corals can recruit new zooxanthellae\nDifferent clades of zooxanthellae may colonize, potentially providing better heat tolerance\nCorals slowly regain their color as symbiont populations rebuild\nEnergy reserves must be sufficient to survive the bleached period\nRecovery can take weeks to months under optimal conditions\n\n\n\n\nDescribe three examples of how positive interactions (e.g. mutualisms, facilitation, indirect and direct, trophic and non-trophic) in the coral reef community are key to its success and persistence in the face of increasing human disturbance.\n\nCoral-Zooxanthellae Mutualism\n\nZooxanthellae provide up to 90% of coral energy needs through photosynthesis → rapid calcium carbonate deposition → Creates the physical structure that supports the entire reef community\nCorals provide protection and nutrients to zooxanthellae\n\n\nBranching Coral-Damselfish Mutualism\n\nDamselfish use coral branches as shelter and nesting sites → they defend their territory against coral predators and algae-eating fish → protects coral from predation and algal overgrowth\nFish waste provides nutrients to coral: important when reefs face stress from pollution or warming\n\n\nCoral-Coralline Algae Facilitation\n\nPresence indicates suitable habitat conditions for coral growth\nrelease chemical cues that guide coral larval settlement\nprovide stable substrate for coral recruitment\n\nHelp cement reef structure against storm damage\n\n\nrelationship is crucial for reef recovery after disturbance events\n\n\n\n\nWhat is the blue water paradox of coral reefs? Give two major mechanisms that solve this paradox–i.e., explain how to apparently counteracting facts can co-exist.\n\nThe Paradox: Coral reefs are highly productive ecosystems existing in nutrient-poor tropical waters\nMajor mechanisms that resolve this paradox:\n\nEfficient Nutrient Recycling: tight coupling (=rapid transfer through trophic levels) between producer &amp; consumer minimize nutrient loss\nPhysical Processes and Reef Structure\n\nComplex reef structure creates eddies and upwelling → Enhances particle capture from passing water; Promotes retention of nutrients within the reef system\n\n\n\n\n\n\n"},"Martin-Heidegger":{"title":"Martin Heidegger","links":["Phenomenology","Torii-Moi","Immanuel-Kant","Plato","Lit-285-Existentialism","Rigor-Ambiguity-Axis"],"tags":["People","Philosophy/Metaphysics"],"content":"\n\n                  \n                  Heidegger was undebatably a Nazi. \n                  \n                \n\nSee Phenomenology.\nBackground, via Moi §\nMotivation. Up until now the best metaphysics was by Kant, who wanted to studying things in themselves…but like Plato’s cave you cannot perceive things in themselves. Heidegger then came along and revolutionized metaphysics by saying that studying nothing is also important, and also that it has Existentialist implications.\ndef. Wissenschaft is any discipline that uses systematic, Rigorous knowledge-gathering, broader than science, including math, analytic philsophy.\nhyp. Any metaphysical inquiry is an inquiry into the whole of being. From the specific, it induces questions onto the whole of existence, to ask “what being is”\nhyp. Nothingness is an entity to be studied.\n\nNothingness is not a negation of all things.\nNothingness is difficult to study as an entity, because once you make it an entity it ceases to be nothing\n\ndef. Dasein and Sein.\n\nSein is a thing that exists\nDasein is the self-perception of Sein that it exists (=consciousness) + questioning, desiring, etc.\n\nIt is capable of percieving the nothing.\nIt is capable of asking questions about the being\n\n\n\nhyp. Boredom, Anxiety are emotional states useful for studying being and nothingness\n\nProfound Boredom (everything is uninteresting) encapsulates the whole of being into one emotion\nAbstract Anxiety (fear of nothing in particular) holds Dasein out into the nothing\n\n"},"Matching-Problems":{"title":"Matching Problems","links":["Property-Exchange","Stable-Marriage-Problem","Maximum-Flow-Problem"],"tags":["Computing/Algorithms"],"content":"\nUnpartitioned Matching: Priority Matching Algorithm\nPartitioned Matching:\n\nGale-Shapely Algorithim\nApplication of Ford-Fulkerson Maximum Flow Algorithm \n\n\n"},"Math-581-&-582-Mathematical-Finance,-Derivatives":{"title":"Math 581 & 582 Mathematical Finance, Derivatives","links":["Probability","Expected-Value","Stochastic-Process","Quadratic-Variation","Stochastic-Calculus","Role-of-Money","Present-Value-Calculations","Future-Value-Calculations","Interest-Rate","Banker's-Rule","Annuity","Perpetuity","Bonds-(Finance)","Portfolio-Theory-(Markowitz)","Dividend-Discount-Model","CAPM-Model","Market-Beta","Measuring-Security-Performance","Binomial-Security-Pricing-Model","No-Arbitrage","Options-(Finance)","Black-Scholes-European-Option-Pricing-Formula","Risk-Neutral-Assumption","Risk-Neutral-Derivation-of-BSM","Binomial-Option-Pricing-Model","Greeks-(Option)","Delta-and-Gamma-Hedging","Warrants-(Finance)","Merton-Jump-Diffusion-Model","Forwards","Futures"],"tags":["Courses"],"content":"\n\n                  \n                  For Stochastic Calculus \n                  \n                \n“Young man, in mathematics you don’t understand things. You just get used to them.”\n\nMathematical Background §\n\nProbability\n\nConditional Expectation for Stochastic Calculus\n\n\nStochastic Process\n\nQuadratic Variation\nStochastic Calculus\n\n\n\nBasic Concepts in Money §\n\nTime Value of Money\n\nPresent Value Calculations\nFuture Value Calculations\nInterest Rate\nBanker’s Rule\n\n\n\nBasic Finance §\n\nAmortizing Securities\n\nAnnuity\nPerpetuity\nBonds (Finance)\n\n\nPortfolio Theory (Markowitz)\n\nDividend Discount Model\nCAPM Model\n\nMarket Beta\n\n\nMeasuring Security Performance\n\n\nBinomial Security Pricing Model\nNo-Arbitrage (=Law of One Price)\n\nDerivatives §\n\n\nOptions (Finance)\n\nBlack Scholes European Option Pricing Formula\n\nRisk-Neutral Assumption and Risk-Neutral Derivation of BSM\nBinomial Option Pricing Model derivation of BSM\n\n\nGreeks (Option)\nDelta and Gamma Hedging\nWarrants (Finance)\nMerton Jump Diffusion Model\n\n\n\nForwards\n\nFutures\n\n\n\nPayoff: the gross outcome of an investment or trade. The amount you earned from that trade, regardless of commissions, extraneous costs, etc.\n\n\nProfits: the gross outcome of an investment or trade, including commissions, extraneous cost\n\n→ the distinction happens in Options (Finance). Payoffs don’t consider the option premium, while the profits do.\n\n\n\nReturn: short for “rate of return”. The percentage of profit (not payoff!) per original investment. Quoted in percentage (%) or log-returns.\n\n"},"Math-582-Financial-Derivatives":{"title":"Math 582 Financial Derivatives","links":["Binomial-Tree-Model-of-Security-Pricing"],"tags":["Courses"],"content":"\nBinomial Tree Model of Security Pricing\n"},"Math-is-an-Abuse-of-Notation":{"title":"Math is an Abuse of Notation","links":[],"tags":[],"content":""},"Mathematical-Induction":{"title":"Mathematical Induction","links":[],"tags":["Math"],"content":"Weak Induction [=Normal Induction] §\n\nBase case\n\nP(1) is true \n\nInductive Hypothesis (IH)\n\nAssume P(k) is true\n\nInductive Step\n\nP(k) is true ⇒P(k+1) is true\nStrong Induction §\n\nBase cases\n\nP(1)…P(n) is true\n\nInductive Hypothesis (IH)\n\nAssume P(k) is true\n\nInductive Step\n\nP(k) is true ⇒P(k+1) is true"},"Mathematical-Optimization":{"title":"Mathematical Optimization","links":["Lagrangian-Optimization","Linear-Programming","Convex-Programming","tags/task"],"tags":["Math/Calculus","task"],"content":"\nLagrangian Optimization\nLinear Programming\nConvex Programming\n\nKKT conditions\n\n\n #task understand kkt conditions\n"},"Mathematical-Proof":{"title":"Mathematical Proof","links":["Mathematical-Induction","Proof-by-contradiction"],"tags":["Math"],"content":"Types of proofs:\n\nMathematical Induction\nProof by contradiction\nContrapositive Proof: prove p⟹q by proving ¬q⟹¬p\n"},"Matricies-Differentiation":{"title":"Matricies Differentiation","links":[],"tags":["Math/Calculus"],"content":"Slides\n\nScalar/Vector (result is row vector)\n\n∂x∂y​=(∂x1​∂y​,…,∂xn​∂y​)\n\nVector/Scalar (result is column vector)\n\n∂x∂y​=​∂x∂y1​​⋮∂x∂yn​​​​\n\nVector/Vector (result is matrix). Also called the Jacobian Matrix\n\n∂x∂y​=​∂x1​∂y1​​∂x1​∂y2​​⋮∂x1​∂ym​​​∂x2​∂y1​​∂x2​∂y2​​∂x2​∂ym​​​⋯⋯⋱⋯​∂n∂y1​​∂xn​∂y2​​∂xn​∂ym​​​​\nTips &amp; Tricks §\nIf A⋅b=c with matrix A, column vectors b,c then:\n∂b∂c​=A"},"Matrix-Chain-Multiplication":{"title":"Matrix Chain Multiplication","links":["Dynamic-Programming"],"tags":["Computing/Algorithms"],"content":"Matrix Chain Multiplication. Given matricies A0​,…,An−1​ with dimensions (m0​×m1​),(m1​×m2​),…,(mn−1​×mn​), which order should we multiply the matricies in order to minimize the number of scalar multiplications? Description from course\nIdea:\n\nSearch every subproblem but with Dynamic Programming.\nfor every chain of matricies, get the split with minimum multiplication required.\nIterate for increasing gap (=chain length)\n\n\n"},"Matrix-Multiplication":{"title":"Matrix Multiplication","links":["Parallel-Algorithms","Inner-Product","tags/processors"],"tags":["Computing/Algorithms","processors"],"content":"Traditional (Textbook) algorithm is O(n3)\nWe have gotten it down to ≈O(n2.37)\nalg. PInnerProduct. Use PSum from Parallel Algorithmss. Calculates Inner Product of two vectors\n\nSpan T∞​(n)=O(logn)\n#processors p=O(lognn​)\nWork: W∞​(n)=O(n)\n\nalg. Matrix Multiplication.\n\nT(n)=O(n3)\n\ndef MatMult(A, B)\n\tfor i=1 to n\n\t\tfor j=1 to n\n \n\t\t\t# calculate inner product\n\t\t\tc_ij = 0\n\t\t\tfor k=1 to n\n\t\t\t\tc_ij += a_ik * b_jk\n\t\t\tend for\n\t\tend for\n\tend for\nalg. Parallel Matrix Multiplication.\ndef PMatMult(A, B)\n\tparallel for i=1 to n\n\t\tparallel for j=1 to n\n\t\t\tc_ij = 0\n\t\t\tfor k=1 to n\n\t\t\t\tc_ij += a_ik * b_jk\n\t\t\tend for\n\t\tend for\n\tend for\nalg. Parallel Recursive Matrix Multiplication. A Divide-and-Conquer algorithm.\nIdea:\n\nC=(C11​C21​​C12​C22​​),A=(A11​A21​​A12​A22​​),B=(B11​B21​​B12​B22​​),\n(C11​C21​​C12​C22​​)=(A11​A21​​A12​A22​​)(B11​B21​​B12​B22​​)=(A11​B11​+A12​B21​A21​B11​+A22​B21​​A11​B12​+A12​B22​A21​B12​+A22​B22​​)\ndef PRMatMult(A, B, C)\n \n\t# base case\n\tif n=1\n\t\treturn a * b\n\t\t\n\t# parallel recursion\n\tspawn PRMatMult(A_11, B_11, D_11)\n\t...# 8 parallel threads\n\tspawn PRMatMult(A_22, B_22, E_22)\n\tsync\n \n\t# add matricies D and E together\n\tparallel for i=1 to n\n\t\tparallel for j=1 to n\n\t\t\tc_ij = d_ij + e_ij\n\nSpan T∞​(n)≤T∞​(2n​)+O(1)=O(logn)\nWork W∞​(n)≤8⋅W∞​(2n​)+O(n2)=O(n3)\n"},"Maxima-of-Point-Set-Algorithm":{"title":"Maxima of Point Set Algorithm","links":[],"tags":["Computing/Algorithms"],"content":"Maxima of a point set - Wikiwand\nProblem. Let P=(x1,y1),(x2,y2),…,(xn,yn) be a set of n points in the two-dimensional Euclidean plane. A point (xi,yi) dominates another point (xj,yj) if xi&gt;xj and yi≥yj or xi≥xj and yi&gt;yj (that is, if it is up and to the right). A point is maximal if it is not dominated by any other point.\nIdea:\n# A[(int, int)] is the structure of the array\n \nfunction FindMaximal()\n\tmax_y = -Infinity\n\t\n\t\n\t# Sort the points in decreasing order by x-coordinate\n\tReverseMergeSort(A[])\n\tmax_y = A[0]\n\tresult = [A[0]]\n\t\n\t# traverse in reverse x direction (right to left)\n\tfor p in A\n\t\t# if y is bigger than anything seen before it&#039;s maximal\n\t\tif p.y &gt; max_y then\n\t\t\tresult.append(p)\n\t\t\tmax_y = p.y\n\t\t\t\n\treturn result\n\nReverseMergeSort O(nlogn)\nFindMaximal T(n)=O(n)+O(nlogn)=O(nlogn)\nCorrectness (Brief argument):\n\nThe rightmost element must be the maximal element\nIf the current element has a higher y value than any other element on the right side of it, it must be a maximum\nIf the current element has a lower y value than another point on the right of it, it is dominated by that point and thus cannot be a maximum\n\n\n"},"Maximum-Flow-Problem":{"title":"Maximum Flow Problem","links":["Directed-Graph","Depth-First-Search","Shortest-Path"],"tags":["Computing/Algorithms"],"content":"Q. Maximum Flow Problem. Given a Directed Graph whose edges are labeled with flow capacity, what is the maximum flow that can be pushed from vertex s to t?\nQ. Minimum Cut Problem. Given a Directed Graph whose edges are labeled with flow capacity, and a source and target vertex s,t, how do you partition the graph into A,Aˉ such that the flow capacity from A→Aˉ is minimized?\n⇒ The two questions give the same answer. i.e…\n\nthe flow rate between A→Aˉ is the maximum flow rate of the graph\nFor the edges cut by the min-cut…\n\nall forward edges operate at full capacity. (The sum of forward edges is the maximum flow capacity of the whole graph.)\nall backward edges operate at zero capacity\n\n\n\nalg. Ford-Fulkerson Flow Maximization. (FFF-Max) Worked Example\n\nIdea: Choose some suboptimal flow, then increamentally find a better solution.\n\n\nChoose some suboptimal flow through the graph.\nConstruct a residual network.\n\nFor edges who have residual capacity, keep those edges but labeled with that residual capacity.\nFor every edge, construct a reversed edge with the used capacity (this is useful later)\n\n\nIf there is still a path from s→t in the residual network, then more flow can be pushed.\n\nThis path is called the ”flow augmenting path”\nThe minimum capacity of any edge in the augmenting path is the ”bottleneck” of the path.\n! Choice of the augmenting path is important. Discussed below.\nRewire the original graph so that…\n\nforward edges have their flow decreased by the bottleneck amount\nbackward edges—decrease flow of the forward edge represented by that backward edge by that bottleneck in the original graph\n\n\n\n\nRepeat calculation until there is no path s→t in the residual graph.\n\n\nalg. Minimum Cut. How to remove the minimum number of edges such that there is no flow from source s to sync t?\n\nRun the Flow Maximization algorithm\nConstruct a residual graph of the final max-flow graph.\nFrom the source vertex, perform a search (DFS/BFS) and mark all reachable edges.\n⇒ The edges between reachable and unreachable verticies is the min-cut edges.\n\n\n\nValue of a cut is the sum of forward edges (=edges in the direction reachable → unreachable)\n\nIn case of a min-cut one needs to minimize this value\n\n\nValue of Min-cut v is equal to the maximum flow f thru the graph\n\n(⇒) Given a min cut s→t and its value v, maximum flow thru s→t is f≥v because backward edges (15 is graph above) is\n\n\nA min-cut may not be unique\n\nTo find all of them, residual → run reachability → traverse cut edges to the nodes (may be multiple!)→ run reachability\n\n\n\nalg. Choice of Augmenting Path. There are three common options:\n\n! Assumptions:\n\nAll capacities are integers\nf∗ is the ideal flow rate\n\n\n\n\nNaive Method\n\nFinding any path using BFS takes O(E) time (fully connected graph)\nIteration count: O(f∗)\nTotal: O(Ef∗)\n\n\nAugmenting path with largest bottleneck.\n\nCan use modified Shortest Path algorithm with O(ElogV) time complexity\nIteration count: O(Elogf∗)\nTotal: O(E2logf∗)\n\n\nBFS shortest augmenting path (Edmond-Karp)\n\nTakes O(E)\nIteration count: O(VE)\nTotal: O(VE2)\n\n\nState of the art: O(VElogV), and even O(E+ϵ)\n\nDiscussion §\nFormalization §\n\nthe answer is given as a flow function f:E↦R&gt;0.\n\ni.e. “assign each edge a flow value”\n\n\nall vertices except the source and target vertices s,t must have net zero divergence (=net zero inflow/outflow)\n\ni.e. “the water can’t disappear or appear randomly in any vertex.”\n\n\n\nApplied Problems §\n\n\n                  \n                  k things with a separate set of l things, suspect flow/cut problem.\n                  \n                \n\nQ. Edge Matching Problem. (=Maximum Cardinality Matching) Given an undirected, partitioned graph, return the maximum number of edges that don’t share a vertex \n\nIdea: reduce to a max-flow problem. \nalgorithm:\n\nFor a graph partitioned A,B, make all edges A→B directed.\nConstruct start and end node s,t, and construct s→a∈A, t←b∈B\nCalculate the max-flow from s→t\nThe edges that are used in the max-flow problem is the solution.\n\n\nComplexity: O(VE)\n\nQ. Advertiser and Viewer Demographic Matching Problem\nQ."},"Maximum-Likelihood-Estimator":{"title":"Maximum Likelihood Estimator","links":["Likelihood-(Statistics)"],"tags":["Math/Statistics"],"content":"def. The Maximum Likelihood (Statistics) Estimator is an estimator θ^ that maximizes L.\n\nIt also works for log likelihood, because the natural log is a monotonic function:\n\nθ^MLE​​=θmax​ Ln​(θ;x1​,...,xn​)=θmax​ lnLn​(θ;x1​,...,xn​)​\nUnder certain regularity conditions, we can find the MLE by finding stationary points in the log likelihood. These are called the likelihood equation:\n∂θ∂lnLn​(θ)​=set to0\nTo consider whether this stationary point is the maximum (as opposed to a miminum) either:\n\ntake the second derivative…\n…or find out via other means\n\n\n\n                  \n                  Properties of MLEs: \n                  \n                \n\n\nMLEs are always a function of a sufficient statistic.\nMLEs are not necessarily unbiased.\nMLEs may not reach the CRLB in variance.\n\nthm. Functional Equivariance of MLE: Given parameter θ and let α=g(θ). Then:\ng(θ^MLE​)=g invertibleα^MLE​\n⇒ the estimator for any function over the parameter can then be found easily.\nthm. Asymptotic Normality of MLE. [=Fisher’s Approximation]\nlet data generated by a univariate single parameter distribution X1​,…,Xn​∼iidf(x1​,…,xn​,θ).\nlet also that θ^MLE​ is found by the likelihood equation ∂θ∂s​=0. Then both are equivalently true:\nθ^MLE​⟶n→∞​N(θ0​,I(θ0​)1​)\nn​(θ^MLE​−θ0​)⟶n→∞​N(0,I(θ0​)1​)"},"Mean-Squared-Error":{"title":"Mean Squared Error","links":[],"tags":["Math/Statistics"],"content":"def. Loss function is a function that encapsulates the “closeness” of the estimate and ground truth. For example a loss function may measured the geometric closeness by:\nL(θ,e)=(θ−a)2\ndef. Mean Squared Error (MSE) is the arithmetic mean of the loss function defined:\nMSE(θ^)​=EX​[(θ^−θ)2]=∫x=−∞∞​(θ^−θ)2fX∣θ=θ^​(x)​\n\n\n                  \n                  Info \n                  \n                \n\nThe following shows why MSE is useful; it is just a sum of variance and bias squared.\nEX​[(θ^−θ)2]=Var[θ^]+Bias[θ^]2"},"Measuring-Security-Performance":{"title":"Measuring Security Performance","links":["Risk-(Finance)","Measuring-Security-Performance","Market-Beta"],"tags":["Economics/Finance"],"content":"Measurement Metrics. The following are used commonly to measure performance of funds. It does not imply, however, that they are meaningful, useful, or correct. Most of them compare risk to return\n\nMeasuring Security PerformanceMeasuring Security Performanceio]]\nJenson’s Alpha. Measures performance against CAPM’s Predictions.\n\nα=(ri​−rf​)−β(rm​−rf​)\nSharpe Ratio §\ndef. Sharpe Ratio. The sharpe ratio for portfolio with return Rp​\nSP​=σp​E[Rp​−μf​]​=σp​μp​−μf​​​​\n\nUsed to compare securities given their risk &amp; returns\nMeasures the “risk-normalized return”\nHigher means a better risk/return profile\n\nTreinor’s Ratio §\nMeasures the return per unit of market risk taken. Higher is better.\nTM=βE[Excess return]​\nReminder: Market Beta is about how correlated the asset is to the market.\nEBITDA Multiple §\ndef. Enterprise value. The amount of money you have to pay to buy off the company, including its financial oblications (net debt)\nEnterprise Value = Equity Value + Net Debt\nwhere Net Debt = SR,LR debt + min.inst.pref.stock−cash\n\nNet Debt &gt; 0 when firm has more debt\nNet Debt &lt; 0 when firm has more cash\n\ndef. EBITDA Multiple.\nEBITDA Multiple=EBITDAEV​=ProfitValue​\n⇒ Think: For two firms…\n\n…if the market values one firm higher [=EV is higher] than another firm,\n…even if their profit is relatively smaller [=relative EBITDA], it implies that:\n…the market thinks the firm has growth potential.\n\nThe EBITDA Multiple is a good measure of how good the operations are of a company because…\n\nEBITDA doesn’t deduct financing costs.\nEBITDA can compare companies in similar industries about how good their operations are.\n"},"Memory-Access-Control":{"title":"Memory Access Control","links":[],"tags":["Computing"],"content":"\nRead\n\nConcurrent Read\nExclusive Read\n\n\nWrite\n\nConcurrent Write\nExclusive Write\n\n\n"},"Merton-Jump-Diffusion-Model":{"title":"Merton Jump Diffusion Model","links":[],"tags":["Economics/Finance"],"content":""},"Methodology-of-Philosophy":{"title":"Methodology of Philosophy","links":["Phenomenology","Analytic-Philosophy"],"tags":["Philosophy/Epistemology"],"content":"Taken an Modified from Wikipedia link above.\n\nGeometric/Mathematical\nPhenomenology\nVerification/Falsification\nConceptual Analysis (Analytic Philosophy)\nCommon Sense/Intuition\nLinguistic\n"},"Michel-Foucault":{"title":"Michel Foucault","links":["Postmodernism","Centralized-Power","Checks-and-Balances","G.-W.-F.-Hegel","Social-Institution"],"tags":["Philosophy","People"],"content":"Notes from What Power Is — Michel Foucault\nBasics\n\n20C thinker\nPostmodernism\n\nMisconceptions of Power\n\nHierarchical power is only a part\n\n→ instead, power is an oceanic force of nature\n\n\nTheoretical vs Empirical\n\nFocault talks both about theoretical and empirical power\nEmperical: sexuality, prison systems, etc. (historical crystallizations)\nTheoretical: generalized, abstract conception, a “theory of power”\n\n\nPrior political analyses of power: misconception\n\ne.g. 1984, Leviathan, Marx’s class relations\ncharacterized as the struggle between powerful vs not powerful\nmost power here is negative power (punishes)\noriginates probably from the western legalistic tradition\n→ but majority of power is not this (misrepresentation)\n\n\n\nImmanent “microphysics” of power\n\ninvisible, but real and measurable\npower is a quality of the social world (not metaphysical, natural force like Nietzche)\nintentional &amp; non-subjective (e.g. party DJ putting on “Killing me softly”)\n\nintentional: you are technically free.\nnon-subjective: people will find you uncool\n\n\n\nResistance\n\nResistance is not exterior to power, but is intrinsic\nrevolutionaries → if there is no resistance, there is no power\nrelations of power is only allowed in a free society\n\n“free” in that you’re technically free to resist\n\n\n\nFull Definition of Power\n\n“Power must be understood in the first instance as the multiplicity of force relation imminent in the sphere in which they operate”\ne.g. dressing for school\n\nlegal, etc. (traditional)\nschool clique\n“what’s cool”\n\n\n⇒ They all operate simultaneously\n\nRelations of Power\n\nLocal force relations act on each other (even in the absence of the individual to act on)\n\ne.g. war. Discipline &amp; punish: perpetual battle\n\n\nstruggle, strengthen, weaken. etc\nThey can also bond &amp; combine\n\ne.g. fashion + peers, school + parents kinda match\n\n\n\nInstitutional Crystallization (social)\n\n“tactics” of power\nlocal micro-physics of power → coalesce to create school &amp; state, bigger power\nnobody individually can predict the effects of micro-power\ne.g. legal, agreement, new forms of government\n"},"Microeconomic-Market-Equilibrium":{"title":"Microeconomic Market Equilibrium","links":["Profit-Function","Utility-Maximization"],"tags":["Economics/Micro-Economics"],"content":"Perfect Competition §\nConditions:\n\n\nZero profit condition: π=px−c(x)=0\n\n\nPricing at marginal cost:\n\n! only when MC is constant…\np=MC\n\n\n\nTheory of the Firm but with…\n\nFixed costs, i.e. π=px−wl−rk−FC\nFirms will enter and exit\n\n⇒ supply price and quantity is at π=0\n\n\n→ market price and individual firm’s quantity supplied are determined by this formula.\nThis is equivalent to the minimum average cost condition minx​AC(x).\n\n\n\nUtility Maximization but…\n\nConsider only one good, x1​\nThere are more than one consumers. x1market demand​=x1​×# of consumers\n\n\n\nMarket equilibrium: \n\nPrice is set at p∗ because it is fully determined by the firm.\n\nThe quantity supplied changes by firms entering and exiting (not by individual firms ramping up or cutting down on production!)\n\n\nQuantity demanded (=aggregate quantity supplied) of x∗ is fully determined by the consumer.\n\n\n\nShort Run §"},"Microeconomics":{"title":"Microeconomics","links":["Philosophy,-Political-Science,-Economics","Decentralization"],"tags":["Economics/Micro-Economics"],"content":"\nCourse process.\nEconomics as a Science §\nEconomics is a science as it creates hypotheses and conducts experiments if they are correct.\n\nSince you can’t often create tests IRL, econometrics deals with interpreting real-world data as a substitute for an experiment\nExperimental economics is a small discipline where you actually conduct experiments\n\n(This is not as scientific as it seems. See Philosophy, Political Science, Economics)\nBuilding Blocks of Microeconomics §\nAssume: People are rational in their pursuit of perceived self-interests. Such people are called economic agents.\n→ …thus people respond to incentives\n→ …then as a consequence of multiple economic agents interacting, there will emerge social consequences.\nEconomic Modeling §\nModeling in economics is a process of distilling the essence of a real-world phenomenon, in order to make predictions about the real world.\n\n\n                  \n                  Model → Optimization → Equilibrium \n                  \n                \n\n\nModel: form a model that describes an economic phenomena (make assumptions, Ceteris Paribus)\nOptimization: consider how economic agents will interact within such assumptions &amp; models. Use math if necessary.\nEquilibrium: predict what will ultimately happen in such a model? What is the ending point? (make predictions)\n\n(Economically, an equilibrium (=Pareto Equilibrium) is a point where no agent can be better off without making another agent worse off)\n\n\n                  \n                  doesn’t need to have realistic assumptions. It just needs to predict reality really well.\n                  \n                \n\n→ Consider: A good pool player doesn’t need to know kinematics to score well. Similarly an economic model doesn’t need realistic assumptions if it can realistically predict real-world economic phenomena.\nOther Key Points §\n\nThe world isn’t a zero sum game. Adam Smith: think of purchasing a piece of bread at 2.Yougetthebread,andyouarebetteroff.Thebreadmakervaluesthebreadatlessthan2, so they’re better off.\nAttribution Bias. People’s actions are due more to the sum of the incentives acting on them, rather than their inherent quality. Don’t attribute it to their personality.\nDecentralization. The rules of the economic game shapes a bottom-up spontaneous order that shapes the world and rations limited resources.\n"},"Microphysics-of-Power":{"title":"Microphysics of Power","links":["External/Michel-Foucault"],"tags":["Philosophy/Political-Philosophy"],"content":"Michel Foucault\nDominance §\nPrestige §"},"Minimal-Spanning-Tree-Problem":{"title":"Minimal Spanning Tree Problem","links":["Graph","Tree","Priority-Queue","Disjoint-Set","Greedy-Algorithm","Shortest-Path"],"tags":["Computing/Data-Structures","Computing/Algorithms"],"content":"Q. Minimum Spanning Tree. From a connected, undirected, edge-weighted Graph, make a subset graph that:\n\nconnects all vertices together\nminimum possible total edge weight\n⇒ will always be a Tree\n→ there could be multiple spanning tree (MST is not unique)\n\nalg. Prim’s Algorithm\n\nIdea: Gradually build one big tree\n\nChoose random vertex, then add all edges connected to it to a Priority Queue.\nUse the lightest edge in the priority queue and connect to that vertex.\n\n\n\nalg. Kruskal’s Algorithm.\n\nIdea: Build many trees, and gradually combine them.\n\nChoose lightest edge, then create a tree with that (using a Disjoint Set)\nIf the vertices are already part of a small tree…\n\nif they’re different trees, combine the two trees with that edge\nif they’re same trees, don’t connect.\n\n\n\n\nTime complexity: O(ElogV)\n\nlem. Exchange Argument. Correctness proof for both Prim’s and Kruskal’s algorithm.\n\nIdea: If we have MST, choosing the lightest edge to connect them (greedily) is MST.\n\nAssume MST T∗.\nSplit into two components C1​,C2​.\nAmong edges that connect C1​,C2​ you choose the lightest edge e∗.\n⇒ Then, C1​,C2​,e is a minimum spanning tree.\n\n\n\nDiscussion §\n\nPrim’s and Kruska’s algorithms are both Greedy Algorithms\nIf edge weights are distinct, there is a unique minimum spanning tree\nDoesn’t have anything to do with Shortest Path algorithms.\n\nApplications §\nQ. Minimal Edge Removal to Acyclic Graph. Given a connected, undirected, weighted graph G=V,E we will remove edges F such that the remaining graph will have no cycles. What is F with the minimum total weight sum?\n\nCompute maximum spanning tree on G\n\nthis can be done by modifying Prim’s or Kruskal’s algorithm…\n2….or by running minimum spanning tree on a new graph with negated edge weights\n\n\nF is the set of edges not contained by this MST\n"},"Moment-(Probability)":{"title":"Moment (Probability)","links":["Distribution-(Math)","Taylor-Approximation"],"tags":["Math/Statistics"],"content":"def. Moment Generating Functions:\nMX​(t)=E[exp(tX)]\n…where MX​ is differentiable k-times around zero to be able to generate the k-th moment.\nYou can build an MGF from a pmf of pdf:\n\nfor pmfs: MX​(t)=∑​etxi​p(xi​)\nfor pdfs: MX​(t)=∫−∞∞​etxf(x)dx\nIt can generate the k-th moment like such:\n\nE(Xk)=∂xk∂k​MX​(0)=MX(k)​(0)\nthm. Linear Combination of MGFs. Given r.v. W=X1​+⋯+Xn​, and Xi​ which are iid,:\nMW​(t)=MX1​​⋅MX2​​⋯MXn​​\nthm. Uniqueness Theorem of MGF. let X,Y have CDFs FX​,FY​ then:\nMX​(t)=MY​(t)⟺FX​=FY​\ni.e. this moment generating function fully characterizes the distribution of the random variable.\nthm. Taylor Expansion of MGF. For RV X\nMX​(t)​=E[etX]=1+tE[X]+2!t2​E[X2]+3!t3​E[X3]+⋯​​\nIntuition for Moments §\nMotivation. Moments are a convenient way to characterize a distinction. The foundation is from this realization (1) The Uniqueness Theorem and (2) Taylor expansion.\nThis means that a linear combination of E[X],E[X2],E[X3],… uniquely describes the distribution of X. It’s also nice that there’s a nice visual explanation for each.\nTypes of Moments §\nFor RV X:\n\nRaw Moments: E[X],E[X2],E[X3]…\nCentral Moments: E[X]=:μ,E[(X−μ)2],E[(X−μ)3],…\nStandardized Moments: E[X],E[(X−μ)2],E[(σX−μ​)3]\n\nSpecial Moments §\n\n0th moment E[X0]=1 i.e. second probability anxiom\n1st moment: E[X]=μ, the mean, location of the pdf\n2nd (central) moment: E[(X−μ)2]=σ2, the variance, the spread of the pdf\n\nsquaring disregards signs, so the farther mass is from μ the higher variance\n\n\n3rd (standardized) moment: E[(σX−μ​)3] the skew, relative tailed-ness of the pdf\n\ncubing preserves signs, so the left-tails negatively contribute, and right-tails positively contribute\n\n\n4th (standardized) moment: E[(σX−μ​)4] the kurtosis, the absolute tailed-ness of the pdf\n\n4th power disregards signs, so the farther mass is from the μ of higher variance. Similar to variance, but higher punishment.\n\n\n\nMethod of Moments §\nMotivation. An alternative way to get an estimator quick and dirty, as opposed to MLEs.\nalg. Method of Moments (MOM):\nlet X1​,…,Xn​∼iidf(x;θ1​,…,θp​). Then to get estimators for θ1​,…,θp​:\n\nlet μi​=E[Xi]=gi​(θ1​,…,θn​)\ngather data on Xˉi to get an empirical estimate mi​\nObserve that because E[Xi]≈Xˉi, this means that μi​≈mi​\nlet function gi​ which maps θ1​,…,θn​↦gi​​mi​ can be inverted\nGet system of equations for as many i’s as necessary\nSolve the system of equations as θ^i​=gi−1​(m1​,…)\n"},"Monetary-Policy":{"title":"Monetary Policy","links":["Money-(Medium-of-Exchange)","Gold-Standard","Balance-Sheet","Interest-Rate","Federal-Reserve","Monetary-Policy","Investment-Bank"],"tags":["Economics/Macro-Economics","Economics/Finance"],"content":"Central Bank Prints Money §\nMotivation. From Money (Medium of Exchange). This is exactly how the central bank prints money, under these rules:\n\nGold Standard: Central bank guarantees that you can give it cash and it will give you gold in return. Central bank is the only bank that can print money (=reserves)\nBalance Sheet must always be balanced, except for that of households\n\nCentral Bank Prints Money (M0) §\n\n\nPrivate bank A is established. Owner invests 100worthofgoldintothebank,with100 equity balancing it out\nPBA deposits gold in the central bank, and in return it gets $100 in cash\nHHA asks for a loan of $90 dollars. PBA grants it.\n\nDepositing in a Private Bank in a Checking Account (M1) §\n\nPBB is establsiehd, has already performed the depositing gold into the CB for cash.\n\nHHA takes the cash from the loan from PBA, and deposits into PBB.\nHHB asks for loan of 180fromPBB,andgetsitincash.Now,HHAcanwrite∗checks∗againsttheir90 in their deposit, which can be used in transactions. We therefore have two types of money in circulation:\n\n\n\nM0: all the paper cash printed and exists in bank vaults or household’s hands\nM1: M0 + all the checks written by (accreddited) private banks that can be used in place of paper cash\nWe also have:\nM2: M1 + all the savings account, etc. and other illiquid forms\ndef. Liquidity. How easy it is to convert to M0. M2 is very illiquid, while M0 is very liquid.\n\nCentral Bank Interest Rates §\nThe minimum Interest Rate declared by the Federal Reserve.\n\nThis is what we mean when we normally say ”The Fed is increasing instrest rates” as Monetary Policy\nCalled different things in different countries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNationName of RateCentral BankU.S.Federal Funds RateFederal Reserve (Fed)EurozoneMain Refinancing Rate / Deposit RateEuropean Central BankUKBank RateBank of EnglandJapanPolicy Balance RateBank of Japan\nMotivation. The central bank’s only ability is to print money. But via this power, it can control the real interest rates. This is called the open market operations. Before that, however, we need to understand overnight loans\nOpen Market Operations §\nOvernight Loans §\n\nContinuing scenario:\n\nHHA requests to withdraw $20 from their checking account in PBA\nPBA doesn’t have enough cash on hand, so goes to PBB and asks “can I borrow 10cashjustovernight,atrateR$?”\nPBA gets the cash, and gives HHA the cash withdrawal\n\nOpen Market Operations §\nThis rate R is by definition the nominal interest rate. The central bank can control this rate because:\n\nCB prints more money\nSupply of money for PBs increases, Demand for overnight loans (=cash) by PBs decreases\n(Law of supply and demand) Price of money in overnight loans, R, decreases.\nThus more money means lower nominal interest rates.\n\nPrivate Bank Operations §\n\nAs seen here, banks are ultimately in the business of “borrow in SR, lend in LR” and pocket the interest rate spread (=difference in interest rates from borrow side and lend side)\nInvestment Banks are a little different, because they deal in stocks. They act as market makers for stocks, and also pocket the underwriting spread (=difference in prices from buy side and sell side)\nIn times of crises:\n\nPeople withdraw more (bank run)\nSell loan at haircut → Bank equity decreases to match\nIf selling loans and cash and reserves can’t keep up with increasing liabilities we have bankruptcy\nTo prevent this: 1) Federal Deposit Insurance Company (FDIC) and 2) Required hold in reserves (~10%)\n\n\n\n\nRelation to Fiscal Policy §\nNominal government budget constraint at time τ=t:\n Nominal govn’t spending Pt​⋅Gt​​​+ Bond payback (1+R)Bt−1​​​Spending Side​= Nom. Tax Rev. Pt​⋅Tt​​​+ Bond issue Bt​​​+ Money from C.B. (Mt​−Mt−1​)​​Revenue Side​\nwhere △Ms=Mts​−Mt−1s​  is the central bank purchasing government bonds\nThe minimum Interest Rate declared by the Federal Reserve.\n\nThis is what we mean when we normally say ”The Fed is increasing instrest rates” as Monetary Policy\n"},"Money-(Disambiguation)":{"title":"Money (Disambiguation)","links":["Money-(Medium-of-Exchange)","Role-of-Money","Balance-Sheet","Gold-Standard","(Article)-Minimum-Viable-Superorganism--Melting-Asphalt"],"tags":["Economics/Finance"],"content":"Money has three roles:\n\nMedium of Exchange → Money (Medium of Exchange)\nStore of Value → Role of Money\nUnit of Account → Balance Sheet\n\nProperties Required in Physical Money §\n\nDivisible ⇒ numerical money. You can divide it into as much as reasonable in transaction\nScarce ⇒ gold/silver is often used\nDesirable ⇒ again, gold/silver.\nDurable ⇒ metallic goods\n\nSee the Gold Standard for more information\nSociology Perspective: §\nSee (Article) Minimum Viable Superorganism  Melting Asphalt\n\n\n                  \n                  Money is industrial-grade prestige status \n                  \n                \n"},"Money-(Medium-of-Exchange)":{"title":"Money (Medium of Exchange)","links":["Monetary-Policy","Money-(Disambiguation)","Information-Asymmetry","Market-Risk","Escrow","Intertemporal-Consumption-Leisure-Optimization-(Full-General-Equilibrium)","John-Meynard-Keynes","New-Keynesian-Business-Cycles-Model"],"tags":["Economics/Finance"],"content":"See also Monetary Policy\nMotivation. We will only discuss money as a medium of exchange in here. For other roles of money, see Money (Disambiguation)\n\nP: Price level\nπ:=Pt​Pt+1​−Pt​​: inflation rate\nR=r+πe: real interest rate (Fisher Identity)\n\nTransaction (Medium of Exchange) §\nMotivation. To solve the problem of the double coincidence of wants, we need a medium of exchange. This can be solved by credit or money.\n\nCredit is when people keep track of how much one ows another, and by doing so without any medium of exchange, can transact.\nMoney is when people exchange actual paper bills during transaction.\nWe consider credit first, because it is simpler and more natural.\n\nCredit Market §\n\nq is the cost of credit, e.g. transaction costs, Information Asymmetry, technology costs,…\nX: the amount of credit in circulation\n\n\nCredit demand:\n\nif R&lt;q, credit is expensive, use cash (pay now)\nif R&gt;q, credit is cheap, use credit (pay later, unused cash earns interest R, but cost increases by q)\n! ⇒ In reality, most people use credit and money both, which means R≈q\n\n\nCredit supply:\n\n∝ competitiveness, information friction, fintech\n∝1/aggregate risk (=Market Risk), regulation\n\n\n\n\nMoney Market §\nMotivation. It’s hard to find credit-worthy counterparties, e.g. you don’t know if a stranger will pay you back, or if a Escrow will still exist in the future. A strong institution that can issue a medium of transaction you can trust is essential for larger transactions. Thus we have money.\nMoney Quantity v⋅MInflation Identity πMoney Demand MdMoney Supply Ms​:=P⋅(Y−X∗(R)​ Q credit in circ. ​):=P=v+M−(Y−X∗)​:=P⋅ liquidity func. L(Y,R)​​≡M∗​​\n\nv is the velocity, i.e. “how many times $1 bill gets used per year”\nX∗ is the amount of credit in circulation\nL(⋅) is the liquidity function, i.e. how easy it is to convert into cash\nMoney supply is just the fixed amount the central bank prints.\nMoney demand is the amount of money people want to hold:\n\n∝ Y, since more goods means more transactions occurring (rotates clockwise)\n∝1/R, as R risers just use credit with cost q and get interest on cash → credit demand increases → cash demand decreases (rotates counter-clockwise)\n\n\n\n\nNeutrality Vs Non-Neutrality of Money §\ndef. Monetary Neutrality is a hypothesis of classical macroeconomists that suggest changing the money supply has no effect on the Intertemporal Consumption-Leisure Optimization (Full General Equilibrium) model. Consider the case where the central bank increases money supply:\n \nAs the money supply is increased, prices are simply increased to match.\nIntution. This is equivalent to central banks saying “we will put another zero at the end of every dollar bill starting next month.” All prices and businesses will be ready to also put another zero at the end of their prices and contracts, and nothing will really change.\nThis assumption is challenged by John Meynard Keynes who suggested firms are not always free to change prices at will. This is modeled in the New Keynesian Business Cycles Model."},"MongoDB-Reference":{"title":"MongoDB Reference","links":["Regular-Expressions"],"tags":["Computing/Data-Science"],"content":"All You Need is… §\n{json}db.bib.find({ title:/[dD]atabase/, price:{$lt:50} })\n\nRegex enclosed in /regex/\nthe argument for find is\nSyntax error: NoSQL queries will return none. SQL will return error\nMongoDB (NoSQL!)\n\nSchema: Document⊂Collection⊂Database\n\ndocument is a single json object (with _id as unique identifier in collection)\n\n\n\n\n\nIf You want more… §\n\n{json}mydb.mycollection.find() ← all documents\nSelection in {js}find(…)\n\n{js}find({ title: &quot;databases&quot; }) ← accepts JSON.\nstring pattern matching is in Regular Expressions. {js}title: /[dD]atabase/\nmultiple patterns: and operation by default.\n\n→ but JSON must have unique keys. {price:…, price:…} doesn’t work (silent error)\nalternative: $and: […, …]\n\n\nwhen there arrays: an ∃ operation.\n{js}$elemMatch: { title: /Section/ } ← performs match per array element\n{js}&quot;arr.0&quot;: &quot;match&quot; ← index matching for array\nbuilt-in functions\n\n{$lt:…}\n{$and:[…, …]}\n\n\n\n\nProjection in {js}find(…, { _id: false, attr1: true, …})\nSort by {js}find().sort({ ISBN: 1 })\n\n1 is ascending (0 → ∞, a → z)\n-1 is descending\n\n\n\nMisc Facts §\n\nQuery strings are valid JSON objects\nprint it to console:{js}printjson(db.collections.find().toArray())\n\nEven More: Aggregation Pipeline §\n\nEach step of the pipeline transforms the json object in some way.\nAggregation steps can include:\n\nSelection: {js}$match\nProject: {js}$project\nSort: {js}$sort\n{js}$addFields\n{js}$unwind\n{js}$lookup\n{js}$group\n\n\n\n"},"Monopoly":{"title":"Monopoly","links":["Market-Power","Uncompensated-Demand-curve","Cost-Function","Unconstrained-Maximization","Price-Elasticity-of-Demand","Pareto-Efficiency","Compensating-and-Equivalent-Variation"],"tags":["Economics/Micro-Economics"],"content":"Definitions §\n\nMonopoly is when a single firm has all the Market Power. Monopolies normally have:\n\nHigh barriers to entry\nProduction in elastic portion of demand\nSingle firm with market power\n(usually) economies of scale\n\n\nA Natural Monopoly is when it’s hard for firms to achieve break-even without a monopoly market strucutre\n\nMarket Demand for Monopolies §\n\n\n                  \n                  MR &gt; 0, i.e. where ε &gt; 1\n                  \n                \n\n\nMonopolies face the whole market demand curve.\nMarginal Revenue (MR) curve will slope down with the same intercept and twice the gradient of the Demand curve.\nVisual Profit Maximization when MC=MR (See below for math)\n\nFirm produces xM (=monopoly quantity)\nMarket price settles at pM (=monopoly price)\nFirm makes profit of c+d\nDeadweight loss is e\n\nDWL happens because monopolist increase price more than optimum\n\n\n\n\n\n\nProfit Maximization for Monopolies §\nProfit maximization for monopolies:\nmaxx​p(x)⋅x−C(w,r,x)\n\np(x) is the inverse of the Ordinary Demand of goods\nC(x) is the Cost Function that is derived w.r.t x\n\n! Not simply wl+rk!\n\n\nTo solve, use Unconstrained Maximization\n\nFirst Order Condition: ∂x∂π​=0\n\nThis is the same condition as MR=MC condition.\n! In third-degree price discrimination, MR=MC can only be used if MC is constant.\n\n\nWhen elasticity is constant ⇒ Use the Elasticity condition: p(x)=1+ϵd​1​MC​\n\nElastic: p&gt;MC thus excess profit\nUnit elastic: p=MC thus zero profit\nInelastic: p&lt;MC thus negative profit (monopolist doesn’t produce)\n\n\n\n\n\n\nPrice Discrimination for Monopolies §\nFirst Degree Price Discrimmination §\nEverybody pays exactly how much they’re willing to pay.\n\n\nDemand curve is same as marginal revenue curve\nPareto efficient because there is no deadweight loss; everybody pays exactly as they want to pay, and no extra profit is lost\n\n→ but the benefit goes to the monopolist only\n\n\n\nTwo-part Tariff §\n\n\nConsumers are made to pay amount A to enter the market\nThen charge consumers p=MC\nSame as Equivalent Variation\n\nProcess of enforcing the tariff\n\nConsumers at utility u1​ˉ​ and only buys good x2​\nFirm enters the market and starts selling good x1​ at price p=MC. The consumer gains utility u2​ˉ​\nFirm realizes they could get more profit. They charge amount A to enter the market. Thus the consumer loses amount of difference.\nConsumers are back to u1​, but they’re still buying x1​ because they’re just as happy as when they only bought x2​.\n\nYou can get market ticket price by either\n\nusing Equivalent Variation on the right graph\nusing integration on the left graph\n\nThird Degree Price Discrimination §\n\nIdea: Charge differently based on consumer characteristics (=based on Price Elasticity of Demand)\n⇒ Split consumers into two groups A,B with elasticity ϵDA​,ϵDB​ and…\n\nIf MC is constant → use the elasticity condition\nIf MC is not constant → use normal profit maximization:\nmaxxA​,xB​​π=pA(xA)⋅xA+pB(xB)⋅xB​Total Revenue​−C(xA+xB)\n\n\nYou will eventually get MRA=MC and MRB=MC\n"},"Monotonic-Transformation":{"title":"Monotonic Transformation","links":["Utility-Function"],"tags":["Math","Economics"],"content":"def. Monotonic Transformation. A transformation of a Utility Function is one that preserves the order of preferences of the original function. i.e., f is a monotomic transformation of utility function u(x) iff:\n∀(xA,xB), if u(xA)&gt;u(xB)⟹f∘u(xA)&gt;f∘u(xB)\nSimple monotonic transformations:\n\nAdding or subtracting a constant\nMultiplying or dividing by a positive constant\nFor always positive functions, exponentiation to a constant\nFor always positive functions, logarithm to a constant base\n\nthm. monotonic transformation equivalence. If one utility function is a monotonic transformation of another, you may treat it for all practical purposes as the same utility function."},"Moore’s-law":{"title":"Moore’s law","links":[],"tags":["Computing/Computer-Architecture"],"content":"\n\nMoore’s law is the observation that the number of transistors in an integrated circuit (IC) doubles about every two years. Moore’s law is an observation and projection of a historical trend. Rather than a law of physics, it is an empirical relationship linked to gains from experience in production.\nThe observation is named after Gordon Moore, the co-founder of Fairchild Semiconductor and Intel (and former CEO of the latter), who in 1965 posited a doubling every year in the number of components per integrated circuit, and projected this rate of growth would continue for at least another decade. In 1975, looking forward to the next decade, he revised the forecast to doubling every two years, a compound annual growth rate (CAGR) of 41%. While Moore did not use empirical evidence in forecasting that the historical trend would continue, his prediction held since 1975 and has since become known as a “law”.\nMoore’s prediction has been used in the semiconductor industry to guide long-term planning and to set targets for research and development, thus functioning to some extent as a self-fulfilling prophecy. Advancements in digital electronics, such as the reduction in quality-adjusted microprocessor prices, the increase in memory capacity (RAM and flash), the improvement of sensors, and even the number and size of pixels in digital cameras, are strongly linked to Moore’s law. These ongoing changes in digital electronics have been a driving force of technological and social change, productivity, and economic growth.\nIndustry experts have not reached a consensus on exactly when Moore’s law will cease to apply. Microprocessor architects report that semiconductor advancement has slowed industry-wide since around 2010, slightly below the pace predicted by Moore’s law. In September 2022 Nvidia CEO Jensen Huang considered Moore’s law dead, while Intel CEO Pat Gelsinger was of the opposite view.\nWikipedia\n"},"Moral-Hazard":{"title":"Moral Hazard","links":[],"tags":["Economics/Game-Theory"],"content":"e.g. Insurance. People with insurance tend to take more risks because they know they’re insured."},"Multi-Sensory-Memory":{"title":"Multi-Sensory Memory","links":[],"tags":["Psychology"],"content":""},"Multidimensional-Expressions":{"title":"Multidimensional Expressions","links":["Database-Management-System"],"tags":["Computing/Linguistics/MDX"],"content":"def. Online Analytical Processing (OLAP) is a form of data storage and management (similar to a  relational database), but with multiple dimensions. While RDBMS is general-purpose, OLAP is more specific to data aggregation and analysis, mostly for business purposes."},"Multinomial-Distribution":{"title":"Multinomial Distribution","links":["Binomial-Distribution"],"tags":["Math/Common-Distributions"],"content":"def. Multinomial Distribution. For outcome space Ω, k mutually exclusive events each with probabilities p1​,…,pk​ s.t. ∑i=1k​pi​=1, let random variables Xi​ denote the number of outcomes whose probability is pi​, when n trials are done:\nX1​,...,Xk​∼Multinomial(n,k,p1​,...,pk​)P(X1​=x1​,...,Xk​=xk​)=x1​!,...,xk​!n!​⋅p1x1​​×⋯×pkxk​​\n⇒ Multinomial is a generalization of the Binomial Distribution."},"Multivariable-Calculus":{"title":"Multivariable Calculus","links":[],"tags":["Math/Calculus"],"content":"\nHessian Matrix\nQuadratic Form\n"},"Multivariate-Ordinary-Least-Squares-Regression":{"title":"Multivariate Ordinary Least Squares Regression","links":["Omitted-Variables","Ordinary-Least-Squares-Regression"],"tags":["Math/Statistics"],"content":"Yi​=β0​+β1​X1i​+β2​X2i​+β3​X3i​+ϵi​\n\n\n                  \n                  Omitted Variables in the regression. Refer to the document in that case.\n                  \n                \n\nVariance of Parameter Estimators §\nIn this multivariate model the variance of β1​^​ is:\nVar(β1​^​)=N⋅Var(X1​)⋅(1−R12​)σ^2​\nwhere\n\nσ^2 is the variance of the regression\nR1​ is the coefficient of determination in the auxiliary regression X1i​=γ0​+γ1​X2i​+γ2​X3i​+ϵi​. It measures multicolinearity\n\n&amp; It measures how much of X1​ can be explained by X2​,X3​\nIn perfect multicolinearity R1​=1 then X2​,X3​ will perfectly determine X1​, thus X1​ is not relevant anymore.\n&amp; Equivalently R2​ is the coefficient of determination from X2i​=γ0​+γ1​X1i​+γ2​X3i​+εi​, etc. etc.\n\n\n1−R12​1​ is known as the variance inflation factor. The higher the multicollinearity, the more inflated is the variance of parameter estimators.\nMulticollinearity is not a problem iff:\n\nit occurs only between control variables\nVar(β1​^​) is small enough; i.e. β1​^​ is statistically significant.\n\n\nN is the sample size\n\nCoefficient of Determination §\nIn multivariate situations (as opposed to bivariate) the coefficient of determination of the whole regression (R2) is more troublesome because\n\nAdding more variables will only increase R2\nThus one is incentivized to increase the number of (possibly irrelevant) independent variables.\nTo mitigate this, we report the adjusted R2 values instead, with a penalty for each additional independent variables used.\n\nHypothesis Testing regarding Coefficients §\nStandardizing Coeffiefficients §\nExample. In the model GDP=β0​+β1​Life Expectancy+β2​Literacy Rate, if we want to compare the effects of life expectancy and literacy, we cannot simply compare the values of β1​,β2​. This is because their units are different, i.e. β1​ is in YearDollars​ and literacy is in Percentage PointDollars​. Thus we need to standardize them:\nβistd​^​=Var(βi​^​)​βi​^​−E(βi​^​)​\nwhich shows “how much Y increase in units of σY​ does one unit σXi​​ increase in X cause?”\nRemark. Only X1​ and X2​ need be standardized to compare β1​^​ and β2​^​’s effects. Y need not be standardized.\nHypothesis Testing about Coefficients §\nLet the model Yi​=β0​+β1​X1,i​+β2​X2,i​+β3​X3,i​+ϵi​. Sometimes we may want to check if β1​^​=?β2​^​, or β1​^​=β2​^​=?0. In these cases we use a hypothesis test. Let Runrestricted2​ be the R2 of this regression. Now, before we do anything we need to…\n\n\n                  \n                  X_{1},X_{2}.\n                  \n                \n\nCase 1: H0​:β1​^​=β2​^​=0. Then the model under null would change to:\nYi​=β0​+β3​X3,i​+ϵi​\nWe run regression on this new model and get Rrestricted2​\nRemark. This is not equivalent to running a t-test on HA​:β1​^​=0∧^β2​=0 because X1​,X2​ may be multicollinear.\nCase 2: H0​:β1​^​=β2​^​. Then the model under null would change to:\nYi​=β0​+β1​(X1,i​+X2,i​)+β3​X3,i​\nWe run regression on this to also get Rrestricted2​.\nWe can observe that in both cases, Runrestricted2​&gt;Rrestricted2​, always, because “restricting” the model will lead only to less (coefficient of-) determination. Now, the bigger this difference is, the more likely that the null is false. We formalize this using the F-test:\ndef. F-Test. For the F-statistic defined as:\nFq,N−k​:=(1−Runres2​)/(N−k)(Runres.2​−Rrestr.2​)/q​\nwhere\n\nq is “how many equal signs in null hypothesis”\nk is the degrees of freedom (=number of coefficients in the _un_restricted model)\nThen:\n\n{H0​H1​​elseif F&gt;K​\nwhere K is the critical value. The critical values are:\n\nK=3.00 in case 1 (H0​:β1​^​=β2​^​=0)\nK=3.84 in case 2 (H0​:β1​^​=β2​^​)\n"},"Music-190FS-Music-and-Medicine-in-European-Renaissance":{"title":"Music 190FS Music and Medicine in European Renaissance","links":["assets/190---Analysis-of-Penelope-Gouk.pdf","assets/190---Renaissance-in-Medieval-Korea.pdf"],"tags":["Courses"],"content":"Writing Assignments §\n\n190 - Analysis of Penelope Gouk.pdf\n190 - Renaissance in Medieval Korea.pdf\n\nCourse Readings §\n\nIntroductions and Origins\n\nSelections from Plato, Aristotle, and Saint Augustine, in Oliver Strunk, ed., Source Readings in Music History (New York: W. W. Norton, 1950), pp. 3–24 and 73–75.\n\n\nAntiquity and the Middle Ages: number in celestial and social harmony\n\nMartin West, “Music Therapy in Antiquity,” in Music as Medicine, ed. Peregrine Horden (Aldershot: Ashgate, 2000), pp. 51–67.\nMarsilio Ficino. The Book of Life, trans. Charles Boer (Irving, TX: Spring Publications Inc., 1980), pp. 3–20 and 158–164.\n\n\nRenaissance perspectives I: Marsilio Ficino\n\nSelections from The Letters of Marsilio Ficino, trans. members of the Language Department of the School of Economic Science, London (London: Shepheard-Walwyn, 1975–2015), vol. 1: No. 5 “Medicina corpus, musica spiritum, theologia animam curat” pp. 39–40; No. 7 “De divino furore,” pp. 42–48; No. 92, “De musica,” pp. 141–144.\nMarsilio Ficino, All Things Natural: Ficino on Plato’s Timaeus, trans. Arthur Farndell (London: Shepheard–Walwyn, 2010), chapters 29–33, pp. 51–71.\nD.P. Walker, Spiritual and Demonic Magic: From Ficino to Campanella (Notre Dame: University of Notre Dame Press, 1975), pp. 3–29.\n\n\nRenaissance perspectives II: music and melancholy\n\nPeter Amman, “Music and Melancholy: Marsilio Ficino’s Archetypal Music Therapy,” Journal of Analytical Psychology 43 (1998): 571–588.\nPenelope Gouk, “Music, Melancholy, and Medical Sprits in Early Modern Thought,” in Music as Medicine, ed. Peregrine Horden (Aldershot: Ashgate, 2000), pp. 173–194.\n\n\nEarly modern science and sound I: the philosophy of human and celestial bodies\n\nStillman Drake, “Music and Philosophy in Early Modern Science,” in Music and Science in the Age of Galileo, ed. Victor Coelho (Dordrecht: Kluwer Publishers, 1992), pp. 3–16.\nRoseen Giles, “The Inaudible Music of the Renaissance: From Marsilio Ficino to Robert Fludd,” Renaissance and Reformation 32, no. 2 (2016): 129–166.\n\n\nEarly modern science and sound II: Kepler, Galileo, and the Scientific Revolution\n\nPenelope Gouk, “The role of harmonics in the scientific revolution,” in The Cambridge History of Western Music Theory, ed. Thomas Christensen (Cambridge: Cambridge University Press, 2002), pp. 223– 245.\nOwen Gingerich, “Kepler, Galilei, and the Harmony of the World,” in Music and Science in the Age of Galileo, ed. Victor Coelho (Dordrecht: Kluwer Publishers, 1992), pp. 45–63.\n\n\nMusic, healing, and the senses\n\nRobert E. Butts, “Tickles, Titillations, and the Wonderful Accidents of Sound: Galileo and the Consonances,” in Music and Science in the Age of Galileo, ed. Victor Coelho (Dordrecht: Kluwer Publishers, 1992), pp. 115–127.\nLinda Phyllis Austern, “Musical Treatments for Lovesickness: The Early Modern Heritage,” in Music as Medicine, ed. Peregrine Horden (Aldershot: Ashgate, 2000), pp. 213–245.\n\n\nMedicine and art in the age of the Enlightenment: pills to purge melancholy\n\nPenelope Gouk, “Music’s Pathological and Therapeutic Effects on the Body Politic: Doctor John Gregory’s Views,” in Representing Emotions: New Connections in the Histories of Art, Music, and Medicine, eds. Penelope Gouk and Helen Hills (Farnham: Ashgate, 2005), pp. 191– 207.\nCharles Brotman, “The Undulating Self: The Rhythmic Conception of Music and the Emotions,” in Representing Emotions: New Connections in the Histories of Art, Music, and Medicine, eds. Penelope Gouk and Helen Hills (Farnham: Ashgate, 2005), pp. 209–221.\n\n\nHealth and physiognomy: music and the body  \n\nMartha Feldman, The Castrato: Reflections on Natures and Kinds (Berkeley: University of California Press, 2015), preface [e-book available via library catalogue]\nBelcastro, Todero, Fornaciari, and Mariotti, HFI and castration: the case of the famous singer Farinelli, Journal of Anatomy 219 (2011): 632–37.\nSigma Xi, Medical Insights into the Castrati in Opera, American Scientist 75, no. 6 (1987): 578–83.\n\n\nMedicine, art, and science: the 19th century\n\nDavid Gentilcore, “Ritualized Illness and Music Therapy: Views of Tarantism in the Kingdom of Naples,” in Music as Medicine, ed. Peregrine Horden (Aldershot: Ashgate, 2000), pp. 255–272.\nCheryce Kramer, “Music as Cause and Cure of Illness in Nineteenth- Century Europe,” in Music as Medicine, ed. Peregrine Horden (Aldershot: Ashgate, 2000), pp. 338–352.\nJames Kennaway, Bad Vibrations: the history of the idea of music as cause of disease (Burlington, VT: Ashgate 2012), excerpts from chapters 1 and 2.\n\n\nContemporary perspectives in music therapy\n\nMichael P. Steinberg, “Music and Melancholy,” Critical Inquiry 40, no. 2 (2014): 288–310.\nBjörn Lemmer, “The rhythm of the heart—the tempus of music—Mozart, Ligeti, and the Rat,” in Music that Works: Contributions of Biology, Neurophysiology, Psychology, Sociology, Medicine and Musicology, ed. R. Haas and V. Brandes (Vienna: Springer, 2009), pp. 167–178.\nPeter Burke, “Rituals of Healing in Early Modern Italy, “in The Historical Anthropology of Early Modern Italy (Cambridge: Cambridge University Press, 1987), pp. 207–220.\n\n\n"},"Nash-Equilibrium":{"title":"Nash Equilibrium","links":[],"tags":["Economics/Game-Theory"],"content":"def. Nash Equilibrium is a set of strategies (one for each player) which is the best response strategy of each other’s move; i.e. you can’t deviate without destabilizing the equilibrium\nalg. Finding the Nash Equilibrium in a payoff matrix.\nCorner Method to find NE in payoff matrix.\n1. For each player:\n2. For each other player’s move; highlight the line of the best response\n→ When both the left and top lines are highlighted that is NE.\nPure Strategy Nash Equilibirum §\ndef. PSNE. s=(s1​,s2​) is PSNE for two players 1,2 iff:\ns1​s1​​=argmins∈S1​​c1​(s,s1​)=argmins∈S2​​c2​(s1​,s)​​\nHow to Find NE §\n\nIn Simultaenous Games: Use the Corner method\nIn Sequential Games: Make the Decision Tree into a Payoff Matrix, and use the Corner method:\n\ne.g. Driving left or Right\n\n\nSubgame Perfect Nash Equilibrium (SPNE) §\ndef. A Non-credible threat is when the follower in a sequential move game says: “If I can’t win, I’ll take you down no matter how much it costs to me.”\n\nIn the above game, (R,R) is an example of a non-credible threat because when Player 1 chooses L, Player 2’s best response is L—but Player 2 threatens to go R. This is because they want the best possible outcome for them, R,(R,R)\n\ndef. Subgame Perfect Nash Equilibria (SPNE) are NE where the follwer will only choose strategies that are best for them, and can’t threaten the leader beforehand with non-credible threats.\nTo find the SPNE of a game, use Backwards Induction:\n\nDetermine the last player’s best strategy\nThe second-last player knows what the last player will do. Then determine what the second-last player will do.\nContinue solving until the first player.\n\n\nSPNE={(d,d),(d,d)}, payoff is (1,0) which is a lot worse off that global optimal."},"Natural-Experiments":{"title":"Natural Experiments","links":[],"tags":["Math/Statistics"],"content":"Need not true randomness, only exogenity\ne.g. crime vs police; terror alerts cause police assignment to increase; terror is exogenous."},"Neat-Projects":{"title":"Neat Projects","links":["Elegance"],"tags":["Computing"],"content":"Having a good intution for something that is “neat” is important in the hacker culture. This means you know what things to build, that is either helpful to the world or helpful to your understanding. This is the counterpart to ”Elegance” in math or pure computer science/code itself."},"Necessary-Lies-of-Civilization":{"title":"Necessary Lies of Civilization","links":[],"tags":[],"content":""},"Neoliberalism":{"title":"Neoliberalism","links":[],"tags":["Philosophy/Political-Philosophy","Economics"],"content":""},"Neural-Networks,-Backpropagation-and-Gradient-Descent":{"title":"Neural Networks, Backpropagation and Gradient Descent","links":["Logistic-Model","logic-gates","Information-Theory"],"tags":["Computing/Maching-Learning"],"content":"Motivation 1. Consider the Logistic Model for classification. We observe the limitations of this model as:\n\nThe decision boundaries are linear, and thus can only solve linearly separable problems.\n\nSpecifically: it can simulate OR, AND logic gates… \n…but it can’t simulate XOR because it’s not linearly separable.\n\n\nAs seen here, we haven’t considered how to transform the ‘input space’ into the ‘feature space.’ This means we still perform terribly without this ‘magic,’ even as simple as classifiying digits in MNIST.\nMotivation 2. Consider how the logistic model, with its weights and bias, can be hooked up together. This resemples the neural connections of the human brain.\n\nFeed Forward Neural Network §\ndef. Neural Network. A feed forwarnd neural network connects the operations of multiple Logistic Models (=neurons) together.\n\n\n                  \n                  Notation of quantities are as follows: \n                  \n                \n\nVrow,column(layer)​\nLet the no. of neurons (=dimensions) of each of the layers be, from the input layer: D(0),D(1),…,D(L+1). The components of the neural net are:\n\nInput neurons x in input space x∈RD(0) (layer k=0)\n→ h(0)=x\nHidden neurons (layer k=2…L)\n→ h(k+1)=g(a(k+1))=g(W(k+1)h(k))\nOutput later neuron (layer k=L+1)\n→ f(x)=h(L+1)=σ(a(L+1))=σ(W(L+1)h(k))\nThe parameters of the neural net are:\nWeight parameters. Layer k’s outgoing weights are packaged into a nice W(k) matrix with shape Dk+1×Dk, where Wi,j(k)​ refers to the weight connecting neuron j in layer k to neuron i in layer k+1. (This backward-ness of the indexing comes from the matrix multiplication shown later.)\nBias parameters. Layer k injects a bias, b into each of the neurons in layer k+1. In matrix multiplication terms this is simply multiplying by one, as it is thusly shown in the image.\nActivation Function. Each neuron in layer k+1, having gotten its weighted input sum passes through an activation function. Without this activation function the whole NN will collapse into a simple linear map.\nVisualization. The following visualization lacks the bias term for simplicity.\n\nMathematical Backing §\nIntuition. Why is this supposedly random arrangement of logistic neurons so effective expeirmentally? Here is a possible reason: neural networks can simulate any function.\nthm. Universal Approximation (non-rigorous statement). Given enough depth and width, and given “normal” activation functions (sigmoid, tanh), a neural network can simulate any function to arbitrary precision.\nBackpropagation §\nMotivation is simple. How do we find W(1)…W(L+1),b(1)…b(L+1)? To do that we must minimize the loss function, i.e. find the gradient of the loss function.\ndef. Loss Minimization of a NN. Let NN be f:x∈RD(0)↦h(L+1)∈RD(L+1)function ℓ is the loss of the neural network with parameters θ=(W(1)…W(L+1),b(1)…b(L+1)). Given training data and label {(xt​,yt​) for t∈1…T} The optimal parameters are:\nθ∗=argθmin​T1​∀t∑​ℓ(f(x(t)​∣θ),y(t)​)+λΩ(θ)\n\n\n                  \n                  Choice of loss function \n                  \n                \nWhile we can choose any loss function, from now on we will simply use the cross-entropy. similar to the loss function here, but only with one data so the second summation disappears.\n\nFor the loss function we have:\nℓ(f(x∣θ),y)​=−ln∀k∈[K]∏​f(x∣θ)kyk​​=−∀k∈[K]∑​f(x∣θ)k​⋅yk​=−lnf(x∣θ)y∗​​​\nwhere we let y⋆=argmaxy as above.\ndef. Gradient Descent. When a analytic solution is not possible, we can minimize a function (=loss function) by numerical methods:\n\nRandomly select parameters θ\n? Find the gradient of the loss function with one training example ∂θ∂​ℓ(f(x∣θ),y)\nMove a little bit (η, learning rate) in the direction of the gradient\nRepeat for every training example (x,y)\nThe question then is how to calculate ∂θ∂ℓ​ from step 2. Here we use backpropagation:\n\nalg. Backpropagation. We can derive the formulae for the derivatives ∂θ∂ℓ​ using the chain rule. First, on the last layer k=L+1: 1\n∂h(L+1)∂ℓ​=−hy∗(L+1)​e(y∗)​\nthen for the previous layer k=L: 2\n∂h(L)∂ℓ​1×D(L)​​=∂h(L+1)∂ℓ​⋅=g′(⋅)∂a(L+1)∂h(L+1)​​​⋅=WL∂h(L)∂a(L+1)​​​=−hy∗(L+1)​e(y∗)​1×D(L+1)​⋅g′(a(L+1))D(L+1)×D(L+1)​⋅W(L)D(L+1)×D(L)​​\nWhile we can extract the differentials wrt W(L) by: \n∂W(L)∂ℓ​​=∂h(L+1)∂ℓ​⋅g′(⋅)∂a(L+1)∂h(L+1)​​​⋅(h(L))⊤per row, all layers∂W(L)∂a(L+1)​​​=−hy∗(L+1)​e(y∗)​1×D(L+1)​⋅g′(a(L+1))D(L+1)×D(L+1)​⋅(⋅)D(L+1)×D(L)×D(L+1)​​​\nAnd extract the differentials wtr b(L) by:\nWe can continue this until necessary according to the following visualization.\nVisualization. \ndef. Gradient Descent. A form of optimization. Average the gradient of all the datapoints.\nθk+1​←θk​−α​ average over all data N1​i=1∑N​​​∇θk​​ℓ(f(xi​∣θk​),yi​)​\nwhere α is a chosen learning rate.\nFootnotes §\n\n\nDeep Learning Slides ↩\n\n\nDeep Learning Slides ^9wnois ^qrkwvn ^ek9eg9 ^bvhuk5 ^bgjjby ^1t1avf ^p87mcc ^xibkdm ↩\n\n\n"},"Neurons-and-Glia":{"title":"Neurons and Glia","links":[],"tags":["Biology/Neuroscience"],"content":"\nNeurons\n\nFunction: Motor neuron/Sensory neuron\nComponents:\n\nDendrites w/ Dendritic spines that increase surface area\nCell body (=Soma)\nAxon\n\nAxion Hillock\nMyelin Sheath (말린 부분) made of Schwann cells + Nodes of Ranvier (the inbetween nodes)\nPresynaptic terminal\n\nIncreases the speed of impulse transmission: Saltatory conduction\n\n\n\n\n\n\nSystemic: efferent /afferent neuron\n\n\n\n\n\n\nGlia\n\nMicroglia: immune system\nOligodendrocytes (@central nervous system), Schwann Cells (@Periphery) build &amp; maintain myelin sheath\nRadial glia: guide development &amp; movement during embryo\n\n\n\n\nBlood-Brain Barrier\n"},"Neuroscience-102":{"title":"Neuroscience 102","links":["Neurons-and-Glia","Brain-Anatomy","Vision-(Neuroscience)","Sleep-(Neuroscience)"],"tags":["Biology/Neuroscience","Courses"],"content":"Neurons and Glia\nBrain Anatomy\nVision (Neuroscience)\nSleep (Neuroscience)"},"New-Keynesian-Business-Cycles-Model":{"title":"New Keynesian Business Cycles Model","links":["Real-Business-Cycle-Model","Coordination-Failure-Business-Cycle-Model","Intertemporal-Consumption-Leisure-Optimization-(Full-General-Equilibrium)","Money-(Medium-of-Exchange)"],"tags":["Economics/Macro-Economics"],"content":"Motivation. Unlike the Real Business Cycle Model or the Coordination Failure Business Cycle Model, in reality it’s pretty hard for companies to change prices, wage contracts to change, etc. This is called the menu cost, and we assume this is so big that firms can’t change the prices P in the short term.\nWe still have the same Intertemporal Consumption-Leisure Optimization (Full General Equilibrium) model and the Money Market but with the key difference: Price levels P are fixed.\n\nNow assume that for some stupid reason the central bank increases money supply in order to decrease interest rates. Note that:\n\nass. Sticky Prices. Price level Pˉ is fixed no matter what.\nass. Demand-Determined Output. The output is always demanded, not supplied. At a given r, the Y∗ is read off from the Yd curve, not the Ys curve.\nThe labor demand curve is irrelevant. Read the N∗ off the production function F(⋅) based on Y∗ from the Yd curve\nThen:\n\n\n\nCentral bank sets target interest rate r∗, and matches the Ms to target that interest rate\nHouseholds…\n\nDemand more at lower r∗\nDecrease labor supply (work tomorrow instead)\n\n\nFirms…\n\nStart producing more to match the higher demand Y∗ at new lower r∗\nEmploy more people at new higher wage w∗ to meet higher demand\nLiquidy demand expands as rates decrease and output increases, to meet the new Money supply at the same Pˉ\n\n\n"},"Nimf-Anthy-installation":{"title":"Nimf-Anthy installation","links":[],"tags":["Computing"],"content":"Nimf-Anthy Installation §\nHow do I restore the default repositories?\n하모니 공지 - 하모니카 저장소 공개키 서명 문제 해결방법\n하모니카(HamoniKR) 하모니 - apt update시 인증서 문제\nInstallation\nsoftware-properties-gtk # open the gui repsitories \ncurl -sL https://apt.hamonikr.org/setup_hamonikr.jin | sudo -E bash - # add the Hamonikr repos to apt sources\nsudo apt install ca-certificates # update certificate authority files\nsudo apt install update # update apt cache\nsudo add-apt-repository ppa:hodong/nimf # add nimf dev&#039;s repo to apt source\n# and the logout and back in.\nThe hamonikr repo and ppa:hodong/nimf has different versions names. The binaries are probably the same but apt will complain about dependency versions. Installing the hodong/nimf repo will resolve the dependency issue."},"No-Arbitrage-Condition":{"title":"No Arbitrage Condition","links":["Options-(Finance)","Free-Cashflow"],"tags":["Economics/Finance"],"content":"\nPayoff: the gross outcome of an investment or trade. The amount you earned from that trade, regardless of commissions, extraneous costs, etc.\nProfits: the gross outcome of an investment or trade, including commissions, extraneous cost\n\n→ the distinction happens in Options (Finance). Payoffs don’t consider the option premium, while the profits do.\n\n\nReturn: short for “rate of return”. The percentage of profit (not payoff!) per original investment. Quoted in percentage (%) or log-returns.\ndef. No arbitrage condition (=Law of One Price means without any risk, there cannot be excess return (=return above risk-free rate).\n\n\nlet security A have payoff pA​, cost cA​ and B payoff pB​, cost cB​.\nIf pA​=pB​ then cA​=cB​\nAlternative formulation:\nIf security A is risk-free and has payoff pA​…\n…then the current price of A should simply be the DCF of pA​ discounted at the risk-free rate\n\nRisk-Neutral Assumption §\nMotivation. People in general are risk-averse. Observe in the following example1: Consider:\n\nA bond with principal \\1thatmaydefault,andincaseofdefaultonlypays60%$ of the principal\nA put option that acts as insurance on this bond, that pays nothing if solvent, and 40% if default.\nYour portfolio contains both. Assume for simplicity risk-free rate r=0.\n\nOur objective is to price the put option so that the law of one price applies. The naive approach:\np=0.1 is the historical default rate from data.\nThe expected payoff of bond calculated from p=0.1 is 0.96\nThe expected value of put calculated from p=0.1 is 0.12\nTotal payoff of portfolio is always \\1$ regardless of solvency\nAnd both sum up to \\1$ which is same as total payoff, thus LoP applies\n! …but this is NOT the observed market price of bond: \\0.88$! The investors are demanding a risk-premium (=acting irrational) because they’re risk-averse\nSo we try a different method:\nFrom the market price \\0.88wecalculatethe∗∗impliedrateofreturn∗∗:0.88=(1-q)\\cdot1+q\\cdot 0.6,andwefindq=0.3$\nCalculate the put option from this q to get price \\0.12$\nAnd both sum up to $1 which is same as total payoff thus LoP applies\n…ANDitisusingthemarketpriceofthebond,0.88.\n\ndef. Risk-Neutral Assumption. A method of pricing derivatives despite the fact that investors are acting irrationally, that makes arbitrage impossible (=makes law of one price hold.)\nFootnotes §\n\n\nQuantitative Risk Management ↩\n\n\n"},"No-Arbitrage":{"title":"No-Arbitrage","links":[],"tags":["Economics/Finance"],"content":"def. Arbitrage Portfolio. Portfolio with value Π(t) at time t is an arbitrage portfolio if:\nΠ(t)=0⟹ price doesn’t go down P(Π(T)≥0)=1​​ and  price guarateed to rise P(Π(T)&gt;0)&gt;0​​\nAlternatively we can define it as:\n if started negative... Π(t)&lt;0​​⟹ ...will be non-negative P(Π(T)≥0)=1​​\ndef. Law of One Price (LOP). Two portfolios with the same future value must have the same value to begin with. if time t&lt;T then:\nΠA​(T)=ΠB​(T)⟹ΠA​(t)=ΠB​(t)\nthm. (Arbitrage is equivalent to LOP) If there is no arbitrage portfolio, then the law of one price holds.\nProof. Contrapositive: If law of one price doesn’t hold, there is an arbitrage portfoilo. Let portfolios A,B such that\nΠA​(T)=ΠB​(T) and ΠA​(t)&lt;ΠB​(t)\nThen construct the following new portfolio that:\n\nLong position on A\nShort position on B\nThen\n\n\nAt time t, value is  Asset ΠA​(t)​​− Cash Debt ΠA​(t)​​− Asset Debt ΠB​(t)​​+ Cash ΠB​(t)​​= Assets ΠA​(t)−ΠB​(t)​​=0\nAt time T, value is  Asset ΠA​(T)​​− Cash Debt ΠA​(t)er(T−t)​​− Asset Debt ΠB​(T)​​+ Cash ΠB​(t)er(T−t)​​= =0 ΠA​(T)−ΠB​(T)​​+ &gt;0 (ΠB​(t)−ΠA​(t))er(T−t)​​&gt;0\n\nThe cash investments are risk free, thus this is determined.\n\n\nTherefore this portfolio is an arbitrage portfolio.■\n\nOne can similarly prove the other way. Thus LOB is equivalent to No Arbitrage condition."},"No-Regret-Dynamics":{"title":"No-Regret Dynamics","links":[],"tags":["Economics/Game-Theory"],"content":"Motivation. Suppose you have to choose which route to take every day driving to work. You devise a complicated strategy, but one day your neighbor and coworker, who takes the same route to work every day, says “I don’t have a strategy, I just take this one route every day.” Wouldn’t it be regretful if it turns out, in total, your route took more time than your co-worker?\ndef. Online Decision Making Game.\n\nPlayer has N actions to choose from, X={1,…,N}\nAt time t…\n\nPlayer constructs a distribution pt over the set of actions X\nAdversary chooses a loss for each action taken, li​∈{0,1}, for every action i∈X where 0 represents no loss and 1 represents a loss. (In our example, think of it as traffic conditions causing a delay.)\nPlayer’s distribution pt is realized into action kt∈X. The player incurs a loss of lktt​.\n\n\nThe player’s goal is to minimize total loss, which we will define shortly.\nWe play for time t=0,…,T. Number of iterations T is predetermined.\n\nWe characterize the loss if we always chose action i (like how our neighbor always takes the same route) for time t=1,…,T as:\nLiT​:=t=1∑T​lit​\nThus we can define also:\n\nLminT​ is the minimum total loss if we could only choose one action every time\nLALGT​:=∑t=1T​lALGt​ is the total loss of player playing strategy of ALG, lALGt=1​,lALGt=2​,….\n\ndef. External Regret. For a player playing strategy ALG, the regret for this strategy is:\nRALG​:=LALGT​−LminT​\ni.e. the difference between total loss of algorithm and the best total loss of one-action-every-time strategy.\nAlgorithms for Online Games §\nalg. Greedy algorithm. This algorithm chooses the action whose one-action-fits-all-time loss is smallest\n\nInitially: x1=1\nAt time t, choose xi​ such that we take the minimum possible LiT​. In other words:\n\nxit​ such that =argmini​LiT​\n\nBreaks ties determinimistically, with the action with lowest index.\n\nMotivation. This algorithm is really bad. Instead, we can try to confuse our adversary by mixing our strategy, i.e. a randomized algorithm.\nalg. Randomized Weighted Majority.\n\nInitially, play i with probability pi1​=N1​ for all i∈X\nAt time t…\n\nwit​={wit−1​(1−η)wit−1​​if incurred loss, i.e. lit−1​=1if incurred loss, i.e. lit−1​=1​\n\nwhere η is the discount factor\n\n\nCalculate this new weight for every strategy i, and then play i with probability\n\npit​=Wtwit​​\n\nwhere Wt:=∑i∈X​wit​\n\nThis is a much better algorithm; in fact we can show how small its regret is.\nthm. (Regret Bound of Randomized Weighted Majority) For η≤21​, the loss of RWM algorithm satisfies:\nLRWMT​≤(1+η)LminT​+ηlnN​\nProof. Let Ft denote the fraction of weights that are discounted because they incurred a loss. We show this is equal to the expected loss at timestep t:\nFt​:=Wt∑i;li​=1​wit​​=i;li​=1∑​Wtwit​​=i∈X∑​Wtwit​​lit​=i∈X∑​pit​lit​=E(lt)​​\nNow, we can express Wt+1 using Ft way, by splitting the summation into those that incurred a loss and those that didn’t.\nWt+1​:=i∈X∑t​wit​=(1−η)∑i;li​=1​wit​i;li​=1∑​wit​(1−η)​​Wt−∑i;ili​=1​i;li​=0∑​wit​​​=(1−η)FtWt+(Wt−FtWt)=Wt−ηFtWt=Wt(1−ηFt)​​\nWe can now construct the inequality:\nmaxi​wit+1​(1−η)LminT​LminT​ln(1−η)​≤Wt+1≤Wt+1=W1(1−nF1)×⋯×(1−nFT)=Nt=1∏T​(1−ηFt)=lnN+t=1∑T​ln(1−ηFt)≤lnN+t=1∑T​ηFt=lnN+ηt=1∑T​Ft=lnN+ηLRWMT​​sum is greater than maxmax weight is one with min lossW1=NTaking log on both sides∀x∈R,ln(1−x)≤−x​​\nAnd with some algebra and inequality: ∀z∈R,−ln(1−z)≤z+z2\nLrwmT​≤ηlnN​−ηLminT​ln(1−η)​≤ηlnN​−(1+η)LminT​\n■"},"Nomura-Traning-&-LLL":{"title":"Nomura Traning & LLL","links":["Security-(Finance)","Bonds-(Finance)","Equity","Credit-Rating","Investment-Bank","Lifecycle-of-a-Trade","Interest-Rate-Arbitrage","Probability","Expected-Value","Stochastic-Process","Quadratic-Variation","Stochastic-Calculus","Markov-Chain","Basel","Portfolio-Theory-(Risk)","Data-Architecture","Cloud-Architecture"],"tags":["Courses"],"content":"Security (Finance)\n\n\nFixed Income: Bonds\n\n\nEquity\nCredit Rating\nRole of Investment Banks\nLifecycle of a Trade\n\n\nFixed Income\n\nBonds: Govn’t (National/Municipal), Corporate\nStructured Products\nEmerging Market Debt\nRates Desk\n\nGovn’t Bonds\nBond futures\nInterest Rate Derivatives\n\n\n\n\n\nCredits Desk\n\n\nInterest Rate Arbitrage (Including Currency Carry Trade)\n\n\nPrime Client: institutional clients (hedge funds)\n\n\nCertificate of deposit\n\n\n Prime client\n\n\n Total return swap\n\n\n credit default swap insurance\n\n\n Money market funds\n\n\n quantitative easing\n\n\nMath §\n\nProbability\n\nConditional Expectation for Stochastic Calculus\n\n\nStochastic Process\n\nQuadratic Variation\nStochastic Calculus\n\n\nProbabilistic Models\n\nMarkov Chain\n\n\n\nRisk §\n\nBasel\nPortfolio Theory (Risk)\n\nEngineering §\n\nData Architecture\nCloud Architecture\n"},"Nonlinear-Models":{"title":"Nonlinear Models","links":[],"tags":["Math/Statistics"],"content":"Polynomial Models §\nYi​=β0​+β1​X1i​+β2​X1i2​+⋯+ϵi​\n\nThis is still bivariate! it’s just Y vs X1​\n\nLogarithmic Models §\n\nLinear-Log: Yi​=β0​+β1​lnXi​+ϵi​\nLog-Linear: lnYi​=β0​+β1​Xi​+ϵi​\nLog-Log: lnYi​=β0​+β1​lnXi​+ϵi​\nBe careful since:\n\n\nUnits of coefficients are different\nVariables must all be positive\nThere isn’t a statistical test to choose which model to use\n\nExample: Wage vs Height §\nRegression Table: \n\nNo Log: “Wage increases by \\0.412per\\text{inch}$ increase in height”\nLinear-Log: “Wage increases by \\frac{29.316}{100}=\\0.293per1%$ increase in height”\nLog-Linear: “Wage increases by 0.033×100=3.3% per inch increase in height”\nLog-Log: “Wage increases by 2.362% per 1% increase in height”\n"},"Norm-(Math)":{"title":"Norm (Math)","links":["방무창","Homogenous-Function"],"tags":["Math"],"content":"Norms are a measure of a number or vector’s distance from the origin.\nFor Scalars §\ndef. Absolute Value or L1 Norm. For real or complex number x its norm is simply the distance from the origin.\nFor Vectors in n-dim Space §\ndef. Euclidean Distance or L2 Norm. For vectors, the euclidean norm of a vector x in Rn is:\n∣∣x∣∣2​=(∀i∑​xi2​)1/2\nFor Matricies §\nMotivation. As 방무창 explained, matricies also live in vector spaces Rm×n. Thus it would also have a notion of “distance from origin.” The matrix norm must satisfy the following conditions for it to be reasonable:\n\n∣∣A∣∣≥0\n∣∣A∣∣=0⟺A is all zeros\n∣∣α⋅A∣∣=∣α∣⋅∣∣A∣∣ (absolutely homogenous)\n! ∣∣A∣∣+∣∣B∣∣≥∣∣A+B∣∣ (triangle ineuqality)\n\ndef. Frobeinus Norm. For a matrix A of shape n×m the Frobeinus norm is:\n∣∣A∣∣F​:=​∀i,j∑​Aij2​​1/2\nVisualization. matrices - What is the difference between the Frobenius norm and the 2-norm of a matrix? - Mathematics Stack Exchange"},"Normal-Distribution":{"title":"Normal Distribution","links":["Normal-Distribution","Approximating-Distributions","Standardizing-a-Random-Variable","Chebyshev's-Inequality","CS-675-HW1","Change-of-Variable-(Probability)","Besset-Correction"],"tags":["Math/Common-Distributions"],"content":"def. Normal Distribution. A random variable X distributed over a Normal distribution with mean μ and standard deviation σ is denoted:\nXfX​(x)P(a&lt;X&lt;b)​∼Normal(μ,σ)=σ2π​1​⋅e−21​(σx−μ​)2=∫ab​σ2π​1​⋅e−21​(σx−μ​)2dx​\ndef. Cumulative Distribution Function (CDF) of a Normal Distribution\nΦ(z):=∫−∞z​2π​e−t2/2​dt\nObserve:\n\nz→+∞,Φ(z)→1\nz→−∞,Φ(z)→0\nΦ(−z)=1−Φ(z)\n\ndef. Standard Normal Distribution. A standard normal distribution is a normal distribution where μ=0,σ=1\nX∼Normalstd.​(0,1)\n\n\n                  \n                  Tip \n                  \n                \nYou can approximate a bunch of distributions using the Normal.\n\nrmk. Linear Transformation of Normal Distribution. If X∼N[μ,σ2], then\n\nE(aX+b)=aE(X)+b=aμ+b\nVar(aX+b)=a2Var(X)=a2σ2\n\n\n⇒ Thus (aX+b)∼Normal(aμ+b,a2σ2)\n\n\n\nremark. Exponentiating Transformation of Normal Distribution. If X∼N(μ,σ2)\nE[eX]=eμ+2σ2​\n(using Law of Unconscious Statistician)\nrmk. Standardizing the Normal Distribution. Given X∼N(μ,σ2):\n\nY=σX−μ​ has the standard normal distribution\nThe pdf is as follows:\n\nP(a&lt;X&lt;b)=∫ab​σ2π​1​⋅e−21​(σx−μ​)2dx   ⇒   ∫σa−μ​σb−μ​​σ2π​e−x2/2​dz\n\nSee also Standardizing a Random Variable\n\nrmk. Empirical Rule: Rule of thumb for calculating probabilities (integrals) of normal distributions\n\n! Generalized version: Chebyshev’s Inequality\n1 std. dev. away is ≈66%; 2 std.dev. away is ≈0.95%\n\nBox-Mueller Transform §\nMotivation. Computers can easily sample from a uniform distribution, but it cannot randomly generage a normal distribution. The Box-Mueller Transform is a method of transforming a uniform unit random variable into a standard normal random variable. (From Problem 3)\nthm. Box-Mueller Transform. Let uniform distrubtions U1​∼Unif[0,1],U2​∼Unif[0,1]. Then let:\nRΘ​=−2lnU2​​=2πU1​​​\nAnd then let:\nZ1​Z2​​=RcosΘ=RsinΘ​​\nThen Z1​,Z2​ are standard normal distributions.\nproof. First, U1​,U2​ to R,Θ. Consider that\n\nU1​=2πΘ​, dΘdU1​​=2π1​\nU2​=exp(−2R2​), dRdU2​​=Rexp(−2R2​)\nUsing Change of Variable (Probability)s we have:\nfΘ​(θ)=2π1​\nfR​(r)=rexp(−2r2​)\nThen, from R,Θ to Z1​,Z2​:\nNote that:\n\n\nZ12​+Z22​=R2(cos2Θ+sin2Θ)=R2 thus R=Z12​+Z22​​\n\ndZ1​dR​=Z1​(Z12​+Z22​)−1/2=RZ1​​\nSymmetrically dZ2​dR​=RZ2​​\n\n\nZ1​Z2​​=RcosΘRsinΘ​=tanΘ thus Θ=arctan(Z1​Z2​​)\n\ndZ1​dΘ​=1+Z12​Z22​​1​⋅Z2​(−1)Z1−2​=−Z12​+Z22​Z2​​=−R2Z2​​\nSymmetrically dZ2​dΘ​=Z12​+Z22​Z1​​=R2Z1​​\nNow, calculate the jacobian for a multivariate change of variables:\n\n\n\n∣J∣​=​∂z1​∂r​∂z1​∂θ​​∂z2​∂r​∂z2​∂θ​​​=​RZ1​​−R2Z2​​​RZ2​​R2Z1​​​​=R3Z12​​+R3Z22​​=R1​​​\nAnd thus the join probability being:\nfΘ,R​(θ,r)​=fΘ​(θ)⋅fR​(r)⋅∣J∣=2π1​rexp(−2r2​)⋅r1​=2π1​exp(−2z12​+z22​​)=2π​1​exp(−2z12​​)⋅2π​1​exp(−2z22​​)=fZ1​​(z1​)⋅fZ2​​(z2​)​​\nThis show both that:\n\nfZ1​​,fZ2​​ is the standard normal pdf\nZ1​,Z2​ are independent because the joint pdf is a simple product of each pdf. ∎\n\nEstimators §\nlet\n\nX∼N(μ,σ2)\nX1​,…,Xn​∼iidN(μ,σ2)\n⇒ Log likelihood:\n\nlnLn​(μ,σ2∣x1​,...,xn​)=−2n​ln(2π)−2n​ln(σ2)−2σ21​i=1∑n​(xi​−μ)2\nScore §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne R.V.Multiple Datas(μ)=σ2x−μ​sn​(μ)=σ2nμ−∑i=1n​xi​​s(σ)=σ3(x−μ)2​−σ1​sn​(σ)=σ3∑i=1n​(xi​−μ)2​−σn​\nMLEs §\n\nμ^​=n1​i=1∑n​xi​\nσ^2=n1​i=1∑n​(xi​−xˉ)2\n\n! divisor for variance MLE is not n−11​ as opposed to Besset Correction\n\nFisher Information §\n\nUnknown μ, known σ2\n\nI(μ)=σ21​In​(μ)=σ2n​\n\nKnown μ unknown σ2\n\nI(σ2)=2σ21​In​(σ2)=2σ2n​\nMultivariate Normal Distribution §\ndef. Multivariate Normal Distribution.1\n scale down (2π)2d​∣Σ∣21​1​​​e−21​(X−μ)TΣ−1(X−μ)\nwhere\n\nd is the dimension\nΣ is the covariance matrix\n\nThe image shows part of a larger equation or expression, as indicated by the curly brace on the right side, but the complete right-hand side is not visible in this image.\nFootnotes §\n\n\nMultivariate Normal (Gaussian) Distribution Explained - YouTube ↩\n\n\n"},"Normalization":{"title":"Normalization","links":[],"tags":["Computing/Maching-Learning"],"content":"A trick in deep learning to speed up training computation.\nLinear Normialization §\nFor each\nf(x)i​=γ⋅σxi​−E[x]​+β\nwhere γ,β are learnable parameters\nBatch Normalization §"},"Numpy-Arrays":{"title":"Numpy Arrays","links":[],"tags":["Computing/Linguistics/Python"],"content":"\nImplemented using arraylist, so append is O(n)\n\nThus, use python lists until you know for sure when the array won’t grow anymore, and convert to numpy array.\n\n\n{python}arr = np.zeros() or {python}arr = np.empty() to initialize.\n\n! np.arrange(3) initializes an int, while np.arrange(3.) initilizes a float.\n\n\n{python}arr = np.zeros_like(ref_arr) will initiallize arr with the same dimensions and datatype as ref_arr: \nSequence based initialization: \n\nLinspace vs arrange differences. Conclusion: linspace is better: \n\n\nIndexing arrays (is a pointer, except the “fancy indexing”)\n\n"},"Numpy-Axis":{"title":"Numpy Axis","links":["Linear-Algebra"],"tags":["Computing/Linguistics/Python"],"content":"In numpy, dimensions of a tensor is called the axis.\n\nTensors are equivalent to lists of lists.\n\nThe higher the axis, the more “inner” list we are talking about.\nEach axis’s value determines the number or elements that array has\n\n\nA tensor of shape (=dimension) (a,b,c) has a rows (axis 0), b columns (axis 1), and c of axis 2.\n\nFor example, a tensor with shape (10,2,20)\n\n\n\n\nTensor Multiplication §\nWhen matrix A with shape (a1,a2) is multiplied by tensor B of shape (b1,b2):\n\na2 must be equal to b1\nresulting matrix will have shape (a1,b2). (in Linear Algebra terms, it is a composite linear transformation of both A and B).\nWhen you have multiple matrix multiplications you can combine that into one.\n"},"Numpy-Datatypes":{"title":"Numpy Datatypes","links":[],"tags":["Computing/Linguistics/Python"],"content":"Numpy has native datatypes which it is fastest at. It also can work with other python datatypes but it is slower. Datatypes are what dtype field in the np.array class is."},"Numpy-Self-Study":{"title":"Numpy Self-Study","links":["Numpy-Datatypes","Numpy-Arrays"],"tags":["Computing/Linguistics/Python"],"content":"\nNumpy Datatypes\nNumpy Arrays\n"},"Object-Detection":{"title":"Object Detection","links":[],"tags":["Computing/Maching-Learning"],"content":"Object detection, i.e., drawing bounding boxes on “interesting” parts of an image is a harder problem than simple classification of images, because you need to identify which parts are interesting."},"Object-Oriented-Programming":{"title":"Object Oriented Programming","links":[],"tags":["Computing/Linguistics"],"content":"\nDynamic dispatch: determine at runtime which implementation of a polymorphic function to call.\n\nvtable: implementation of dynamic dispatch where the compiler inserts a pointer to a object type determining code, and then pointing again to the correct object implementation\n\n\n"},"Oligopoly":{"title":"Oligopoly","links":["Bertrand-Price-Competition","Cornout-Quantity-Competition"],"tags":["Economics/Macro-Economics","Economics/Game-Theory"],"content":"Definitions §\n\nBest Response Curve: A function that determines one’s best response value of the strateigic variable, given the opponent’s strategic variable value.\n\nThis is equivalent to the pure strategy function (see definition)\n\n\nDr denotes residual demand for one firm\n\nAssumptions §\n\nAssume that products are identical [=no differentiation]\nAssume there are two firms in the market\nAssume that MC is constant\n\nTypes §\n\nBertrand Price Competition\n\nProduct Differentiation\n\n\nCornout Quantity Competition\n\nSequential (=Stackleberg)\n\n\n\n\nThere are thus 2 ways to fill in the gap between 2-firm oligopoly and perfect compeition:\n\nN-firm Oligopoly as an extension of Cornout’s 2-firm oligopoy analysis\nMonopolistic Competition, where firms strategically set differentiation and price\n"},"Omitted-Variables":{"title":"Omitted Variables","links":[],"tags":["Math/Statistics"],"content":"Motivation. What if there are external variables that are related to Y? We want to characterize how off we might be if we omitted a variable.\nOmitting a Variable from a True 2-var Model §\nAssume the true model should have the following:\nYi​=β0​+β1​Xi​+β2​ Omitted Zi​​​+ϵi​\nWe didn’t take into account Z, and we mistakenly used the following model:\nYi′​=β0omit​+β1omit​X1,i​+ϵi​\nThen, our estimators β0​^​omit and β1​^​omit will be different from β0​^​,β1​^​. We can state precisely how off they will be (=Omitted Variable Bias):\nβ1​^​omit=β1​^​+ Bias β2​^​δ1​^​​​\nwhere δ1​^​ is from the auxiliary regression\nZi​=δ0​^​+δ1​^​Xi​+τi​\nOmitting a Variable from a 3-var Model §\n\nTrue model: Yi​^​=β0​^​+β1​^​X1,i​+β2​^​X2,i​+β3​^​X3,i​+ϵi​^​\nOmitted model: Yi′​^​=β0′​^​X1,i​+β2′​^​X2,i​+ϵi′​^​\nEstimator relationships:\n\nβ1′​^​=β1​^​+β3​^​δ1​^​\nβ2′​^​=β2​^​+β3​^​δ2​^​\n\n\nAuxiliary regression: X3​=δ0​^​+δ1​^​X1​+δ2​^​X2​\n\nGeneral Form §\nFor true model:\nYi​^​=β0​^​+β1​^​X1,i​+⋯+βn−1​^​Xn−1,i​+βn​^​Xn​\nwhere Xn​ is the omitted variable. Our omitted model:\nYi​^​\nβi′​^​=βi​^​+β^​omit​δi​^​\nwhere\nXomit​=δ0​^​+δ1​^​X2​+⋯+δ^n−1​Xn−1​"},"Online-Matching":{"title":"Online Matching","links":["Matching-Problems"],"tags":["Computing/Algorithms","Economics/Game-Theory"],"content":"Motivation. Suppose you are Google. Many advertisers come to you and want to advertise on google searches by users. Now, users come it throughout the day, one by one, searching for whatever they want to search. You can show them one advertisement. How do you maximize the advertisement to show to the users?\n\ndef. Optimal Online Matching Algorithm. The optimal algorithm is just one that knows in advance what all users will search, and thus simply finds a maximum partition matching.\nWhole Online Matching §\ndef. Greedy Online Matching algorithm. When a new user from R comes in and declares its edges, it simply matches to any valid advertiser L.\n\nThis simply finds the maximal (not maximum) matching.\nThis is in the worst case two times worse than optimal online matching. Proof:\n\nWhen a new edge v comes available, ALG matching it to the “wrong” vertex u′ will at most block two “correct matchings” (from optimal).\nTherefore OPTALG​≤2 ■\n\n\n\n\nFractional Online Matching §\nWe may sometimes allow for fractional matchings, i.e. consider an edge “full” at weight 1, but the weight can be anywhere from [0,1]. We still benchmark fractional online matching with the original optimal whole matching algorithm.\ndef. Greedy Fractional Online Matching Algorithm. let r∈R encountering nr​ edges. For every unfilled edge, equally fill nr​1​ to each.\nThis algorithm is pretty bad. Consider the following worst case scenario:\n\n\nEach set A,B,C,D have n verticies\nAppears in order: c1​,…,cn​,d1​,…,dn​\nOPT=2n which matches all of A to C, and all of B to D.\nFor ALG:\n\nconsider the time when ci​ is matching:\n\nequally gives n+11​ to all of b1​…bn​s and ai​.\nThis happens for ever ci​, i.e. n times\nAfter the last vertex in C, total n finishes matching, all of b1​…bn​ have filled n+1n​.\n\n\nNow, each of di​ only has n1​ of bi​ to match with.\nTotal fully matches ci​∈C, and n1​ of di​∈D. Thus ALG=n+(n⋅n1​)=n+1\n\n\nThus OPTALG​=n+12n​&lt;2. Pretty bad.\ndef. Water-filling Algorithm. The algorithm works like this: for every r∈R that appears, it outputs water at a constant rate. Fill the vertices with least water first at equal rate:\n\nIn the example above of the worst-case greedy algorithm, water-filing does a better job.\nFirst consider ci​ filling ai​ and all of b1​…bn​.\nb1​…bn​ is already filled up to level x (we know they are all filled up to equal amounts, because B and C are fully connected.)\nai​ is empty because it is only connected to ci​\nTotal water output by ci​ is 1.\nTotal n+1 verticies.\nWe know that water level will rise above x for everybody (until some yi​) because ai​ can’t hold that much\n\nThen, denoting the rate of water output by ci​ as dtdx​:\n\n filled by ai​ x​​+ filled by all vertex dtdx​(n+1)​​dtdx​​= total water 1​​=n+11−x​​​\nThen, let y be the height of water on b1​…bn​ after c1​…cn​ are done filling. Summing this up for all c1​…cn​ from x=0…y and t=0…n:\n∫x=0y​1−x1​dx[−ln(1−x)]0y​ln(1−y)y​=∫t=0n​n+11​dx=n+1n+1​=1=−1=1−e1​​​\nAnd then the remaining e1​ for each bi​ will be matched to di​. Total matched is:\nALGOPTALG​​= by C n​​+ by D n(e1​)​​=n(1+e1​)=2nn(1+e1​)​=2ee+1​​​\n(Note that we still compare with whole matching optimum.)\ndef. Ranking Algorithm.\ndef. Bid Scaling Algorithm."},"Optimal-Stopping-Problem":{"title":"Optimal Stopping Problem","links":[],"tags":["Economics/Game-Theory"],"content":"Motivation. Imagine a gambling situation, where there is a sequence of prizes inside boxes. The gambler knows the distribution of these boxes, but is only shown one at a time. They can claim only one box, and once a box is opened the prize must be claimed or trashed. How can the gambler act?\n\ndef. (Optimal stopping problem) let prizes of random variables X1​,X2​…,Xn​ be distributed F1​,F2​,…,Fn​. The gambler only knows the distribution of each of these boxes, and the order in which the boxes are shown is shuffled randomly.\nthm. Prophet Inequality. There is a strategy for the gambler to achieve at least 21​ of the optimal revenue, i.e.:\nE(payoff)≥21​E(maxi=1n​Xi​)\nwhere…\n\npayoff is the payoff to the gambler\nlet X∗:=maxi=1n​Xi​, a random variable. This is what the “Prophet” gets, i.e. a optimal strategy.\nAdditionally, the theorem states that this strategy is a optimal cutoff strategy, which is one that stops if the payoff from the current opened box is larger than predetermined cutoff w.\n\nProof. We know that payoff=base payoff+excess payoff where\n\nbase payoff is w\nexcess payoff is Xj​−w, where Xj​ is the box we stop at\nWe also know that these two are random variables:\n\nw={w0​if X∗≥welse​\nexcess payoff={E((Xj​−w)+)0​if stopped at Xj​if never stopped​\nNow, the expected payoff is:\nE(payoff)= expected base P(X∗≥w)⋅w​​+ expected excess j=1∑n​P(stopping at Xj​)⋅E((Xj​−w)+)​​​​\nWe know that the probability of stopping at Xj​ (from the first case of excess payoff) is:\nP(stopping at Xj​)​=P(maxi=1j−1​Xi​&lt;w)≥P(maxi=1n​Xi​&lt;w)=P(X∗&lt;w)​...that boxes before j where &lt;w...that all boxes are &lt;wby definition of X∗​​\nThus:\nE(payoff)≥P(X∗≥w)⋅w+ take out since X∗ is not relevant to j P(X∗&lt;w)⋅j=1∑n​E((Xj​−w)+)​​\n(lemma 1)\nOn the other hand, the expected prophet payoff is\nE(X∗)​=E(w+maxj=1n​(Xj​−w))≤w+E(maxj=1n​(Xj​−w)+)≤w+j=1∑n​E((Xj​−w)+)​by definition of (…)+sum greater than max​​\n(lemma 2)\nNoticing lemma 1 and lemma 2 both have term ∑j=1n​E((Xj​−w)+), we can organize for that:\nP(X∗&lt;w)E(payoff)−w⋅P(X∗≥w)​≥j=1∑n​E((Xj​−w)+)≥E(X∗)−w\nSimplifying we get\nE(payoff)≥21​E(X∗)\n■"},"Optimistic-Nihilism":{"title":"Optimistic Nihilism","links":[],"tags":["Philosophy"],"content":""},"Options-(Finance)":{"title":"Options (Finance)","links":["Derivatives-(Finance)","No-Arbitrage","Binomial-Option-Pricing-Model"],"tags":["Economics/Finance"],"content":"An option is a contract that gives the holder the right to exercise the option (=buy/sell a stock) at the strike price at or before the strike date.\n\nIt is a type of Derivative\nThe person who gives out the option contract is the writer.\nThe person who gets the option contract is the holder.\n\nTypes of Options §\n\nExercise timing\n\nAmerican option: anytime on or before the strike date.\nEuropean option: at the strike date only.\n\n\nExercise type\n\nCall option: you hold the right to buy the stock at the strike price.\nPut option: you hold the right to sell the stock at the strike price.\n\n\n\nDefinitions §\n\nS(t): price of underlying asset, at time t\nC(t): price of a call option, at time t\nP(t): price of a put option, at time t\nK: strike price of a call or put option\n\nIntuition. (Payoff Diagram for Options)\n\nCall Option §\nAssume continuous compounding at rate r between time period [t0​,t1​]. Assume No-Arbitrage.\n\nPayoff: max[S(t1​)−K,0]\n\n0 is when the stock price as decreased, so you don’t decide to sell.\naccording to the law of one price, must equal to =C(t1​)\n\n\nProfit: max[S(t1​)−K,0]−C(t0​)exp[r(t1​−t0​)]\n\nFirst term is the payoff\nSecond term is because you borrowed money at the risk-free rate r to buy the call option.\nWe don’t know what C(t0​) is yet…see Binomial Option Pricing Model\n\n\n\nthm. Put-Call Parity. let call option entered at time t0​ and put at time t0​. Then\n Call C(t0​)​​+ Cash Ke−r(t1​−t0​)​​= Put P(t0​)​​+ e−q(t1​−t0​) units of underlying S(t0​)e−q(t1​−t0​)​​\nIntuition. You can always use a stock plus a put option to simulate an equivalent call option, and v.v.\n\nProof Outline. let ΠA​ long 1 call, have Ke−r(t1​−t0​) units of cash. Let ΠB​ long 1 put, have e−q(t1​−t0​) units of underlying stock. At time t1​ they will have equivalent value, thus at time t0​ their value must be same by LOP.\nthm. Put-Call-Forward Relation.\n\nForward entered at t=0 and ends at t=T, with strike price K\nC(t),P(t) are call and put entered at time t, with strike price K\n\nC(t)−P(t)=FT​(t)\nProof Outline. \nVarious Portfolios that Can Be Constructed from Options §\ndef. Moneyness of Option. A call option’s moneyness at time t is: \n\nIf St​&gt;K: call is in (=on) the money\nIf St​=K: Call is at the money\nIf St​&lt;K: Call is out of the money, i.e. worthless.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrategyWhen you predict…Profit Diagrams (Not Payoff)StraddleUnderlying will move up/down a littleStrangleUnderlying will move up/down a lotBull/Bear SpreadUnderlying will go up (bull) / down (bear)Butterfly SpreadUnderlying will not move much (long) / move a lot (short)"},"Order-Statistic":{"title":"Order Statistic","links":[],"tags":["Math/Statistics"],"content":"Motivation. When you have iid random variables X1​,…,Xn​ with a known pdf, you sometimes want the pdf of the minimum, maximum or k-th smallest/biggest of the realizations.\ndef. k-th order statistic X(k)​is the k-th smallest of iid rv X1​,…,Xn​. Equivalently, X(1)​ is the smallest, and X(n)​ is the largest.\nthm. pdf of order statistic. For iid rv X1​,…,Xn​:\nP(X(k)​∈[x,x+ϵ])​=P(one of the X’s∈[x,x+ϵ] and exactly k−1 of the others &lt;x)=i=1∑n​P(Xi​∈[x,x+ϵ] and exactly k−1 of the others &lt;x)=nP(X1​∈[x,x+ϵ] and exactly k−1 of the others &lt;x)=nP(X1​∈[x,x+ϵ])P(exactly k−1 of the others &lt;x)=nP(X1​∈[x,x+ϵ])((k−1n−1​)P(X&lt;x)k−1P(X&gt;x)n−k)​\nAnd the pdf is:\nf(k)​(x)=nf(x)(k−1n−1​)F(x)k−1(1−F(x))n−k"},"Ordinal-Allocation":{"title":"Ordinal Allocation","links":["Utility-Function","Fairness-(Economics)","tags/task"],"tags":["Economics/Game-Theory","task"],"content":"Motivation. We move on from cardinal allocations where the Utility Function is numerically defined, into a situation where there is only an ordinal ranking by the agents. This is more realistic in real life, since most people are able to identify which item they prefer even if they can’t assign it a numerical utility value.\nIn order for any of this to be remotely fair, we need to allow probabilistic allocation. This means that the allocation solution P is for each item a and agent i, gives a probability Pi,a​ that the item is allocated to the agent:\nPi,a​:=P(a is allocated to i) s.t.∀a∑​Pi​=1\nFor convenience, let’s also define a cumulative distribution function, for each agent.\nPi​(t)=a⪯t∑​P(a)\nWe might visualize two different allocation P,Q for which for agent i the cumulative allocation function Pi​,Qi​ is visualized like this:\n\nNow we can define a few properties of baysian allocations.\ndef Stochastic Dominance (SD). Allocation Pi​ stoastically dominates distribution Qi​ for agent i iff\n∄a such that  probability to get at least a or better a′&gt;a∑​Pi​(a′)​​&lt;a′&gt;a∑​Qi​(a′)\nExample. Agents 1..4 want items a,b with the preferences shown below.\n\n\nFirst allocation P is stochastically dominated by second allocation Q for agents 1,2 because:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability under..aa∨ba∨b∨∅ (=get sth)P125​126​1Q21​21​1\nThere is no item where getting P gets clearly better than Q. Therefore Q stochastically dominates P.\ndef. (Preferences in Ordinal Allocation) This is how we define preferences in ordinal situations. P stochastically dominates Q for i is equivalent to saying P≻i​Q.\n\n! But note that we cannot say anything about indifference. Stochastic dominance is defined over a partial ordering (we won’t go into more detail) which means there cannot be two different sets i is indifferent about.\n\nWe define Fairness (Economics) in the following ways:\ndef Ordinal Efficiency (OE). Allocation P is pareto-optimal when there does not exist any other allocation Q which is a pareto improvement than P:\n\nNobody are worse off: ∀i,Qi​⪰Pi​\nSome are improved: ∃i∗Qi∗​≻Pi∗​\n\ndef. Ex-Post Pareto Optimality. Allocation P is Ex-post PO when every possible matching in the distribution P is Ordinally Efficient.\nThere are two definitions of envy-freeness in probabilistic allocations:\n\ndef. Weak Envy-Free (WEF). Allocation P is WEF when there exists no Pj​ that stochastically dominates for Pi​.\ndef. Strong Envy-Free (SEF). Allocation P is SEF when it stochastically dominates all other agent’s allocation.\n\nRandom Dictatorship §\n\n Proofs of various ordinal allocation schemes#task\n\nthm. RD is DSIC. Trivially. You are best off when you choose your item on the first try. Induction.\nthm. RD is Ex-post PO. Trivially. If there are at least as many agents as items, then every agent picks their favorite among that’s left, and nobody can really improve by swapping. Induction.\nthm. RD is WEF. (will not prove)\nthm. (Not strong envy-free.) Counterexample.\n\n\nagent 1’s allocation’s cumulative allocation is (21​,64​,1)\nagent 3’s allocation’s cumulative allocation w.r.t. 1’s preferences is (0,65​,1)\n\nthm. (RD is not ordinally efficient.) Counterexample. (Same example from above)\n\nProportional Eating §\nthm. PE is not truthful. Counterexample. \nthm PE is SEF if all agents have different preference orderings (WEF if exists two agents with same ordering). Proof sketch by induction:\n\nLet S1​ be the timepoint a1​ is fully eaten, S2​ when a2​ is fully eaten, and so on.\nLet agent i have preference a1​≻a2​≻⋯≻am​ of m items of set T.\nThen, agent i eats a1​ first. Everybody eats at the same rate, so there is no agent who can eat even more than agent i. Agent i eats it from time 0 to S1​:\n\n∀j,S1​=Pi,a1​​≥Pj,a1​​\nThen, i moves on to eat a2​ from time S1​ to S2​. No other agent can eat more of a1​ AND a2​ than agent i (unless they both eat identically):\n∀j,Pi,a1​​+Pi,a2​​&gt;Pj,a1​​+Pj,a2​​\nInduce into all m items:\n∀j,∀a∑​Pi,a​&gt;∀a∑​Pj,a​\n(equality when i,j have identical preferences). Considering all of these cases together:\n∀a,∀ja′&gt;a∑​Pi,a​&gt;a′&gt;a∑​Pj,a​\nTherefore i does not envy anybody else. ■\nthm PE is Ex-Post PO. We will go through a lemma first.\ndef. In allocation P, call a2​→τa1​ (”a2​ tau-s a1​”) iff there exists an agent i which prefers a2​ over a2​, but has also eaten some of a1​ (and obviously ate a2​ beforehand). Allocation-wise, a2​≻i​a2​ and Pi,a1​​&gt;0.\nLemma. If an allocation P is not Ex-Post PO, then there exists a cycle, i.e. a1​→τa2​→τ…→τa1​\nThen Proof by contradiction. Assume PE produces and allocation P that is not EX-post PO; by the lemma this allocation must have a tau-cycle:\nFor the tau’ing aγ−1​→τaγ​ in this cycle, Find the agent iγ​ that tau’ed these items. Let the timepoint when i starts eating aγ​ be Sγ​. By this time aγ−1​ must have been fully eaten; thus Sγ−1​&lt;Sγ​.\nWe can do this for every tau’ed agent in the cycle. But then S1​&lt;Sγ​, but by cycle S1​&gt;Sγ​. This is a contradiction. Thus allocation P which is produced by PE must be Ex-Post PO. ■\nProof Sketch of Lemma. Assume allocation P which is not ex-post PO, i.e. there exists another allocation Q where some agent i is better off in Q, i.e. Qi​≻Pi​ (=Qi​ stochastically dominates Pi​.) Focus on this agent i.\nThen, there must exist for this agent a pair of items (call them a2​,a1​) where:\n\na2​≻i​a1​\nQi,a2​​&gt;Pi,a2​​ because for i Q is better because it eats more of the better one.\nQi,a1​​&lt;Pi,a1​​ because probability for each row must both sum to 1.\n\nThus Pi,a2​​&gt;0 and in alloc. P, a2​→τa1​. Then, find another agent j that Qj,a2​​&lt;Pj,a2​​. This j exists because probability of each column sums to 1 (for each allocation).\nNow, for this agent j, find item a3​ that satisfies:\na3​≻j​a2​\nPj,a3​​&lt;Qj,a3​​\nThis item a3​ must exist because row must sum to one. Then Pj,a2​​&gt;0 and in alloc. P, a2​→τa1​\nWe can continue on but we must form a cycle of tau-ing because there are finite number of items. ■\n"},"Ordinary-Least-Squares-Regression":{"title":"Ordinary Least Squares Regression","links":["Mean-Squared-Error","Confidence-Intervals","Hypothesis-Testing","Central-Limit-Theorem","Student's-t-test","Student's-T-Distribution"],"tags":["Math/Statistics"],"content":"Motivation. Let’s say that there is a relationship between GDP per capita and life expectancy. Maybe god has declared a perfect formula describing this relationship:\nGDP Per Capita=200⋅Life Expectancy+1000+Noise\nWhile we humans may never truly know the parameters of the formula, 200 and 1000, we can still make a good guess about it. Therefore, assuming this is a linear relationship, we have the Bivariate Ordinary Least Squares Model.\nYi​=β0​+β1​Xi​+ϵi​\nwhere (X1​,Y1​),…,(Xn​,Yn​) are observations (=regressors).\nthm. Parameter OLS Estimator for N observations (Xi​,Yi​):\nβ1​^​β0​^​​:=∑i=1n​Xi​−Xˉ∑i=1n​(Xi​−Xˉ)(Yi​−Yˉ)​:=Yˉ−β1​^​Xˉ​​\nProperties.\n\nPredictor: Yi​^​=β0​^​+β1​^​Xi​\nResidual: ϵ^:=Yi​−Y^ is the estimator for the error term, i.e. how good the predictor is.\nRegression Variance: σ^2=N−k∑i=1N​ϵi​^​2​=N−k∑i=1N​(Yi​−Yˉ)2​ where k is the number of parameters (k=2 in this case)\n\nbasically the Mean Squared Error. The lower the better.\nThis is also the standard error of the residuals.\nIn Stata, it’s called the Root MSE.\n\n\n\nEvaluation of Estimators.\n\nMean of β1​^​: E(β1​^​)=β1​+ρX,ϵ​σX​σϵ​​\n\nThus bias is ρX,ϵ​σX​σϵ​​\nIf ρX,ϵ​=0 then exogenous (good!)\nIf ρX,ϵ​&gt;0 then there’s some 3rd factor positively correlated with X, thus bias is positive.\nIf ρX,ϵ​&lt;0 then v.v.\n&amp; Thus the bias characterizes exogeniety\n\n\nVariance of β1​^​: Var(β1​^​)=N⋅Var(xi​)σ^2​\n\nThis is also called precision\nVar(β1​^​)​ is also called standard error of β1​^​.\n! For random variables, Var(X)​ is called standard deviation. For estimator random variables, it is called standard error. An abuse of terminology.\n\n\n\nConfidence Intervals and Hypothesis Testing §\nSee also: Confidence Intervals and Hypothesis Testing\nMotivation. Assume we have our estimators for our sample size N using OLS, β0​,β1​^​^​. Now, assuming we have the true population data (impossible in real life) and take 100 samples of size N from the whole population, we get 100 different tuple of estimators (β0​^​,β1​^​). If we plot these on a graph, we get an approximate bell curve. This is due to the Central Limit Theorem. Knowing this fact, we can deduce if there is a correlation between X and Y.\n\nRemark. N≥30 is the minimum required for CLT. N≥100 is a conservative requirement for CLT to apply.\nRemark. We will only look at β1​^​ since it is the more important parameter.\nHypothesis Testing §\ndef. The Null hypothesis in regression is H0​:β1​=0, i.e. there is no correlation.\ndef. Regression T-test. See Student’s t-test. A T-test is a test for rejecting the null hypothesis. let the T-statistic T=∣Var(β1​^​)​β1​^​−β1Null​​∣. Then\n{H0​H1​​if T&gt;Kotherwise​\n\nThe cutoff value K is determined by how powerful (=α) you want the test to be. This is determined by the Student’s T-Distribution.\n\nThis is because T=d​tN−1​\n\n\nThis is Student’s t-test but with only one random variable.\nNormally, we set the cutoff K=2, i.e. two standard deviations away. This is around an α=0.05 test.\n\nConfidence Interval §\nIntuition.\nBias:\nstandard error of regression: mean squared of residuals; also the estimator for the error\nStandard error of…\n\nResiduals → standard error of regression\nbe\nVaraince of β1​^​ is also the variance of CLT limit with the same sample size\n\nOmitted Variables §\nMotivation. Let’s say that there is a relationship between GDP per capita and life expectancy. Maybe god has declared a perfect formula describing this relationship:\nGDP Per Capita=200⋅Life Expectancy+1000+Noise\nWhile we humans may never truly know the parameters of the formula, 200 and 1000, we can still make a good guess about it. Therefore, assuming this is a linear relationship, we have the Bivariate Ordinary Least Squares Model.\nYi​=β0​+β1​Xi​+ϵi​\nwhere (X1​,Y1​),…,(Xn​,Yn​) are observations (=regressors). The OLS algorithm will minimize the squared sum of residuals:\nminβ1​^​,β0​^​​i=1∑N​ϵi​^​2\nwhere ϵi​^​:=Yi​−=Y^(β0​^​+β1​^​Xi​)​​. Note the square.\nthm. Parameter OLS Estimator for N observations (Xi​,Yi​)\nβ1​^​β0​^​​:=∑i=1N​(Xi​−Xˉ)2∑i=1N​(Xi​−Xˉ)(Yi​−Yˉ)​:=σX2​σXY​​=ρXY​σX​σY​​:=Yˉ−β1​^​Xˉ​​\nProperties.\n\nPredictor: Yi​^​=β0​^​+β1​^​Xi​\nResidual: ϵ^:=Yi​−Y^ is the estimator for the error term, i.e. how good the predictor is.\n\nN1​∑iN​ϵi​^​=0 i.e. the mean of residuals is zero in OLS algorithm.\n\n\nOLS results in assuming that X is exogenous, i.e. ρX,ϵ​=0.\n\nρ\n\n\nRegression Variance: σ^2=N−k∑i=1N​ϵi​^​2​=N−k∑i=1N​(Yi​−Yˉ)2​ where k is the number of parameters (k=2 in this case)\n\nbasically the Mean Squared Error. The lower the better.\nσ^ is also the standard error of the residuals.\nIn Stata, σ^ is called the Root MSE\n\n\n\nEvaluation of Estimators.\n\nMean of β1​^​: E(β1​^​)=β1​+ρX,ϵ​σX​σϵ​​\n\nThus bias is ρX,ϵ​σX​σϵ​​\nIf ρX,ϵ​=0 then exogenous (this is the definition of exogeniety)\nIf ρX,ϵ​&gt;0 then there’s some 3rd factor positively correlated with X, thus bias is positive.\nIf ρX,ϵ​&lt;0 then v.v.\n&amp; Thus the bias characterizes exogeniety\n\n\nVariance of β1​^​: Var(β1​^​)=N⋅Var(X)σ^2​\n\nThis is also called precision\nVar(β1​^​)​ is also called standard error of β1​^​.\n! For random variables, Var(X)​ is called standard deviation. For estimator random variables, it is called standard error. An abuse of terminology.\n\n\n\nOther Data when Regression is Run §\nThere are a bunch of other variables that may matter, that is not included in the above core set of variables of the regression. Here are a few:\ndef. Coefficient of Determination (R-Squared). Intuition: The proportion of the variation in Y^ that can be determined from X. We define it as:\nR2:=1−SStot​SSres​​\nwhere\n\nSSres​:=∑i​ϵi​^​2=∑i​(Yi​−Yi​^​)2 residual sum of squares\nSStot​:=∑i​(Yi​−Yˉ)2 total sum of squares\n\nExample. A R2 of 0.67 means that around 67% of the variation in Y^ is explained by X. Therefore, the higher it is, the better the regression is (=has more explanatory power).\nRemark. It can also be shown that:\nR2=ρY,Y^2​\nMeasurement Error §\nMotivation. There are measurement errors in every data; it can be both in the independent variable X or the dependent variable Y. We can characterize measurement error (in this case a measurement error in X) as:\nXi​=Xi∗​+vi​\nwhere vi​ is randomly distributed with E(vi​)=0 and std. dev. σv​, which is the measurement error. In this case, the regression in the model Yi​^​=β0​^​+β1​^​Xi​+ϵ^i​ will also change, into:\nβ1​^​=n→∞​β1​σv2​+σX∗2​σX∗2​​\nWe can extract a couple of facts from this relationship:\n\nAttenuation bias: the greater the σv​, the closer β1​^​ is to zero.\nif σv​=0 then β1​^​=β1​, i.e. no measurement error\n\nAlternatively, if there is a measurement error in Yi​, then:\n\nMeasurement error: Yi​=Yi∗​+vi​ where E(vi​)=0 and std. dev. σv​\nError term vi​ is absorbed by error term ϵi​^​\nThis will increase σ^ (variance of regression) and thus Var(β1​^​), but does not bias the estimator.\n\nIssues to Watch out for §\nHeteroskedasticity is when the variance of data is different for some subsets of data than other subsets. ⇒ Use heteroskedatic-consistent standard errors by using robust in stata. This does not affect value of β1​^​ (how!) and does not bias it.\nAutocorrelation often occurs in time series data where the error terms are sticky. For example, attendance at a NY yankees game will be sticky, because people who watched a good year will probably come back next year, even if the yankees aren’t as good as the year before. → used lagged variables"},"Parallel-Algorithms":{"title":"Parallel Algorithms","links":["Memory-Access-Control"],"tags":["Computing/Algorithms"],"content":"Terminology\n\nTp​(n): span; time taken for p-processes with input size n\n\nYou may choose p=∞ i.e. T∞​(n) to analyze if unlimited parallelism was possible.\n\n\nWp​(n): work; total time by all individual process; Wp​(n)≤p⋅Tp​(n)\nspeedup: speedup factor compared to a single-process algorithm\nspawn: start a new process\nsync: wait until all processes are finished\n\nParallel algorithms…\n\nFor Memory Access Control we will concern ourselves with concurrent read &amp; exclusive write model of computation\nare easy to make from recursive algorithms.\n\nalg. PSum. Normal algorithm is O(n). Following is a parallel algorithm:\nfunction psum(l, r)\n\tif l=r\n\t\treturn A[l]\n\tspawn ls = psum(l, m)\n\tspawn rs = psum(m+1, r)\n\tsync\n\treturn ls + rs\n\nSpan: T∞​(n)≤T(2n​)+1=O(logn)\nParallel algorithm recursion tree: \n\ndepth is logn, each takes constant time ⇒ T∞​(n)=O(logn)\n\n\n…if number of processors are limited to p, then: \n\nTp​(n)=O(pn​+logp) where pn​ is the time for subproblems that cannot be parallelized (problem size is pn​) and logp is the parallel algorithm runtime.\nWp​(n)=O(p(pn​+logp))=O(n+plogp)\n\n\n…if number of processros p=lognn​, then:\n\nTp​(n)=O(logn)\nWp​(n)=O(n)\n\n\n"},"Pareto-Efficiency":{"title":"Pareto Efficiency","links":["Utility"],"tags":["Economics"],"content":"Pareto Efficiency is a state where you cannot make one person better off without making another person worse off.\nWe will define pareto efficiency when there are n divisible items and n agents.\ndef. Pareto Efficiency. Allocation x1​​,…,xn​​ is pareto efficiet where there exists no alternative allocation y1​​,…,yn​​ such that both:\n​∃i∗ s.t. ∀i,​yi∗​​⪰i∗​xi∗​​yi​​⪰i​xx​​ one agent benefitsothers still not worse​​"},"Path-Alignment":{"title":"Path Alignment","links":[],"tags":["Computing/Algorithms"],"content":"Homework"},"Percieved-value—Real-value":{"title":"Percieved value—Real value","links":[],"tags":["Philosophy/Marxism"],"content":"Perceived value—Real Value §\n(Use-value vs Exchange-value)\nDiscrepancy decreases in the long run.\nPeople will notice that you brought value to their lives, but it may take time."},"Performance-(Computing)":{"title":"Performance (Computing)","links":[],"tags":["Computing/Computer-Architecture"],"content":"Perfomance measurement: Execution Time and Throuput/Bandwidth. (Decreasing execution time always improves thruput) And intuitively performance is inversly related to execution time: Performance=Execution Time1​.\nMeasuring Performance §\n\nProgram Execution Time (is the sum of…)\n\nCPU Time (e.g. 10 picoseconds)\n\nUser CPU Time\nSystem CPU Time\n\n\nI/O Time\n\n\nClock Cycle (= tick, clocks, cycles) e.g. one tick on this CPU is 250 picoseconds.\nInstruction Count: Number of instructions in a program.\n\n⇒ Clock Per Instruction (CPI) / IPC (Instructions Per Clock)\nThus\nCPU Time (s)=# of cycles×Tick Time (s)# of cycles=# of Instructions×avg. CPI\nAnd:\nCPU Time (s)=Clock Rate (Hz)# of Instructions×CPI​\nThe Three Factors of Performance: Clock Rate / Instruction Count / CPI\n$$\n\\text{Performance}= \\frac{\\text{Seconds}}{\\text{Program}}\\\\=\\frac{\\text{Instructions}}{\\text{Program}}\\times \\frac{\\text{Clock Cycles}}{\\text{Instruction}}\\times \\frac{\\text{Seconds}}{\\text{Cycle}}\\\\\\\\\n=\\text{Instruction Count} \\times \\text{CPI} \\times \\text{Clock Rate}\n\n\n$$\n\nList of Common Misconcpetions Performance §\n\nImproving one aspect doesn’t improve overall performance porportionally\n\nAmdahl’s Law:\nExecution timeafter improvment​=Amount of improvmentExecution timeaffected by improvement​​×Execution timeafter improvment​\n→Some perfomance targets are impossible with partial improvements. See example on p.50."},"Permutation-Test":{"title":"Permutation Test","links":[],"tags":["Math/Statistics"],"content":""},"Perpetuity":{"title":"Perpetuity","links":["Sequence-Summation"],"tags":["Economics/Finance"],"content":"A∞​=(1+r/k)P​+(1+r/k)2P​+⋯=r/kP​\n(Geometric Sequence Summation)"},"Personal-Computing":{"title":"Personal Computing","links":["Tools-and-Structures-Define-Your-Capacity","Living-With-the-Internet","Lifelong-Learning","Alan-Kay","Trust","Decentralization","Multidependence","Everything-is-a-File","Reliable,-Replicable-and-Scalable","(Article)-Evergreen-notes","Fetishization","Parasocial-Relationship"],"tags":["Computing/Human-Interface","Logistics"],"content":"\nThe Personal Computer is an extension of the brain, a Bicycle of the Mind.\nThe personal computer is not a tool. Instead, it is a person you have a relationship with.\n\nThis is because you do not fully control your computer (!=definition of a tool). You influence your computer, and the computer influences you back.\nThe relationship can be positive or negative.\n\n\nThe personal computer is the interface and vehicle with which you explore the internet universe.\n(DevonThink) How To Become A Hacker This is the gospel for behaving within a culture of hackers. Things that particularly stand out is about asking good questions, discipline and competence, and Lifelong Learning.\nAlan Kay considers Trust as the core variable in different computing and information systems in general ((DevonThink) What does Alan Kay think about programming and teaching programming with copilots and LLMs of today? - Quora)\n\nInvestment in a fast, good personal computer is worth it. It accelerates everything you do, for a long period of time, and you’re pleasant and happy during the process.\nPrinciples §\n\nLifelong Learning\n\n(DevonThink) How do you remember all the Linux commands? - Quora\n(DevonThink) Why Windows Causes Stupidity - Why It Matters\n\n\nDecentralization\n\nKey components of your computer systems should follow decentralized models\ne.g. ActivityPub &gt; Twitter\ne.g. IPFS &gt; HTTPS\nThis also comes out of the principle of multidependence.\nOpen-source instead of closed-source\n\ne.g. Linux &gt; macOS\n\n\n\n\nOwn Your data\n\nServices that pay for you to access your own data will eventually disappear. Own your data.\nYour disk space should be ~1/3 empty. Keep it at that to allow for flexibility but not wasted space.\nPlaintext is the best format.\n\n\nEverything is a File\n\nDon’t use Windows objects, proprietary databases for notes or databases that you don’t own. Systems that don’t embrace this approach will eventually likely fail.\n\n\nSoftware Maturity\n\nInstall mature software only (Operating Systems, etc.)\nan extension of having Reliable, Replicable and Scalable things\n\n\nUse Software As They’re Intended\n\nespecially: business software shouldn’t be used for personal things.\ne.g. Obsidian &gt; Notion (business pivot)\n\n\nKnow to Surface Data\n\nPhotos go into where you view photos the most (Apple Photos library). Documents go where documents are surfaced the most (DevonThink).\n⇒ Data should be surface-able, like (Article) Evergreen notes\n\n\nLiving with Generative AI\n\nA “language calculator”\nEverybody gets a 100 interns. (GPT: The Second Renaissance - No Boilerplate)\n\n\nTechnology Fetishization (resist)\n\nTechnology is always a means to an end.\n“Tech youtuber” videos, etc. are entertaining, but when you see tech not as simply a hobby of means and consider it more important than it is, it makes your relationship with tech parasocial.\n\n\n"},"Phenomenology":{"title":"Phenomenology","links":["Celine-Wei","Phenomenology","G.-W.-F.-Hegel","Edmund-Husserl","Martin-Heidegger","Maurice-Merleau-Ponty","Jean-Paul-Satre","External/Michel-Foucault","Internal/Judith-Butler","Jacques-Derrida","Plato","Meditation"],"tags":["Philosophy"],"content":"\n\n                  \n                  Celine Wei: It Feels Right.\n                  \n                \n\nNotes from What is Phenomenology? The Philosophy of Husserl and Heidegger - YouTube\nHistory of Phenomenology\n\n18C Kant proposed, and briefly picked up by Hegel\nBut only until Edmund Husserl did it become mainstream\n\nStudent: Martin Heidegger\n\n\nGermany → France\n\nMaurice Merleau-Ponty, Jean-Paul Satre\n\n\nModern philosophers: Michel Foucault, Judith Butler, Jacques Derrida\n\nDefining Phenomenology\n\n\nGreek word for “Appears”\n\n\nFocus on the first hand experience\n\n\nexperientialist &gt; rationalist\n\n\ne.g. time\n\nrationalist: measurement of numerical time\nexperientialist: subjective personal time\n\ne.g. the “pretty girl minute”\n\n\n\n\n\ne.g. fear\n\nrationalist: physiological change, observable behavior of beings in fear\nexperientialist: how fear colors perceptions, the conscious experience of fear\n\n\n\nReversal of Plato\n\nMost of philosophy until Husserl has been about following Plato\nRepresentational Theory\n\nPlato’s Cave analogy(representational theory)\nHumans have\nIt’s a tragedy; our senses only allow incomplete access to reality\nPeaked in Decartes’ mind-body dualism\n\n\n\n\n\nHusserl’s ”Transcendental Phenomenology”\n\nObjective study of the subjective\nTheory of “Intentionality” = “about-ness”\n\nCoined by Brentano (teacher)\nconsciousness cannot be isolated; it’s always interacting with its subjects\nstudy of how the object of consciousness interacts with the structure of consciousness\n\n\nwhether the object is a fantasy/reality/dream/memory doesn’t matter\nPhenomenological method\n\nBracketing: setting aside judgements, filters, and gathering the raw experience;\nEidetic reduction (≈imaginary variation): reduce to essence\n\nMess with the attributes of the phenomenon\ne.g. Fear ⇒ attributes: lack of choice, freeze\n\n\nEnd goal of phenomenology: getting to the universal, objective understanding of the concept\n\n\n\n\n\nHeidegger disagrees: ”Existential Phenomenology”\n\nHusserl: trying to make a science of the consciousness\nHeidegger’s ontological twist:\n\nthe goal is instead understanding the nature of being\nexperience and consciousness cannot be separated\nentanglement varies between people\ne.g. fear of you, an animal, or an Aztec warrior is different\n\n\n\n\n\nEastern connection (China, India)\n\ne.g. Meridian system by China, Chakra by India\nLooks stupid from rationalist perspective\n⇒ but it’s the mapping of the first-person experience of the body\ne.g. Meditation: observe the experience\n\na form of bracketing (stopping judgement, and just experiencing thoughts and emotions)\nZen buddism\nDaoism may have influenced Husserl directly too\n\n\n\n\n"},"Phil-345-Dupre-vs-Cartwright":{"title":"Phil 345 Dupre vs Cartwright","links":[],"tags":[],"content":"Who would win in a fight between Dupre and Cartwright?\nI take issue with both Dupre and Cartwright’s characterization of a machine or mechanism in the context of economics. Considering economics as a problem-solving field concerned with production and distribution (as defined by Samuelson), it seems unwise to address the mechanism problem (as Dupre defines it through essentialism, reductionism, and determinism) by introducing additional machines beyond the universe itswelf, whether these are Cartwright’s model-machines or Dupre’s pluralistic “family of machines.” A more parsimonious approach would be to consider how the universe-as-machine can generate new “submachines” (e.g., organisms, human societies, artifacts, or cultural constructs).\nI argue for an instrumentalist approach, drawing on Rosenberg’s concept of supervenience and the (albeit limited) success of econometrics. These provide both a theoretical framework and a methodological approach for knowledge production without requiring us to possess Laplacian omniscience. Given that economics aims for useful models rather than fundamental truths (conceding to Dupre’s point about the inseparability of normativity from economic facts, while still viewing normative judgments as epiphenomena, much like economic phenomena themselves), I propose that economics should view this pluralistic abstraction of the “machine” merely as a tool for constructing a better society (with subjectivity in defining “better”)."},"Phil-345-Essay-1":{"title":"Phil 345 Essay 1","links":[],"tags":[],"content":"Essay 1 §\nFriedman argues that a theory should be accepted if it makes well confirmed predictions for its “intended domain.” Do economic models such as Akerlofs or Schelling’s support or undermine Friedman’s “methodology of positive economics?”\nI claim that neither Akerlof’s nor Schelling’s models comply with Friedman’s methodology of positive economics, but they should nevertheless be considered useful for their potential. I invoke Sugden’s conception of credible worlds (CW) to argue that while both models cannot be empirically verified, they represent parallel worlds that could become credible with more refinement and thus should be considered useful.\nThe scientific process is often simplified as a linear chain of hypothesis, modeling, testing, and accepting or rejecting. This view is oversimplified not only because it assumes linear theory generation without providing a method for generating hypotheses, but also because it presumes the entire process is conducted by one person or a small team over a short period. In reality, a falsifiable scientific hypothesis is generated through a tortuous, winding path, encountering dead ends and following beliefs, myths, and hunches, often involving decades or even centuries of random search by scientists worldwide. From this viewpoint, Friedman’s methodology is both:\n\nValid and consistent with existing paradigms of the philosophy of science.\nUtterly useless for contemporary positive economics because much of economics is still in its infancy as a science\n\nFriedman’s argument that “theory is to be judged by its predictive power for the class of phenomena which it is intended to explain” aligns with concepts like falsifiability, instrumentalism, and robustness, as well as methods in the hard sciences like physics. (Friedman, 1953) Within the scientific process, this corresponds to the “verification/falsification” step. However, inventing hypotheses ripe for such verification is often the more difficult step, especially for economists, considering that (i) economics rests on human psychology, and despite J.S. Mill’s hopes, we lack a precise theory for it, and (ii) there are too many variables, as highlighted by Rosenberg. (Friedman, 21; Rosenberg, 300) Friedman glosses over this: ”[…] besides [empirical evidence’s] obvious value in suggesting new hypotheses […],” despite it being the most crucial inductive step, often referred to as creativity or, in Sugden’s Schema 3: Abduction, the inductive leap from real-world evidence to abstraction in a credible, parallel world. (Friedman, 7; Sugen, 20) Induction from the real to the CW is one type of induction; the induction from the credible to the real is the popular one—namely, empiricism—with the former having taken a backseat.1 While Friedman offers a good process for the latter, he fails to address the more crucial former step: the generation of hypotheses, or, in Sugden’s conception, the induction of CWs.\nAs Sugden argues, the importance of Akerlof’s and Schelling’s papers is that they “give structure to a statement that is often made about the real world,” i.e., they induce a credible world from real-world phenomena. (Sugden, 3) A market for used cars is a “how-possibly” explanation for a theory of asymmetric information, as is the checkerboard argument for segregation. Their significance lies not in their findings but in their inductive leap from evidence (e.g., segregation exists) into a CW. While not yet fully fleshed out to take the second inductive step of prediction and being “abstract and unrealistic,” they have “introduced to economics the concept of asymmetric information,” sparking off a whole branch of economics: the economics of information. (Sugden, 2) The value of Akerlof’s paper is in abstracting from real-world phenomena to conceptualize asymmetric information. Refining, extending, and verifying is the work of subsequent research, and Akerlof is not required to provide a full outline; his role was to generate or inspire hypotheses regarding asymmetric information.2\nEconomics is, compared to other subjects, in its infancy; it must be given freedom to explore, conceptualize, formalize, and extend without immediate regard for testability—not because it is unimportant, but because the universe is too complex for one theory or concept to emerge fully formed from a few papers. Only after centuries of barely testable macroeconomic policies have we gained enough tools (statistics, surveys, computing power) to consider verifying classical economics. There is hope that game theory, a relatively recent branch of economics, can generate testable hypotheses in fields such as auctions or signaling games—a significant achievement and a stepping stone. Possibly, in the future, we may have enough concepts and tools to generate an economic equivalent of general relativity, with great predictive power and generality, as Friedman envisioned. Right now, however, Schelling’s and Akerlof’s models solidly contribute to the process of scientific discovery as nascent seeds of hypotheses that may, in the future, generate “fruitful theories,” yielding precise predictions over a wide area.\nEssay 2 §\nDefend or Criticize: The most reasonable conclusion to draw from Reiss’s “Explanation Paradox” is that models in economics do not provide scientific explanations, but that they are nevertheless accepted by economics as explanatory because ‘explanation’ in this discipline isn’t scientific, it’s just plausible story telling.\nI claim that Reiss’s characterization of the explanation paradox and its last arm—only true accounts explain—to be an unreasonable imposition on causal explanations and scientific theories in general. This is because:\n\nStorytelling has been an indispensable part of science\nScience includes systematic classification between good and bad stories—not true or false\nScientific stories are not necessarily monist and reductionist\n\nI. For Storytelling\nIn the discussion of the third arm of the explanation paradox, Reiss doubts economists’ intuitions of credibility, stating that “the ‘credibility’ of an account of a phenomenon […] is not per se a reason to accept it as an explanation.” (Reiss, 56) However, the history of science is replete with scientists seeking credible accounts—a good story. From Archimedes’ insight into density in a bath, Kekulé’s ouruborus dream leading to the benzene ring, to Descartes’ coordinate system inspired by a fly on the wall, storytelling has been central. These compelling stories were neither fully evidenced nor objective from the outset, yet they inspired causal explanations. We then filter out the unfalsifiable and empirically test the remainder; only then do we consider them scientific theories or true-enough accounts.\nFrom this perspective, a scientist essentially performs a random search for CWs, relying solely on skillful storytelling, and eliminating those that fail falsifiability tests, and finally evidencing them through experiments. In such conception of science we need not reject, unlike Reiss’s suggestion, the importance of’aesthetic sensibility’ and intuitive recognition of theory as elegant or beautiful by a subject expert in determining which CWs are good explanations.3 For instance, contrary to Reiss’s interpretation, the Galilean conception that the universe is pythagorean is aesthetically pleasing, and evidenced extensively, but still rigorously unjustified according to his criteria.4 (Reiss, 56)\nIndeed, the scientific revolution was not necessarily a deductive proof of the scientific process but instead a method of sorting the evidentiary and useful out of a plural set of stories, an identification of the way of knowing that produces, historically, useful and good knowledge. What sets apart human science from simply an automatic theorem prover is that we need induction—creativity—, like Galileo and Newton’s idea to use mathematics to describe the physical world. Friedman even acknowledges that empiricism “can never reduce [a multitude of possible theories] to a single possibility […] consistent with the finite evidence,” and that “the choice among alternative hypotheses […] must [be] arbitrary, though there is general agreement [of] relevant considerations”—as we have selected mathematical beauty for physics, we aim to find a useful heuristic for economics, and we are doing just that, as outlined below. (Friedman, 5)\nII. For Pluralism\nIt is also false that storytelling is a monistic endeavor; economics especially prides in its pluralist family of theories. Reiss claims, via Kitchen’s notion of argument patterns, that models contain generalized abstract argument structures, and good scientific argument patterns are simpler and more fruitful, i.e. reductionist and monistic. Reiss posits that “The most important inference rule in economics is ‘Solve the model using an equilibrium concept’ […] but of course there are many equilibrium concepts [and] no clear rules which among a set of Nash equilibria to select […] the justification for using [one over another] is very thin.” (Reiss, 58) This is, also, a mischaracterization. Scientific fields often comprise multiple stories: in physics, of general relativity and quantum mechanics, or psychology, of behaviorism and cognitivism.\nUltimately, the CWs of science cannot at all simultaneously be reduced into simpler or abstractly general statements from the outset; a grand unified theory of economics is impossible without first resorting to the “higher-level” storytelling. The strictly reductionist view also manifests in Reiss’s repeated characterization of “contemporary economics,” referring only to neoclassical economics while in reality, there is no one unifying story of contemporary economics: computing and math in algorithmic game theory, dialectical methods in Marx’s historical materialism, psychology’s methods in behavioral economics, or statistics in quantitative finance all are equivalently powerful; to argue for reductionist stories alone is akin to ontological dictatorship.\nMuch of economics is simply CWs, some verified, others instrumentally useful, and the remainder neither verified nor useful—that we hold on to with a certain trust in the aesthetic sensibility of a scientist5, that it may be stepping stones in building more verifiable and useful theories. Science has never been a grand unifying endeavor nor has it aimed to be; it simply searches for credible explanations, respecting the universe’s complexity and emergence, while giving us a method for distinguishing between the credible and good, and the credible but bad.\nBibliography §\nReiss, Julian. “The Explanation Paradox.” Journal of Economic Methodology 19, no. 1 (2012): 43–62. https://doi.org/10.1080/1350178X.2012.661069.\nFriedman, Milton. Essays in Positive Economics. Chicago: University of Chicago Press, 1953.\nSugden, Robert. “Credible Worlds: The Status of Theoretical Models in Economics.” Journal of Economic Methodology 7, no. 1 (2000): 1–31. https://doi.org/10.1080/135017800362220.\nAkerlof, George A. “The Market for ‘Lemons’: Quality Uncertainty and the Market Mechanism.” The Quarterly Journal of Economics 84, no. 3 (1970): 488–500. https://doi.org/10.2307/1879431.\nSchelling, Thomas C. Dynamic Models of Segregation. 1978.\nFootnotes §\n\n\nThe reason for this may be that hard sciences have simpler laws and thus the first induction step is easier, while data is hard to gather, leading to a focus on experiments and testing; on the other hand economics has much more complex laws and more numerous variables, making the first inductive step harder ↩\n\n\nThis is remarkably similar to how physics progressed in the beginning of the scientific revolution. It took decades to centuries to conceptualize location (via cartesian coordinates), velocity, acceleration, or momentum; decades more to formalize it, and some more to finally result in the empirically testable hypothesis of Galilean relativity. Equivalently it took years of observations by Tico Brahe to verify the planets, years more by Kepler and his conception of “orbits” lacking mechanism, until Newton’s testable hypothesis of gravitation. ↩\n\n\nReiss lapses into deferring to such sensitiblites quoting Friendman: a good model is “‘simpler’ the less the initial knowledge needed to make a prediction […] more ‘fruitful’ the more precise the resulting prediction.” ↩\n\n\nIt even surprises the hard scientists; the “Unreasonable Effectiveness of Mathematics in the Natural Sciences” is a phrase coined by physicist Eugene Wigner in his influential 1960 essay, reflecting on the surprising correspondence between mathematical concepts (credible world)—often developed without any consideration for physical application—remarkably correspond to and predict natural phenomena (real world) ↩\n\n\nWhile Reiss claims that “the credibility of an account of a phenomenon of interest to an individual or group of researchers is not per se a reason to accept it as an explanation of the phenomenon,” I consider this to be putting science on an undeserving pedestal of objectivity. Science comprises not more than few researchers, with individual biases, “upbringing and educational background,” creating good stories based upon their derived nature and nurture, via the limited tools we have of falsification and empiricism, believe will hold—as we did believe for centuries of the “irrefutable truth” of Newtonian mechanics, the Central Dogma in biology or now, general equilibrium theory, all of which has been overturned or minimized. Our search for truth is indeed a process of iterative improvement, from a subjective story (of the credible world) to an objective one (of the real world), a process of telling better stories, and without either the creativity and rigor of our bards of science, we couldn’t have gotten this far. ↩\n\n\n"},"Phil-345-Philosophy-of-Economics":{"title":"Phil 345 Philosophy of Economics","links":["(Article)-Essays-in-Positive-Economics","Deductive-Nomological-Model","Pluralism-vs.-Monism","How-Possibly-vs.-How-Actually","Reductionism","Determinism","Essentialism","Supervenience","Analogical-Reasoning","paradoxes-of-EUT","Philosophers-of-Science","Instrumentalism-vs-Realism","Epistemic-Natural-Selection","Phil-345-Dupre-vs-Cartwright","Phil-345-Sugden-vs-Cartwright","External/Phil-345-Hausman","Phil-345-Essay-1","Phil-345-Essay-2","Phil-345-Final-Essay","Phil-345-Weekly---Reductionism"],"tags":["Courses"],"content":"\n\n                  \n                  Keynes \n                  \n                \nThe master-economist must possess a rare combination of gifts …. He must be mathematician, historian, statesman, philosopher—in some degree. He must understand symbols and speak in words. He must contemplate the particular, in terms of the general, and touch abstract and concrete in the same flight of thought. He must study the present in the light of the past for the purposes of the future. No part of man’s nature or his institutions must be entirely outside his regard. He must be purposeful and disinterested in a simultaneous mood, as aloof and incorruptible as an artist, yet sometimes as near to earth as a politician.\n\n\n(Article) Essays in Positive Economics\n\nProblems of Economics §\n\nLacks explanatory power\nLittle predictive power\nFailed microfoundations of macroeconomics\nEmpirical falsification of Rational Choice Theory\n\nModels &amp; Explanation §\nConcepts §\n\nDeductive-Nomological Model\nPluralism vs. Monism\nHow-Possibly vs. How-Actually\nReductionism\nDeterminism\nEssentialism\nSupervenience\nAnalogical Reasoning\nCredible-worlds\nExplanation Paradox\nApproximation vs. Charaterization\nparadoxes of EUT\nPhilosophers of Science\n\nSynopsis §\nWhat is economics?\n- Friedman: Instrumentalism vs Realism\n\nWhat is a model?\n\nHausman, D. M. (1994). Why look under the hood.\nGibbard, A. and Variant, H. (1978). Economic models.\n\n\nExample economic models\n\nAkerlof, G. (1970). The Market for ‘Lemons’: Quality Uncertainty and the Market Mechanism.\nSchelling, T. (1971) Dynamic Models of Segregation\n\n\nHow to think of models\n\nSugden, R. (2000) Credible Worlds: The Status of Theoretical Models in Economics\nCartwright, N. (1995). Ceteris paribus laws and socio-economic machines.\nDupré, J. (2001) Economics without Mechanism\n\n\nExplanations seem impossible…\n\nReiss, J. (2012). The explanation paradox.\nBecker, G. S. (1962). Irrational behavior and economic theory.\n\n\nGeneral Equilibrium Theory?\n\nHausman, D. M. (1981). Are general equilibrium theories explanatory?\nRosenberg (1983): If Economics Isn’t a Science: What Is It?\n\n\n\nHistory of Science §\n\nDevelopment of Science\n\nKuhn, T. (1962) The Structure of Scientific Revolutions (Selections)\nLakatos, I. (1995) The Methodology of Scientific Research Programmes,”\nBlaug, M.(1976) Kuhn vs. Lakatos or Paradigms vs Research Programmes in the History of Economics\nStanfield, R. (1974) Kuhnian Scientific Revolutions and the Keynesian Revolution\nPheby, J. (1988) Kuhn and Economics.\n\n\nReductionism: the microfoundations project\n\nHoover, K. (2001). Does Macroeconomics Need Microfoundations?\nElster J. (1988) The Nature and Scope of Rational-Choice Explanation.\nReiss, J. (2013). Chapter 3: Rational Choice Theory.\nEpistemic Natural Selection\n\n\n\nConcepts §\nAssignments §\n\n\nPhil 345 Dupre vs Cartwright\n\n\nPhil 345 Sugden vs Cartwright\n\n\nPhil 345 Hausman\n\n\nPhil 345 Essay 1\n\n\nPhil 345 Essay 2\n\n\nPhil 345 Final Essay\n\n\nPhil 345 Weekly - Reductionism\n\n\nPhil 345 Weekly - Reductionism\n\n\nModel\n\n\nMechanism\n\n\nExplanation\n\n"},"Phil-345-Sugden-vs-Cartwright":{"title":"Phil 345 Sugden vs Cartwright","links":[],"tags":[],"content":"In a fight between Sugden and Cartwright, who would win?\nI claim Sugden illustrates a more refined view of the function of economic sciences in knowledge generation. Through a survey of characterizations of the economic process he distinguishes the real and theoretical world (and how they may collide), with a respect for the emergent behaviors of socioeconomic systems shown by e.g. Schelling. Deductive steps are performed only in the “imaginary” or structured world, while the inductive leaps are clearly earmarked for analysis and evaluated with the robustness and credible-worlds criterion.\nOn the other hand Cartwright presents a naïve understanding of socioeconomic/game theories especially in her comparison to physics; to search for an Aristotelian nature of things relies heavily on our unjustified attempt at finding the core mechanism of things; in realty much of physics and mathematics relies on descriptive definitions/proofs, and theories that are mathematically true without representation (“shut up and calculate”). The dichotomy of regularity and laws is especially jarring as it relies on our sense of “aesthetics” of a claim, a sense modern physical sciences often discredit—nature is often neither simple nor reducible. Sugden’s analysis better respects such complexities by providing a tighter analytic framework than such intuition."},"Philips-Curve":{"title":"Philips Curve","links":["Money-(Medium-of-Exchange)","Gross-Domestic-Product","New-Keynesian-Business-Cycles-Model"],"tags":["Economics/Macro-Economics"],"content":"\nMotivation. There is this interesting data that shows negative correlation between unemployment and inflation. This shows a few things:\n\nMonetary Neutrality doesn’t seem to hold in the real world, because nominal prices are correlated with real variables\nWe need to model this!\nSince unemployment is a counter-cyclical variable and it’s easier to have a positive correlation equation, we instead consider GDP vs Inflation:\n\ni=a(Y−YM)+b⋅ie\nwhere:\n\ni is the inflation, or rate of price change\nie is the expected inflation. This can be whatever, but we assume for now it is constant.\na,b are coefficients. We can assume 0&lt;a and 0&lt;b&lt;1.\nY−YM, as shown in New Keynesian Business Cycles Model, is the output gap\nThus:\nThe first term shows sensitivity of inflation to the output. This is because the higher the output, the more firms increase prices\nThe second term shows the sensitivity of inflation to inflation expectation. This is because as firms expect prices to go higher in the future, they also increase prices now to match.\n\n"},"Philosophers-of-Science":{"title":"Philosophers of Science","links":["Epistemic-Natural-Selection"],"tags":["People","Philosophy/Epistemology"],"content":"\nKuhn (Revolutions in Science)\nLakatos (Scientific Research Programme)\nFeyerabend (Against Method)\nMy perspectice: Epistemic Natural Selection\n"},"Philosophy,-Political-Science,-Economics":{"title":"Philosophy, Political Science, Economics","links":["Economics/MicroEconomics/Game-Theory"],"tags":["Economics/Game-Theory","Philosophy/Political-Philosophy"],"content":"Why? §\nLack of Scientific Rigour in Economics §\nEconomics doesn’t have predicting power (the test of correctness for scientific theories)…\n\ncouldn’t predict recessions\ndoesn’t pursue distributive justice\nbetter for designing economics/social institutions, maintaining social order, or incentivizing individuals in society\n\nThis is because in economics the strategy of people are not considered:\n\n\n                  \n                  parametric agents.\n                  \n                \n→ In reality, individuals act based on how they think others may act; i.e. they are strategic agents.\n\n→ Conventional economic theory is useless as a predictor—it’ll only satisfy one’s curiosity\n→ The correct/better theory for modeling economics is Game Theory—the theory of strategic interactions\n"},"Physical-Data-Organization":{"title":"Physical Data Organization","links":[],"tags":["Computing/Data-Science"],"content":"\nStorage ranges from Fast &amp; Small → Slow &amp; Big \n(DevonThink) Numbers Every Programmer Should Know By Year\n\nIdea: a trip to the next hierarchy is orders of magnitude slower.\n\n\n\nAnatomy of a Hard Drive §\n\nParts of a mechanical hard drive: \n\nAccess Time=Seek+Rotation+Transfer\nAll data is transferred in blocks! (512B~4KB)\nRecords (=Tuples) can be fixed length of dynamic length\n\nBLOB fields: e.g. images. These link out to external locations\n\n\n\nStoring Many Tuples in One Block §\n\nOften many tuples will fit in one block. There are multiple schemes to lay them out.\n\nN-ary Storage Model (NSM) §\n\nData stored from the beginning of the block\nIndex stored at the end of the block\nEvery update/delete operation will reorganize everything! → Use gaps inbetween records (=sparse block)\nHard to cache, because queries will often only access a few columns\n\n\nPartition Attributes Across (PAX) §\n\nCluster columns together\nVariable length columns will have index at the end\nKeep the fields together (=dense block)\n\n\nColumn Stores §\n\nStore the whole table by columns\ne.g. Apache Parquet\n"},"Pipelining":{"title":"Pipelining","links":[],"tags":["Computing/Computer-Architecture"],"content":"\nIn computing, a pipeline, also known as a data pipeline, is a set of data processing elements connected in series, where the output of one element is the input of the next one. The elements of a pipeline are often executed in parallel or in time-sliced fashion. Some amount of buffer storage is often inserted between elements.\nComputer-related pipelines include:\nInstruction pipelines, such as the classic RISC pipeline, which are used in central processing units (CPUs) and other microprocessors to allow overlapping execution of multiple instructions with the same circuitry. The circuitry is usually divided up into stages and each stage processes a specific part of one instruction at a time, passing the partial results to the next stage. Examples of stages are instruction decode, arithmetic/logic and register fetch. They are related to the technologies of superscalar execution, operand forwarding, speculative execution and out-of-order execution.\nGraphics pipelines, found in most graphics processing units (GPUs), which consist of multiple arithmetic units, or complete CPUs, that implement the various stages of common rendering operations (perspective projection, window clipping, color and light calculation, rendering, etc.).\nSoftware pipelines, which consist of a sequence of computing processes (commands, program runs, tasks, threads, procedures, etc.), conceptually executed in parallel, with the output stream of one process being automatically fed as the input stream of the next one. The Unix system call pipe is a classic example of this concept.\nHTTP Pipelining, the technique of issuing multiple HTTP requests through the same TCP connection, without waiting for the previous one to finish before issuing a new one.Some operating systems may provide UNIX-like syntax to string several program runs in a pipeline, but implement the latter as simple serial execution, rather than true pipelining—namely, by waiting for each program to finish before starting the next one.\nWikipedia\n"},"Pluralism-vs.-Monism":{"title":"Pluralism vs. Monism","links":[],"tags":["Philosophy/Epistemology"],"content":""},"Poisson-Distribution":{"title":"Poisson Distribution","links":["Poisson-Limit-Theorem"],"tags":["Math/Common-Distributions"],"content":"See also: Poisson Limit Theorem\ndef. Poisson Distribution. A random variable X which model the number of events in a fixed interval of time, where each event is rare and there is a large number of events, is well modeled by a Poisson Distribution with the intensity of the event λ:\nX∼Poisson(λ)P(X=k)=e−λk!λk​\n\nE(X)=λ\nSD(X)=λ​\nP(X&lt;n)=∑k=0n−1​e−λk!λk​\n\nEstimators §\nlet X1​,…,Xn​∼Poi(λ)\nlnLn​=∏i=1​xi​!λ∑i=1n​xi​enλ​\nsn​=λ∑i=0n​xi​​−n\nλ^MLE​=n∑i=1n​xi​​=Xˉn​\n\n\n\n\n\n\n\n\n\n\n\n\n\nsinglemultipleI=λ1​In​=λn​"},"Poisson-Limit-Theorem":{"title":"Poisson Limit Theorem","links":["Poisson-Distribution"],"tags":["Math/Probability"],"content":"Poisson Limit Theorem (Simplified) §\nthm. Poisson Limit Theorem. With a random variable X∼Binom(n,p), as n→∞,p→0, X≈Poi(np), since:\nX=I1​+⋯In​ where each is an indicator of success of the i-th event, and as limn→∞,p→0​ this defines the Poisson Distribution.\nPoisson Scatter Theorem §\ndef. An experiment process which adheres to the following criteria is a Poisson Scatter process.\n\nNumber of hits are finite\nNo multiple hits on one point\nHits are homogeneous and independent (any non-overlapping region’s hit number is independent.)\n\nTHM Poisson Scatter Theorem. In a poisson scatter process:\n\nNumber of hits over area R is a Poisson Random variable\nThe number of hits in each disjoint region is independent of each other (definiiton #3)\nThe rate of hits (λ) is proportional to its area\n\nPoisson Addition Rule §\nthm. If X∼Poi(λX​) and Y∼Poi(λY​) and X⊥Y then:\nX+Y∼Poi(λX​+λY​)\n\n\n                  \n                  Info \n                  \n                \nNote also that the bionmial distribution as something like this too. If X∼Bi(nX​,p) and Y\\sim \\text{Bi}(n_Y,p)\n$$Y\\sim \\text{Bi}(\\lambda_Y) and X⊥Y then $X+Y\\sim \\text{Bi}(n_X+n_Y,p)\n\n## Poisson Point Process\n\ndef. Memory-less-ness (continuous). $X$ is memoryless iff:\n\\forall s,t&gt;0,\\ \\ \\mathbb{P}(X&gt;s+t|X\\geq s)=\\mathbb{P}(X&gt;t)\n\n&gt; [!info] Read it like this:\n&gt; \nLHS: “probability of waiting $t$ **more** minues, given that you’ve **already** waited $s$ minutes.”\nRHS: “probability of just waiting $t$ minutes in total.”\n\nGeometric and **_Exponential_** distributions satisfy this property. See the following.\n\ndef. **Poisson Point Process (PPP)**. PPP can be described in the following _three equivalent definitions_ in region $R$ with intensity $\\lambda$ _(per unit)_:\n1. Given a region $R_i$, let random variable $N(R_i)$ be defined as the number of events in the region. If..:\n\t- $\\forall i\\in R, \\ \\ N(I_i)\\sim \\text{Poi}(\\lambda\\cdot {|R_i|})$, where $|R_i|$ is the “size” of the region and $\\lambda$ the intensity\n\t- $N(R_i)$ are all independent of each other\n\t- ⇒ …then the event occurs in a PPP\n2. A process where the waiting time $W_i$ between two sequential events is distributed as an exponential distribution $\\forall i\\in R,\\ \\ W_i\\sim \\text{Exp}(\\lambda)$\n3. Total region size $X$ for $r$ events is distributed over gamma $X\\sim\\Gamma(r,1/\\lambda)$"},"Political-Compass":{"title":"Political Compass","links":["Ideology"],"tags":["Philosophy/Political-Philosophy"],"content":"A list of maps that categorizes the political ideologies across axes\nTradition/Secular Vs. Survival/Self-Expression w.r.t. Culture/Religion §\n"},"Portfolio-Theory-(Markowitz)":{"title":"Portfolio Theory (Markowitz)","links":["Efficient-Market-Hypothesis","Dividend-Discount-Model","Random-Variable","Risk-(Finance)"],"tags":["Economics/Finance"],"content":"Assume the Efficient Market Hypothesis.\n\n\n                  \n                  Info \n                  \n                \nHow to maximize returns while minimizing risk [=volatility =std. dev.]?\n→ Since risk/return is proportional to each other, you choose one optimization goal.\n\n\nSingle Stock (See Dividend Discount Model)\nPrice of stock i at time t: Si​(t)\nReturn of stock i at time t: Ri​(t)=Si​(t0​)Si​(t)−Si​(t0​)​+Si​(t0​)Di​​\n\nSi​,Di​,Ri​ are all Random Variables\n\n\nExpected Return of stock i: μi​=E[Ri​]\nVolatility (=Risk) of Stock i: σi​=Var[Ri​]​\n\nTwo-stock Portfolios §\n\nCovariance of stocks i,j: σi,j​=Cov[Ri​,Rj​]\nCorrelation of stock i,j: ρi,j​=σi​,⋅σj​σi,j​​ such that −1≤ρ≤1\nWeights w,1−w for (R1​,μ1​,σ1​),(R2​,μ2​,σ2​)\nPortfolio Expected Return: μp​=wμ1​+(1−w)μ2​\nPortfolio Variance:\n\nσp2​​=w2σ12​+(1−w)2σ22​+2w(1−w)ρ⋅σ1​σ2​=(σ12​−2ρσ1​σ2​+σ22​)w2+2(ρσ1​σ2​−σ22​)w+σ22​​function of w​​\n\nMinimum Variance Portfolio:\n\nW1−W​=σ12​−2ρσ1​σ2​+σ22​σ22​−ρσ1​σ2​​=σ12​−2ρσ1​σ2​+σ22​σ12​−ρσ1​σ2​​​​\nN-Stock Portfolio §\nR=​R1​⋮RN​​​  μ​=​μ1​⋮μN​​​  w=​w1​⋮wN​​​  V=​σ1​⋮σN,1​​…⋱…​σ1,N​⋮σN,N​​​\n\nKnown information: σi​,σij​,ρij​,μi​ are all known\nMultiple Stocks (=Portfolio)\n\nPosition of portfolio p at time t (=amount invested): Vp​(t)=∑i=1N​ni​Si​(t)\nPortfolio Return at time t Rp​(t)=wT⋅R\nPortfolio Expected Return at time t: μp​(t)=w⋅μ​\nPortfolio Variance: σp2​=∑i=1N​wi2​σi2​+2∑1≤i&lt;j≤N​wi​wj​ ρ σi​σj​=wTVw\n\n\n\ndef. Feasible Portfolios. Set of tuples (σ=risk,μ=expected return) given we have many assets weighted w.\n\nFp​={(σp​,μp​)∣w∈R} ← Short-selling is allowed\nLower the correlation [= the more negatively correlated] ∝ the better diversification is.\n\nWhen two stocks have perfectly negative correlation ρ=−1, the efficient frontier touches the y-axis (=σp​=0)\n\n\n\nEfficient Frontier §\nminw​ σp​=wTVw​ such that wT1=1, wTμ​=μp​\n\n“Minimize the variance σ such that the sum of weights are 1 and the portfolio return is μ (a constant)”\nKey Assumption: neither the returns μi​ or risks σi​ are identical, and there is no perfect correlation ρi,j​=±1\nMinimum for given return level μp​ at:\n\nw=AC−B2C−Bμp​​V−1e+AC−B2μp​A−B​V−1μ​\n\nEfficient Frontier:\n\n{(σp​,μp​)∣σp2​=AC−B2Aμp2​−2Bμ_p+C​}\n\n…where A,B,C are scalars:\n\nA=eV−1e\nB=μ​TV−1e=eTV−1μ​\nC=μ​TV−1μ​\n\n\n\n\n"},"Portfolio-Theory-(Risk)":{"title":"Portfolio Theory (Risk)","links":["Stochastic-Process","Black-Scholes-European-Option-Pricing-Formula","Normal-Distribution"],"tags":["Economics/Finance"],"content":"def. Risky Portfolio. A portfolio contains a risk factors (multivariate random variable) Zt​​, and has a total value Vt​, changing along indexed time t. The measurable function f is the map from risk factors and time: \n(Zt​,t)↦f​Vt​\nConsider Zt​,Vt​ a Stochastic Process {Zt​}t=0,1,…​,{Vt​}t=0,1,…​ with adapted filtration {Ft​}t​.\nWe then consider the changes to this porfolio:\nChange Xt+1​Loss Lt+1​​:=Zt+1​−Zt​:=−[ future value f(t+1,zt​+Xt+1​)​​− current value f(t,zt​)​​]​​\nwhere zt​ is the realized value of Zt​ at time t information Ft​\ndef. First-order Approximation of Risky Portfolio wrt to risk factors in vector Zt​ is:\nLt+1​Δ​:=−​ time derivativeft(1)​∣t,zt​​​​+∀i∑​ risk factor derivative fzi​(1)​∣t,zt​​Xt+1,i​​​​\nThe goal is then to:\n\nModel the distribution of X, the risk factors\nAnalytically solve the distribution of L or linear approximation LΔ\nExample 1. Stock portfolio with underlyers {St,i​}t∈N​ with each λi​ positions, then:\n\n\nRisk factors: Zt,i​=eSt,i​, log prices per custom\nLoss: Lt+1​=−∑∀i​λi​⋅St,i​⋅(eXt+1,i​−1)\nLinear Loss: Lt+1Δ​=−Vt​∑∀i​wt,i​Xt+1,i​\n\nwhere weights wt,i​:=Vt​λi​St,i​​\nModeling X as mean vector μ and covariance matrix Σ:\n\n\n\nE[Lt+1​Δ​]=−Vt​ dot w⊤μ​​Var[Lt+1​Δ​]=Vt2​w⊤Σw​​\nEasy!\nExample 2. European Call Option. Recall the Black Scholes European Option Pricing Formula. We take the rate r, volatility σ and underlyer St​ as the risk factors, pack them into vector Zt​=(lnSt​,r,σ)⊤. The option price is denoted C. Then:\nModeling risk factor changes:\nXt+1​=​lnSt​St+1​​rt+1​−rt​σt+1​−σt​​​\nThus deriving linearized loss:\nLt+1​Δ​=−( &quot;Theta&quot; Ct(1)​​​+ &quot;Delta&quot; CSt​(1)​​​St​Xt+1,1​+ &quot;Rho&quot; Cr(1)​​​Xt+1,2​+ &quot;Vega&quot; Cσ(1)​​​Xt+1,3​)\nwhere each partial derivative has a special name.\nExample 3. Loan Portfolio with default risk. We consider giving out a bunch of loans to different people at time t, and maturity t+1:\n\nPrincipal ki​\nPV of principal = Exposure ei​:=1+rki​​\nDefaults or not: RV Yi​ is 1 if default, 0 is solvent\n\n! Default doesn’t mean they don’t pay you, instead they pay you (1−δi​)ki​.\nδi​ is the “loss ratio” given default\nProbability of default pi​\nThis is the risk factor Z=(Y1​,Y2​,…)⊤\nFor simplifying notation, given default the expected PV, book value is:\n\n\n\nE[Lossi​∣Yi​=1]:=δi​pi​ei​\n\nThis is not the value of the loan, since (as we see later) risk-neutrality means there must be a premium on the current price due to default risk\nNow we have expectation of value, and value:\n\nE[Vt​]Vt+1​​=∀i∑​(1−δi​pi​)ei​=∀i∑​( if solvent (1−Yi​)ki​​​+ if default Yi​(1−δi​)ki​​​)=∀i∑​ki​(1−δi​Yi​)​​\nNow, we use expectation for Vt​ and realized value for Vt+1​, because the former must be calculated at time t, and the only thing we know is expectation; the latter is calculated at t+1 which means Yi​s are all known, thus the realized value is known.\nFinally:\nLt+1​​=Vt​−1+rVt+1​​=∀i∑​(1−δi​pi​)ei​−∀i∑​=ei​1+rki​​​​(1−δi​Yi​)=∀i∑​(δi​ei​Yi​−δi​ei​pi​)​​\nDistribution of Loss from Distribution of Underlyer §\nLoss Lt+1​:=−[ future value f(t+1,zt​+Xt+1​)​​− current value f(t,zt​)​​]\nRecall the definition of loss above. To determine this Lt+1​’s distribution we must\n\nModel RV Xt+1​’s distribution\n\nUsually historical data is used\n\n\nModel RV (=mapping function) f(t+1,zt​+Xt+1​)’s distribution from it\n\nValuation models, like the BSM are used\nIn general we have three methods to do this:\n\n\n\n1. Analytical Method §\n\nChoose Xt+1​ and f such that Lt+1​ is analytically tractable\n\nAssume Lt+1​Δ​ is a good enough approximation\nAssume Xt+1​ is a Normal Distribution\nThus we will have a general form of:\n\n\n\nLt+1​Δ​=−(ct​+∂t∂b​Xt+1​)\ncomparing to the example 1 above, the constant term ct​=0, b are the weights\n2. Historical Simulation §\n3. Monte Carlo Simulation §"},"Post-Treatment-Variable":{"title":"Post-Treatment Variable","links":[],"tags":["Math/Statistics"],"content":"Post-treatment variables are variables that confound the “controlling for endogenous factors by including them in the regression” technique. They can cause problems is two ways:\nMediator §\nMotivation. Suppose regressing Earnings vs Tutoring:\nEarningsi​=β0​+β1​isTutored+ϵi​\nBut secretly, there was a correlation where:\n\nTherefore, E[β0​^​]=γ1​+αγ2​\nNow, simply controlling for reading scores like this:\nEarningsi​=β0​+β1​isTutored+β2​readingScores+ϵi​\nIs not effective, because there’s a multicollinearity between isTutored and readingScores. In this case, E[β1​^​]=γ1​, and this doesn’t capture the αγ2​ portion.\nConfounder §\nCollider Bias §\nCollider bias\n\nA stupid model:\nYi​=β0​+β1​X1i​+β2​X2i​\nwill have biases:\n\nE[β1​^​]=γ1​+αρ1​ρ2​​\nE[β2​^​]=γ2​+ρ1​ρ2​​\n\nExample. Suppose we are regressing: Flu vs. Car accident. We also try to control for being being infected while being hospitalized:\nFlui​=β0​+β1​hadAccident+β2​isHospitalized+ϵi​\nSuppose, however, reality went like this:\n\nCar accidents lead to hospitalization, but don’t cause the flu\nFevers lead to both hospitalization and cause flu testing\nThen:\n\n\nE[β1​^​]=γ1​−αρ1​ρ2​​ i.e. accident and flu is\nE[β2​^​]=γ2​+ρ1​ρ2​​\n"},"Postmodernism":{"title":"Postmodernism","links":[],"tags":["Philosophy"],"content":""},"Postructuralism":{"title":"Postructuralism","links":["Decentralization","External/Michel-Foucault","Jacques-Derrida","Jean-Baudrillard","Noam-Chomsky"],"tags":["Philosophy/Epistemology"],"content":"\nPoststructuralism is Decentralization (of power, of…)\n“Microphysics of power is too strong. We shouldn’t trust social institutions as they are agents of power. Everything is relative and a social construction.”\nMoral relativism\n\nRelated People\n\nMichel Foucault, Jacques Derrida, Jean Baudrillard\nNoam Chomsky hates it. See Debate Noam Chomsky &amp; Michel Foucault - On human nature [Subtitled] - YouTube\n"},"Potential-Game":{"title":"Potential Game","links":["Traffic-Routing"],"tags":["Economics/Game-Theory"],"content":"def. Potential Game. A game is a potential game iff there exists a potential function that satisfies the following, for any player i that changes their strategy from si​→si′​:\nChange in Potentialϕ(si′​,s−i​)−ϕ(si​,s−i​)​​=Change in Costci​(si′​,s−i​​)−ci​(si​,s−i​​)​​\ndef. This function Φ(si​,s−i​) is a potential function. \n\ns−i​ denotes the strategies of the remaining players.\n\nthm. Potential Games always has a PNE. Namely, for convex potential functions that have a global minima, that minima is the equilibrium.\n\nThis NE is also achievable; let the system run, and it will reach NE (best-response dynamics)\nEvery local minimum of the potential function is a NE\nIntuition. Consider:\n\n\nWhen a player improves their strategy to reduce their own costs, the potential decreases\nThis repeats until nobody can switch their strategy\n\nAn example of potential game analaysis for computing NE is Traffic Routing"},"Power-Consumption-(Computing)":{"title":"Power Consumption (Computing)","links":["Performance-(Computing)"],"tags":["Computing/Computer-Architecture"],"content":"1.7. The Power Wall §\nUntil recently power usage and clock rate have increased together. Recently limits were reached with cooling capacity, and thus processors cannot be designed consume more power. Power consumption is summarized as:\nPower∝21​×Capacitive Load×Voltage2×Frequency\n\nCapacitive load depends on each transistor\nNote the voltage is squared\nFrequncy is the CPU clock rate\n\nMisconceptions §\n\nComputers at low utilization don’t always use little power\n\nPower doesn’t really scale linearly with performance; even computers at idle use lots of power. e.g. Google’s servers use 33% of peak power at 10% utilization.\n\nPerformance helps energy efficiency\n\nDesigning for performance means the task takes less time, and thus total energy consumption decreases.\n\nUse all three performance metrics: frequency, CPI, Instruction count\n\ne.g. MIPS (million instructions per second) is a very bad metric for measuring performance. It’s defined as MIPS=Execution time×106Instruction count​. and is also equal to CPI×106Clock rate​. So MIPS will always not take into consideration at least one of the three performance metric, and thus can be very decieving."},"Present-Value-Calculations":{"title":"Present Value Calculations","links":["Security-(Finance)","Role-of-Money","Future-Value-Calculations","Interest-Rate","Inflation"],"tags":["Economics/Finance"],"content":"In pricing the value of a security, we consider that humans discount future returns.\n\nReason we need to discount value: Role of Money\nThis is the reverse of Future Value Calculations.\n\ndef. Discounted Cash Flows (DCF) are used to price assets.\n\nEach cash flow is discounted by the discount rate. They are summed to get the present value of all the cash flows\nWe always want asset price↑, risk↓\nrate of return ∝ volatility ← knowing how to trade off these two is important\n\nTo calculate the DCF of some future cash flow FV’s current present value PV:\nPV​=(1+kr​)t×kFV​=(1+q)nFV​​\n\nr is the Annual Percentage Rate [= quoted rate]\nk is how many times to compound every year\n\n→ thus r/k is the interest rate per compound period (=q)\n\n\nt is the number of years\n\n→ thus t×m is the number of **total compounding periods (=n)\n\n\n\nYou can calculate the present value of multiple identical cash flows of amount C:\nPV​=(1+r)1C​+(1+r)2C​+⋯+(1+r)nC​=rC​(1−(1+r)−n)​\nWhat is the value of Interest Rate r?\n\nIn individual investments/loans, r determined usually by looking at similar assets in the market\nr∝ Inflation; if inflation is high, money isn’t worth much in the future, so lender demands more interest rate\n\nNet Present Value §\nNPV(r)=PV(r)−Initial Investment"},"Presentation":{"title":"Presentation","links":[],"tags":[],"content":""},"Prestige":{"title":"Prestige","links":["Microphysics-of-Power","Cheat-Codes-of-Life"],"tags":["Sociology"],"content":"\n\n(DevonThink) Social Status: Down the Rabbit Hole | Melting Asphalt\n\n\n(DevonThink) Social Status II: Cults and Loyalty | Melting Asphalt\n\n\nPrestige is the currency of the social world, indicator of social status. It’s one of the two ways power is allocated and distributed.\n\n\n“Red Paperclip Theory of Prestige.” The important part of pretige is that you can trade one type of prestige for another type. Use money to get a job, use social prowess at work to get a fancier job title, etc.\n\n\nMoney is industrial-strength prestige.\n\nFor that matter, all of Cheat Codes of Life is about congenital prestige; it’s embedded in our biology to consider these cheat codes as pretigous.\n\n\n"},"Price-Controls":{"title":"Price Controls","links":[],"tags":["Economics/Micro-Economics"],"content":"Four types of price controls exist:\n\nPrice Floors\nPrice Ceilings\nTaxes\nSubsidies\n\n"},"Price-Elasticity-of-Demand":{"title":"Price Elasticity of Demand","links":[],"tags":["Economics/Micro-Economics"],"content":"ϵD​​:=%Δp%Δx​=pΔp​xΔx​​=xp​ΔpΔx​→xp​dpdx​=dlnpdlnx​​​\n\nElasticity can be:\n\n∣ϵD​∣&gt;1…Elastic\n∣ϵD​∣=1…Unit Elastic\n∣ϵD​∣&lt;1…Inelastic\nReminder that elasticity is almost always negative (for a downward-sloping demand curve (=ordinary good))\n\n\nElasticity of demand can vary along a single demand curve\n\nThere is an “elastic portion” and “inelastic portion”\nElasticity determines if a firm should produce more or less to increase revenue. \n\n\n"},"Price-Mechanism":{"title":"Price Mechanism","links":[],"tags":["Economics","Economics/Game-Theory"],"content":""},"Price-to-Earnings-Ratio":{"title":"Price to Earnings Ratio","links":["Measuring-Security-Performance"],"tags":["Economics/Finance"],"content":"def. Price to Earnings Ratio (P/E Ration).\nP/E=Earnings per ShareShare Price​=Net IncomeMCAP​\n⇒ Think: for two firms…\n\n…if the market values the shares higher,\n…even though the earnings are low,\n…the market thinks the firm has growth potential.\n\nHow Good is the P/E Ratio? §\n\nThe P/E ratio contains finance information so it’s a noisier measure compared to the Measuring Security Performance.\nIt’s a better measure for the pure returns you get on the share.\n→ EBITDA is a bigger, general rule of thumb, while P/E could be better for a small invester.\n\n\n\n                  \n                  fundamental analysis.\n                  \n                \n\n\n\n                  \n                  \\frac{\\text{Company Size}}{\\text{Profitability}}, thus measuring the growth of the company.\n                  \n                \n"},"Principles-and-Techniques-of-Debugging":{"title":"Principles and Techniques of Debugging","links":["Algebraic-Type-System","Formal-Methods","Debugger","Git"],"tags":["Computing/Engineering"],"content":"25 Debugging Techniques Every Software Developer Should Master\nPrinciples §\n\nObservability. Program should output its progress so you can see which codepaths are being taken\n\nLogging: delinate between log levels: TRACE/INFO/DEBUG/WARN/ERROR/FATAL\nTight REPL\n\n\nRoot-cause. Always find the root cause of why the bug happened. You shouldn’t be satisfied with a surface-level reason\nVerification. Code should be verified\n\nTechniques §\n\nSystematic Reduction. This is done both in time-axis and space-axis:\n\nTime: which version (commit) introduced the bug?\nSpace: which line of code introduced the bug?\n\n\nStatic program analysis. Ranges from simple to proper:\n\nLinters\nUsing typed languages (Algebraic Type System)\nFunctional Programming languages\nFormal Methods\n\n\nTesting. With tests, you can make changes without fear that it’ll break something without you knowing\nTight REPL. Your debugging harness should be setup for a very tight REPL loop. The longer it takes to get the feedback, the less efficient, and more frustrating.\n\nTools §\n\nDebugger\n\nA time-travel debugger is the best, but if not a normal debugger will do\nUse breakpoints to narrow down exactly where the problem occurs\n\n\ngit bisect\n\nIdentify exactly which change introduced the bug\n\nalso reason why you should merge instead of rebase (^pzjio5)\n\n\n\n\n"},"Priority-Queue":{"title":"Priority Queue","links":[],"tags":["Computing/Data-Structures"],"content":"Time Complexity §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperationfind-maxdelete-maxinsertincrease-keyMergeBinary-HeapΘ(1)Θ(logn)O(logn)O(logn)Θ(n)Fibbonacci HeapΘ(1)O(logn)Θ(1)Θ(1)Θ(1)\nTournament Tree §\n\n\nGetting the largest item is O(n) which is same as the naive approach but…\nGetting the next-largest element after having built the tree is O(logn) time\n\nGetting the next-k largest elements is O(klogn) time\n\n\n"},"Prisoner's-Dillemma":{"title":"Prisoner's Dillemma","links":["Game-Theory","Econ-361-Distributive-Justice","Elinor-Ostrom","John-Rawls","Robert-Axelrod","Robert-Frank","Prisoner's-Dillemma"],"tags":["Economics/Game-Theory"],"content":"def. Prisoner’s Dillemma is a symmetric game where the dominant strategy for both players is to defect, but collaboration would have yielded greater results. It is one of the most important problems that Game Theory aims to solve, and have applications in many fields. It is also known as the collective action problem or the free-rider problem. It is the cause of the Tragedy of the Commons in the scaled form.\n\nEcon 361 Distributive Justice is a course that dealt with this in-depth, thru the lens of PPE.\nElinor Ostrom\nJohn Rawls\nRobert Axelrod\nRobert Frank\n\ndef. Coordination Device is a mechanism, any rule, external entity, etc. that can make both players cooerate.\n\n¶a [[Focal Point|Focal Point]Prisoner’s Dillemmaoordination device.\n\nWhen you ask two strangers to meet up at New York on Jan 1st., without giving them information, and without letting them coordinate, they will likely meet up in Times Square, at Midnight. This is a focal point.\nA Focal Point is a 결집점 in a game with no coordination; a conspicuous point, either by nature or culture, that we can identify."},"Private-Equity-Firms":{"title":"Private Equity Firms","links":["Initial-Public-Offering"],"tags":["Economics/Finance"],"content":"Funds that have a lot of money, and use that to buy out a public company to take it private → improve its operations → IPO again to sell it at a higher price (often a big failure)"},"Private-Property":{"title":"Private Property","links":["Proletatriat-(Marxism)"],"tags":["Philosophy/Marxism"],"content":"\nthe theory of the Communists may be summed up in the single sentence: Abolition of private property\n\n\nBut does wage-labour create any property for the labourer? Not a bit. It creates capital, i.e., that kind of property which exploits wage-labour, and which cannot increase except upon condition of begetting a new supply of wage-labour for fresh exploitation\n\n\nWhen, therefore, capital is converted into common property, into the property of all members of society, personal property is not thereby transformed into social property. It is only the social character of the property that is changed. It loses its class character.\n\n\nFinally, communism2 is the positive expression of the abolition of private property and at first appears as universal private property.\n"},"Probabilistic-Generative-Models":{"title":"Probabilistic Generative Models","links":["Maximum-Likelihood-Estimator","Normal-Distribution","Covariance","Softmax-and-Sigmoid"],"tags":[],"content":"Motivation. Inspiration is from Maximum Likelihood Estimators. Consider classifying into two classes, C1​,C2​ If we consider w, the weights as a parameter is a probability distribution for class C1​, classification output as such:\n\np(C1​∣x): given datapoint x the probability (distribution) of it being in class 1. The pdf of this is what we want to find.\np(x∣C1​): given that data is in class 1, what is the distribution of the datapoints? We know this from the data.\nWe will construct the former from the latter.\n\nFrom x as a Multivariate Normal Distribution §\nIf we have x as a multivariate normal distribution, it has the probability distribution:\np(x∣C1​)=(2π)2d​∣Σ∣21​1​e−21​(X−μ1​)TΣ−1(X−μ1​)\nwhere\n\nμ1​​ is the mean point of datapoints in class 1\nΣ is the Covariance matrix for this distribution\nBoth off these we do not know yet. Then using Bayes’ Rule:\n\np(C1​∣x)​=p(x∣C1​)p(C1​)+p(x∣C2​)p(C2)p(x∣C1​)⋅(C1​)​=∑∀j​exp[ let aj​ lnp(x∣Cj​)p(Cj​)​​]exp[lnp(x∣Ck​)p(Ck​)​ let ak​ ​]​=σ(ak​∣a1​,…,an​)​by Bayes’add exp &amp; log for sigmoidby def of sigmoid​​\nwhere\n\nSee ^unj2yy for the definition\nSoftmax ensures this is a probability distribution.\nNow, we calculate ak​. Substitute the multivarate normal pdf into ak​:\n\nak​​=lnp(x∣Ck​)p(Ck​)= let C ln(2π)D/21​+ln∣Σ∣1/21​​​−21​(x⊤−μk⊤​)Σ−1(x−μk​)+lnp(Ck​)=C−21​x⊤Σ−1x+21​x⊤Σ−1μk​+21​μk⊤​Σ−1x let wk0​​ −21​μk⊤​Σ−1μk​+lnp(Ck​)​​=C−21​x⊤Σ−1x+ let wk​ μk⊤​Σ−1​​x+wk0​​=wk​⊤x+wk,0​−21​x⊤Σ−1x+C​Σ−1 is symtrc​​\nFinally, we plug this into our softmax function to get:\np(Ck​∣x)​=∑∀j​exp(wj​⊤x+wj,0​−21​x⊤Σ−1x+C​)exp(wk​⊤x+wk,0​−21​x⊤Σ−1x+C​)​=∑∀j​exp(wj⊤​x+wj,0​)exp(wk​⊤x+wk,0​)​​​\nFinding the Optimal Parameters w §"},"Probabilistic-Principle-Component-Analysis":{"title":"Probabilistic Principle Component Analysis","links":["Principle-Component-Analysis","Maximum-Likelihood-Estimator"],"tags":["Computing/Maching-Learning","Math/Statistics"],"content":"Motivation. Principle Component Analysis was about transforming data into the explanatory variables (=principle components). PPCA is mathematically similar, but it thinks of each source variable as a random variable.\ndef. Probabilitic Principle Component Analysis (PPCA) model. We have observed data {x1​,…,xN​} with xn​∈Rdim(xn​) and we want to extract latent features into h∈Rdim(h). Naturally, dim(xn​)=d≫q=dim(h).\nxn​=Whn​+μ+ϵn​\nwhere\n\nhn​∼N(0,Id​) i.e. multivariate standard normal distribution\nϵn​  N(0,σ2Id​) noise, i.e. multivarite normal distribution with mean 0, variance σ2 for each\nW∈Rdim(xn​)×dim(hn​)\nObserve that then xn​∼N(μ,C) where C=WW⊤+σ2Id​\nObjective is to find the optimal W, noise variance σ2 and μ given observed data. We use MLEs for this. Steps are briefly outlined below. Objective function f is the log-likelihood:\n\nf=lnL=−2N⋅dim(xn​)​log(2π)−2N​log∣C∣−21​n=1∑N​(xn​−μ)⊤C−1(xn​−μ)\nThe analytic solution using eigendecompotision is:\nμMLE​σMLE2​WMLE​​=N1​n=1∑N​xn​=d−q1​i=q+1∑d​λi​=Uq​(Λq​−σMLE2​Iq​)1/2R​​\nwhere\n\nΛq​=diag(λ1​,…,λq​) in decreasing order, is from a spectral decomposition of sample covariance N1​∑n=1N​(xn​−μMLE​)(xn​−μMLE​)⊤=UΛU⊤\nR∈Rq×q is an arbitrary orthogonal rotation matrix\n"},"Probability":{"title":"Probability","links":["Stochastic-Calculus","Cardinality"],"tags":["Math/Probability"],"content":"def. An Outcome Space (Ω) is the set of all possible outcomes of an experiment\ndef. An Event (A) is a subset of an outcome space. (die face is even)\nExample. For a 6-sided die:\nΩAA​={1,2,3,4,5,6}={2,4,6} ⊂Ω​​\ndef. Probability for countable sets. When all outcomes in Ω are equally likely, and Ω is a finite set,\nP(A)=#Ω#A​\nwhere #A denotes the number of elements in set A (its cardinality).\nProperties.\n\nP(AC)=1−P(A)\nIf A⊆B then P(A)≤P(B)\nIf A⊆B then P(B∩AC)=P(B)−P(A)\nP(A∪B)=P(A)+P(B)−P(A∩B)\nTrivial cases:\nP(∅)=0\nP(Ω)=1\nA⊆B is equivalent to A→B (i.e. if A then B)\n\n…for Stochastic Calculus §\nMotivation. We need a sigma algebra to define a probability, because there’s problematic set theory problems with uncountably infinite sets.\nSigma-Algebra §\ndef. Sigma Algebra. A sigma-algebra on a set Ω denoted σ(Ω) contains certain subsets of A, such that it follows the following properties:\n\n{∅,Ω}⊆σ(Ω)⊆2Ω (biggest and smallest possible sigma-algebras)\nlet A be a subset of Ω. If A∈σ(Ω) then AC∈Ω. (closed under complement)\nlet A,B a subset of Ω. If A,B∈σ(Ω) then A∪B∈σ(Ω) (closed under finite union)\ndef. Atom of a Sigma Algebra F is a non-empty set A∈F where for any other set B∈F, A∩B=∅ or A.\n\n\nAll the atoms in F form a partition on Ω.\nIntuition. The important parts of a sigma-alg is the atoms. Other elements generated by union/complement are there to satisfy measure-theory.\n\nMeasure §\ndef. Measure. let X,σ(X). A measure μ is a function from measure space to to real numbers:\nμ:σ(X)→R\nthat satisfies the following properties:\n\nNon-negative: ∀A∈σ(X), μ(A)&gt;0\nCountable Additivity: μ(A∪B)=μ(A)+μ(B)\nμ(∅)=0\nIntuition. Consider a measure as a means to measure the “size” of the set (not Cardinality). On a real number line, the set (2,5)≡{x∈R∣2&lt;x&lt;5} has length 3. The set (5,9) has length 4. Thus (2,5)∪(5,9)=(2,9) has length 3+4=7.\ndef. A Measure space is simply a set X and its sigma-algebra Σ together in a tuple:\n\n(X,Σ)\nProbability Space §\ndef. A Probability measure on X,σ(X) is a special type of measure P:σ(X)→R that satisfies P(Ω)=1\ndef. A Probability Space is like a measurable space, but also together with the probability function:\n(Ω,F,P)\nInterpretations of Probability (Philosophically) §\nMotivation. What is probability? Interpretations of its definitions seem meaningless without real-life experiments.\nTwo main interpretations of probability are:\n\nThe Objectivist Interpretation—relative frequency of occurrence, if the experiment is conducted indefinitely\nThe Subjectivist Interpretation—degree of belief; how much you would bet on an event.\n"},"Production-Function":{"title":"Production Function","links":["Homogenous-Function"],"tags":["Economics/Micro-Economics","Economics/Macro-Economics"],"content":"xx​=f(l)=f(l,k)​Short RunLong Run​​\nTerminology §\ndef. Marginal Product. The additional output in units of good x giving additional input (labor or capital)\nMPl​=dldf​,MPk​=dkdf​\ndef. Marginal Revenue Product. The additional output in units of dollars ($) given one unit of additional input (labor or capital)\ndef. Law of Diminishing Marginal Product. As the input increases over a certain point, the marginal product of the input decreases. This happens for both labour and capital in production functions.\n\nMathematically, There exists some l∗ such that:\n\n∂l∂MPl​​​l∗​&lt;0\nReturns to Scale §\nProduction functions are one of three types:\n\nDecreasing Returns to scale: tf(l,k)&lt;f(tl,tk)\nConstant Returns to scale: tf(l,k)=f(tl,tk)\nIncreasing Returns to scale: tf(l,k)&gt;f(tl,tk)\n\nMicro Production Function §\nShort Run §\ndef. SR Production function. f(l,k) where l denotes labor input, and k denotes capital input. Range is the units of output good.\n\nShort Run (SR) is defined as the timeframe where l can be varied, but k is fixed.\nAlso known as production frontiers or producer choice sets.\nWhile production frontiers can take on any shape, the most common shape is that of (b) (due to Law of diminishing marginal product).\n\n\ndef. SR Marginal Product of Labor (MPl​) The marginal change in output with a unit more of labor.\n\nIn the short-run production function, that is the gradient of the function, i.e. MPl​=dldf​.\n\n\nLong Run §\n\nMacro Production Function §\nHomogenous Function"},"Productive-Forces,-Relations-of-Production,-and-Historial-Materialism":{"title":"Productive Forces, Relations of Production, and Historial Materialism","links":["Proletatriat-(Marxism)"],"tags":["Philosophy/Marxism"],"content":"Productive Forces, RoP, Historical Progression §\n\n[Capitalism] has pitilessly torn asunder the motley feudal ties that bound man to his “natural superiors”, and has left remaining no other nexus between man and man than naked selfinterest, than callous “cash payment”. […] It has resolved personal worth into exchange value, and in place of the numberless indefeasible chartered freedoms, has set up that single, unconscionable freedom—Free Trade.\n\n\nIt has converted the physician, the lawyer, the priest, the poet, the man of science, into its paid wage labourers.\n\n\nProductive forces are the steady march of technology which drive economic growth through “good ideas” and efficiency.\nRelations of production are the social institutions that facilitate (or hinder) production—the context within which production occurs\n→ Think: patent systems (transforming ideas into property), protection of proptery, chattle slavery\n\n\nDirection of History §\n\nOur epoch, the epoch of the bourgeoisie, possesses, however, this distinct feature: it has simplified class antagonisms. Society as a whole is more and more splitting up into two great hostile camps, into two great classes directly facing each other—Bourgeoisie and Proletatriat (Marxism).\n\nFeudalistic society i.e. guilds + birth-determined rank → Capitalist Society i.e. private property, wage-labor\n→ Productive forces has outgrown capitalism—”specture haunting Europe”\n⇒ Constant change in ideas, innovation, striving for growth, and societal thought, idealology changed to serve its mode of production\n\nAll fixed, fast-frozen relations, with their train of ancient and venerable prejudices and opinions, are swept away, all newformed ones become antiquated before they can ossify. All that is solid melts into air, all that is holy is profaned, and man is at last compelled to face with sober senses his real conditions of life, and his relations with his kind.\n\n⇒ BZ RoP destroys national boundaries, cosmopolitan nature of production as it brings together global supply of goods into global consumer demand (ultimately, the “epidemic of overproduction”)\n\nThe bourgeoisie has through its exploitation of the world market given a cosmopolitan character to production and consumption in every country.\n\n\nIt compels all nations, on pain of extinction, to adopt the bourgeois mode of production; it compels them to introduce what it calls civilisation into their midst, i.e., to become bourgeois themselves. In one word, it creates a world after its own image.\n\n⇒ Ultimately consuming the whole world economy into its RoP\n\nCommunism is the necessary form and the dynamic principle of the immediate future, but communism is not as such the goal of human develop­ meant - the form of human society.9\n\nand communism is thus inevitable:\n\nenough. In order to supersede private property as it actually exists, real communist activity is necessary. History will give rise to such activity, and the movement which we already know in thought to be a self-superseding move­ meant will in reality undergo a very difficult and protracted process.\n\nbecause the brotherhood of man is real\n\nThe brotherhood of man is not a hollow phrase, it is a reality, and the nobility of man shines forth upon us from their work-worn figures.\n\n\n\n[…] in one word, the feudal relations of property became no longer compatible with the already developed productive forces; they became so many fetters. They had to be burst asunder; they were burst asunder.\n\n\na society that has conjured up such gigantic means of production and of exchange, is like the sorcerer who is no longer able to control the powers of the nether world whom he has called up by his spells.\n\n⇒ When PF outgrows RoP, naturally PF wins; i.e. communism is inevitable. Feudalism to Capitalism, Capitalism to Communism\nCurrently we observe this as the epidemic of overproduction:\n\n[…] the epidemic of over-production. Society suddenly finds itself put back into a state of momentary barbarism […] And how does the bourgeoisie get over these crises? On the one hand by enforced destruction of a mass of productive forces; on the other, by the conquest of new markets, and by the more thorough exploitation of the old ones.\n"},"Profit-Function":{"title":"Profit Function","links":["Input-Demand-and-Output-Supply","Hotelling's-Lemma"],"tags":["Economics/Micro-Economics"],"content":"def. Profit Function. Given input and output prices, returns the maximum achievable profit.\nπ(w,r,p)\n(HowTo) Derive Profit Function §\n\nGet Input Demand and Output Supply.\nSubstitute into the profit equation\n\nπ=p⋅x(w,r,p)−w⋅l(w,r,p)−r⋅k(w,r,p)\n\n\n\nProperties §\n\nHD1 in w,r,p\nIncreasing in p\nDecreasing in w,r\nHotelling’s Lemma applies\n"},"Profit-Maximization":{"title":"Profit Maximization","links":["Cost-Minimization"],"tags":["Economics/Micro-Economics"],"content":"See also Cost Minimization\nShort Run (One-input) §\n\\text{max} ~ \\pi=px-wl-r \\bar{k} ~ \\text{such that} ~ x=f(l,\\bar{k})\n$$ ...where $\\bar{k}$ is fixed at the long run optimal point.\nMaximization of profit (reaching maximum iso-profit line) against the [[Production Function]]\n- Uses [[Constrained Optimization]]\n- Optimal is where the production function is tangent to the isoprofit line: $-\\frac{w}{r}=TRS$\n\t- reminder: [[Technical Rate of Substitution|TRS]] is the tangency of the isoprofit line.\n\t- the slope of the isocost line is $-\\frac{w}{r}$\n- Result is the [[Input Demand]]s and Output [[Supply Function]]\n- As in [[Utility Maximization]], you can substitute these results into profit formula $\\pi-px-wl$ to get the [[Profit Function]].\n\nAlternative Characterization\nmax_{x,l}~p\\cdot f(l,\\bar{k})-wl-r\\bar{k}\n- Where $c(w,x)$ is the reformulation of the production function into a function of input $l$, multiplied by $w$ (= $\\text{input quantity}\\times \\text{input price}$ given a certain level of output $x$)\n- Solvable simply by finding where tangent is zero.\n- You will get short run [[Input Demand]]s \n\n## Long Run (Multiple Input)\n\nmax_{x,l,k}~ \\pi=px-wl-rk ~ \\text{such that} ~ x=f(l,k)\n- First Order Condition (FOC) is equivalent to $pMP_{L}=w,pMP_{k}=r$\n\t- i.e. produce when the additional revenue is equal to price of inputs\n\t- i.e. $MR_{L}=MC_{L},MR_{k}=MC_{k}$\n- Gets you [[Input Demand]] functions\n- ! Beware returns to scale: If Increasing Returns/Constant Returns to scale, then only one of the inputs are used. (See below for more details)\n\n## Profit Maximization Problem\n\nThere are two main ways for profit maximization:\n\n- One-step: \n\t- $\\text{max}\\ \\pi=px-wl-rk \\ \\text{ s.t. } x=f(l,k)$ to get profit-maximization condition $p\\cdot \\text{MP}_l=w, \\ \\ p\\cdot MP=r$\n\t- This gets you the input demand functions $l(p,w,r), k(p,w,r)$ and output supply $x(p,w,r)$.\n- Two-step:\n\t  1. $\\text{min} \\ c=wl+rk \\ \\text{ s.t. } \\ x=f(l,k)$ to get cost-minimization condition $-\\frac{w}{r}=\\frac{\\text{MP}_l}{\\text{MP}_k}(=\\text{TRS})$\n\t\t This gets you conditional input demand functions $l(x,w,r), k(x,w,r)$\n\t  1. Solve $p=\\text{MC}^{\\text{LR}}$ by using above conditional input demand functions (recall $\\text{MC}=\\frac{\\delta C}{\\delta x}$)\n     This gets you output supply $x(p,w,r)$\n\n\n### Special Case: Labor and Capital Are Perfect Compliments\nf(x)=min(l^\\alpha,k^\\alpha)\n- When $0&lt;\\alpha&lt;1$ it has decreasing returns to scale\n\t- → simply solve two [[Unconstrained Maximization]] problems:\n\t- $max_{l}\\pi =pl^a-wl-rk$, \n\t- $max_{k}\\pi =pk^a-wl-rk$,\n\t- $l^\\alpha = k^\\alpha$ &lt;- **dont forget!**\n\n### Special Case: Labor and Capital Are Perfect Substitutes\n$$f(l,k)=(l^\\alpha+k^\\alpha)^\\beta\n\nwhen 0&lt;α⋅β&lt;1: decreasing returns to scale\nwhen α≥1: Isoquant is bowed in the wrong direction\n\n→ Use only one of the inputs (corner solution)\n\n\nwhen 0&lt;α&lt;1 but β≥α: Isoquant may or may not be bowed in the wrong direction.\n\n→ May or may not use only one input (corner solution)\n\n\n\nExample plot to demonstrate this\nManipulate[\n Plot3D[(l^alpha + k^alpha)^beta, {l, 0, 5}, {k, 0, 5}, \n  AxesLabel -&gt; {&quot;l&quot;, &quot;k&quot;, &quot;x&quot;}, PlotRange -&gt; All], {alpha, 0.1, \n  5}, {beta, 0.1, 5}]"},"Proletatriat-(Marxism)":{"title":"Proletatriat (Marxism)","links":[],"tags":["Philosophy/Marxism"],"content":"Defining the Proletatriat §\nMarx “calling into being” the class of the proles. The process of their organization organically produces the communist party as a political power.\n\nThe proletariat goes through various stages of development. With its birth begins its struggle with the bourgeoisie\n\n\nDisorganized proles\nOrganized by BZ [=companies, etc.]\n\n\nAt this stage, the labourers still form an incoherent mass scattered over the whole country, and broken up by their mutual competition\n\n\nOrganized Unions\nSeparate, permanent unions to fight for wage\n\n\nNow and then the workers are victorious, but only for a time. The real fruit of their battles lies, not in the immediate result, but in the ever expanding union of the workers.\n\n\nNationally organized communist community\n\n\nIt was just this contact that was needed to centralise the numerous local struggles, all of the same character, into one national struggle between classes.\n\n\nUltimately the formation of the communist party\n\n\nThis organisation of the proletarians into a class, and, consequently into a political party\n\nIt is unfortunately for the bz that the proles—the class whose founding was caused by capital—will eventually be the ones to destroy them:\n\nBut not only has the bourgeoisie forged the weapons that bring death to itself; it has also called into existence the men who are to wield those weapons—the modern working class—the proletarians.\n\n\nWhat the bourgeoisie therefore produces, above all, are its own grave-diggers. Its fall and the victory of the proletariat are equally inevitable.\n"},"Proof-Techniques":{"title":"Proof Techniques","links":["Mathematical-Induction"],"tags":["Math"],"content":"\nMathematical Induction\nBy Contradiction\nBy Contrapositive\n\nAlgebraic\n\nYou can split summations (even infinite ones)\n∏i​eXi​=e∑i​Xi​\nEquivalently, ln(∏i​Xi​)=∑i​(lnXi​) Pushing log thru product makes it a sum\nTelescoping sums and products\n\nUseful when moveing\n\n\nBreak up summations ∑∀i​=∑i≤k​+∑i&gt;k​\nSummation of Squared ∑∀i​​(ai​+bi​)2=∑a2+∑b2+∑i&lt;j​2ai​bi​i=j∑​ai​bi​​​\n\nProbability\n\nTower property E(X)=E(E(X∣F))\nLinearity of expectation. You can switch expectation and summation\nDefinition of expectation is an integral (or summation)\nTwo RVs are identical if they have the same CDF.1\n\nCalculus\n\n∫x​∫y​dydx=∫y​∫x​dxdy switch integrals\n\nInequalities\n\nSum greater than max\nMax greater than averages\nAverage greater than min.\nCombined:\n\n sum i=1∑n​xi​​​≥max(x1​,…,xn​)≥ average n1​i=1∑n​xi​​​≥min(x1​,…,xn​)\nln(1−x)≤−x always.\nFootnotes §\n\n\nWhy CDF? statistics - Why is there a preference to use the cumulative distribution function to characterise a random variable instead of the probability density function? - Mathematics Stack Exchange ↩\n\n\n"},"Property-Exchange":{"title":"Property Exchange","links":["Directed-Graph"],"tags":["Economics/Game-Theory"],"content":"Motivation. Imagine a situation where n people each have their own house. They would like to exchange houses to be better off. How do we go about doing that?\nIn this setting we say:\n\nPreferences are strict (no agent is indifferent about two different items)\n\ndef. Stable allocation (=“in the core”). An allocation a is stable there is no blocking coalition.\nA blocking coalition for allocation a is a subset of agents who, when trading amongest themselves, achieves a higher utility for at least one agent, and doesn’t reduce the utility of anybody else in the subset. \ndef. Top Trading Cycles (TTC) algorithm.\n\nAlgorithm is in multiple rounds\nRound k:\n\nAgents point to favorite remaining house\nThis graph will always have a cycle, because all edges have out-degree 1 (thm)\nTake an arbitrary cycle, and swap houses according to that cycle\nEliminate these people from the graph\n\n\nRepeat\n\nthm. (TTC is strategy-proof w.r.t. individual agents). let allocation A be an allocation generated by TTC. Fixing all other agent’s preferences, no one agent can benefit by lying about their preference ordering.\nProof sketch by induction\n\nRound 1: Agents point to favorite house.\n\nLet those who win in round 1 be W1​; they don’t lie, because they will win their favorite house if they’re truthful.\nThose who lose, will proceed to the next round…\n\n\nRound 2: Agents not in W1​ (=losers) have not gotten their favorite house; let’s say a loser ℓ∈W2​ (a round 2 winner) got their k-th favorite house as a result of the second round.\n\nSay ℓ prefers a house from someone in W1​. ℓ will lie and point to that house to attempt to be included in the cycle.\nHowever, nobody in W1​ pointed to our loser. Therefore the loser will not be able to form a cycle, as long as everybody else is truthful.\n\n\nInduction until final round (assume round 1..k is truthful, then round k+1 is truthful. ■\n\nthm. (TTC is stable). Allocation A generated by TTC has no blocking coalition.\nProof by contradiction. Suppose there was a blocking coalition they are the black dots. We list them by the rounds in which they won in TTC: W1​,W2​,…\n\n\nIf there is a blocking coalition of agents separated in two disparate rounds of TTC i,j such that Wi​,Wj​,i&lt;j, then the coalition’s alternative trading cycle must have an agent who, in the informal trade, pointed from round i to round j. This doesn’t make sense, because they chose their favorite in round i already and won. Thus such a coalition cannot exist.\n\nIt is trivial to see also that a coalition spread across three or more rounds of TTC is impossible.\n\n\nIf there is a blocking coalition of agents who are all in Wk​, the TTC algorithm already trades amongst them. Thus such a coalition cannot exist.\n■\nthm. (TTC is the only stable algorithm) let A be allocation by the TTC algorithm. There exists no other allocation A′ that is stable. (=TTC is the only allocation in the core.)\nProof by Induction and Contradiction. Suppose TTC produced allocation A. Presume there was an alternative allocation A′ that was also stable. Let the agents whose allocation differs between A and A′ be S, the “switchers”. \nRound 1: Let the switchers in W1​ be S1​⊆W1​. Every switcher s∈S1​ would gotten switched to an alternative house in a latter round. But since in the TTC s pointed to another agent in W1​, this is a contradiction. Therefore, no switcher exists in S1​.\nRound 2: Let the switchers in W2​ be S2​. Switcher s∈S2​ cannot have switched with anybody in W1​ because there are no switchers in W1​. (equivalent)\nInduction until final round (assume there are no switchers until round i, then any switcher in round i+1 must have switched with latter round, but impossible.) ■\n\nKidney Exchange §\nMotivation. Let’s think of an alternative situation. A patient needs a kidney, and the patient’s family or friend wants to donate the kidney to them. However the donor’s may not be compatible with the patient. Therefore we need to find other compatible donor(-patient pair) to do a cross-kidney swap.\n\nWhen there are lots of patient-donor pairs, we need to find a trading cycle with compatible donors. We model the situation such that each patient-donor entity has a compatibility list for all other patient-donor pairs. Think of each patient-donor as one entity. \nHowever, Unlike the swapping housing example above we cannot trade along long cycles because we need to operate on all patients simultaneously which is not feasible. Therefore, we only consider a matching; i.e. a pairs of patient-donors do a cross-kidney swap. This is equivalent an (unpartitioned) maximum graph matching problem. Construct an undirected graph:\n\nAll entities point to any subset of compatible entities (obviously not themselves)\nIf both entities point to each other, then create an undirected edge. Remove all other directed edges.\ndef Priority Matching. On an undirected graph G:\nRandomly fix an ordering of agents.\nlet M0​:= set of all possible maximum matchings on G.\n\nObviously, these maximum matchings are of the same degree (=have same number of edge matchings in them)\n\n\nFor i=1…n for every entity:\n\nZi​ ← subset of Mi−1​ that includes agent i in the matching\nIf Zi​=∅ then Mi​← Mi−1​ (if i cannot be included ever, then ignore i)\nIf Zi​=∅ then Mi​ ← Zi​ (make sure i is included in all subsequent matchings)\n\n\nThis will result in Mn​. Every matching m∈Mn​ is a matching.\n\nthm. (PM matchings always match same agents.) All matchings Mn​ returned by PM matches the same agents.\nProof Sketch. Inside the loop:\n\nIf Zi​=∅ then agent i is not included in any m.\nIf Zi​=∅ then agent i is definitely included in all m.\n\nthm. (PM is DSIC) In PM, all agents will always report all of their compatible partners.\nProof Sketch. Reporting a lesser number of edges will only reduce the edges they are connected to, and thus only makes it less possible to be included in a matching."},"Prospectus":{"title":"Prospectus","links":[],"tags":["Courses"],"content":"Topic: Queer culture and sexual space in Tokyo, and their interaction with formal and informal mainstream cultural institutions, through the lens of linguistic anthropology.\n\nQueer culture is not yet accepted as mainstream in Japan, which still has heteronormative and bourgiosie family structure as the default\nBut queer acceptance is making headway through general regard for empathy which I will examine as “Amae”\n\nIdea that Japanese society operates on a culture of “openness” and “reliance,” a stronger motivation than capitalism for Japan yet\nRecently enshirned queer legislation (legal change in gender, same-sex marriage) includes these types of language that emphasize “harmony” and “acceptance” and “feelings of safety”—both a win in that mainstream Japan is willing to accommodate queer people, but also at the cost of normalizing and harmoginizing a diverse set of identities\n\n\nQueer people and organizations have mixed feelings about this. On one hand this is celebrated, but on the other hand it also opens up different avenues of exclusion, based on “making people uncomfortable.”\n\nQueer organizations in Tokyo have existed for long. In most cases it is transgender women or effeminate gay men\n\nEstablished neighborhoods like Shinjuku-nicho-me\n\n\nQueer people have always sought a space, but in a different way than the western narrative of “moving to the city where one can be free”—the city to most Japanese queer people is still repressive and harmoginizing\n\nThere is simultaneous desire to be free from this harmoginizing force and also to be accepted into the mainstream\n\n\nQueer celebrities have been on mainstream television (Matsuko Deluxe) but have been if not controversial, made fun of\n\nThe responses of these queer people to such legislation cause\n\n\n\n\n\nSources\n\n三橋順子 Mitsuhashi Junko—新宿「性なる街」の歴史地理 “Shinjuku’s History and Geography of A Sexual Neighborhood”\n土居健郎 Doi Takeo—Amae no Kouzou \nYue, Leung—Notes towards the queer Asian city\nGraham Kolbeins—Queer Japan\n"},"Pushdown-Automata":{"title":"Pushdown Automata","links":[],"tags":["Computing/Formal-Languages"],"content":"def. Pushdown Autotmaton (PDA)\nM=(Q,Σ,Γ,δ,q0​,z,F)\nWhere:\n\nΓ: stack alphabet\nz: stack bottom market\nδ is a transition function where for q,q′∈Q,σ∈Σ,γ∈Γ\n\n(q,σ,γ)↦{(q′,γ∗),...}\n…where the destination set is a finite set.\n…and strings can be accepted by either final state xor an empty stack (equivalent)\n\n\n                  \n                  We’re going to deal mostly with nondeterministic pushdown automata (NPDA), as they are more useful. \n                  \n                \n\nDescribed mechanically:\n\nInput tape is read only once, left to right\nA read head has finite number of states\nA read head has a stack from which the top letter can be read off\n\n\ndef. the current Configuration of a PDA is described as a tuple\n(q,aw,bx)⊢(q′,w,x)\nwhere ⊢ represents a single step of a PDA.\n\n\n                  \n                  final state or an empty stack; these two definitions are equivalent.\n                  \n                \n"},"Python-Common-Operations":{"title":"Python Common Operations","links":[],"tags":["Computing","Computing/Algorithms"],"content":"\nInitialize integer as positive or negative infinity: {python} i: int = float(&#039;-inf&#039;|&#039;inf&#039;)\n\nCollections §\nSets §\n\nSet difference{python} set(list1) - set(list2)\n\nO(n)\n\n\nSet lookup {python}i in {1, 2, 3}\n\n==Constant time O(1) lookup==\n\n\n\nList §\n\nList Sort {python} list2 = sorted(list1)\n\nO(nlogn)\n\n\nDeduplication{python} list1 = list(set(list1))\n\nO(n)\n\n\nCounter {python}Counter(&quot;mississippi&quot;)\n\nreturns {python}{&#039;i&#039;: 4, &#039;s&#039;: 4, &#039;p&#039;: 2, &#039;m&#039;: 1}\nO(n)\n\n\nAccumulate{python} accumulate(list1)\n\nO(n)\n\n\n\nDictionary §\n\nDefault value for dictionary entries{python}dictionary = collections.defaultdict(list|int)\nHelps when you have a list of dicts, or accumulating a count in a dict.\n\nAll conditions true §\ndef is_increasing(lst):\n    return all(earlier &lt; later for earlier, later in zip(lst, lst[1:]))\n \n# Example usage:\nmy_list = [1, 2, 3, 4, 5]\nprint(is_increasing(my_list))  # Output: True"},"Python-Idioms":{"title":"Python Idioms","links":[],"tags":["Computing/Linguistics/Python"],"content":""},"Python-Package-Management":{"title":"Python Package Management","links":[],"tags":["Computing/Linguistics/Python"],"content":"Python is famous for having many package and environment managers.\n\npip is the default, index: PyPI (python package index)\nconda also does other language dependencies, index: Anaconda Repo, conda-forge\nPython environment is a directory with:\n\n\nBinaries in bin: interpreter\nStandard lib in lib\n3rd party packages in site-packages\n\nwhen you are making a python package you also\nYou can {python}import packages (directory) or modules (single file)\n\n\n\n\nPython packages have directory structures under mypackage\n\npyproject.toml or setup.py: package name &amp; build configuration\nsrc/mypackage\n\n__init__.py is required to make it a package\n__main__.py is optional, allows running by python -m mypackage\n./subpackage to be imported like {python}import mypackage.subpackage\n\n\n\n\nPython modules are the single file mymodule.py anc imported {python}import mymodule\n"},"Quadratic-Variation":{"title":"Quadratic Variation","links":[],"tags":["Math/Calculus"],"content":"def. Total Variation of a function f in interval (a,b) is defined:\nVab​(f):=∣P∣→0lim​k=0∑n−1​∣f(xk+1​)−f(xk​)∣\nwhere P is the mesh size. Formally, the maximum mesh size. P→0 means we are chopping up more finely.\nVisualization. Consider sin(x) in interval (0,2π).\n\ndef. Quadratic Variation of stochastic process {Xt​}t≥0​ (on a probability space) in interval (0,t) is:\n[X]t​=P→0lim​k=1∑n​(Xtk​​−Xtk−1​​)2\nwhere P is also the mesh size as in total variation.\n\nIntuition. [X]t​ is thought of as a measure of how much energy or ADHD a function has in interval (0,t)\nQuadratic variation is also a function itself indexed by t.\n"},"Quantitative-Problem-Solving-Tips-1":{"title":"Quantitative Problem Solving Tips 1","links":[],"tags":["Math","Computing","Meta-Learning"],"content":"\nFind the invariant.\n\nWhat does not change? e.g. radius in a circle\n\n\nFind identities.\n\nCan we keep the problem, but invert our perspective?\n\n\n"},"Quantitative-Problem-Solving-Tips":{"title":"Quantitative Problem Solving Tips","links":[],"tags":["Math","Computing","Meta-Learning"],"content":"\nFind the invariant.\n\nWhat does not change? e.g. radius in a circle\n\n\nFind identities.\n\nCan we keep the problem, but invert our perspective?\n\n\n"},"Random-Variable":{"title":"Random Variable","links":[],"tags":["Math/Probability"],"content":"def. Random Variable A Random Variable X from probability space (Ω,F,P) to measurable space (R,B) is a function:\nX:Ω→R\n\nB is the Borel sigma-algebra on real numbers (=Borel set)\n\nThis is a set of open intervals on R that satisfies σ-algebra properties\n\n\nfor all B∈B, it is true that X−1(B)={ω∈Ω∣X(w)∈B}∈F\n\nYou’re given an open interval (=set) B on real numbers\nGet all ω such that X(ω) is within this range (=pre-image)\nThis set (a subset of Ω) must be in the sigma-algebra of Ω\nThis applies to any range B=(a,b)\n&amp; We can then say ”X is F-measurable” or ”F has enough information to measure X“.\nIntuition. Let there be other sigma-algebras G,H such that G⊂F⊂H, in addition to X being F-measurable. In this case…\n\nX is H-measurable because H has more information than F.\nHowever, X is not G-measurable because G has less information than F.\n\n\n\n\n\ndef. Probability on a Random Variable. Probability function on random variable X is a function PX​:R→[0,1] such that:\nPX​(B):=P(X−1(B))=P({ω∈Ω∣X(ω)∈A})\n\n&amp; i.e., the probability we encounter daily P(X…) is simply a shorthand notation PX​=P∘X−1\nYou put the interval B, and get the probability of all ω in that interval’s pre-image\nthus by abuse of notation we write: PX​(B)≡P(X∈B)≡P(X−1(B))\n\nExample. Let Ω is the trajectory of a coin toss. This is a very big set, and probably also infinite. On the other hand, let X a random variable for\nX={10​if headsif tails​\n\nThis is much simpler and more useful.\nThis means X−1(1)={All trajectories that land in heads}.\nIf X−1∈F, then we can try to measure the probability of P(X−1(1))=P({All trajectories that land in heads})\nBecause we know the pre-image of 1 is in F, we know that P(X−1(1)) is defined.\n\nthm. Addition Rule for Random Variables. For a discrete random variable X:\nP(a≤X≤b)=k=a∑b​P(X=k)\nFunctions of Random Variables §\nMotivation. Functions can be made of random variables; for example let Y=∣X−1∣. In order to investigate Y, need a way to derive the probability distribution of Y from X.\nExample. Let Y be a random variable defined by a function of another random variable X; Y=f(X). Then:\nP(Y=y)=P(f(X)=y)=all x s.t. f(x)=y∑​P(X=x)\nthm. Random variables X,Y are equal when:\n\nRange(X)=Range(Y)\n∀k∈Range​P(X=k)=P(Y=k)\n\nIndicator Functions §\ndef. Indicator Functions. For event A⊂Ω, the indicator function IA​ is a random variable (i.e. function) such that:\nIA​:Ω→{0,1}s.t.IA​(ω)={01​ω∈/Aω∈A​\nRemark.\n\nIndicator functions are useful in probability for solving problems, not for being a fundamental mathematical object.\nRemember that indicator functions are also random variables. All the rules for random variables apply, including the identities for expected values.\n\nProperties. let I an indicator function describing an event with probability p, then:\n\nE(I)=p\nVar(I)=p(1−p)\n"},"Rationality-(Economics)":{"title":"Rationality (Economics)","links":["Utility-Function","Monotonic-Transformation","HD","Marginal-Rate-of-Substitution-(MRS)"],"tags":["Economics"],"content":"Microeconomics Assumptions §\ndef. Rational Preference. Consider two basket of goods A,B. A rational preference is one that is all of the following:\n\nComplete: either A≻B,B≻A,or A∼B\n\nComparable: You have to choose either of the above three\n\n\nTransitive: if A≻B and B≻C, then A≻C\nMonotonic: x1​⪰x2​\n\nThe following are optional conditions\n\n\nConvexity: Averages are bette than extremes. (Preference of variety)\n\n\nThe rational preference assumption and the convexity assumption will together be enough to define an Utility Function.\n\n\nTwo tastes are same if they have the same utility function, or the utility functions are Monotonic Transformations of one another.\n\n\nNotation §\n\nA∼B: A is equally preferable with B\nA≻B: A is strictly preferred to B\nA≿B: A is equally or more preferable to B\nA∼B: A is equally preferred to B (indifferent)\n\ndef. Homothetic Tastes. Two equivalent definitions\n\nTastes are homothetic if the utility function (or its Monotonic Transformation) is HD of degree k&gt;0\nTastes are homothetic if the MRS depends only on x1​x2​​\n\n​u(x1​,x2​) is homogenous of degree k⟹u(tx1​,tx2​)=tku(x1​,x2​)​​\n\nQuasilinear Taste §"},"Readers-Writer-Locking":{"title":"Readers-Writer Locking","links":[],"tags":["Computing/Algorithms"],"content":"Solution to concurrency control problem.\n\nThere are two locks: read lock and write lock\nOnly those with a read lock can read, and write lock can write\nThere must always be either\n\nMany reader locks, no writer lock\nOne writer lock, no reader lock\n\n\n"},"Real-Business-Cycle-Model":{"title":"Real Business Cycle Model","links":["Intertemporal-Consumption-Leisure-Optimization-(Full-General-Equilibrium)"],"tags":["Economics/Macro-Economics"],"content":"Motivation. The real business cycle model asserts that all the cycles are caused by changes in z or:\ndef. Solow Residual (=Total Factor Productivity, TFP). Simply defined as ln(z) from the Intertemporal Consumption-Leisure Optimization (Full General Equilibrium) model.\nImproving Our General Equilibirum Model §\nBefore proceeding, however, we need to add a few more items into the variables that affect our general equilibrium. From General Equilibirum Functions we have:\n\nLabor market\n\nNs(w⊕,r⊕,W⊖)\nNd(w⊖,z⊕)\n\n\nGoods market\n\nYs=z⊕F(K,N⋆⊕LR(w,r⊕LR,W))\nYd=Cd(r⊖,…)+I(r⊖,…)+G\nWe will now consider a change in z′ (not z) and its effects:\n\n\n\nInvestment §\nOn Yd via I, investment decisions. Expand the optimal investment decision condition:\nMPKt+1​​−dzt+1​α Law of Motion Kt+1α−1​​​Nt+11−α​zt+1​↑​α​(1−d)Kt​+I↑1​​1−α​=r=r+d=r+d​​\nObserve that for the optimal investment decision, as zt+1​ increases, I has to increase to too balance the equation, as r,d,Kt​ is held the same. Also note that MPKt+1​​ also increases\nIntuition. Firms want to invest in more capital since productivity of capital in the future will increase.\nConsumption §\nOn Yd via C, current consumption decisions. The firm prices the optimal future wage as:\nwt+1​↑​=∂Nt+1​∂Yt+1​​​MPNt+1​​​=(1−α)zt+1​↑​Kt+1α​Nt+1−α​\nProfits (which go to HHs eventually) also increase:\nπt+1​↑​=zt+1​↑​F(Kt+1​)−wt+1​↑​Nt+1​+(1−d)Kt+1​\nIntuition. As lifetime wealth increases, consumption increases too, with the same reasoning as shown here.\nLaborx §\nOn Nts​. As lifetime wealth increases, consumers reduce their working hours, with the same reasoning as shown here again.\nEffects of Current and Future z Increase §\n\n\nDirect Effects (zt​ increases)\n\nProduction function increases\nYts​ increases as firms take advantage of more efficiency\nNtd​ increases as firms produce more\n\n\nSecondary Effects (zt+1​,wt+1​ increases)\n\nYtd​ increases as explained above (via C,I)\nNts​ decreases as consumers’ lifetime wealth increases\n\n\nEquilibirum condition achieved.\n\nWages are higher\nAssume direct effects are stronger than indirect effects then\n… r1​&gt;r2​ interest rates fall\n… N1​&lt;N2​ employment rises\n\n\n\nUtility Function Change §"},"Recurrence-Relation":{"title":"Recurrence Relation","links":["most"],"tags":["Computing/Algorithms"],"content":"Table of Common Recurrence Relations §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecurrenceAlgorithmSolutionT(n)=T(2n​)+O(1)binary searchO(logn)T(n)=T(n−1)+O(1)sequential searchO(n)T(n)=2T(2n​)+O(1)tree traversalO(n)T(n)=T(2n​)+O(n)quick selectO(n)T(n)=2T(2n​)+O(n)mergesort, quicksortO(nlogn)T(n)=T(n−1)+O(n)Insertion or selection sortO(n2)\nQuick Bits §\n\nT(n)≤T(a⋅n)+T(b⋅n)+cn where a+b&lt;1 is almost always O(n).\n\nMaster Theorem §\nSee Master theorem (analysis of algorithms) - Wikiwand\n⇒ Can be used to solve most recurrence relations.\nFirst, Identify the recurrence relation in the following form\nT(n)=a⋅T(bn​)+O(nclogkn)\nConsider three casesp\n\nCase 1: logb​a&lt;c\n\n⇒ T(n)=Θ(nc)\n\n\nCase 2: logb​a=c\n\n⇒ T(n)=Θ(nclogk+1n)\n\n\nCase 3: logb​a&gt;c\n\n⇒ T(n)=Θ(nclogkn)\n\n\n\nRecurrence Tree §"},"Recursively-Enumerable-Languages":{"title":"Recursively Enumerable Languages","links":[],"tags":["Computing/Formal-Languages"],"content":"Section: Turing Machines\ndef. Language L is recursively enumerable iff there exists a TM M such that L=L(M).\n→ The class of languages that TMs can represent is a recursively enumerable language. You can think of languages which are “countable.”\ndef. Language L is recursive iff there exists a TM M which, for every string w in L, it halts.\n→ Recursive Languages are languages in which a TM always halts.\n\nDistinction between RE and R languages.\n\n\nlemma. If S is a countable set, then 2S [= the set of all subsets of S] may not be countable.\nthm. There exists languages over alphabet Σ that are not recursively enumerable. (proof using lemma.)\nthm. If L is recursively enumerable, Lˉ may not be RE.\nthm. If L and Lˉ are both RE, then L and Lˉ is Recursive.\nthm. If L is recursive, Lˉ is recursive."},"Reduced-Price-of-Capital-Goods":{"title":"Reduced Price of Capital Goods","links":[],"tags":["Economics/Game-Theory"],"content":"The most significant thing about technological improvement is equality.\nNobody cares if you are or aren’t from a noble family; if you have money it’s fine.\n\nThink—pride and prejudice society was rejecting those who have gained prestige with just money.\nThink also: discussion with mom, when she says “technological improvement is irrelevant; where there are people, there is love and relationship.” Only those who are lucky enough to be born in the modern age has that prestige of emotion, etc. Also think: the hindsight paradox.\n\n(DevonThink) My Ordinary Life: Improvements Since the 1990s · Gwern.net (reader mode)"},"Reductionism":{"title":"Reductionism","links":["Emergent-Phenomena"],"tags":["Philosophy/Analytic"],"content":"hyp. Limitations of Reductionism. While reductionism is a useful tool for predicting the behavior of individual components of a system, it falls short in explaining Emergent Phenomena and the complex behavior of systems as a whole. To gain a more complete understanding of the world, it is necessary to incorporate multiple perspectives and embrace the complexity of Emergent Phenomena.\n\n\n                  \n                  analytic truth. It is what we know to be true of the universe through empirical methods of arriving at truth [=ways of knowing]. This is where the ball stops for practitioners of Newton’s Flaming Laser Sword.\n                  \n                \n\n\nMr. Marsh: “You can’t measure it, though; is there an experiment?”\n"},"Refactoring-Reduces-Cognitive-Load":{"title":"Refactoring Reduces Cognitive Load","links":["Working-Memory"],"tags":["Computing"],"content":"Refactoring is useful in coding because it reduces the cognitive load to Working Memory."},"Refinancing":{"title":"Refinancing","links":["Monetary-Policy"],"tags":["Economics/Finance"],"content":"e.g. refinancing a mortgage\n→ When the Monetary Policy decreases and therefore the general interest rate decreases, your mortgage payments can be payed off by another new loan you take."},"Regression-Discontinuity":{"title":"Regression Discontinuity","links":["Dummy-Variables","Controlled-Experiments"],"tags":["Math/Statistics"],"content":"Regression Discontinuity analysis is useful when\n\nThe assignment variable (≈independent variable) is continuous (not binary)\n\n“Treatment” thus simply means after-cutoff\nWe can’t just use Difference of Means because the assignment variable continuously changes in each groups too and thus affects the dependent variable too.\n\n\nNo other factors change when crossing the cutoff\nThen we can use the basic (constant-slopes) RD model:\n\nYi​=β0​+β1​ treatment var. Ti​​​+β2​ assignment var. (X1i​−C)​​+ϵi​\nwhere:\n\n\nC is the cutoff point\n\n\nT is 1 for post-cutoff, 0 for pre-cutoff. Simulates “treatment”\n\n\nX1​ is the assignment variable (non-adjusted).\nwhere coefficients mean:\n\n\nβ1​^​ is the causality of treatment\n\n\nβ2​^​ is the causality of the assignment var\n\n\n\nError term shouldn’t be jumping at the cutoff, i.e. we should have ρX1​,ϵ​=0\n\nIf ρX1​,ϵ​=0 then ϵ=ρX1i​+νi​ thus the basic RD model from above becomes:\n\n\n\nYi​=β0​+β1​Ti​+β2​(X1i​−C)+ρX1i​+νi​\n- → Thus even if $\\rho_{X_{1},\\epsilon}\\neq 0$, $\\hat{\\beta_{1}}$ is unbiased (i.e. still correctly indicates the causality between treatment and $Y$)!\n- But $\\hat{\\beta_{2}}$ no longer indicates causality between $X_{1}$ and $Y_{i}$, instead just indicates. overall correlation.\n\nAdvanced RD §\nWe don’t need to limit ourselves to have the slopes be same before and after discontinuity by using the varying slopes model:\nYi​= intercept before β0​​​+β1​Ti​​ intercept after ​+ slope before β2​​​(X1i​−C)+ slope after β3​​​(X1i​−C)×Ti​+ϵi​\nSmaller windows → probably linear\nIssues with RD Analysis §\n\nSmaller window (=bandwidth): We must look at variables close to the cutoff (because the farther away you go, the more endogenity there might be) But this isn’t always possible because of limited sample size.\nProbably only estimates the Local Average Treatment Effect (LATE), meaning that you can’t generalize the results. (“Are effects of drinking (vial legal age) on grades affect babies? old econometrics professors?“)\nMultiple variables usually determine treatment or not. (medicare and age is clear-cut; SAT and college admission isn’t) → use Fuzzy RD model\n\nOr, we can use the Balace Test to see if the side of the cutoff is truly random\n\n\nError term jumps at discontinuity (the issue from above)\n\nTo check, make sure the frequency of the assignment variable is smooth at cutoff (e.g. the number of people (=samples) with SAT scores just below 1500 and just above 1500 isn’t too different from the rest of the grade.)\n\nIf there is, it might mean that people under 1500 wanted to get into colleges with cutoff score at 1500, so studied a little bit harder.\n\n\nVisualization. \nAnother way to check this is to run regression between covariate (which we suspect to be in the error term) and the RD model: X2i​=γ0​+ jump γ1​​​Ti​+γ2​(X1i​−C)+νi​\n\nA Statistically significant γ1​ indicates X2​ is in the error term\n\n\n\n\n"},"Regular-Expressions":{"title":"Regular Expressions","links":["Finite-Automata","Regular-Languages"],"tags":["Computing/Formal-Languages"],"content":"Section: Finite Automata = Regular Languages\nRegular expressions have three operators:\n+  ∗  ∘  ​UnionZero or MoreConcat​\n\nBegin with ϕ,λ,Σ∈RegEx\nAll above operators on the three, and the alphabet, is a regular expression\n\n\n\n                  \n                  Example \n                  \n                \n→ Odd number of a’s, and then even number of b’s: a(aa)∗(bb)∗\n→ 3 or less a’s and ends in ab: b∗(a+λ)b∗(a+λ)b∗ab\n\nRegEx ≡ DFA §\npf. RegEx ⇒ DFA\n\nFirst define the building blocks\n\n\nRepresentation of ϕ\nRepresentation of {a}\n\nRepresentation of λ\n\n\nThen buid on them recursively:\n\n\npf. DFA ⇒ RegEx\n\nWith a DFA with one final state, convert into a generalized transition graph (GTG) [=edges can be a RegEx]\nIf the GTG has two states:\nUnique RegEx for this DFA: (rii∗​rij​rjj∗​rji​)∗rii∗​rij​rjj∗​\n\nIf GTG has three or more states it is in the form:\n\n\n"},"Regular-Grammar":{"title":"Regular Grammar","links":[],"tags":["Computing/Formal-Languages"],"content":"Section: Finite Automata\ndef. Right Linearity. A grammar G=(V,T,S,P) is right-linear iff all elements in the range of the production rules Range(P) only contain variables on the right side of the result; i.e.:\nS→aP∣aSP→a\n\n\n                  \n                  Observe that a right-linear grammer is same as being able to extend it only on the right. Left-linear grammar is vice versa \n                  \n                \n\ndef. A Regular Grammar is a grammar that is exclusively either right-linear or left-linear.\nRegGrammar ≡ DFA §\npf. Regular Grammar ⇒ DFA\nfor G=(V,T,S,P), for each rule in P, convert a variable into a state; arcs identify the rightside (or leftside) terminal variable\npf. DFA ⇒ Regular Grammar"},"Regular-Languages":{"title":"Regular Languages","links":["Finite-Automata"],"tags":["Computing/Formal-Languages"],"content":"Equivalent to Finite Automata\ndef. Regular Languages are languages that can be described by either\n\nFinite Automata\nRegular Grammar\nRegular Expressions\n\nClosure of Regular Languages §\nRegular languages are closed under the following operations:\n\nUnion, Intersection\nConcatenation, Star\nCompliment\nQuotient\nL1​/L2​={w∣∃x∈L2​ s.t. wx∈L1​} ← left quotient\nL1​\\L2​={w∣∃x∈L2​ s.t. xw∈L1​} ← right quotient\nHomomorphism\n\nHomomorphism is a function h s.t.:\n\n\n\nh:\\Sigma\\mapsto\\Gamma^*\n$$So: $w=a_1a_2…a_n \\Rightarrow$ $h(w)=h(a_1)h(a_2)…h(a_n)$\n\n## Pumping Lemma and Disproving Regularity\n\nlem. **Pumping Lemma.** let regular language $L$, and $w\\in L$ whose length is $m$ or greater. Then $w$ can be decomposed s.t.:\n\n1. $w=x\\circ y \\circ z$\n2. $xy$ has a length no larger than pumping length $m$\n3. $y$ is not an empty string\n\nIn this case, a newly constructed string $w’=x\\circ y^i \\circ z$ must also be in language $L$.\n\nProof by using the pumping lemma should be:\n\n1. Choose **any** single string $w$ in the language and express it in terms of a positive int. $m$\n2. Decompose the string into $x,y,z$, **where** $|xy|\\leq m$\n3. Show that $x\\circ y \\circ z$ is not in $L$, for a **certain** $i=0,1,…$\n   → i.e. pick an i, and show that it doesn’t work if m is positive\n4. Contradiction!\n\n---\n\nIn general, you can disprove regular languages by knowing\n\n- Finite languages are always regular\n- the Pumping Lemma"},"Reification":{"title":"Reification","links":[],"tags":["Philosophy/Marxism"],"content":""},"Relational-Algebra":{"title":"Relational Algebra","links":["Turing-Machine","Database-Management-System","Monotonic-Transformation"],"tags":["Computing/Data-Science"],"content":"(DevonThink) 2. Relational Algebra\nMathematical formulation of operations on relational data.\n\nMade by E.F. Codd\nEquivalent to domain-independent relational calculus\n\nRelational calculus is a specialization of first-order logic\n\n\nIsn’t Turing-complete because it lacks recursion.\n\nBut because of this, relational algebra is decidable\n\n\nHigh level and declarative (procedural implementation is left to DBMS)\nEasy to optimize by the DBMS\n\nDefinitions §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDBMS jargonCommon-sense nameRelationTableAttributeColumnDomainData typeTupleRow\n\nA row is defined as a tuple\n\nDuplicate rows are not allowed\nA tuple is considered same if all attributes are same.\n\n\nSchema: the tuple of attribute names (≈class)\nInstance: the instantiation of the schema (≈object)\nKey (see below for formal definition):\n\ne.g. address(street_addr,city,state,zipcode)\n\n⇒ {street_addr,city,state} is a key\n\n..but {street_addr,state} is not a key\n\n\n⇒ {street_addr,zipcode is also a key\n\n\n\n\n\ndef. Superkey. A set of attributes K is a superkey of relation R if there exists a dependency:\n\ndef. Key. A superkey K is a key of relation R if there is no other superkey with smaller number of attribute elements. (=minimally identifies each tuple/row)\nalg. Determining if a superkey is a key.\n\nReduce one element and see if the key is superkey.\nIf a reduction of one element doesn’t yield a smaller superkey, we got it.\nIf it does, recurse.\n\n\nRelational Operators §\nFundamental Operations §\n\nSelection: σp​⋅R\n\n“Give me the entries of relation R that satisfies condition p”\n\n\nProjection: πL​⋅R\n\n“Give me the attribute L for each entry of relation R”\nDuplicates are filtered out\n\n\nCross Product: R×S\n\nSame as cross product of normal sets.\n⇒ Every row of R per every row of S\n\n\nUnion: R∪S\n\nR,S must have identical schema\nAll rows on R,S combined\nDuplicates are removed\n\n\nDifference: R−S\n\nR,S must have identical schema\nRows in R that are not in S\n\n\nRenaming: ρS​⋅R\n\nRename attribute S of relation R\n\n\n\nDerived Operators §\n\nJoin [=Theta Join]: ⋈p​\n\ndefinition: σp​(R×S)\n“Cross Product the relations, and then filter by condition p”\ni.e. “Join the two databases so based on condition”\n\n\nNatural-Join: ⋈\n\ndefinition: πL​(R⋈p​S)\n\nwhere p pairs common attributes\nwhere L is the union of the attribute names\n\n\n“Theta join, but the duplicate attribute is removed”\n\n\nOuter-Join\n\nLeft outer join: R ⟕ S (leave null entries from R)\nRight outer join: R ⟖ S (leave null entries from S)\nFull outer join: R ⟗ S (leave null entries from R,S both)\n\n\nIntersection: R∩S\n\nR,S must have identical schema\n\ndefined as R−(R−S)≡S−(S−R)\n\n\n\n\nSymmetric Difference: (R−S)∪(S−R)\n\nMonotonicity §\n(Vaguely related to: Monotonic Transformation)\ndef. Monotonic Operation. If input adds more rows, does the output also only add more rows? i.e. f is a monotone operator iff:\nR⊆R′⟹f(R)⊆f(R′)\n\nDifference (R−S) is the only non-monotone operator among simple operators\n\nMonotone for R but not for S\n\n\nIf an operation is not monotone, it must include the difference operator.\n\nExpression Tree Notation §\n\n\nSimpler to use and understand\nDesigned both bottom-up or top-down\n\nProperties of Relational Algebra §\n\n\nSelection Properties\n\nIdempotent: σA​(R)=σA​σA​(R)\nCommutative: σA​σB​(R)=σB​σA​(R)\nConjunction: σA∧B​(R)=σA​(σB​(R))=σB​(σA​(R))\nDisjunction: σA∨B​(R)=σA​(R)∪σB​(R)\nBreaking up: σA​(R×P)=σB∧C∧D​(R×P)=σD​(σB​(R)×σC​(P))\nDistribution over Difference: σA​(R∖P)=σA​(R)∖σA​(P)=σA​(R)∖P\nDistribution over Union: σA​(R∪P)=σA​(R)∪σA​(P)\nDistribution over Intersection: σA​(R∩P)=σA​(R)∩σA​(P)=σA​(R)∩P=R∩σA​(P)\n\n\n\nSelection and Projection Commutativity\n\nπa1​,…,an​​(σA​(R))=σA​(πa1​,…,an​​(R)) where fields in A⊆{a1​,…,an​}\n\n\n\nProjection Properties\n\nIdempotent: πa1​,…,an​​(πb1​,…,bm​​(R))=πa1​,…,an​​(R) where {a1​,…,an​}⊆{b1​,…,bm​}\nDistributing on Union: πa1​,…,an​​(R∪P)=πa1​,…,an​​(R)∪πa1​,…,an​​(P)\n! Projection doesn’t distribute along difference:\n\nπA​({⟨A=a,B=b⟩}∖{⟨A=a,B=b′⟩})={⟨A=a⟩}\nπA​({⟨A=a,B=b⟩})∖πA​({⟨A=a,B=b′⟩})=∅\n\n\n! Projectio doesn’t distribute along intersection:\n\nπA​({⟨A=a,B=b⟩}∩{⟨A=a,B=b′⟩})=∅\nπA​({⟨A=a,B=b⟩})∩πA​({⟨A=a,B=b′⟩})={⟨A=a⟩}\n\n\n\n\n\nRename Properties\n\nρa/b​(ρb/c​(R))=ρa/c​(R)\nρa/b​(ρc/d​(R))=ρc/d​(ρa/b​(R))\nDistribution on Difference: ρa/b​(R∖P)=ρa/b​(R)∖ρa/b​(P)\nDistribution on Union: ρa/b​(R∪P)=ρa/b​(R)∪ρa/b​(P)\nDistribution on Intersection: ρa/b​(R∩P)=ρa/b​(R)∩ρa/b​(P)\n\n\n\nProduct and Union\n\nCartesian Product and Union: (A×B)∪(A×C)=A×(B∪C)\n\n\n"},"Restricted-Boltzmann-Machines":{"title":"Restricted Boltzmann Machines","links":[],"tags":["Computing/Maching-Learning","Math/Statistics"],"content":"def. Restricted Boltzmann Machine.1 Let a partitioned neural network with visible layer x and hidden (=latent) layer h, with weight matrix W and biases c for visible layer and b for hidden layer. Then we can define a probability distribution over the whole network:\np(x,h)=Zexp(−E(x,h))​\nwhere E(x,h) is called the “Energy function”\nE(x,h)​=−h⊤Wx− data bais c⊤​​x− latent bias b⊤​​h​​\nIn most cases, each xi​,hj​∈{0,1} (Bernouilli–Bernouilli) or xi​∈R,hj​∈{0,1} (Gaussian–Bernouilli)\nConditional Inference. Given data x(realized) we obtain the distribution of hidden layer\np(h∣xrealized)p(hj​=1∣xrealized)​=j∏​p(hj​∣xrealized)=σ(bj​+ j-th row Wj,⋅​​​x)​​\nfrom which we can “sample” in the latent distribution to get hrealized.\nAlternatively, given a latent representation hrealized we obtain the distribution of generated “data”:\np(x∣hrealized)p(xj​=1∣hrealized)​=k∏​p(xk​∣hrealized)=σ(ck​+h⊤ k-th col W⋅,k​​​)​​\nTraining. We minimize the log-likelihood to model the data distribution p(x) to fit the data best:\nW,c,bmin​T1​∀t∑​−lnp(x(t))\nFootnotes §\n\n\nNeural networks [5.1] : Restricted Boltzmann machine - definition - YouTube ← this whole playlist is very useful. ↩\n\n\n"},"Revenue-Maximizing-Auctions":{"title":"Revenue-Maximizing Auctions","links":[],"tags":["Economics/Game-Theory"],"content":"Motivation. Sometimes we aim to maximize revenue instead of social welfare.\n\nIn this case, we assume that we know a little bit about the bidder’s private valuations—we know the distributions vi​∼Fi​. (we call this a baysian auction.)\n\nThis is because unless we do this, the revenue maximization is impossible, since it is hard to guarantee DSIC.)\n\n\nThen, in a DSIC auction M=(x(b),p(b)), the revenue maximization goal can be phrased as:\n\nmaxx,p​ E(∀i∑​pi​(v))\nIn order to do to this, we introduce a new concept, and as we will see later it will simplify the above problem.\ndef. Virtual Value. For a bidders’ true valuation vi​ whose CDF is Fi​(vi​), PDF is fi​(vi​), the virtual value is defined as:\nφ(vi​):= want to collect vi​​​− information penalty fi​(vi​)1−Fi​(vi​)​​​\nProperties.\n\nφ(vi​)≤vi​\nφ(vi​) may be negative\n\nIntuition. As annotated above, treat vi​ as the amount of money we would like to collect from bidder i. But this is discounted by a certain factor (=“information penalty”) because we know a bit of information about them—their distribution.\nWe can rephrase the above revenue maximization problem in terms of virtual value as the following:\nthm. (expected virtual welfare is expected revenue)\nE​ revenue ∀i∑​pi​(v)​​​=E​ virtual welfare ∀i∑​φi​(vi​)xi​(v)U​​​\nMotivation. Now the revenue maximization goal is simply to maximize expected virtual welfare. However, we still need to make sure the bidders are truthful, because we need to maximize over v.\n\n⇒ Using Myerson’s lemma, we can create auctions that are truthful. The following are conditions of Myerson’s lemma, applied in the context of virtual value.\nThen, we will give an example of this in a single-item many-bidder auction where these conditions hold.\n\ndef. Vickery Auction with Reserve \n\nCalculate virtual values φ1​,…,φn​ for all bidder’s bids b1​,…,bn​\nGive the item to the bidder with the highest virtual value φ1​, unless the highest bidder’s virtual value is below zero—φ1​&lt;0.\n\nThe the highest bidder’s virtual value is below zero, throw away the item. Auction is over.\n\n\nCalculate the reserve price r:=φ−1(0)\nCharge them max(v2​,r)—either the reserve price, or the second-price\nThis auction will maximize revenue.\n\nVisual Interpretation. An example of three different cases where different distributions lead to different virtual value\n\nthm. (General case of revenue maximization.)\n\nAssuming Fi​ is regular for every bidder i…\n\n\nTransform the truthfully reported valuation vi​ into corresponding virtual value φi​(vi​)\nAllocate by maximizing virtual welfare: x(v)=argmaxX​∑∀i​φi​(vi​)⋅xi​\nCharge payments p(v) according to the\n\ndef. Regular Distribution. A distribution is regular iff if its virtual value function is monotonic.\nthm. If all bidder’s values follows an i.i.d. regular distribution, Vickery Auction with Reserve is DSIC."},"Riemann–Stieltjes-integral":{"title":"Riemann–Stieltjes integral","links":[],"tags":["Math/Calculus"],"content":"def. Riemann-Stieltjes Integral. let functions f(x),g(x):R→R. Then the R-S integral with f(x) as the integrand and g(x) as the integrator is defined:\n∫ab​f(x)dg(x)=n→∞lim​i=0∑n​f(xi​)[g(xi+1​)−g(xi​)]​​\nwhere a=x0​≤⋯≤xn​=b\nVisualization. The value of the integral is the shadow projection of the purple fence (=from x−g plane to the intersecting line of f(x) and g(x)). The lower the slope of g(x), the denser the shadow is. See: Riemann–Stieltjes integral - Wikiwand\n"},"Risk-(Finance)":{"title":"Risk (Finance)","links":["Market-Risk","Variance","Bonds-(Finance)"],"tags":["Economics/Finance"],"content":"def. Risk is related to the uncertainty in payment back when giving a loan.\n\nExample of a riskless investment is investing into the bank upto the federal deposit insurance (~$100K)\nTypes of risk include:\n\nCredit Risk (=counterparty risk): the counterparty can fail to make the payment\nMarket Risk\n\nInterest Risk: the market interest rate might change\nInflation Risk: the inflation is high so the money isn’t worth much\n\n\nOperational Risk\n\n\nIncreased cash flow frequency usually means less risk.\n\ndef. Risk in finance is equivalent to Standard Deviation in mathematics.\n\nRisk is proportional to the duration of investment (See bond yield curve).\n\nBanks broadly have teams to manage:\n\nMarket Risk. How market changes will affect the bank’s positions\nCredit Risk.\n"},"Risk-Neutral-Assumption":{"title":"Risk-Neutral Assumption","links":["Present-Value-Calculations","No-Arbitrage"],"tags":["Economics/Finance"],"content":"Motivation. A risk-neutral world is where there is no risk premium on return. This implies:\n\nAll Present Value Calculations are done at the risk-free rate\nAll assets have risk-free rate of return (=zero riNo-Arbitrageitrage Condition]] holds\n&amp; This is not really realistic, but simplifies calculations a lot.\n\nRisk-Neutral Measure §\nA risk-neutral measure is a probability measure such that E(S0​)=ert, in other words, all securities have risk-free rate of return."},"Risk-Neutral-Derivation-of-BSM":{"title":"Risk-Neutral Derivation of BSM","links":[],"tags":["Economics/Finance"],"content":""},"Robert-Nozick":{"title":"Robert Nozick","links":[],"tags":["Economics/Game-Theory","People"],"content":""},"Rock-Paper-Scissors":{"title":"Rock Paper Scissors","links":[],"tags":["Economics/Game-Theory/Games"],"content":""},"Role-of-Money":{"title":"Role of Money","links":["Money-(Medium-of-Exchange)","Edgeworth-Box","Utility-Function","Future-Value-Calculations","Financial-Market","Use-value,-Exchange-value","Price-Mechanism","Balance-Sheet"],"tags":["Economics/Finance"],"content":"Distributive Justice Class notes for detailed info.\n\nMedium of Exchange. Solves the problem of the double coincidence of wants\n\nSee Money (Medium of Exchange)\nObserve the Edgeworth Box This is a full barter system based on Indifference Curves. This is impossible, so we need a convenient medium of exchange\n\n\nStore of Value. Functions as a claim on future consumption\n\nI give you the perishable food now ⇒ you give me a claim on future consumption. \nTime Value of Money arises ← due to risk aversion and instant gratification (human psychology)\nFinancial Markets use this to convert the storage of value into something actually useful and not just gold sitting in a vault\n\n\nUnit of Account. Helps quantify the value of things\n\nProblematic because Use value, Exchange value distinction\nAlso problematic because the Price Mechanism doesn’t give the pure value of things\n…but is still useful because you do need to write things down in Balance Sheets.\n\n\n"},"Roy's-Identity":{"title":"Roy's Identity","links":["Utility-Function","Uncompensated-Demand-curve"],"tags":["Economics/Micro-Economics"],"content":"thm. Roy’s Identity. Relates income, price, and Indirect Utility Function to a own-price demand function.\nx1​(p1​,p2​,I)≡−∂v/∂I∂v/∂p1​​x2​(p1​,p2​,I)≡−∂v/∂I∂v/∂p2​​​​"},"SQL-Basics":{"title":"SQL Basics","links":["Relational-Algebra","assets/Screenshot-2023-10-13-at-15.18.16.png"],"tags":["Computing/Data-Science","Documentation/Code-Journal","Computing/Linguistics/SQL"],"content":"Based on Relational Algebra §\nSQL is more readable than Relational Algebra.\nSPJ Query §\nSelect-Project-Join (SPJ) query. Is what almost all queries are. in the form of:\nπattr1, attr2, ...​(σcondition​(relation1×relation2×…))\n…is equivalent to…\nSELECT attr1, attr2, ...\nFROM relation1\nWHERE condition\n⇒ Think: you are checking {postgresql} condition on the whole cross product of relation1, relation2, …\nCanonical form of a single query\nSELECT ... AS ..., ... AS ...\nFROM Relation_1 r1, Relation_2 r2 -- Rename the table.\nWHERE ... AND ... -- Where condition\n\t(NOT) IN, (NOT) EXISTS, ALL, SOME (subtable) -- Nested Query\n\t-- exist doesn&#039;t need an attribute\nORDER BY ASC|DESC\nJoins, Unions and Differences §\n\nSELECT ...\nFROM ...\n-- Join query\n[INNER, OUTER] [LEFT, RIGHT, FULL] JOIN \n-- INNER JOIN equivalent to JOIN\n-- LEFT|RIGHT JOIN equivalent to LEFT|RIGHT OUTER JOIN\n(subtable) T -- rename \nON join_condition\n\n-- optional WHERE clause\nWHERE ...\n\n-- Set/Bag operation query\n(subtable) UNION, INTERSECT, EXCEPT (ALL) (subtable)\n\n\ntable references scoping is like other languages\nsubtables can be one tuple &amp; one column (=scalar), where you can do a direct comparison: {postgresql}WHERE age [&gt;=&lt;] (subtable)\n\n…but this will cause runtime error if the subtable has more than one value.\n\n\n{postgresql}SELECT DISTINCT to remove duplicates\n{postgresql}EXCEPT/INTERSECT/UNION ALL is a bag operation that removes by count; Example data\n\nConditionals §\nSELECT\nFROM\n&#039;string literal&#039; -- should be enclosed in single quotes\nWHERE string LIKE &#039;%foobar%&#039;-- pattern matching strings\n\n-- Null checking\nWHERE attr IS NULL\nWHERE attr IS NOT NULL\n-- etc.\nNull Handling Rules. §\n\n\n                  \n                  NULL and Unknown as a 0.5 value\n                  \n                \n\n\nComparing NULL with any value will result in UNKNOWN\nValues as numbers:\n\nTRUE=1\nFalse=0\nUNKNOWN=0.5\n\n\nOperations as functions:\n\n{postgresql}x AND y=min[x,y]\n{postgresql}x OR y=max[x,y]\n{postgresql}NOT x=1−x\n\n\n\nWhat about handling null results?\n\nSolution 1: {postgresql}SELECT COALESCE(col,0.0)\n\n⇒ if {postgresql}col = NULL, will return 0.0\n\n\nSolution 2: {postgresql}SELECT NULLIF(col,&#039;n/a&#039;)\n\n⇒ if {postgresql}col = &#039;n/a&#039;, will return NULL\n\n\n\nSQL Extensions §\nAggregation §\nSELECT COUNT(*)|COUNT(DISTINCT col)|AVG(col)|MIN(col)|MAX(col)\nFROM ...\nWHERE ...;\n\nAn aggregation in a HAVING clause applies only to the tuples of the group being tested.\nAny attribute of relations in the FROM clause may be aggregated in the HAVING clause, but only those attributes that are in the GROUP BY list may appear unaggregated in the HAVING clause (the same rule as for the SELECT clause).\n\nGrouping §\nSELECT aggr_func FROM [JOIN ON | UNION | ...] WHERE\n-- Group By\nGROUP BY col1, col2 -- will group if *both* columns are same\nHAVING condition_on_group\n\nCompute Order:\n\n{postgresql}GROUP BY\n`{postgresql}HAVING\n{postgresql}SELECT\n\n\naggr_func is computed for each group\n\ne.g. {postgreSQL}SELECT age, AVG(pop) FROM User GROUP BY age computes the average popularity for each age.\ne.g. {postgresql} SELECT uid, MAX(pop) FROM User is wrong ← aggregate function and column cannot be used together (which uid?)\ne.g. {postgresql}SELECT uid, age FROM User GROUP BY age is wrong ← which uid?\n“\n\n\n\nVariables §\nTwo forms of “variables”\n\nNamed Subqueries (will evaluate by macro expansion)\nViews (is a window into a table)\n\nProvides Logical data independence\n\n\n\n-- Named subqueries\nWITH subtable_name AS (subquery)\n\n-- Views\nCREATE VIEW view_name AS (subquery)\nDROP VIEW -- dropping a view\nData Input/Output §\nINSERT INTO ... (subquery) ...\nDELETE FROM ... [WHERE ...]\nUPDATE ... SET ... [WHERE ...]\nAdvanced Topics §\n\nTransaction\n\nAllows multiple queries to be treated as one (helps when making new tables with foreign key relations)\n\n\nObject-Relational Mapping\n\nConvert between object-oriented language’s object data into SQL-compatible tuples, and vice versa\n\n\nCasting\n\nConvert between datatypes {postgresql}CAST(num AS FLOAT)\n\n\n\nRecursion §\n\nYou can define new subtables with recursion\n\n{postgresql}WITH RECURSIVE subtable AS (recursive_query)\n\n\nSQL does fixed point recursion\n\n…i.e. recurses until the table doesn’t seem to change anymore\n\n\nLinear vs Non-linear recursion\n\nLinear: a single recursive call\nNon-linear: a tree-like recursion; multiple recursive calls\n\n\nMutual Recursion\n\nTwo tables are defined upon each other\n\n\n"},"SQL-Constraints":{"title":"SQL Constraints","links":[],"tags":["Computing/Data-Science"],"content":"Constraint Checking §\nSchema Declaration §\nCREATE TABLE User(\n\tuid INTEGER NOT NULL PRIMARY KEY, -- primary key\n\tage INTEGER NOT NULL UNIQUE -- not a key, but is unique\n\t\t    CHECK(age IS NULL OR age &gt; 0), -- constriant\n);\n\nCREATE TABLE Group(\n\tgid CHAR(10) NOT NULL PRIMARY KEY,\n\tname VARCHAR(100) NOT NULL UNIQUE\n);\n\nCREATE TABLE Member (\n\tuid INTEGER NOT NULL REFERENCES Group(uid), -- foreign key\n\tgid CHAR(10) NOT NULL, \n\tPRIMARY KEY(uid, gid) -- another way to write key\n\tFOREIGN KEY(gid) REFERENCES Group(gid) -- anoter foreign key\n);\n\nWhen constraint check fails during a query, will reject (maybe cascade, or set null)\nDeferred constraint checking: constraint check only happens for a full declaration or transaction\n{postgresql}CHECK will accept NULL values even if not clearly specified\n\nAssertions §\nCREATE ASSERTION assertion_name CHECK condition"},"SQL-Query-Optimization":{"title":"SQL Query Optimization","links":[],"tags":["Computing/Data-Science"],"content":"\n\n                  \n                  Problem: How to actually process this: \n                  \n                \n\n\nBasic Strategies §\n\nSQL Query rewriting\n\nMake everything into joins!\n\n\nDe-correlation: Correlated subqueries are de-correlated using “Magic” de-correlation\nIterated (=pipelined) algorithm processes\n\nProcess one tuple, up the chain, one at a time\nWill start producing results faster, but may not be fast in total\n\n\nBottom-up Evaluation\n\nProcess the bottommost query, then up one level, etc.\nUse temporary files to store intermediate results\n\n\n\nHeuristics-based Optimization §\n\nIdea: estimate size of intermediate results to calculate total operation count\n\nCardinality estimation\n\n\nGiven knowledge: ∣πA​R∣,∣R∣\nPrinciples:\n\nPreservation of Value\n\n\n\nSelection\n∣σA=val​∣∣σA=val​∣​≈∣R∣⋅∣πA​R∣1​=Distict A values in RSize of R​≈∣R∣⋅(1−∣πA​R∣1​)​​\n\ndivide by the “selectivity factor”\n\nConjunction, Disjunction (AND, OR operations)\n\nBest when A,B are independent columns\n\n∣σA=u∧B=v​∣∣σA=u∨B=v​∣​≈∣R∣⋅∣πA​R∣⋅∣πB​R∣1​≈∣R∣⋅(∣πA​R∣1​+∣πA​R∣1​−∣πA​R∣⋅∣πB​R∣1​)​ConjunctionDisjunction​​\nRange\n\nWithout max,min values: just say 31​\nWith max=hi(R.A) and min=lo(R.B)\n\n&amp; sometimes, highest and lowest is “invalid” → use second highest &amp; lowest\n\n\n\n∣σA&gt;v​∣≈∣R∣⋅max−minmax−v​\nJoins Estimation §\nNatural Join\n\nAssumption: containment—every tuple in smaller table joins with bigger table\n⇒ selectivity factor is the bigger one one\n\n∣R⋈S∣≈∣R×S∣⋅max(∣πA​R∣,∣πA​S∣)1​\nMulti-way Join\n\nAssumption: preservation of value sets—non-join attribute doesn’t lose values\n⇒ reduce by selectivity factor for every join\n\n∣R(A,B)⋈S(B,C)⋈T(C,D)∣≈∣R×S×T∣⋅selectivity of first join∣πB​R∣,∣πB​S∣1​​​⋅selectivity of second join∣πC​S∣,∣πC​T∣1​​​\nProjection over Join\n\nDue to assumption of preservation of value sets…\n…when R(A,B),S(B,C), A does not appear in S. Therefore we estimate:\n\n∣πA​(R⋈S)∣≈∣πA​S∣\n\n\n                  \n                  Nowadays people use histograms and ML for better estimation \n                  \n                \n\nJoin Plans §\nQ. given n relations to join, how to join?\n\nBrute Force: (n−1)!(2n−2)!​\nLeft-Deep Plan: n! \nGreedy: n2 \nDynamic Programming:\n\nNeed to consider: interesting orders (=sorted? deduped? etc.) need to be considered too!\n\n\n\n"},"SQL-Query-Processing-Algorithms":{"title":"SQL Query Processing Algorithms","links":["Sorting-Algorithms"],"tags":["Computing/Data-Science"],"content":"Notation §\n\nB(R): block count of table R\n∣R∣: tuple count of table R\n\n∣R∣&gt;&gt;B(R) because many tuples fit in a block\n\n\nM: number of available blocks in memory\nComplexity is always the number of I/Os\n\nSorting Based Algorithms §\nalg. External Merge Sort\n\nIdea: Merge Sort, but with limited memory \n\n\nWe are trying to sort relation R on disk with M blocks of main memory\nPass 0: we read M sequential blocks at a time, sort (using any Sorting Algorithms) and read onto disk into different runs. The number of runs is n.\nPass 1: We merge teach runs in memory. We need one block to write out the result, and n blocks to merge simultaneously.\n\nThus n=M−1\n\n\nRepeat passes until there is only one output run. This is the sorted run.\n\n\nComplexity\n\nNumber of passes: =⌈logM−1​⌈MB(R)​⌉⌉+1\nI/O Complexity: ≈O(B(R)logM​B(R))\nMemory requirement: M (as much as possible)\n\n\n\nalg. Sort-Merge Join\nIdea: while sorting the table, simultaneously join them in the process: \n\nRun external merge sort passes until the no. of total runs to merge is less than or equal to M−1\nMerge each table in-memory, and compare them to the condition. Join if necessary.\n\n\nComplexity for 2 passes:\n\nIO Complexity in 2 passes: O(B(R)+B(S))\nMemory requirement: M&gt;⌈MB(R)​⌉+⌈MB(S)​⌉=B(R)+B(S)​\n\n\nComplexity for &gt;2 passes: same as external merge sort\n! May degrade complexity if, e.g. the whole table needs joining.\n\nHash Based Algorithms §\nalg. Hash Join\n\nIdea: use a O(1) hash function that partitions the table into block size M−1 partitions, then merge: \n\n\nProbe: Partition both tables R,S into block size M−1 different partitions using the same hash function\nLoad one partition of R into memory. Stream one block of S at a time, and join.\n\n\nComplexity\n\nI/O: 3(B(R)+B(S))\nMemory requirement: M&gt;min[B(R),B(S)]​+1\n\n\n\nIndex Based Algorithms §\nEquality and Range: use B+ Trees\nIndex Nested Loop Join\nZigzag Join"},"SQL-Transaction-Guarantees":{"title":"SQL Transaction Guarantees","links":["Atomic","Directed-Acyclical-Graph","Readers-Writer-Locking"],"tags":["Computing/Data-Science","Computing/Algorithms"],"content":"SQL Transactions must be: ACID\n\nAtomic: Done or not done. Atomic\nConsistent: Don’t do partial writes\nIsolated: Transaction Serializability:= transactions should seem like they are executed in isolation\nDurable: Crashes should be recoverable\n\nIsolation §\nConcepts §\n\nSerial Schedule: execute transactions in order. Don’t interleave anything.\nConflicting Operations: two transactions conflict if any of the operations they have on same data conflict iff one of the operations is a write operation\n\nDirty Write: T1​.r(A) → T2​.w(A)\nDirty Read: T1​.w(A) → T2​.r(A)\n&amp; Two transactions are T1​,T2​ is conflict-serializable iff they have conflicts, but we can interleave some operations but still seems like it’s a serial schedule. SEREALIZABLE guarantee above is equivalent to this.\n\n\nConflicting Transactions:\n\nSee Isolation (database systems) - Wikiwand\nNon-repeatable Reads: T1​ reads X→ T2​ writes X and commits → T1​ reads X again (different value!)\nPhantom Reads: T1​ gets range → T2​ inserts between this range and commits → T1​ gets same range (different value!)\n\n\nPrecedence graph: a schedule with no cycles in the precedence graph (=graph is DAG) is conflict-serializable\n\nA path in an acyclic precedence graph is a conflict-serializable schedule\nExample: \n\n\n\nLevels of Isolation Guarantees §\n\nRead Uncommitted: lock &amp; release immediately\nRead Committed: don’t release write locks until commit\nRepeatable Reads: long duration locks\nSerializable: long duration locks on ranges\n\n\nSerializable Guarantee §\nImplemented thru Strict 2-phase locking (S2PL). Rules:\n\nOne writer, multiple readers: Readers-Writer Locking\n2-Phase Locking: For each transaction, you must lock everything first (=locking phase) then unlock everything together (=unlock phase): \nStrict Locking: Release write-locks (=exclusive-locks) only at commit or abort time\n\nGuaranteed recoverable (no cascading rollbacks)\nExample:\n\n\nRigorous (=Strong) Locking: Release all locks only at commit or abort time.\n\nPrevents Deadlocks\nAlso Recoverable.\nExample:\n\n\n\nRecovery §\nComputation Model: \nNaive Recovery §\n\nForce: on commit, (“force”) dirty flush to disk\nNo Steal: don’t (“stealthily”) flush before committing\n\nSmarter: Undo/Redo Logging §\n\nlog start of transaction\nFor each write/delete, log the old and new values\nOn commit, write all logs to disk\nProperties\nWrite-ahead logging: before disk flush, log must be flushed first\nNo force: commit even without flushing\nSteal: flush to disk anytime (just log it!)\n\n\nFuzzy Checkpointing §\n\nLog begin checkpointing and log open transactions\n\nStart Checkpoint CHK1, Open transactions: S,T\n\n\nflush all current transactions (=transactions at “begin” point) at leisure\nLog finish checkpointing (and pointer for convenience)\n\n`Finish Checkpoint CHK1. Started at \n\n\n\nRecovery\n\nAnalyze &amp; Redo Phase\n\nFind last completed checkpoint Finished Checkpoint CHK1…\nGo to the start of that checkpoint Start Checkpoint CHK1, Open: S,T…\nAnalyze open transactions at time of crash U={S,T}\n\n! Start with checkpoint’s open transactions\nWhen you encounter close transactions Close S, remove from U={S​,T..,U}\nWhen you encounter new open transactions Begin T, add that to U={S,T..,addedU​​}\n\n\nAll operations between Start Checkpoint… and end of log is replayed\n\n\nUndo Phase\n\nFrom the last log item until whenever:\n\nFor each open transaction at time of crash T∈U\n\nUndo all of its operations in reverse order…\nUntil you reach Begin T\n\n\n\n\nStop when U is exhausted\n\n\n\nRecoverability §\nFor multiple transactions to be recoverable:\n\nT1 writes → T2 reads, then must T1 commits → T2 commits\nT1 writes → T2 writes doesn’t even matter\n\nEd discussion:\n"},"Sacred—Profane-Dialectic":{"title":"Sacred—Profane Dialectic","links":[],"tags":["Sociology"],"content":""},"Scheduling-Problem":{"title":"Scheduling Problem","links":["Greedy-Algorithm","Dynamic-Programming","Subset-Sum"],"tags":["Computing/Algorithms"],"content":"Scheduling problem solution formulations\n\nSort by some quantity\nSchedule accordingly\nProve correctness by these strategies\n\nEvent Scheduling §\nQ. Interval Scheduling (Greedy Algorithm)\n\nDynamic Programming idea works\n\ndp[i] = max(dp[]+1\nO(n3) time\n\n\nGreedy Algorithm is even better\n\nIdea: sort by\n\n\n\nalg. Interval Scheduling with n rooms\nQ. Multiple Room Interval Scheduling\nQ. Interval Coloring\nQ. Interval Cover Problem\nJob Scheduling §\nQ. Minimize Makespan (=maximum time on any machine) of n machines\n\n\nNP-Hard problem (reduce from 21​-Subset Sum)\n\nalg. Approximate Greedy Makespan: Always choose the least loaded machine\n\nIs a 2-level approximation \nalg. Better Approximate Greedy Makespan: Assign from longest to shortest job, to least loaded machine every time.\n\n\nalg. Minimize lateness of jobs\n\n\nInterval Scheduling\n\nschedule n jobs\n\nno dependencies\n\n\n=&gt; how many jobs can you complete? (total time doesn’t matter!)\nconflict when:\n\nStart[i] &lt;\n\n\nDynamic Programming solution is easy\n\nOPT = max(1+OPT(jobs that don’t conflict with job 1) vs OPT(2..n))\n\n\n=&gt; but O(n3) or even with improvement O(n2)!\n\n\n\nGreedy solution:\n\njob that finishes first = job that leaves the most amount of remaining time\nimplementation\n\nSort by finish time\nchoose earlyest finish time\nchoose next earliest finish time that doesn’t conflict with most recently finished job\nDone!\n\n\n=&gt; O(nlogn) time (because sorting)!\nProof by induction: Exchange Argument\n\nbase: Y∗ optimal solution has y1​inY∗\n\nif you have x1​ that finish time earlier than y1​…\nremoving y1​ and adding x1​ is also optimal solution\n\n\n\n\n\n\n\nTry: dp solution\n\n\nvariation: make span algorithm (greedy is close to optimal, but not optimal)\n\n\ninterval scheduling, but with n meeting rooms\n\ngreedy still words!\n\n\n\nminimum total completion time.\n\n\nminimize lateness\n\nsort by ascending lateness&lt;&lt;\n\n\n"},"Securities-and-Exchange-Commission":{"title":"Securities and Exchange Commission","links":[],"tags":["Economics/Finance"],"content":"\nThe SEC is an independent agency of the United States federal government, and it is responsible for enforcing federal securities laws, proposing securities rules, and regulating the securities industry.\n"},"Securitization":{"title":"Securitization","links":["Bonds-(Finance)"],"tags":["Economics/Finance"],"content":"def. Securitization is the process of making a one-time loan product into a split product that pays fixed income and is tradable. Example.\n\n\nA company wants a $100 loan from the bank to build a factory\nThe bank sets up a special purpose vehicle (SPV), which actually lends the money to the factory\nSimultaneously, the SPV issues Bonds worth $10 each that investors can buy, to get the cash to give to the company\nBank makes profit in the credit spread between the loan and the bond.\n\n"},"Security-(Finance)":{"title":"Security (Finance)","links":["Bankruptcy","Loans","Bonds-(Finance)","Equity","Derivatives-(Finance)","Futures","Options-(Finance)","Securitization","Capital-(Marxism)","Valorization,-Surplus-Value"],"tags":["Economics/Finance"],"content":"All securites are a form of loaning money to a company.\ndef. Capital Structure. Order of payouts when company goes Bankrupt.\n\nLoans\nFixed Income\n\nBonds: No voting rights, 1st seniority cap structure, fixed interest\n\n\nEquity\n\nPreferred Shares: limited voting, 2nd seniority cap structure, fixed dividends (mostly)\nCommon Shares: full voting, last seniority cap structure, unknown dividends\n\n\nDerivatives\n\nFutures\nOptions\n\n\nStructured Products\n\nSecuritization\n\n\n\nBasically capital to be valorized. Interest is surplus value."},"Sensuous-Activity":{"title":"Sensuous Activity","links":["Alienation-(Marx)","External/Private-Property","Capital-(Marxism)","Species-being"],"tags":["Philosophy/Marxism"],"content":"Sensuous Activity §\nSenses define reality…\n\nAll his human relations to the world - seeing, hearing, smelling, tasting, feeling, thinking, contemplating, sensing, wanting, acting, loving - in short, all the organs of his individuality, like the organs which are directly communal in form, are in their objective approach or in their approach to the object the appropriation of that object. This appropriation of human reality, their approach to the object, is the confirmation o f human reality.\n\n…but it’s estranged for now…\n\nThis material, immediately sensuous private property is the material, sensuous expression of estranged human life. Its move­ment - production and consumption - is the sensuous revelation of the movement of all previous production, i.e. the realization or reality of man.\n\n…and Private Property is a core “sensuous” response by capitalism…\n\nPrivate property has made us so stupid and one-sided that an object is only ours when we have it, when it exists for us as Capital (Marxism) or when we directly possess, eat, drink, wear, inhabit it, etc., in short, when we use it. […] Therefore all the physical and intellectual senses have been replaced by the simple estrangement of all these senses - the sense of having.\n\n…and removal of private property is the “emancipation” of our senses\n\nThe supersession of private property is therefore the complete emancipation of all human senses and attributes […] The senses have therefore become theoreticians in their immediate praxis. They relate to the thing for its own sake, but the thing itself is an objective human relation to itself and to man, and vice-versa.\n\nEmancipation of the Senses §\nThe socialized and trained “senses” determine the progress of the Species-being\n\nTherefore this relationship reveals in a sensuous form, reduced to an observable fact, the extent to which the human essence has become nature for man or nature has become the human essence for man. It is possible to judge from this relationship the entire level of development of mankind. It follows from the character of this relationship how far man as a speciesbeing, as man, has become himself and grasped himself\n\nThis material, immediately sensuous private property is the"},"Sequence-Summation":{"title":"Sequence Summation","links":[],"tags":["Math/Calculus","Math"],"content":"Geometric Series: a,ar,ar2,…,arn\nSn​=a⋅1−r1−rn​\nIf it converges, the infinite sum:\nS∞​=1−ra​\nArithmetic Sequence: a,a+d,a+2d,…a+nd\nSn​=2n​⋅(2a+(n−1)d)\nOther Summations: See Convergence tests - Wikiwand\nProperties of Summations §\n(i=0∑n​ai​)2=i=0∑n​ai2​+i=j∑n​ai​aj​\nConvergence Tests §\nDivergence Test §"},"Set-Cover":{"title":"Set Cover","links":[],"tags":["Computing/Algorithms"],"content":"Q. Set Cover. We have a universe set U and a bunch of sets S1​,…,Sn​ in the universe. (It is guaranteed that the union of all these sets will cover U). I want to cover all elements of the universe. Which sets should I choose do do this minimally?\n\ne.g. How many classes to visit to see all Duke students?\n\nalg. Approximate Set Cover.\n\nChoose the largest available set Si​\nRemove elements from that set from the universe\nRepeat until no element is left.\n"},"Set-Theory":{"title":"Set Theory","links":[],"tags":["Math"],"content":"Partitions and Probability §\ndef. Sets B1​…Bn​ is a partition of B if and only if:\n\nB1​…Bn​ are pairwise disjoint\nB1​∪…∪Bn​=B\n\nCOR. if B1​…Bn​ is pairwise disjoint P(B1​∪…∪Bn​)=P(B1​)+…+P(Bn​)\nCOR. if B1​…Bn​ is a partition of Ω then P(B1​∪…∪Bn​)=1\nSet Operations §\n\n∣A∣ Cadinality of set A\nA×B: Cartesian product of sets A,B\n2A: all the subsets of A [= power set of A]\n\n"},"Shepard's-Lemma":{"title":"Shepard's Lemma","links":["Expenditure-Minimization","Uncompensated-Demand-curve","Expenditure-Function","Cost-Minimization","Input-Demand-and-Output-Supply"],"tags":["Economics/Micro-Economics"],"content":"thm. Shepard’s Lemma (Expenditure Minimization). Relates Hicksian Demand and Expenditure Function\nh1​(p1​,p2​,uˉ)=∂p1​∂E​h2​(p1​,p2​,uˉ)=∂p2​∂E​​​\nthm. Shepard’s Lemma (Cost Minimization). Related conditional Input Demand and the Cost Function\nlc​(w,r,xˉ)kc​(w,r,xˉ)​=∂w∂C​=∂r∂C​​​"},"Short-selling":{"title":"Short-selling","links":[],"tags":["Economics/Finance"],"content":"= “selling an asset you don’t owe”\n= you borrow the asset to immediately sell it on the market;\n→ then when the price drops, you buy the share from the market to return it\n"},"Shortest-Path":{"title":"Shortest Path","links":["Priority-Queue","No-Arbitrage","Dynamic-Programming"],"tags":["Computing/Algorithms"],"content":"Summary of All Shortest Path Algorithms §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShortest Path (SP) AlgorithmsBFSA*Dijkstra’sBellman FordFloyd WarshallComplexityO(V+E)O(ElogV)O((V+E)logV)O(VE)O(V3)Recommended graph sizeLargeLargeLarge/MediumMedium/SmallSmallAll-PairsOnly works on unweighted graphsNoOkBadYesCan detect negative cycles?---✓✓SP on graph with unweighted edges✓(Best)✓✓✓(Bad)✓(Bad)SP on graph with weighted edgesMust expand graph✓(Best)✓✓✓(Bad)\nSingle Source Shortest Path (SSSP) §\nalg. BFS Shortest Path\n\nAssumption: Graph doesn’t have weights\nIdea: BFS, but every time you encounter a tense edge, relax it\n\n\nalg. Dijkstra’s Shortest Path\n\nAssumption. graph has non-negative edge weights\nIdea. BFS Shortest Path, but you choose the next vertex by how likely they are to be the shortest path (=maintain a Priority Queue based on path length)\n\nSee Dijkstra’s Algorithm - Computerphile - YouTube\n\n\nComplexity\n\nTime: O((∣V∣+∣E∣)⋅log∣V∣)\n\n\n\nalg. A-Star (A*) Shortest Path\n\nAssumption. graph has non-negative edge weights\nIdea. Dijkstra using Priority Queue, but the priority is calculated on a heuristic\n\nSee A* (A Star) Search Algorithm - Computerphile - YouTube\n\n\nComplexity.\n\nTime: O(∣E∣log∣V∣) using binary heap\n\n\n\nalg. Bellman–Ford Shortest Path\n\nIdea:\n\nRepeat BFS edge relaxation for ∣V∣−1 times. Get shortest path\nRepeat BFS edge relaxation again, for ∣V∣−1 times. But this time, if any cost values are updated, the node is part of a negative cycle. Mark cost to that node as ∞.\nSee Bellman Ford Algorithm | Graph Theory - YouTube\nAble to deal with negative edges/negative cycles\nOften used in finance for identifying No-Arbitrage opportunities.\n\n\nComplexity. Time: O(V⋅E)\n\nAll Pairs Shortest Path (APSP) §\nalg. Floyd-Warshall Shortest Path\n\nIdea: Dynamically Programmed algorithm. from i to j, compare the two paths:\n\ni→j using only vertices 1,…,k−1\ni→k→j, but only using vertices 1,…,k−1\nTake the smaller of the two.\n\n\nExample: \nDP Table: memo[k][i][j] is the shortest path from i→j using nodes 1..k\nSee also: Floyd–Warshall algorithm - Wikiwand\n\n\nComplexity. Time: O(V3), Space O(V2) by retaining most recent only\n\nTips and Tricks §\n\nIf the problem demands you multiply edge weights instead of summing them, use log of weights instead to transform it back into a problem with summation of edge weights. (See Example)\n"},"Signaling-Game":{"title":"Signaling Game","links":[],"tags":["Economics/Game-Theory"],"content":"\nSignaling game because one is attempting to signal to another authentically.\nHandicap principle - Wikiwand is an instance of the signaling game. Animals will signal their sexual prowess at the cost of actual function."},"Simultaneous-Equation-Model":{"title":"Simultaneous Equation Model","links":["Two-Stage-Lease-Squares-Regression"],"tags":["Math/Statistics"],"content":"Simultaneous Equation Models use Two-Stage Lease Squares Regression to analyze circular causality.\n\n\nThe two dependent variables with circular causality is Y1​,Y2​\nY1​,Y2​ are caused respectively by Z1​,Z2​\nW cause both Y1​,Y2​.\nThe equation is thus:\n\nY1i​Y2i​​=β0​+β1​Y2i​+β2​Wi​+β3​Z1​+ϵ1i​=γ0​+γ1​Y1i​+γ2​Wi​+γ3​Z1​+ϵ1i​​​\nThe process for regression is same as the Two-Stage Lease Squares Regression:\n\nFor the first equasion Y1i​:\n\nFirst stage: regress Y2​without Y1​:\n\n\n\nY2i​=π0​+π1​Wi​+π2​Z1​+νi​\n2. Second stage: regress $Y_{1}$ againt $\\hat{Y_{2}}$ (the original equation):\n\nY1i​=β0​+β1​Y2i​^​+β2​Wi​+β3​Z1​+ϵ1i​\n\nPerform the same 2SLS regression for the second equation as well\n"},"Ski-Rental-Algorithm":{"title":"Ski Rental Algorithm","links":[],"tags":["Economics/Game-Theory"],"content":"Motivation. At a ski resort you are trying to decide whether to buy a ski or rent:\n\nYou will be there for T days\nIt costs \\1torentaski,and$B$ to buy the ski\n\nB≪T\n\n\nOn day ℓ you will wake up and say: “I don’t want to ski anymore” and go home\nWhat should be your strategy? An online algorithm answers this question.\nThe OPT algorithm is our benchmark, and it has information that we don’t have. The OPT algorithm for this problem is:\n\ndef. Optimal Ski Rental Algorithm.\nOPT={if ℓ&gt;Bif ℓ&lt;B​buyrent​=min{l,B}\nDeterministic Ski Renter Algorithm §\nExample. Suppose our strategy is to rent B−1 days, and buy on the Bth day. Then:\n\nif ℓ&gt;B then we will have rented for B−1 days, and bought on the B-th day so the total cost is 2B−1\nif ℓ&lt;B then we end up just renting for ℓ days, so the total cost is ℓ\n\nOPTALG​​={if ℓ&gt;Bif ℓ&lt;B​B2B−1​&lt;2ℓℓ​=1​&lt;2​​\nSo even in the worst case, the algorithm will be less than 2⋅OPT.\nthm. (Deterministic Algorithm Bound) Let a ALGk​ algorithm denote a deterministic algorithm that rents for k−1 days, and buys the ski at day k. For any k:\nALGk​≥2⋅OPT\nProof. Knowing k from the algorithm design, adversarial input ℓ←k will be ALG(k)≥(2−B1​)OPT(k). The deterministic algorithm is to rent until k−1th day, and buy on the k-th day.\n\nThe worst case adversarial situation for ℓ is ℓ=k because this will force the user to rent for the maximum k−1 days, and then to buy as well.\n\nOPT=min{k,B}\nALG=(k−1)+b\nTherefore:\n\nOPTALG​​=min{k,B}k−1+B​=max{1+Bk−1​,1+KB−1​}=1+≥1max{Bk−1​,kB−1​}​​≥2​​\nSo even the best deterministic algorithm pays ≥2⋅OPT. ■\nThis is devastating; we can’t get past two-times better than the optimal. Instead, we should mix our strategies to overcome the adversary.\nProbabilistic Ski Rental Algorithm §\nExample. Let us imagine a probabilistic rental algorithm that:\n\nWith probability 21​, stops renting and buys at day 43​B\nWith probability 21​, stops renting and buys at day B\nThis algorithm behaves differently depending on the true day you stop skiing, ℓ:\n\n\nCase ℓ&lt;43​B:\n\nOPT=ℓ\nE[ALG]=ℓ\nOPTE[ALG]​=1\n\n\nCase 43​B&lt;ℓ&lt;B:\n\nOPT=B\nE[ALG]=21​buy at 43​B(43​B+B)​​+21​buy at B(B+B)​​=815​B\nOPTE[ALG]​=815​\n\n\nCase B&lt;ℓ:\n\nOPT=ℓ\nE[ALG]=21​ rent ℓ​​+21​ rent then buy (43​B+B)​​\nOPTE[ALG]​= this is case B&lt;ℓ21​+78​ℓB​≤21​+78​​​=711​\nTherefore we have the worst of these cases bound:\n\n\n\nOPTE[ALG]​≤815​\nWhich is slightly better than deterministic. This is, however, nowhere close to how good we can be:\ndef. Optimal Probabilistic Ski Algorithm. The optimal algorithm is to, on waking up every day:\n⎩⎨⎧​If day&lt;BIf day=B​then {Buy with probability B1​Rent with probability 1−B1​​ then just buy​\nThis algorithm will have:\nOPTE[ALG]​≤e−1e​≈1.58\n(We won’t prove this)"},"Sleep-(Neuroscience)":{"title":"Sleep (Neuroscience)","links":[],"tags":["Biology/Neuroscience"],"content":"\nSuprachaismatic Nucleus (SCN)\nSpecial ganglion cells in the retina\n\nwith their own pigment (melanopsin)\nlocated nearer to nose in the eye\naverage intensity over time\n\n\nTranscription of PER and TIM proteins vary\n\n"},"Sliding-Window-Technique":{"title":"Sliding Window Technique","links":[],"tags":["Computing/Algorithms"],"content":"“Maximum consecutive something”/“Maximum Substring” ⇒ think sliding window.\n\nSliding windows can stretch or shrink in length\nIt’s a kind of greedy algorithm\nyou can do many things in O(1) time.\nIt can do flipping and stuff too!\n\nExample of complex sliding window problem:\n#1004 - Maximum Consecutive Ones with Flipping"},"Slow-Feature-Analysis":{"title":"Slow Feature Analysis","links":[],"tags":["Computing/Maching-Learning"],"content":"Motivation. In a time series data, the latent features change more slowly than the actual data. For example in a video of a running zebra, the pixels of the video rapidly changes, but the ‘feature’ that there is a zebra in the frame does not change. We take advantage of this.\ndef. Slow Feature Analysis (SFA). We have continuous time series data x(t)\n\nWe extract them into latent features over time h(t) via a set of non-linear functions g.\n\nThese non-linear functions are somewhat arbitrarily chosen. It can simply be something like xi2​ or xi​+xj​, etc.\n\n\nThen we linearly combine these to get the output signal via matrix W\n\nx(t)⟶g​h(t)⟶W​y(t)\nObjectiv e. Our goal is to find W such that y(t) changes very slowly. Thus the objective is to minimize:\nWmin​Et​[(dtd​y(t))2]≡Wmin​T1​∫0∞​ rate of change of y y′(t)​​dt​ total rate of change ​\nunder constraints\n3. Zero mean: Et​(y(t))=0 ← because shifts doesn’t matter\n4. Unit variance: Et​(y(t)2)=1 ← because amplitude doesn’t matter"},"Social-Choice":{"title":"Social Choice","links":[],"tags":["Economics/Game-Theory"],"content":"Motivation. Social choice is about converting many people’s preferences into one big social preference. This is obviously how many collective decision making works: elections, markets, auctions, govn’t policty, etc.\nThe problem is, as we will prove, social choice is really hard, and it’s really hard to avoid dictatorships. But there are mechanisms to avoid this too.\nFormalizing Social Choice §\ndef. Social Choice Situation. We have to choose between candidates (=alternatives) in set A:\n\nAn ordering ≻=(total order on A).\nL={all possible ordering of A}. It is simply a permutation set on A, thus ∣L∣=∣A∣!\nThere are n voters. Each voter i has one ordering ≻i​.\ndef. Social Welfare Function F:Ln→L i.e. given everybody’s prefs, come up with a collective pref\ndef. Social Choice Function f:Ln→A i.e. given everybody’s prefs, come up with a collective single candidate\nTwo functions have a simple distinction:\nWelfare function: full ordering, collectively.\nChoice function: just choose one collectively.\nWe first discuss the welfare function in detail.\n\nImpossibility (Welfare) §\nProperties of a social welfare function are:\n\nUnanimity: if all voters have the same preference ≻, then that is the collective order too. F(≺,…,≺)=≺\nDictatorship: if voter i’s preferences are just taken as the collective order, disregarding everybody else’s preferences, it’s a dictatorship. Dictator i: ∀≺1​,…,≺n​∈L just take F(…)=≺i​\nIndependence of Irrelevant Alternatives (IIA). Let candidates a,b∈A and preferences of voters ”…”, and observe F which ≺∗​=F(…).\n\nSuppose we create a new rankings ≺∗′​=F(…′), where each voter i voted a≺i​b and maintains their preference ranking of a and b in the new ranking: a≺i​b⟺a≺i′​b\nThen collective ranking between a and b should also be maintained: a≺∗​b⟺a≺∗′​b\nthm. Arrow’s Impossibility Theorem. Every social welfare function F over choices ∣A∣≥3, that satisfies unanimity and IIA, must be a dictatorship.\nProof.\n\n\n"},"Social-Construct":{"title":"Social Construct","links":[],"tags":["Sociology"],"content":""},"Social-Institution":{"title":"Social Institution","links":[],"tags":["Sociology"],"content":"hyp. A Social institution is defined by the rules that govern it. (=it is a descriptive definition, not a constructive definition)\n\nUS is defined by the Constitution &amp; laws\nThe maintenance of a social institution is done by a stable equilibrium where everybody upholds the law and sanctifies it, and nobody dares to deviate.\nThe president swears to uphold the constitution, not govern well.\nWhen conflict arises people look at the constitution debate about the law, not debate each other.\n"},"Sociology-110D":{"title":"Sociology 110D","links":[],"tags":["Courses"],"content":"EQUIRED COURSE TEXTS AND TECHNOLOGY:\n(1) Conley, Dalton. 2015. You May Ask Yourself: An Introduction to Thinking Like a Sociologist (4th\nEdition). New York: W.W. Norton.\nAvailable at reduced cost as an e-book or online rental\n(http://books.wwnorton.com/books/webad.aspx?id=4294989745). There are many inexpensive\nused copies of the fourth and third editions at Amazon.com, etc.\nA list of required pages/sections will be available on Sakai.\nHenslin, James (Editor). 2007. Down to Earth Sociology: Introductory Readings (14__th Edition).\nNew York: Free Press"},"Sociometry":{"title":"Sociometry","links":[],"tags":["Sociology"],"content":"The use of graph theory tools to analyze social networks, etc."},"Softmax-and-Sigmoid":{"title":"Softmax and Sigmoid","links":["Floating-Point-Numerical-Stability"],"tags":["Computing/Maching-Learning"],"content":"def. Sigmoid Function is a simple S-shaped function.\nσ(z):=1+ez1​\n\ndef. Softmax function takes in a vector z of dimension K and normalizes it into a probability distribution with K different outcomes:\nz=​z1​z2​⋮zn​​​→σ(z)i​=∑j=1K​ezj​ezi​​for each i\nSometimes if we input each zi​ separately we see it as a function that already knows z: \nσ(zk​∣z1​,…,zn​)=∑i=1n​exp(zi​)exp(zk​)​\nand it is the single-element (scalar) σk​, i.e. probability of class k.\nNumerical Instability §\nWhen calculating softmax in floating point, when the values get big there are instabilities. Instead, observe:\nsoftmax(x)i​​=∑j​exj​exi​​=1⋅∑j​exj​exi​​=CC​∑j​exj​exi​​=∑j​Cexj​Cexi​​=∑j​exj​+logCexi​+logC​​​\nWhich means we can add any constant logC to prevent overflow/underflow and thus infs."},"Solow-Growth-Model":{"title":"Solow Growth Model","links":["Utility-Maximization","Cobb-Douglas-Utility","Unconstrained-Maximization"],"tags":["Economics/Macro-Economics"],"content":"\nAssumptions §\n\nass. Investments are equal to savings, i.e. all savings are invested. I=S\nass. Y=F(Kt​,Nt​,zt​)\n\nIntertemporal Utility Maximization §\nass. Households perform intertemporal utility maximization. Similar to Utility Maximization but according to the following general form of intertemporal utility maximization:\nc1​,c2​max​c1α​c21−α​,∀t Ct​+St​=wt​Nt​+rt​Kt​\nThe equality is the intertemporal budget constraint that holds from t=t0​,t1​,….\nLimit the time horizon to just two periods, t and t+1. Then:\n\nCapital stock is initially Kt​=0\nNo savings at second (=last) period St+1​=0\nThen the two constraints:\n\n\nCt​+St​=wt​Nt​\nCt+1​=wt+1​Nt+1​+(1+r)St​\nEquating for St​ we get:\n\n NPV (1+r)1​​​ Savings carried over (Ct+1​−wt+1​Nt+1​)​​= Save at t wt​Nt​−Ct​​​\nThis is the intertemporal budget constraint. Now solving for Ct+1​ the maximization problem for the above Cobb-Douglas Utility function to get Unconstrained Maximization:\nCt​max​Ctα​[ appreciated savings (1+r)wt​Nt​−(1+r)Ct​​​+ income wt+1​Nt+1​​​]1−α​ Ct+1​ ​\nwe get the first order condition (by optimizing the log of the utility function and getting first derivative):\nα(Yt​+1+rYt+1​​)=Ct​\nwhere Yτ​=wτ​Nτ​\nCapital Accumulation §\nass. Law of Motion (Capital) Capital stock is acculated and depreciated via the following equation:\nKt+1​= Capital stock at t Kt​​​(1−δ depreciation )+ investment at t It​​​\nThen consider It​=St​=sYt​ where s is the savings rate. Then\nKt+1​Nt​Kt+1​​kt+1​Nt​Nt+1​​​=(1−δ)Kt​=(1−δ)Kt​+szNt​f(kt​)=(1−δ)kt​+szf(kt​)=(1−δ)kt​+szf(kt​)​+szF(Kt​,Nt​)​=Nt​(Nt​1​F(Kt​,Nt​))=Nt​(f(kt​,1))(1) CRS(2) Intensity​\nthm. Law of Motion (Capital, intensive) i.e. per person:\n future stock kt+1​​​(1+n)kt+1​​= leftover (1−δ)kt​​​+ inflow szf(k)​​=1+n1−δ​kt​+1+n1​it​​​\n\nkt​:=Nt​Kt​​ i.e. capital intensity, in (2) Intensity\nn:=Nt​Nt+1​−Nt​​ i.e. population growth rate\nf(kτ​) is the individual production function, i.e. per worker (Nτ​=1)\nit​:=szf(kt​), investment (=saving) in intensive form\n(1) CRS works because of assumption of constant returns to scale: δF(Kt​,Nt​)=F(δKt​,δNt​)\n\nImplications §\ncorr. Capital change can be simply caldualted:\n△k:=kt+1​−kt​=−δkt​+szf(kt​)\n\n\nCapital accumulation cannot explain per capital GDP growth\n\n…due to diminishing MPL​,MPK​ in F(⋅)\n\n\nExplains why per capita GDP converges to a certain point (intersection point in right graph)\nExpalins why total GDP grows as population increases, but also that population growth does not cause per capital GDP growth\n\nSteady State Capital §\nWe consider the steady state point, the intersection point in the right graph. We assume:\n\nk^:=kt​kt+1​−kt​​ i.e. capital growth rate\nzf(kt​):=zkα i.e. Cobb-Douglas production function\n\nk^​:=kt​−δkt​+szf(kt​)​=−δ+sktα−1​​​\nAt steady state k^stst​=0. Thus:\nδkstst​​=skststα−1​=(szδ​)1/α−1​​\nReformulation with Productivie Hours of Labor §\nlet\n\ng=zt​zt+1​−zt​​ the growth rate of z.\nk~:=zNK​ disembodied intensive capital i.e. “body-hours,” i.e. when productivity is disembodied.\n\nThis also implies kt+1​~​=zt​Nt​(1+n)(1+g)Kt​​\nWe have\nthm. Law of motion for disembodied capital:\n\n\n\nkt+1​~​△kt+1​~​​=(1+n)(1+g)1−δ​kt​~​+(1+n)(1+g)sf(kt​~​)​=(1+n)(1+g)(1−δ)−(1+n)(1+g)​kt​~​+[…]​​\nAt steady state △k~=0 i.e. k~t+1​=k~t​ and we get:\nk~stst​=(n+g+δ+ngs​)1/(1−α)\nDynamics §\nVarious Growth Rates §\nObserve first that if k~ can reach a steady state, then growth at the steady state is also zero:\nf(k)​=k=k~=0\nDenote growth rates:\n\nln(N)=n, population grwoth\nln(z)=g, productivity growth\nln(y~​)=0, as we showed.\nExample.\n\nln(Y)​=ln(zNzNY​)=ln(zN⋅zNY​)=ln(zN⋅y~​)=ln(z)+ln(N)+ln(y~​)​=g+n​​\n\n\n                  \n                  \\ln(\\tilde{y})=0.\n                  \n                \n\nConvergence &amp; Endogenous Growth §\n\nObserve.\n\nRed is developing, blue is developed nation.\nReason for convergence is dinimininshing marginal product of capital MPK​\n\nDifferently-shaped Production Functions §\n"},"Soren-Kierkegaard":{"title":"Soren Kierkegaard","links":[],"tags":[],"content":""},"Sorting-Algorithms":{"title":"Sorting Algorithms","links":["Priority-Queue"],"tags":["Computing/Algorithms"],"content":"\n\n                  \n                  Abstract \n                  \n                \nBest sort algorithm (merge sort) will take O(nlogn) at worst.\n\nQuick Sort §\nIdea:\n\nChoose pivot\nEverything left of pivot is smaller, everything right of it is bigger (O(n))\nCall quicksort on the left side and right side\n\nChoosing a good pivot\n\nRandom\n\nWith high probability pivot within 31​∼32​\nLikely Complexity: T(n)≤T(32​n)+cn=O(n)\n\n\nDSelect i.e. Median of medians approach \n\nRank is guaranteed to be between 41​∼43​ of the array slice\nComplexity: T(n)≤T(5n​)+T(43​n)+cn=O(n) ……first term: DSelect, second term quicksort\n\n→ T(n)&lt;A⋅n for some A≥20c (=O(n))\n\n\n\n\n\nMerge Sort §\nIdea:\n\nSplit into two\nCall mergesort on each left, right side\nmerge left, right side in O(n) time\n\nalg. Parallel Merge\nIdea: For the 8 elements shown below, can we find out how to place the element in parallel (8 simultaneous processes)? → We can!\n\nLeft half: binary search on right half for correct place\nRight half: binary search on left half for correct place\n\n\ndef PMerge(A[1..n], m)\n\t# Ind[i] stores the final index of the i-th element\n\t\n\t# left half\n\tparallel for i=1 to m\n\t\t# search in right half\n\t\tInd[i] &lt;- BinarySearch(in A[m+1..n] for A[i]) + i\n\t# right half\n\tparallel for j=m+1 to n\n\t\t# search in left half\n\t\tInd[i] &lt;- BinarySearch(in A[1..m] for A[j]) - m + j\n\tsync\n \n\t# place each item in their ordered location\n\tparallel for i=1 to n\n\t\tB[Ind[i]] &lt;- A[i]\n\tparallel for i=1 to n\n\t\tA[i] &lt;- B[i]\n\t\n\nSpan O(logn)\n\nthen we have…\nalg. Parallel Merge Sort\ndef PMergeSort(A[1..n])\n\t# base case\n\tif n &gt; 1\n\t\tm = ceiling(n/2)\n\tspawn PMergeSort(A[1..m])\n\tspawn PMergeSort(A[m+1..n])\n\tsync\n\tPMerge(A[1..m],A[m+1..n])\n\nSpan T∞​(n)≤T∞​(2n​)+O(logn)=O(log2n)\nWork W∞​≤2W∞​(2n​)+O(nlogn)=O(nlog2n)\n\nSecond term is work done by parallel merge procedure: correctly placing n items, each takes O(logn).\n\n\n\nalg. Inversion Counting. In an unsorted array A[1..n] count how many pairs A[i],A[j] are not in the correct order\n\nIdea: Mergesort, but during the merge step check how many times you need to reverse the pairs\n\n\nPartially Sorted §\nUses of partially sorted arrays:\n\nTop k items: e.g. Google Search, College Admissions\nk-th largest/smallest item\n\nk-th smallest item (=item of rank k) =&gt; use Tournament Tree"},"Sparse-Coding-and-Dictionary-Learning":{"title":"Sparse Coding and Dictionary Learning","links":[],"tags":["Computing/Maching-Learning"],"content":"Intuition. In many cases, data is represented by a sum of commonly-existing components. For example, a number is simply a combination of pen strokes.\n\nWe formalize these properties into the following:\n\nEach component is a element of a common language, i.e. a dictionary\nData is constructed from a weighted sum of dictionary items\n\nOften, only a few types of pen strokes are used for a single number. Thus we want to only choose (=weight) few components; i.e. enforce sparsity in latent representation\n\n\n\ndef. Sparse Coding &amp; Dictionary Learning. Given a dictionary D, let x(n) be a datapoint and h(n)(x(n)) be the corresponding latent representation obtained by:\nh(n):=argh(x)min​ reconstruction loss 21​∥x−D⋅h(x)​ reconstruction ​∥22​​​+λ∥h(x)∥1​​ sparsity penalty ​\ni.e., the latent representation h always minimizes the reconstruction loss. This is an odd way to define a latent representation, but it is useful in that given a dictionary h always minmizes reconstruction loss—part of the training is built into the inference.\n\nWe also enforce that the columns of D is of norm 1, because if we don’t the sparsity objective can ‘cheat’ and increase the numbers in D to compensate for the sparsity.\nThis optimization is done by the Iterative Shrinkage and Thresholding Algorithm (ISTM) because the sparsity penalty makes it difficult to use standard tools.\nObjective. Now, how do we get D? Sometimes it’s obvious (e.g., sentences are composed of words) but in most cases not. We obtain optimal D with the following objective:\n\nL=Dmin​⟨h(x)min​21​∥x−D⋅h(x)∥22​+λ∥h(x)∥1​⟩T​\nSite Unreachable"},"Species-being":{"title":"Species-being","links":["Humanism"],"tags":["Philosophy/Marxism"],"content":"= Marxist Humanism\n\nMan is a species-being, not only because he practically and theoretically makes the species - both his own and those of other things - his object, but also - and this is simply another way of saying the same thing - because he looks upon himself as the present, living species, because he looks upon himself as a universal and therefore free being.\n\n\nFor in the first place labour, life activity, productive life itself appears to man only as a means for the satisfaction of a need, [But] the whole character of a species, its species-character, resides in the nature of its life activity, and free conscious activity constitutes the species-character of man. Life itself appears only as a means of life.\n\n\nThe object of labour is therefore the objectification of the specieslife of man: for man reproduces himself not only intellectually, in his consciousness, but actively and actually, and he can there­ fore contemplate himself in a world he himself has created.\n"},"Sponsored-Search-Auction":{"title":"Sponsored Search Auction","links":[],"tags":["Economics/Game-Theory"],"content":"def. Sponsored Search Auction. This is a different game layout. Imagine a search engine selling advertisement slots. \n\n\nThere are k ad slots, with click-thru rate, largest to smallest: α1​,…,αk​\nThere are n bidders, for whom the value-per-click is v1​,…,vn​. Each bids bi​ to get any slot.\nThen, if bidder i is assigned slot l and charged pi​(b) \n\nAllocation matrix is x and xil​=1\nprice matrix is p and pil​ is the price charged\n\n\n\ndef. Generalized Second Price Auction."},"Spontaneous-Organization":{"title":"Spontaneous Organization","links":["Role-of-Money"],"tags":["Economics/Game-Theory","Philosophy/Political-Philosophy"],"content":"Money is an example of spontaneous organization: Devonthink"},"Stable-Marriage-Problem":{"title":"Stable Marriage Problem","links":["CS535-HW5"],"tags":["Computing/Algorithms","Economics/Game-Theory"],"content":"def. Stable Marriage Problem. Given equal number of heterosexual men and women, where each person has ranked all members of the opposite sex in order of preference, how can we marry the men and women together such that there are no non-married pair (m,w) who would both marry each other than their current partners (=in a unstable marriage)?\nExamples.\n\nResident—hospital matching\nhigh school—student matching\n\ndef. Gale–Shapely Stable Matching Algorithm (GSSMA) (=propose-accept algorithm) \n\nEvery day…\n\nEach man who isn’t engaged proposes to their top women that didn’t reject them before\nEvery women who gets proposed to by m1​,m2​,…\n\n…if they’re already engaged to some m0​: accept max[m0​,…], reject everybody else.\n…if they’re not engaged to some m0​: accept max[m1​,…], reject everybody else\n\n\n\n\nRepeat until completes\n\nthm. (GS always terminates at full matching)\nLemma. If woman w rejects man m, in all future scenarios w is engaged to m′ who she likes better than m.\nProof by Contradiction. Suppose there is a man m who is not matched; i.e. m is rejected by everybody.\n\nBy the lemma, all women are engaged to somebody better than m.\nBut if all women are engaged, and there are equal number of men and women, m cannot exist. ■\n\nthm. (GS matching is stable)\nProof by contradiction. Assume there exists an unstable match (m1​,w2​),(m2​,w1​), but w1​≻m1​​w2​ and m1​≻w1​​m2​:\n\n\nm1​ must have proposed to w1​ and been rejected before proposing to w2​ by the structure of the algorithm\nBy lemma above, w1​’s final fiancé, m2​, must have been preferable to m1​. But this is not true. This is a contradiction.■\n\ndef. Valid Partner. w is a valid partner of m if there exists any stable matching where (m,w) is a match (doesn’t have to be found by GS).\nthm. (Proposer wins in GS). GS where men propose, men get the best valid partner possible. In other words all men m with their set of valid partners Sm​⊆Women, never get rejected by any women in Sm​.\nProof by Contradiction. Assume there exists a valid rejection of a man’s proposal by a woman. Consider the first timepoint when a man m gets rejected by a valid partner w.\n\nw must have rejected m because she is already engaged with m′ who she prefers more. (‡)\nw is engaged to man m′. m′’s most preferred partner in Sm​ is w by definition of this timepoint (†)\n\nNow, consider a completely different stable match (not by GS). There must exist a stable matching where (m,w) is a match because w is a valid partner of m. In this matching, let w′ be m′’s match.\nNow, w′ is not the most preferred valid partner Sm′​ because m′ prefers w the most, as we saw in (†). w also is unsatisfied with m because she prefers m′ more (‡). Therefore this is not a stable matching; this is a contradiction. ■\n\nRemark. Extensions of this stable matching problem is done in CS535 HW5."},"Standardizing-a-Random-Variable":{"title":"Standardizing a Random Variable","links":["Normal-Distribution"],"tags":["Math/Probability"],"content":"\nAny random variable X can be standardized\n\n! It doesn’t have to be normally distributed\n\n\nFor a single random variable:\n\nY=SD(X)X−E(X)​\n\nFor X1​…Xn​\n\n"},"Stat-230-Probability":{"title":"Stat 230 Probability","links":["Probability","Conditional-Probability","Independence-(Math)","Random-Variable","Expected-Value","Variance","Distribution-(Math)","Exponential-Family","Binomial-Distribution","Multinomial-Distribution","Hypergeometric-Distribution","Poisson-Distribution","Exponential-Distribution","Uniform-Distribution","Normal-Distribution","Gamma-Distribution","Approximating-Distributions","Central-Limit-Theorem","Poisson-Limit-Theorem","Confidence-Intervals","Change-of-Variable-(Probability)","Joint-Distributions","Covariance","Correlation","Conditional-Distribution"],"tags":["Courses"],"content":"Probability Basics §\n\nProbability\nConditional Probability\nIndependence (Math)\nRandom Variable\nExpected Value, Variance Identities\n\nDistribution Manipulation §\n\nDistribution (Math)\n\nDistribution (Math)\nExponential Family\n\n\nDiscrete Distributions\n\nBinomial Distribution\nMultinomial Distribution\nHypergeometric Distribution\nPoisson Distribution\n\n\nContinuous Distributions\n\nExponential Distribution\nUniform Distribution\nNormal Distribution,\nGamma Distribution\n\n\nApproximating Distributions\nCentral Limit Theorem\nPoisson Limit Theorem\n\nAdvanced Probability §\n\nConfidence Intervals\nChange of Variable (Probability)\nJoint Distributions\nCovariance, Correlation\nConditional Distribution, Conditional Expectation\n"},"Stat-432-Statistics":{"title":"Stat 432 Statistics","links":["Estimator","Sufficiency","Likelihood-(Statistics)","Fisher-Information","Maximum-Likelihood-Estimator","Consistency","Chebyshev's-Inequality","Moment-(Probability)","Confidence-Intervals","Hypothesis-Testing","Student's-t-test","Wilcoxon-Signed-Rank-Test","Wilcoxon-Rank-Sum-Test","Permutation-Test","Bernouilli-Distribution","Chi-Squared","Student's-T-Distribution"],"tags":["Courses"],"content":"\n\n                  \n                  All models are wrong, but some models are useful. \n                  \n                \n\nBasic Statistics §\n\nEstimator\nSufficiency\nLikelihood (Statistics)\nFisher Information\n\nEstimators §\n\nMaximum Likelihood Estimator\nConsistency\n\nChebyshev’s Inequality\n\n\nMethod of Moment (Probability)\n\nStatistical Testing §\n\nConfidence Intervals\nHypothesis Testing\n\nStudent’s t-test\nWilcoxon Signed Rank Test\nWilcoxon Rank Sum Test\nPermutation Test\n\n\n\nNew Distributions §\n\nBernouilli Distribution\nChi-Squared\nStudent’s T-Distribution\n"},"Statistical-Inference-Using-ARMA":{"title":"Statistical Inference Using ARMA","links":["Hypothesis-Testing"],"tags":["Math/Statistics"],"content":"Motivation. Given some data over time X1​,…,Xn​ we want to know if we can use AR/MA to model this. Normally this follows:\n\nPrelimiary Analysis\n\nIs the series stationary?\n\nPlot on graph/Numerical tests\nIf not, can you difference it to get a staionary model? (ARIMA)\n\n\nLogarithmic?\n\n\nAnalysis in the Time Domain: Can we use tools in the Time Series? i.e. Rejecting that it’s just totally random noise\n\nAutocorrelation Function plot (ACF Plot).\nAutocorrelation Test\nPortmanteau tests (Box-Jenkins, Ljung-Box tests)\n\n\nModel Fitting: Choose p,q parameters, i.e how far to look back. Maybel also d.\n\nlook at ACF plots to choose\n\n\nPrediction\n\nAnalysis in the Time Domain §\nlet {Xt​}t∈N​ a weakly stationary time series. We collect samples X1​,…,Xn​. Then:\n\nμ^​=Xˉ=n1​∑∀i​Xi​\nγ^​(h)=n1​∑t=1n−h​(Xt+h​−μ^​)(Xt​−μ^​)\nρ^​(h)=γ^​(0)γ^​(h)​\nMotivation. To see if we can use our tools to model the data, we need to see if the time series has some pattern in it, and not just random noise. The hypothesis that it’s just random noise is called:\ndef. SWN hypothesis. This is the Null Hypothesis when we’re checking if our time series has patterns. For given lag h, does it have any autocorrelation?\n\nH0​:ρ(h)=0\nIf (for many h, see below portmanteau tests) we must accept the null, then this time series is not analyzable at all.\nTo develop a formal statistical test with this hypothesis, we use an important property of the estimated autocorrelation function ρ^​ that can help us identify things about {Xt​}:\nthm. Asymptotic normality of causal-weak-stationary Time Series. Let {Xt​}t∈N​ be:\n\nCausal: Xt​=∑i=0∞​ψi​ϵt−i​+μ\n\nμ: mean of Xt​, in case mean is not zero\n∑i=0∞​∣ψi​∣&lt;∞\n\n\nHas SWN innovations: {ϵi​}t∈Z​∼SWN(0,σϵ2​)\nBehaves Nicely: E(ϵt4​)&lt;∞ or ∑i=0∞​iψi2​&lt;∞ (latter always satisfies by ARMA)\nthen:\n\nn​(ρ^​(h)−ρ(h))→dNh​(0,W)\nwhere\n\nρ^​(h)=[ρ^​(1),…,ρ^​(h)]⊤, same for ρ(h)\nW is a matrix with elements: Wij​=∑k=1∞​(ρ(k+i)+ρ(k−i)−2ρ(i)ρ(k))(ρ(k+j)+ρ(k−j)−2ρ(j)ρ(k)).\nExample. ==Under Null hypothesis H0​== a lag h we have ψ1​=1,ψi=1​=0 and thus:\n\nn​(ρ​^​(h))→dNh​(0,Ih​)\nThis means that under the null hypothesis ρ^​(h)∼N(0,n1​). Thus we need to see how much the autocorrelation (per h) deviates from normal distribution. With α=0.05, P(∣ρ(h)^​∣&gt;n​1.96​)=α=0.05 and thus this is the rejection region. This is shown as the dotted bands below:\n\nFormally:\ndef. Autocorrelation Test. let time series {Xt​} with h-lag ACF ρ(h). We gather n data points X1​,…,Xn​. Then the γ-level test is:\nδ:{H0​:ρ(h)=0 iff ∣ρ^​(h)∣&lt;n​Φ−1(α/2)​H1​:ρ(h)=0 iff ∣ρ^​(h)∣≥n​Φ−1(α/2)​​\nPrediction §\nMotivation. The MSE predictor for a ARMA(p,q) model is simply:\ndef. MSE Predictor for ARMA(1,1)"},"Statistical-Triple":{"title":"Statistical Triple","links":[],"tags":["Math/Statistics"],"content":"A probability triple where the outcome space is instead a sample space, defines a statistical experiment.\n(Ω,F,P) where Ω is the sample space, F=2Ω, and P:F→[0,1]\n\n\n                  \n                  Relationship between probability and statistics \n                  \n                \n\n"},"Stochastic-Calculus":{"title":"Stochastic Calculus","links":["Riemann–Stieltjes-integral","Reimann-Integral","Stochastic-Process"],"tags":["Math/Calculus"],"content":"def. Ito Integral. Let the following:\n\nBrownian Motion B={Bs​∣s≥0}, an Ito process X={Xs​∣s≥0}\n0=t1​&lt;t2​&lt;⋯&lt;tn​=t and thus ti​=nt​i\nThen the Ito integral Yt​ of X(t) with respect to B(t) is defined as such:\n\nY(t)=∫0t​X(s)dB(s):=n→∞lim​i=0∑n​X(ti−1​)[Bti​​−Bti−1​​]\n\nVisualization. Think of the 3D visualization of the Riemann–Stieltjes integral, but both f(x) and g(x) are zigzags.\nThe solution to the Ito integral is also a stochastic process Y={Yt​∣t≥0}. It also has the following properties because the integrator is Bt​\n\nYt​ is a martingale\nE(Yt​)=0\nlet Zt​=∫0t​X(s)2dB(s)\n\nIto Isometry: Var(Yt​)=E(Zt​) \nYt​∼N(0,E(Zt​))\n\n\n\n\n\nRemark. We often use shorthand notation (abuse of notation) to write an integral term like this:\n\ndBt​:=∫0t​1dBs​:=∑i=1∞​1⋅(Bti​​−Bti−1​​)\nXt​dBt​:=∫0t​Xs​dBs​:=∑i=0∞​Xt​(Bti​​−Bti−1​​)\ndt:=∫0t​1dt:=∑i=0∞​1(ti+1​−ti​)=t by definition of the Reimann Integral\nXt​dt:=∫0t​Xs​ds:=\nNow, these also exist but they are trivial (Ito Isometry):\ndBdt=dtdB=0\n(dt)2=0\n(dB)2=dt\n\nReference Table of Common Ito Integrals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic IntegralResultVariance∫0t​dBs​Bt​t∫0t​sdBs​tBt​−∫0t​Bs​ds31​t3∫0t​Bs​dBs​21​Bt2​−21​t21​t2∫0t​Bs2​dBs​31​Bt3​−∫0t​Bs​ds3t2∫0t​eBs​−21​sdBs​eBt​−21​t−1et−1\nMotivation. Unfortunately we cannot calculate ito intergrals directly. For example, as we saw in the above example finding ∫0t​B(s)dB(s) using just the definition was a laborious process. In order to make this easier, we use the following lemma.\nthm. Ito’s Lemma. (Ito’s Chain Rule) The stochastic calculus chain rule. Stated in two ways.\n\nX(t) is an Ito process dX(t)=a(X,t)dt+b(X,t)dB\nf(X,t) is a normal function\nThen:\n\ndf(X,t)=(ft​+afx​+2b2​fxx​)dt+bfx​dB\n\nfx​=∂x∂f​∣x=X​, ft​=∂t∂f​\n\nExample. Suppose we want to find the integral ∫0t​Bu2​−udBu​. Since it’s nearly impossible to find it from the definition of the Ito integral, let us use Ito’s lemma. Arbitrarily set an ordinary function f(x,t)=3x3​−tx.\n\n! Now, it is important to note that we normally don’t know what function to use, but in this case assume we are given a nice f(x,t) that will lead to the answer. Given this function, we then have:\nfx​=x2−t, fxx​=2x, and ft​=−x\nBecause B is a standard brownian motion it conforms to the form dB=0⋅dt+1⋅dB which means for Ito’s lemma a=0,b=1.\nThen, we use Ito’s lemma:\n\ndf(B,t)f(Bt​,t)−\\cancelto0f(B0​,0)3Bt3​​−tBt​​=(ft​+afx​+2b2​fxx​)dt+bfx​dB=\\cancelto0(−x+0+21​⋅2x)dt+(x2−t)dB=(B2−t)dB=∫0t​Bu​−udBu​​​\n■\nReference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotion: Xt​Differential: dXt​=udt+udBt​Bt​dBt​Bt2​2Bt​dBt​+dtBt2​−t2Bt​dBt​Bt3​3Bt2​dBt​+3Bt​dteBt​eBt​dBt​+21​eBt​dteBt​−21​teBt​−21​tdBt​e21​tsinBt​e21​tcosBt​dBt​e21​tcosBt​−e21​tsinBt​dBt​(Bt​+t)e−Bt​−21​t(1−Bt​−t)e−Bt​−21​tdBt​\nthm. Ito’s Product Rule. for two stochastic processes Xt​,Yt​\nd(Xt​Yt​)=Xt​dYt​+Yt​dXt​+dXt​dt​\n\nIto Processes §\ndef. Ito Process. An Ito process X(t)={Xt​∣t≥0} is a type of Stochastic Process that is defined by a certain a(Xt​,t),b(Xt​,t):\nX(t)=X(0)+∫0t​a(Xs​,s)ds+∫0t​b(Xs​,s)dB\n\na(Xt​,t) must be integrable in the ordinary sense\nb(Xt​,t) must be integrable in the stochastic sense\n\nIt actually needs to be integrable twice\nAs an abuse of notation we can write:\n\n\n\ndX=a(Xt​,t)⋅ds+b(Xt​,t)⋅dB\n\nThis is called a Stochastic Differential Equation (even if it’s not actually a differential equation).\nEssentially, a function of an Ito process is also an Ito process.\n\nProperties.\n\nE(dY∣Ft​)=(ft​+afx​+2b2​fxx​)⋅dt\nVar(dY∣Ft​)=(bfx​)2\n"},"Stochastic-Process":{"title":"Stochastic Process","links":["Expected-Value","Markov-Chain","Stochastic-Process","Stochastic-Calculus","Quadratic-Variation"],"tags":["Math/Statistics"],"content":"def. Filtration on a probability space is (Ω,F,{Ft​}t≥0​,P) where filtration {Ft​}t≥0​ is an increasing collection of σ-algebras where: \nfor s≤t,Fs​⊆Ft​\nExample. Consider a two-step coin-flip.\n\n\nω∈Ω are the final future outcomes, HH, HT, TH, or TT.\nF1​ “cut” the outcomes in half, and now we know if we’re in the blue universe (first roll is heads) or the red universe (first roll is tails)\nF2​ “cut” the outcomes again into quarters, and now we know specifically what happened.\nVisualization. See the pie cutting diagram. Obviously, F1​⊂F2​ because if we know the final outcome, we know if the first roll was heads or tails. Rigorously, since σ-algebras need to be closed under union, all “lower resolution” events are contained within the “higher resolution” event.\n\ndef. Adapted process = Natural Filtration. Process {Xt​}t≥0​ is adapted to filtration {Ft​}t≥0​if Xt​ is Ft​-measruable for every t. Or vice versa, the filtration is the natural filtration of the process.\nExample. In the above coin-flip example, let random variables X0​,X1​,X2​ be:\nX0​X1​X2​​=1 for all outcomes={23​if first roll headsif first roll tails​=⎩⎨⎧​4567​if HHif HTif THif TT​​≡E[X0​∣F0​]≡E[X1​∣F1​]≡E[X2​∣F2​]​​\nThen we can easily see Xi​ is Fi​-measurable, and that this process is adapted to the filtration (=filtration is the natural filtration for process X).\nthm. From the adapted=natural process-filtration definition and the filtration definition, we have that filtration {F}: \nFt​=σ(X0​,…,Xt​)\n\n! σ(Xt​)=Ft​, because the former just knows about the current Xt​, and the latter knows about all the past X1​,…,Xt​ thus Ft​=σ(X0​,…,Xt​). This becomes important in the Markov property.\ndef. Time Series is a stochastic process indexed by discrete (integer) time points.\nSee discussion: Is a time series the same as a stochastic process? - Cross Validated\nExample. Process {St​}t=0,1,…​where it is sum of the sequence of random variables: R1​,…,Rn​:\n\nSk​={∑i=1k​Ri​S0​​if k&gt;0if k=0​\nMemoryless, True Fair-Game §\ndef. Martingale. A process {Xt​}t≥0​ is a martingale iff for s&lt;t \nE[Xt​∣Fs​]=Xs​\nIntution. True Fair-game process is a martingale. This doesn’t mean fair game in that when you begin the game and there is no unfair advantage/disadvantage (false fair-game), but that it is a fair game at every single point in time where you roll.\ndef. Standard Markov. A process {Xt​}t≥0​ is a Markov process iff for s&lt;t\nP(Xt​≤a∣σ(X0​,…,Xs​))=P(Xt​≤a∣σ(Xs​))\nIntuition. Path-agnostic process is Markov. This means that giving the path of how you got to current point s doesn’t matter to how the future behaves.\nMotivation. A process being a Markov process is a really powerful statement since we can use the relatively computationally less complex tools of Markov Chains for analysis.\nthm. If for process {Xt​}t≥0​, for every s≤t, σ(Xs​)⊆σ(Xt​) holds then it is a Markov process. ∀s≤t, σ(Xs​)⊆σ(Xt​) is not trivial, because natural filtration contains full past history. This property holding means that current state Xt​ encodes the past state completely, which is not necessarily true. See the Random walk with drift example.\nExamples. Consider these four games:\n\nUrn Game §\nExample. Two red balls and two black balls are in an urn. You pick a ball without replacement for four times. {Ct​}t=1,2,3,4​ where Ci​ is 1 if i-th pick is red, and 0 if black.\nNot Markov §\nP(C3​=1∣F2​)=⎩⎨⎧​021​1​if {RR∗}if {RR∗}if {RR∗}​\nbut\nP(C3​=1∣σ(C2​))(1)(2)​={P(C3​=1∣{∗R∗})P(C3​=1∣{∗B∗})​if {∗R∗}if {∗B∗}​(1)(2)​=P(C3​=1∣{BR∗})⋅P(C1​=1∣C2​=R)+P(C3​=1∣{RR∗})⋅P(C1​=1∣C2​=R)=(21​⋅32​)+(0⋅31​)=31​ is computed similarly=32​​Use Bayes’ Rule​​\nThus L(C3​∣F2​)=d​L(C3​∣σ(C2​)), meaning their distributions are not the same, and thus not path agnostic.\nNot Martingale §\nE[C2​∣F1​]={E[C2​∣{B∗}]=32​E[C2​∣{R∗}]=31​​if {B∗}if {B∗}​\nbut\nC2​={10​if {∗B∗}if {∗R∗}​\nThus trivially not martingale.\nBetting Game §\nExample. Player wins or loses on each round i, but depending on if they won or lost the previous round i−1 they bet more or less, and thus on this round i the payoff changes.\nXn​​=Xn−1​+bn​Rn​=X0​+R0​R1​+⋯+Rn−1​Rn​​since bn​=Rn−1​​​\nwhere\nbn​Rn​​={21​if Rn−1​=1if Rn−1​=0​cockyinsecure​={1−1​p=21​p=21​​​​\nNot Markov §\n\nConsider the game tree. Simply observe the distribution of X3​ and see how well it predicts X4​:\n\nL(X4​∣F3​) has 23 cases for each path\nL(X4​∣σ(X3​)) has 7 cases since 101∗ and 011∗ both equal 1\n\nIs Martingale §\nE[Xt​∣Fs​]​=E[X0​+R0​R1​+⋯+Rs−1​Rs​+Rs​Rs+1​+⋯+Rt−1​Rt​∣Fs​]=Xs​X0​+⋯+Rs−1​Rs​​​+Rs​E[Rs+1​]+E[Rs+1​Rs+2​]+⋯​=Xs​​​\nsince E[Rs​Rs+1​]=0 because\nRs​Rs+1​=⎩⎨⎧​1⋅11⋅−1−1⋅1−1⋅−1​p=41​p=41​p=41​p=41​​\nRandom Walk with Drift §\nExample. Consider process {Xt​}t=0,1,…​ where:\nXn​Rn​​=Xn−1​+Rn​+0.1=X0​+R1​+⋯+Rn​+0.1n={1−1​p=21​p=21​​​​\nIs Markov §\nP(Xn+1​≤a∣Fn​)​=P(Xn​+Rn+1​+0.1≤a∣Fn​)=P(Rn+1​≤ all known under Fn​ a−Xn​−0.1​​∣Fn​)=P(Rn+1​≤a−Xn​−0.1)​since Rn+1​⊥Fn​ ​​\nIsn’t Martingale §\nE[X2​∣F1​]={1.2−0.8​if {1∗}if {0∗}​\nbut\nX1​={1.1−0.9​if {1∗}if {0∗}​\neach case is off by 0.1 due to drift.\n\n\nRi​ are identically distributed.: Stationary\n\n\nE(Wk+1​∣W1​…Wk​)=Wk​ then: Martingale\n\n\nP(Wk+1​=wk+1​∣W1​=w1​,…Wk​=wk​)=P(Wk+1​=wk+1​∣Wk​=wk​): Markov\n\n\n                  \n                  random variable - Why do stochastic processes involve time? - Cross Validated\n                  \n                \n\n\n\nStrong Markov Property §\nMotivation. Consider the following special Markov Chain with just two states, 0,1 and deterministic transitions:\n\nIf at 0, next step is 1\nIf at 1, next step is 0 except:\n\non the first time it arrives, it stays there for one more timestep.\nThus discrete process {Xt​} has distribution:\n\n\n\nXt+1​=⎩⎨⎧​101​if Xt​=0if Xt​=0, t=1if Xt​=0, t=1​Special case​\nThis is standard markov, because Xt+1​ only considers Xt​, the current step. But doesn’t it feel weird that the process depends on what timestep it is on? Shouldn’t a true memorlyless process be independent even of what time it is?\ndef. Stopping Time. Let {Ft​} filtration adapted to process {Xt​}. A random variable T=0,1,⋯&lt;∞ is a stopping time wrt {Ft​} iff:\n{T=n}∈Fn​\nIntutition. Consider stopping time of the first time process its state i, ie T=min{n≥1∣Xn=i​}. Then {T=n}∈Fn​, i.e. at time step n filtration Fn​ is enough to show it’s time to stop.\nExample.\n\nValid: “Stop after exactly 10 iterations”, “Stop the first time state is 10”\nInvalid: “Stop at the biggest value ever” (Impossible), “Stop when the next state is 10” (Impossible)\ndef. Strong Markov. Let T be some stopping time of your choice on process {Xt​} Then {Xt​} has the strong markov property iff for ∀t:\n\n Process starting from t=0 L(XT+t​∣FT​)​​= Process starting at T L(Xt​∣X0​=XT​)​​\nIntuition. Like in the motivation section, the distribution of Xt​ at any timepoint should be independent of the timestep t it is in.\nWeiner Process §\nStandard Weiner Process §\nMotivation. Assume there are random variables X1​… as:\nXi​={n​1​−n​1​​0.5 probability0.5 probability​\n\nlet Wk​ be a stochastic process such that:\n\nW(n)(t)=⎩⎨⎧​0R1​+⋯+Rt​affine extension on closest interval ​if t=0if t∈Nelse​\nThen we get the Weiner process as:\nB(t)=n→∞lim​W(n)(t)\ndef. Brownian Motion. (=Weiner process) B(t) (written as Bt​) is a set of random variables continuous-time indexed and has the following properties:\n\nIs a continuous process\nB0​=0\nBt​−Bs​∼d​N(0,t−s)\n\nThus, Bt​=d​N(0,t)\n\n\nAny interval Ba​−Bb​ and Bc​−Bd​ where [a,b] and [c,d] do not overlap is independent.\nNotation wise, we think of B={Bt​}t≥0​, i.e. a set of random variables indexed by t.\n\nRemark. Brownian Motion can also be defined as an Ito Process that satisfies the following stochastic differential equation:\ndB=μ⋅dt+σ2dB\nTrivially, in the case of standard brownian motion, μ=0,σ2=1 Thus dB=0⋅dt+1⋅dB.\nProperties. Brownian Motion satisfies the following properties:\n\nMartingale Property: Brownian motion is martingale: ∀s≤t,E(Bt​∣Fs​)=Bs​ where Fs​=σ(Bs​)\nMarkov Property:\n\n∀s≥0,{Bs+t​,t≥0} is independent of Fs​ and…\n{Bs+t​,t≥0}=d​{Bt​}\nAlternatively: P(Xu​∈A∣σ(Xs​))=P(Xu​∈A∣Xt​) where any combination s≤t≤u\n\n\nScaling Invariance: If Bt​ is a brownian motion, then aB(a2t)​ is also a brownian motion ∀a&gt;0\nQuadratic Variation property: [B]t​:=∑i=1n​(Bti​​−Bti−1​​)2=t\n\nWith Scale and Drift §\ndef. Weiner Process with Drift and Scaling (WPDS). Such is a Weiner process X(t) that has the following properties:\n\nIs a continuous process\nX(0)=x0​\nX(t+Δt)−X(t)=d​N(μΔt,σ2Δt)\nAny interval X(a)−X(b) and X(c)−X(d) where [a,b] and [c,d] do not overlap is independent.\n\n\nProperties:\n\nμ: drift; the higher, the more it climbs \nσ2: scaling; the higher, the more volatile (See y-axis:) \n\n\n\nthm. Standard WP to Scale and Drift. Given:\n\nB(t) is a standard Weiner process\nX(t) is a Weiner process with drift μ and scaling σ2, with initial value x0​\n⇒ Then the following relationship holds (two equivalent definitions)\n\nX(t)dX(t)​=d​x0​+μt+σB(t)=μ⋅dt+σ⋅dB(t)​​\nGeometric Brownian Motion §\nthm. Geometric Brownian Motion. Given X(t) is a WPDS μ,σ2,x0​ then the following is a geometric brownian motion with initial value S(0): (two equivalent definitions)\nG(t)dG(t)​=G0​exp[X(t)]=G0​exp[x0​+μt+σB(t)]=(μ+2σ2​)G(t)⋅dt+σG(t)⋅dB(t)​​\n\n\n                  \n                  Geometric to WPSD \n                  \n                \nThe following is equivalent:\n\ndS(t)S(t)​=(μ+2σ2​)S(t)⋅dt+σS(t)⋅dB(t)=S0​exp[(μ−2σ2​)t+σB(t)]​​\nProperties of Stochastic Processes §\ndef. Adapted process. A stochastic process {Xt​}t≥0​ is adapted to filtration set {Ft​}t≥0​ if ∀i,Xi​ is a Fi​-measurable function.\nIntuition. Recall that a sigma algebra F can be thought of as “resolution of information”. Xi​ is realized and its information is Fi​. The series of filtrations F1​⊂F2​⊂⋯⊂Ft​⊂⋯⊂F correspond to the series of random variables X1​,X2​,…; for each timestep, the information gets higher and higher resolution.\nMartingale Process §\nMotivation. Many stochastic processes, including the standard brownian motion Bt​ has the property that you can’t predict future trajectory based on information from its past trajectory. We formalize this as a martingale.\ndef. Martingale. A stochastic process {Xt​}t≥0​ on a filtered probability space (Ω,F,{Ft​},P) is a martingale w.r.t. its adapted filtration {Ft​} if:\n∀s≤t,E(Xt​∣Fs​)=Xs​\nThis is equivalent to saying\n\n∀s≤t,E(Xt​)=E(Xs​) by taking expectation on both sides of above\n∀s≤t,E(Xt​−Xs​)=0\n\nThis is a simple proof that brownian motion is a martingale; E(Bt​−Bs​=d​N(0,t−s))=0\n\n\n\nQuadratic Variation §\nMotivation. Quadratic variation is not about variation of probability distributions. It’s a way to measure how “shaky” a function (in this case, a Brownian motion) is in a given interval.\ndef. Quadratic Variation. For a stochastic process {Xt​}, its quadratic variation on interval [0,t], [X]t​ for 0=t0​&lt;t1​&lt;⋯&lt;tn​=t, is:\nn→∞lim​i=1∑n​(Xti​​−Xti−1​​)2"},"Stream-of-Content":{"title":"Stream of Content","links":[],"tags":["Computing/Internet"],"content":"In the internet, the problem is not finding content, but finding good content. Therefore, pick sparingly what you consume.\n\nUse RSS feeds to choose good sources to read\nWhen you have a list of content, pick out what you want, like how you would scoop a bucket from the river.\n\nDon’t try to read the whole thing. It’s not efficient or useful.\nChoose what seems enticing or relevant\nApplies to articles, books, movies, tv shows, games, etc.\n\n\n"},"Strings-in-Rust":{"title":"Strings in Rust","links":["Executable-and-Linkable-Format"],"tags":["Computing/Linguistics/Rust"],"content":"\nAll rust string types use UTF-8\nstr is a string literal. You never use this\n\nEncoded into the program executable (in the .data portion of the ELF binary)\n\n\n&amp;str is a string slice\n\nThis is the borrowed type. You may use this, but not often\nIt references either (1) the above str encoded in the binary or (2) a String class\n\n\nString is a string class\n\nIt has length, size, etc. all the information\nThis is the **owned type\nYou should probably use this.\nYou can always pass a &amp;String into a &amp;str, because &amp;str is just a reference; in this case it refers to the String that is allocated on the heap.\n\n\n\n\n\n                  \n                  Abstract \n                  \n                \nA String is a wrapper around str a string literal. The literal can live in stack or heap, depending on your program. The size is predetermined on compile-time, but you can change its contents.\n"},"Structural-model-of-the-psyche":{"title":"Structural model of the psyche","links":[],"tags":["Psychology"],"content":"Id vs Superego\n\nId: Emotions, desires ,etc.\nSuperego: Moral values, etc.\n"},"Student's-T-Distribution":{"title":"Student's T-Distribution","links":["Besset-Correction"],"tags":["Math/Common-Distributions"],"content":"Motivation. A Generalization of the standard normal\ndef. Student’s T Distribution.\ntn−1​∼σ^/n​∑i=0n​(Xn​ˉ​−μ)2​\nwhere:\n\nXi​∼N(μ,σ2)\nσ^ is the Besset-corrected standard deviation estimator\n\n\n\n\n                  \n                  Tip \n                  \n                \nIt’s called a Student’s t because it was from a beer brewery company, and the company didn’t want the in-house statistician to publish it so he published it secretly.\nIntroduction to the t Distribution (non-technical) - YouTube\n"},"Student's-t-test":{"title":"Student's t-test","links":["Student's-T-Distribution"],"tags":["Math/Statistics"],"content":"See also Student’s T-Distribution\ndef. let X1​,…,Xn​∼iidN(μ,σ2). We are comparing two hypothesis:\n\nH0​:μ≤μ0​\nH1​:μ&gt;μ0​\nThen, let T=σ^/n​Xˉn​−μ0​​ where σ^=n−1E[(Xˉn​−Xi​)]​ is the besset-corrected variance estimator. We know that T∼tn−1​. Thus the γ=1−α-level student’s (=Gosset’s) t-test is:\n\nδ:{H0​H1​​else if T&gt;tn−1​(1−α)​"},"Subject-Object-Dialectic":{"title":"Subject-Object Dialectic","links":["G.-W.-F.-Hegel","Jean-Paul-Satre","Simone-de-Beauvoir"],"tags":["Philosophy/Metaphysics"],"content":"\nG. W. F. Hegel\nJean-Paul Satre\nSimone de Beauvoir\n"},"Subset-Sum":{"title":"Subset Sum","links":["Common-Graph-Problems"],"tags":["Computing/Algorithms"],"content":"Q. Subset Sum Problem. Given a set of real numbers X={x1​,…,xn​} is there a subset of X that sum exactly to a certain integer k\n\nReduce from Vertex Cover\n"},"Substitution-Effect-(SE)":{"title":"Substitution Effect (SE)","links":[],"tags":["Economics/Micro-Economics"],"content":"Imagine you travel to the Cayman Islands with your twin. You both start with the same income. You choose to rent a car with upfront cost, while your brother chooses to take a taxi. Remember, you both have the same income; only different opportunity cost of miles traveled.\ndef. Substitution effect is when you change the consumption of a good due to a change in opportunity costs.\nObserve that you are still on the same indifference curve; the utility is the same.\n\nThe size of the effect has to do with how substitutable the two goods are; the more subsitutable the two goods, the bigger the substitution effect is (obvious verbally and graphically):\n"},"Sufficiency":{"title":"Sufficiency","links":["Exponential-Family"],"tags":["Math/Statistics"],"content":"Recall Exponential Family\nSufficiency §\ndef. let statistic T=T(X); and Xi​ depends on unknown parameter θ. Then T is a sufficient statistic for θ if P[X=x∣T=t] does not depend on θ.\n\n\n                  \n                  Info \n                  \n                \nIf the distribution of Xi​’s depends on unknown parameter θ, but the distribution of Xi​ given T=t does not depend on θ, it must be the case that all information about θ is in T. Once the value of T is given, then θ becomes irrelevant in determining the value of X.\n\n\n\n                  \n                  Info \n                  \n                \nThere’s only one minimally sufficient statistic, and all other sufficient staistics are a function of this.\n\nthm. Fischer-Neymann factorization. T is a sufficient statistic iff:\nfX​(X=x∣θ)=u(X)⋅v(T(X),θ)\ni.e. the joint density function can be factored into a a function of u,v where:\n\nu(X) does not depend on θ\nv(T(X),θ) depends on θ, and depend on X only through statistic T\n⇒ Intuitively, this means when\n\n\n\n                  \n                  Info \n                  \n                \nVery convenient for determining if statistic is sufficient or not.\n\nlem 1. let θ^1​ an estimator for θ and T a sufficient statistic for θ\nthen θ^2​=E[θ^1​∣T] is also a sufficient estimator.\nthm. Rao-Blackwell theorem. Continuing from above,\nMSE[θ^2​]≤MSE[θ^1​]\n\n\n                  \n                  Info \n                  \n                \ni.e. if you throw more data into a statistic, it often becomes a better statistic.\n\n\nIn the exponential family of distributions, you can mostly do one iteration of Rao-Blackwell algorithm to get a pretty good estimator.\n\nBias of θ^ and Bias of Rao-Blackwellized E[θ^∣T=t] is the same\n\n\nIf T is a minimally sufficient statistic, E[θ^∣T=t] is the best you will do\n"},"Supervenience":{"title":"Supervenience","links":[],"tags":["Philosophy/Epistemology"],"content":""},"Sustainable-Development-Goals":{"title":"Sustainable Development Goals","links":["United-Nations"],"tags":["Economics"],"content":"By the United Nations"},"Symbolic-AI":{"title":"Symbolic AI","links":[],"tags":["Computing/Maching-Learning"],"content":"def. Symbolic AI are AI systems that do not use any of the neural network/deep learning methods, but instead use human-interpretable symbols, logic and rules."},"Symbolic-Interactionism":{"title":"Symbolic Interactionism","links":["How-Not-Why","Stagecraft","Social-Roles"],"tags":["Sociology"],"content":"\n\n                  \n                  How Not Why\n                  \n                \n\nDefined social roles and relationship status is how people interact. It’s not the direct mediation of desires or personalities, but simply the interaction of Stagecrafted symbols.\n\nSocial Roles themselves are the symbols that we interact with\nStagecraft is the process by which I personally use symbolic interactionism\nIt is not the direct human-to-human relationships that matter to people, but that\n"},"Targeting-a-Niche":{"title":"Targeting a Niche","links":[],"tags":["Computing/Internet"],"content":"(DevonThink) &gt; The more precise and niche the words I input, the better the internet would ma… | Hacker News"},"Task-Perspectives":{"title":"Task Perspectives","links":[],"tags":[],"content":"Actionable/Waitable §\nis not blocked\nnot done\nfilter by function task.file.filename !== &quot;Foundational Goals.md&quot;\nstarts on or before today"},"Taxation":{"title":"Taxation","links":["Laffer-Curve"],"tags":["Economics"],"content":"Laffer Curve"},"Taxonomic-Rank":{"title":"Taxonomic Rank","links":[],"tags":["Biology"],"content":"\n\nDomain\nKingdom\nPhylum\nClass\nOrder\nFamily\nGenus\nSpecies\n"},"Taylor-Approximation":{"title":"Taylor Approximation","links":[],"tags":["Math/Calculus"],"content":"f(x)≈f(a)+f′(a)(x−a)+2!f′′(a)​(x−a)2+3!f′′′(a)​(x−a)3+⋯+n!f(n)(a)​(x−a)n\nf(x)=n=0∑∞​n!f(n)(a)​(x−a)n"},"Technical-Notes-Index":{"title":"Technical Notes Index","links":["Nimf-Anthy-installation","Hacking-Flash-Apps","Yokosuka","Atsugi","lightdm---How-do-I-hide-a-particular-user-from-the"],"tags":["Computing"],"content":"Tasks for Atsugi-hamonikr §\n\n add volume keybindings to script\n make (sudo) script for nimf-anthy installation (see below)\n Wezterm configs (see)\n\n\nNimf-Anthy installation\nHacking Flash Apps\nSetting cinnamon settings in the command line\nMaizuru: Personal computer\nYokosuka: Personally managed server\nAtsugi: Remotely managed servers\nYokosuka\nAtsugi\nGeneral References §\nGitHub - tmux-plugins/tmux-resurrect: Persists tmux environment across system restarts.\nGitHub - pyoky/vimrc: The ultimate Vim configuration: vimrc\nHow To Configure WebDAV Access with Apache on Ubuntu 14.04 | DigitalOcean\nInstalling with Docker\nInstall Docker Engine on Ubuntu\nHTTP server on Docker with HTTPS\nlightdm - How do I hide a particular user from the"},"Technical-Rate-of-Substitution":{"title":"Technical Rate of Substitution","links":[],"tags":["Economics/Micro-Economics"],"content":"TRS=−∂f/∂k∂f/∂l​=−MPK​MPL​​\nIsoquant\n\nSlope of isoquant is the technical rate of substitution\n\nIsocost\n\nc(l,k)=wl+rk\n"},"The-Human-Condition":{"title":"The Human Condition","links":[],"tags":["Philosophy"],"content":""},"Time-Complexity":{"title":"Time Complexity","links":["Recurrence-Relation"],"tags":["Computing/Algorithms"],"content":"Assumptions:\n\nRAM is constant time access\nMultiplication, addition, etc. are constant time\nConstant time operations are O(1)\nWe concern ourselves with asymptotic behavior, i.e. as n→∞\n\nRecall there are only two models of computation\n\nTime complexity of loops: The time complexity of a loop is proportional to the number of iterations multiplied by the time complexity of the code inside the loop.\nTime complexity of recursion: The time complexity of a recursive function can be analyzed using a Recurrence Relation, which expresses the time complexity of the function in terms of its input size and the time complexity of its subproblems.\n\nRules of Computing Complexity §\n\nRule of sum: If an algorithm performs two precedural operations, and the total time taken by each operation is proportional to the size of the input n, then the time complexity is O(f(n)+g(n)), where f(n) and g(n) are the time complexities of the two operations.\nRule of product: If an algorithm performs two nested operations, and the time taken by the inner operation is proportional to the size of the input n, and the outer operation is performed n times, then the time complexity is O(f(n)∗g(n)), where f(n) is the time complexity of the outer operation and g(n) is the time complexity of the inner operation.\nBig O notation ignores constant factors: When calculating the time complexity of an algorithm, we can ignore constant factors, such as the time taken by basic arithmetic operations, since they do not affect the overall growth rate of the function.\nBig O notation ignores lower-order terms: When calculating the time complexity of an algorithm, we can ignore lower-order terms, such as constants or logarithmic terms, since they become insignificant as n grows larger.\n\nNotation §\nComplexity is most often denoted in poly-logarithmic time:\nO(nklogjn)\nTypes of Complexity Analysis\n\nBest case analysis\nWorst case analysis\nAverage case analysis\nAmortized Analysis\n"},"Time-Series":{"title":"Time Series","links":["Stochastic-Process"],"tags":["Math/Statistics"],"content":"def. Time series defined on a probability space (Ω,{Ft​}t∈Z​,P) with filtration indexed by integers is {Xt​}t∈Z​ with adapted to the filtration.\nMoments of Time Series §\ndef. Moments of time series {Xt​} are:\nMean:μt​Autocovariance:γs,t​​=E[Xt​]=E[(Xt​−μt​)(Xs​−μs​)]​​\ndef. Strong Stationarity. {Xt​} is strongly stationary iff ∀(1…n),∀k\n(Xt1​​,…,Xtn​​)=d(Xt1​+k​,…,Xn+k​)\n\n! This doesn’t mean that moments exist i.e. E[Xt​]=∞ but also strongly stationary.\nThe reason why you don’t write Xt1​​=dXt2​​=d…\nIntution. If inspected for any time period of length n, its same as any other period of length n.\ndef. Weak Stationarity (=Covariance Sationary). {Xt​} is weakly stationary iff\n\n∀t,∀t,s,k, ​ μt​=E[Xt​]γt,s​=γt+k,s+k​​mean is samecovariance is same​​\n\n! This means that moments do exist, i.e. always E[Xt​],E[Xt2​]&lt;∞\nIntution. If inspected at any point, the distribution have the same first and second moments.\nFor a weakly stationary process, the covariance γ(s,t) only depends on the temporal separation ∣s−t∣=: lag. So we write γ(t,t+k)=γ(0,k)=γ(k).\ndef. Autocorrelation Function (ACF). Weakly stationary process {Xt​} with mean μ and variance γ(⋅) is\n\nρ(h):=ρ(Xh​,X0​)=γ(0)γ(h)​\nMotivation. Seems like it’s simply the standardized version of covariance. But ACF is very important because it fully characterizes the dependence structure of the process. ¶we can use the sample autocorrelations, compare with the theoretical ACF and see which model to use. \nSimple Processes §\ndef. White Noise Process. (WN). {ϵt​} is a WN(μ,σ2) process iff\n\nϵt​ has central moments μ,σ2\n\ni.e. Weakly stationary\n\n\n! AND ∀h=0,ρ(h)=0 i.e. no two time points are correlated\ndef. Strict White Noise Process (SWN). {ϵt​} is a SWN process iff:\nϵi​ are all iid\nE[Xt​]=0, Var[Xt​]&lt;∞\ndef. Martingale Difference Process. {Xt​} adapted to filtration {Ft​}is a martingale difference process if:\n\nE[Xt​∣Fs​]=0\nCompare this to a normal martingale process. It’s called martingale difference because it serves as “difference” process, i.e. there exists a martingale process {Mt​} st:\nXt​=Mt+1​−Mt​\n\nCausal Process. §\nMotivation. You may naiively think that {Xt​} being adapted to filtration {Ft​} is enough to make it “causal.” And technically that is true. But consider the case of AR(1). let {Xt​} be AR(1) with coefficient ϕ=2\nXt​=2Xt−1​+ϵt​\nExpanding this recursively we see that this is explosive:\nXt​=i=0∑∞​2⋅ϵt−i​→n→∞∞\nExplosive processes like these have no real applicatbility in statistics. So we consider weakly stationary processes only.\ndef. Causal Process. {Xt​} is a causal process if\n\nIt is weakly stationary\nIt is adapted to filtration {Ft​}.\nThis is equivalent to the following definition: {Xt​} can be written as an infinite series of white noise {ϵt​}∼WN(0,σϵ2​):\n\nXt​=ψ1​ϵt−1​+ψ2​ϵt−2​+⋯=i=0∑∞​ψi​ϵt−i​\nwhere\n\n! we must have ∑i=0∞​∣ψi​∣&lt;∞, else it is not stationary.\nNaturally, causal process can be expressed into an MA(∞) process that isn’t explosive\nAutoCov and ACF is:\n\nγ(h)ρ(h)​=σϵ2​i=0∑∞​ψi​ψi+∣h∣​=∑i=0∞​ψi2​∑i=0∞​ψi​ψi+∣h∣​​​​\nInvertible Process. §\nMotivation. This is also a housekeeping-style definition so that we can construct the current shock based on past data.\ndef. Invertible Process. {Xt​} is an invertible process it can be expressed in the form:\nϵt​=i=0∑∞​πi​Xt−i​\n\nNaturally a AR(∞) process\n"},"Torii-Moi":{"title":"Torii Moi","links":[],"tags":["People"],"content":""},"Tradable-Inflation-Protected-Securities-(TIPS)":{"title":"Tradable Inflation-Protected Securities (TIPS)","links":[],"tags":["Economics/Finance"],"content":"Tradable Inflation-Protected Securities (TIPS) are a type of bond issued by the treasury that are like bonds (issued by govn’t, pays coupon), but its principal changes based on inflation."},"Trade-Through":{"title":"Trade-Through","links":["Efficient-Market-Hypothesis"],"tags":["Economics/Finance"],"content":"is when a transaction occurs at a worse price than the market price. It’s usually prevented by regulation. A type of market inefficiency, a failure of the Efficient Market Hypothesis"},"Traffic-Routing":{"title":"Traffic Routing","links":["Maximum-Flow-Problem","Potential-Game","Nash-Equilibrium"],"tags":["Computing/Algorithms","Economics/Game-Theory"],"content":"Similar to Maximum Flow Problem. An instance of a Potential Game.\nAtomic Traffic Routing §\n\nThis time, there are n cars (not infinitesimally small). If player i chooses path Pi​, then:\n\nme​: traffic (=# of cars) flowing thru edge e\nIndividual cost: ci​(Pi​,P−i​​)=∑e∈Pi​​ce​(me​)\nPotential Function: ϕ(P)=∑∀e​∑j−1me​​ce​(me​)\n\nThis is similar to the integral of costs for each edge: \n\n\n\nthm. (Potential function minimization derives PNE) Minimizing the following potential function will give you the PNE.\nΦ(f)= Over every edge e∈E∑​​​ Area under cost function i=1∑fe​​ce​(i)​​\nProof. The inner summation above is Intuitively the area under the cost, as illustrated above.\nThen, consider an agent i deviated from path Pi​ to Pi​^​, which as a whole is a change from flow f to f^​. Then the difference in potential of these two flows is:\nΦ(f^​)Φ(f^​)−Φ(f)​=Φ(f)+ add deviated edges e∈Pi​^​∖Pi​∑​ce​(f+1)​​− remove undeviated edges e∈Pi​∖Pi​^​∑​ce​(f)​​= i ’s cost at Pi​^​e∈Pi​^​∑​ce​(f^​)​​− ...at Pi​ e∈Pi​∑​ce​(f)​​​​\n\nThus, each user’s goal is to minimize this potential function. Therefore the minimum of the potential function is the pure strategy nash eqilibirum. ■\nthm. (existence of PNE). Proof outline. This potential function will have one global minimum, if the cost functions are monotonically increasing (v.v. strictly increasing) and continuous, making Φ a (v.v. strictly) convex function which has a global minimum (v.v. a single global minimum). This shows that it PNE will always exist for monotonic cost functions. ■\nNonatomic Traffic Routing §\nDefinitions §\n\nflow on edge e is xe​\nCongestion Function ce​(xe​) i.e. time taken\nIndividual Delay for Path P: ∑e∈P​ce​(xe​)\nTotal Delay for all paths: ∑∀cars​∑e∈P​xe​⋅ce​(xe​)\n\nPigou’s Example §\n\n\nNash Equilibrium when\n\nFor all users of edge a, ca​(xa​)≤cb​(xb​)\nFor all users of edge b, cb​(xb​)≤ca​(xa​)\n\n\nTotal delay =(0⋅1)+(1⋅1)=1\n\n\nGlobal Optimum when:\n\nTotal Delay​=(xa​⋅1)+(xb​⋅xb​)=1−xb​+xb2​​​\n⇒ Total delay minimized when half goes thru a, other half goes thru b\nBraess’s Paradox §\ni.e. adding a new road may worsen outcome for Nash Equilibrium (but not in global minimum)\n\n\nConsider before and after adding edge from b→a, a superhighway with zero delay cost.\nNE before adding blue edge:\n\n0.5 on top (s→a→t)\n0.5 on bottom (s→b→t)\nTotal Delay=23​\n\n\nNE after adding blue edge (superhighway):\n\nall passes thru green path (s→b→a→t)\nTotal Delay=2 (worse!)\n\n\n\nFor Social Planner… §\nObserve that congestion network is a convex optimization problem:\n\nTotal Delay function is a convex function\nConstraints (=conversation of flow) are linear:\n\n∀v inflow = outflow\n∑ flow at source/target is 1\nfe​≥0\n\n\n\nFor Obtaining Nash Equilibirum… §\nthm. Routing NE Uniqueness. For every routing problem there is only one NE.\nthm. Obtaining NE of Congestion Network. Minimizing the following function ϕ will yield the unique NE of any congestion network:\nϕ(f​)=e∈E∑​2ae​fe2​​+be​fe​​​\nProof Sketch. The following definition of ϕ will satisfy the definition of a potential function\nϕ(f​)​:=e∈E∑​∫0fe​​ce​(x)dx=e∈E∑​[2a​x2+bx]0fe​​=e∈E∑​2ae​fe2​​+be​fe​​​\n■"},"Training-Tricks":{"title":"Training Tricks","links":[],"tags":["Computing/Maching-Learning"],"content":""},"Traveling-Salesperson-Problem":{"title":"Traveling Salesperson Problem","links":["Minimal-Spanning-Tree-Problem","Depth-First-Search"],"tags":["Computing/Algorithms"],"content":"\n\n                  \n                  Problem: \n                  \n                \nGiven a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city?\n\n\n\nBrute Force: O(∣V∣!)\n\n\nHeld–Karp algorithm: O(n2⋅2n)\n\n\nRecord: O(1.728n)\n\n\nTSP is not in NP, and is NP-Hard\n\n\nMetric TSP (MTSP) §\nalg. Tree-MTSP Approximation.\n\nRun MST\nRun DFS on the MST, record pre-time\nTraverse the graph in the pre-time order\n\n\nIs 2-approximation\n\nalg. Christofids MTSP Approximation"},"Tree":{"title":"Tree","links":["Graph"],"tags":["Computing/Data-Structures"],"content":"def. Tree. A Tree is an undirected connected acyclical Graph\n\nBalanced Tree\nBinary Search Tree\n"},"Turing-Machine":{"title":"Turing Machine","links":["Limits-of-Math-and-Computing"],"tags":["Computing/Formal-Languages"],"content":"A Turing machine:\n\nIs most powerful type of automation (probably.)\nCannot solve the halting problem.\nCan be used to prove Limits of Math and Computing.\n\ndef. Turing Machine M is a turing machine defined as a tuple:\nM=(Q,Σ,Γ,δ,q0​,B,F)\nwhere the previously unencountered symbols are:\n\nQ: States\nΣ: Input Alphabet\nΓ: Tape Alphabet (superset of Σ)\nq0​: Start state\nF: Set of en states\nB: Blank (also written as □)\nδ:Q×Γ→Q×Γ×{L,R}\nConfiguration and transition of a TM is denoted:\nConfiguration: wqv⊢w′q′v′\n\nIn this case the head is reading the first symbol of v, i.e. the right-side letter.\nthe ”⊢” indicates a state transition\n\n\nTransition function δ: δ(q,v)=(q′,v,R)\n\nThe configuration is denoted as (state, write symbol, move left/right)\n\n\n\nTuring Machines as Language Recognizers/Acceptors §\ndef. Language of a turing machine. For turing machine M with language L, String w∈L if:\nL(M)={w∣q0​w⊢vqf​w}\nYou can also think of it as the turing machine “halting” on the final state.\n\nIf TM halts on a final state, w is accepted\nIf TM halts on a non-final state w is not accepted\nIf TM doesn’t halt, w is not accepted\n\nTuring Machines as a Transducer (=Transformation on a language) §\ndef. Turing-Computable. A function f(w) is Turing-Computable if:\n∀w,  q0​w⊢∗qf​f(w)\nExample of a Turing Machine representing a turing-computable function:\n\nTuring Machine Building Blocks §\n\nM1​→M2​\nM1​→xM2​: run if x is the current output ← you can also have multiple conditionals\n\nYou have the building blocks of commonly-used TMs.\n\ns: start, h: halt\nx: write symbol x onto tape\nL,R: Move left, right\nLa​,Ra​: Move left or right until you see a in tape ← make sure there is an a on tape or it won’t halt\nL¬a​,R¬a​Move left or right until you don’t see a in input\n→a,b}→w: symbols a,b are represented as variable w ← avoids having to write two identical machines for each symbol\n\nSome more advanced building blocks:\n\nC: Copy with a zero in the middle. e.g. abba→abba0abba. Tape head starts and ends in the beginning symbol.\nSL​,SR​ Shift left what is on the right, v.v. The symbol on the head is erased.\n\nExample of using building blocks to simplify a turing machine:\n\nTuring Machine Equivalents §\n\nTM with Stay option: δ:Q×Γ→Q×Γ×{L,R,S}\nMultitrack TM: One tape, but split into n cells: δ:Q×Γn→Q×Γn×{L,R}\n\nDiagram\n\n\n\nSemi-infinite TM: The tape is infinite only in one direction\n→ Proof by “folding over” the standard TM into multi-track.\nMulti-tape TM: δ:Q×Γn→Q×Γn×{L,R}n\nOff-line TM: Two-tape; one tape is input, the other the read/write tape. δ:Q×Γ2→Q×Γ×{L,R}2\nNon-deterministic TM\n\nNPDA with 2 stacks\n\n\n\nUniversal Turing Machine §\nEvery TM can be binary encoded by a binary number:\n\nThe Universal Turing Machine is a 3-tape turing machine that simulates a standard Turing Machine M. Each of the tapes are:\n\nTape A: Binary encoding of a simulation of M\nTape B: The tape of M\nTape C: M’s current state\n"},"Two-Stage-Lease-Squares-Regression":{"title":"Two-Stage Lease Squares Regression","links":["Consistency"],"tags":["Math/Statistics"],"content":"Motivation. Instrumental variables are an advanced way of removing endogenity. Suppose you want to see the effects of police on crime rates. But does crime cause more police to be hired, or does more police cause less crime? Because of this circularity, it’s hard to isolate simply the causality from police to crime rates.\nSo to isolated the exogenous variation in police hiring (i.e. get the pure exogenity), we can use a proxy or instrumental variable: firefighters. Exogenous factors both cause firefighters and police to be hired (policy changes, citizen support) but they don’t cause crime rates to increase.\n2-Stage Lease Squares §\nInstrumental variables are implemented using 2SLS.\n\nWe have the first stage (=reduced form), where we regress the independent variable (police) with instrumental variable (firefighters):\n\n police X1i​^​​​=γ0​+γ1​ firefighters Zi​​​+γ2​ some control X2i​​​+νi​\n\nThen we take the ==police estimator X^it​ (not the actual data!)== and then regress dependent (crime) against estimated independent (estimated police):\n\nYi​=β0​+β1​ estimated X^1i​​​+β2​ control X2i​​​+ϵi​\nVisualization. \nWhat Are Good Instrumental Variables? §\n\nInclusion condition: Z needs to have meaningful influence on X\n\nSatisfied when in the first stage γ1​ is significant at 1%.\n\n\nExclusion condition: Z needs to have no infludence on Y\n\nin other words, Z and ϵ should not be correlated.\nBut this cannot be tested directly because ϵ is not observed.\n\n→ thus the exclusion condition is more of a “it’s probably okay” argument\n\n\n\n\n! running regression Y against Z might seem prudent but actually if Z is correlated with X^ and X^ with Y then it will always be significant\nThe best we can do is include more controls we assume are inside ϵ that correlate with Z, such that the new ϵ with more controls are not correlated to Z.\n\n\n\nMultiple Instruments §\nPerform the first stage with each instruments Z1​,Z2​,…:\nX1i​=γ0​+γ1​Z1i​+γ2​Z2i​+⋯+νi​\nand then same for the second stage.\nOver-identification test tests the exclusion restriction. if you’ve gotten the correct multiple instrumental variables. (Overidentification is a good thing.) If we want to see if Z1​ and Z2​ together satisfy the exclusion condition we test:\n\nFirst stage just with Z1​ to get X1​^​Z1​. Then the second stage to get β1​^​Z1​\nFirst stage just with Z2​ to get X1​^​Z2​. Then the second stage to get β1​^​Z2​\nIf β1​^​Z1​≈β1​^​Z2​ then good!\n! but if they’re similar this might just mean that both are bad in similar ways…\nAnd If they’re different, there’s no way of knowing which one is the better one\nAlternatively, run\n\nϵi​^​=α0​^​+α1​^​Z1i​+α2​^​Z2i​+α3​X2i​\nand then F-test: H0​:α1​=α2​=0. A exclusion-condition compatible IV should not be jointly significant. But this is also inaccurate in the same way described above.\nComparison with Ordinary Least Squares §\nWe should use 2SLS instead of OLS when we know that X is very much endogenous, and we have found a good IV, Z that satisfies the inclusion and exclusion conditions. To test if X is endogenous enough for 2SLS to be useful, we use the following:\ndef. Durbin-Wu-Hausman Test of X’s endogenity. Observe first the fact that assuming Z is exogenous:\n\nif X is exogenous: β^​OLS≈β^​2SLS\n\ni.e. ρX,ϵ​≈0 already, so why use IV or 2SLS?\n\n\nif X is endogenous: β^​OLS=β^​2SLS\n\ni.e. ρX,ϵ​=0 so we must use IV or 2SLS!\nThe test has null hypothesis H0​:β^​OLS=β^​2SLS. If we reject the null, then we should use 2SLS.\n\n\n\nBias in 2SLS §\n\ndef. Quazi-instruments are instruments where there exists some (small) ρZ,ϵ​=0 (Usually okay, see below)\ndef. Weak Instruments are instruments where there exists some ρZ,X1​​=0. (Usually bad, see below)\nIt’s sometimes okay to have some correlation between Z and ϵ, but having correlation between Z and X1​ is pretty bad. To see why, observe the 2SLS bias of β1​^​\n\nn→∞lim​β1​^​2SLS=β1​+ bias ρZ,X1​​ρZ,ϵ​​σX1​​σϵ​​​​\nCompare this to vanilla OLS (for Consistency ρX,ϵ​=0):\nn→∞lim​β1​^​OLS=β1​+ bias ρX,ϵ​σX​σϵ​​​​\nThis implies\n\nWhen it has a strong first stage relationship (=ρZ,X1​​ is small) the 2SLS’s β1​^​2SLSisbeer\n&amp; If ρZ,X1​​ρZ,ϵ​​&lt;1 then 2SLS has less bias than vanilla OLS even if ρZ,ϵ​=0\n! If ρZ,X1​​ρZ,ϵ​​&gt;1 then 2SLS amplified any small correlation ρZ,ϵ​ and easily becomes worse than vanilla OLS\n\n\n\n                  \n                  Rule of Thumb for determining weak instruments \n                  \n                \nUse 2SLS with instrument Z when in the first stage regression the test\n\n{&gt;H1​H0​​if γ0​=γ1​=γ2​=⋯if else​&gt;\n\ni.e. an F-test, F&gt;10.\n\n\nEven when ρZ,ϵ​=0, since the above equations are for limn→∞​, when n is small bias still exists (in the same direction as OLS).\n\nThis bias will eventually go away in bigger n (or, of ρZ,ϵ​=, go the opposite way!)\n\n\n\nPrecision of 2SLS §\nRecall multivariate OLS variance of coefficients is:\nVar(β1​^​)=N⋅Var(X1​)⋅(1−R12​)σ^2​\nThe 2SLS variance of coefficient is:\nVar(β1​^​)=N⋅Var(X1​^​)⋅(1−RX1​^​NoZ2​)σ^2​\nThe differences are:\n\nσ^2(=second stage regression variance) may be larger because ϵ has been purged\n\nThis simplies that explainatory power in the observed X1i​ that were correlated with ϵ is purged (=thus total reduced)\n\n\nVar(X1​^​), not Var(X1​) because we use estimates (see above) not actual data during regression\n\nVar(X1​^​) is probably smaller because we purged ϵ-related variance.\n\n\n! RX1​^​NoZ2​ is the R-squared from the new regression\n\nX1,i​^​=π0​+π1​X2i​+ηi​\n- This regression determines how much does $X_{2}$, not $Z$, determines $\\hat{X_{1}}$.\n- $R^{2}$ in this regression thus measures the collinearity of $\\hat{X_{1}}$ and $X_{2}$ (=controls)\n    - btw, $R^{2}$ in the second regression doesn&#039;t mean shit.\n- The lower the explanatory power of $Z$ on $\\hat{X_{1}}$, the higher this value is, and the higher the variance of the 2SLS coefficient is.\n"},"Types-of-Demand-Curves-(MicroEcon)":{"title":"Types of Demand Curves (MicroEcon)","links":["Elasticity-of-Substitution","Uncompensated-Demand-curve","Marginal-Willingness-to-Pay","Cross-Price-Demand-Curve"],"tags":["Economics/Micro-Economics"],"content":"Demand Curves §\nDemand is a relationship between prices, income, and quantity consumed.\nExogenous:(p1​,p2​,I)↔Endogenous:(x1​,x2​)\nProperties relating these variables are:\n\nx1​↔I: Income Elasticity of Demand (YED)\nx1​↔x2​: Elasticity of Substitution (EoS)\np1​↔x1​: Own-price Demand Curves…\n\nUncompensated Demand curve\n\n= Ordinary Demand Curve\n= Marshalian Demand Curve\n\n\nCompensated Demand Curve\n\n= Marginal Willingness to Pay Curve\n= Hicksian Demand Curve\n\n\n\n\np1​↔x2​: Cross-Price Demand Curve\n\np1​↔x2​: Cross Elasticity of Demand (XED)\n\n\n"},"Types-of-Goods-(Economics)":{"title":"Types of Goods (Economics)","links":["Income-Effect-(IE)","Uncompensated-Demand-curve","Game-Theory","Prisoner's-Dillemma"],"tags":["Economics/Micro-Economics"],"content":"Economic Goods §\nNote:\n\nNormal Good ⊂ Ordinary Good\nGiffen Good ⊂ Inferior Good\n\nDepending on Price and Quantity Demanded §\n\nOrdinary Good: Follows the Law of Demand: Price∝1/QDemand​\nGiffen Good: Price∝QDemand​\n\n⇒ Cause: Income Decrease ⇒ Buy cheaper Giffen Good\nIrish Famine. the price of potatoes and meat increased subsequently. Compared to meat, potatoes could be much cheaper as a staple food.\n\n⇒ Potatoes were a Giffen Good\n\n\n\n\n\nDepending on Income and Quantity Demanded §\n\n\n                  \n                  Notation: \n                  \n                \n\nYED:= Income elasticity of demand-\nIncome Effect (IE) is important in understanding economic goods\n\n\n\nNormal Good Income∝QDemand​\n\ni.e. ∂I∂x1​(p1​,p2​,Iˉ)​&gt;0 (derivative from Marshallian Demand)\nNecessity Good (Necessities)\n\n0&lt;∂I∂x1​​&lt;1\n\n\nLuxury Good\n\n1&lt;∂I∂x1​​\n…i.e. a normal good for which the proportional consumption increase exceeds the proportional Increase\n\n\n\n\nInferior Good: Income∝1/QDemand​\n\ni.e. ∂I∂x1​​&lt;0\n\n\nQuasilinear Good: QDemand​ doesn’t change on income change\n\ni.e. ∂I∂x1​​=0\n\n\n\nFor the set of all goods we can thus show the following Venn diagram:\n\nGame Theory Goods §\n\n\nCommon-pool resources are also called Common Goods.\nPublic Goods\n\nThe production of a public good is a Prisoner’s Dillemma situation.\nBasic examples: national defense, street lighting, a good schooling district, etc.\n\n\n"},"Types-of-Loans":{"title":"Types of Loans","links":["Annuity"],"tags":["Economics/Finance"],"content":"Types of Loans\n\nAmortizing Loan (=Annuity): borrower regularly pays back interest and principal\nSimple interest: borrower regularly pays back only interest. Principal is paid back altogether at the end.\n"},"UI-Design-Tips":{"title":"UI Design Tips","links":["dont-make-the-user-think","(Article)-Magic-Ink---Information-Software-and-the-Graphical-Interface"],"tags":["Computing/Human-Interface"],"content":"Principles §\ndont make the user think\n(Article) Magic Ink - Information Software and the Graphical Interface\nStart here §\nDetermine a color pallette\n\nWhat “atmosphere” are you going for? what\n"},"Unapologetic-Use-of-Materials":{"title":"Unapologetic Use of Materials","links":["Jony-Ive"],"tags":["Design"],"content":"As Jony Ive says in designing of iPhohne 5C, using plastic unapologetically."},"Uncompensated-Demand-curve":{"title":"Uncompensated Demand curve","links":["Homogenous-Function","Utility-Maximization","Types-of-Goods-(Economics)","Utility-Maximization-with-Endowments","Income-Effect-(IE)"],"tags":["Economics/Micro-Economics"],"content":"\n\n                  \n                  Abstract \n                  \n                \n\nThe canonical, standard demand curve when we normally say “demand of good x”\nIn theory, it should be the actual demand curves of individuals\nAll Marshalian demand curves are HD0 in (prices,income)\n\n\nHow to Derive OPDC §\nVisually:\n\nPlot the indifference map and budget line for goods x1​ and x_2=\\text{&quot;\\ of other goods”}.Chooseanoptimalbundlex_1atpricep_1.Plotthisontheprice−quantitygraph:A(x_1,p_1)$\nChange the price p1​→p1′​; i.e. change the gradient of the budget. See what substitution and income effects it produces.\nThen find the optimum bundle for the new price: C(x1′​,p1′​). Connect these points together.\n\nAnalytically:\n\nmaxx1​,x2​​ u(x1​,x2​) such that I=p1​x1​+p2​x2​ using Utility Maximization\nyou will get x1​(p1​,p2​,I),x2​(p1​,p2​,I)\n\ni.e. quantity demanded as a function of prices and income\nThis is the demand function → plot this to get demand curve\n\n\n\nAs stated above: OPDC must be HD0; check if it is!\nAnalysis of Goods §\n\nIn a demand curve, Price is the negative gradient of the budget line. (∵g=−p2​p1​​, and we defined p_2=\\1$)\n\nDepending on Price §\n∂p1​∂x1​​ &gt;0&lt;0​⟹Ordinary Good⟹Giffen Good​​\n\nOrdinary goods’s demand functions x1​ are homogenous in (p1​,p2​,I)\n\n→ Inflation in all prices as well as income doesn’t change anything\n\n\n\nDepending on Income §\n∂I∂x1​​ &gt;0=0&lt;0​⟹Normal Good⟹Quasilinear Good⟹Inferior Good​​\n\n\nGraph (a) shows a normal good’s OPDC.\nGraph (b) and (c) shows a inferior good’s OPDC.\nGraph (c) is the strange Giffen Good.\n\nLabor Supply Curve §\nSee also Utility Maximization with Endowments\nDeriving the Labor Supply Curve works similarly to the OPDC, with the following differences:\n\np1​ = (gradient of budget line) = (opportunity cost of leisure in dollars) = ∗∗w (wage)**\nThere’s no exogenous income. You’re endowed a bundle with zero consumption and certain hours of leisure.\n\n→ You can “sell off” your leisure hours at price w\n\n\n\nThus as wages increase (= price of leisure in dollars)\n\n→ the shift in the budget line happens from blue→pink in graph below…\n\n…with a stationary leisure endowment at the x-intercept and a clockwise rotation.\n\n\nThen after you draw the OPDC for leisure, you flip the curve horizontally.\n(since endowment=leisure hours+work hours.)\n\n\n\nGraph (a) shows the case where leisure is a normal good, but IE &gt; SE.\nGraph (b) is Normal good, IE &lt; SE.\nGraph (c) shows when leisure is an inferior good.\n\n\n\n                  \n                  Warning \n                  \n                \n\nWhen discussing labor supply, the Income Effect (IE) is called the wealth effect (WE). We’re not discussing why.\nBackward Bending Supply of Labor §\nIn real life, labor supply curves look like this:\n\nThis is because workers will trade-off leisure and work hours.\n→ the higher the person’s wage, they will work more; but at some point their wage is large enough and they start to enjoy life more. This is called the Backward Bending Supply of Labor.\nInteresting IRL Analysis §\ncdixon | How bundling benefits sellers and buyers\nDeriving Uncompensated Demand §"},"Unconstrained-Maximization":{"title":"Unconstrained Maximization","links":["Critical-Point"],"tags":["Math/Calculus"],"content":"\n\nThe first-order necessary condition for a maximum or minimum. If a function has a maximum or minimum at a certain point, then its derivative at that point is zero\n\nIf f(x) has a maximum or minimum at x=c, then f′(c)=0.\n\n\n\nThe second-order necessary condition for a maximum. If a function has a maximum at a certain point, then its second derivative at that point is less than or equal to zero:\n\nIf f(x) has a maximum at x=c, then f′′(c)≤0.\n\n\n\nThe second-order sufficient condition for a maximum. If a function’s first derivative is zero at a certain point and its second derivative at that point is less than zero, then the function has a maximum at that point:\n\nIf f′(c)=0 and f′′(c)&lt;0, then f(x) has a maximum at x=c.\n\n\n\nTo find the unconstrained maximum of a function, we can set its derivative equal to zero and solve for ‘x’. Then, we use the second derivative test to determine whether each solution is a maximum or minimum.\n\nFind unconstrained maximum of f(x)\n\nset f′(x)=0 and solve for ‘x’.\nUse f′′(x) to verify maxima.\n\n\n\n\n"},"Unemployment":{"title":"Unemployment","links":[],"tags":["Economics/Macro-Economics"],"content":"Unemployment u=N+UU​\nParticipation Rate p=populationN+U​\nVacancy Rate v=U#vacancies​\nMeasured using three variables:\n\nu the unemployment rate\np the participation rate\nv the vacancy rate\n…where…\nU is the number of registered unemployed people\nN is the number of employed people\nThe labor market is tight when Dlabor​&gt;Slabor​ and loose vice versa.\n\nTypes of Unemployment §\n\nCyclical: due to business cycles, ¶during recessions companies lay off workers\nSeasonal: due to natural yearly fluctuations, ¶hotel workers unemployed during off-season\nStructural: due to ¶ training and job-searching imperfections, baked into the system\n"},"Uniform-Distribution":{"title":"Uniform Distribution","links":[],"tags":["Math/Common-Distributions"],"content":"Uniform Distribution §\ndef. Uniform Distribution. X has uniform distribution of it has a uniform density on interval (a,b):\nX∼Unif(a,b)fX​(x)={b−a1​0​x∈(a,b)else​FX​(t)=∫−∞t​fX​(x)dx=⎩⎨⎧​b−at−a​10​t∈(a,b)t&gt;bt&lt;a​​\nE(X)=2a+b​       SD(X)=23​b−a​\nEstimators §\nlet\n\nX∼Unif(a,b)\nX1​,…,Xn​∼iidUnif(a,b)\n\nLog likelihood:\nlnLn​(a,b∣x1​,...,xn​)=ln(b−a)n1​\nscore:\nsn​(a)=b−an​\nsn​(b)=b−a−n​\nMLEs:\na^MLE​=min(X1​,...,Xn​)\nb^MLE​=max(X1​,...,Xn​)"},"Univariate-Distribution-Relationship-Chart":{"title":"Univariate Distribution Relationship Chart","links":["Distribution-(Math)"],"tags":["Math/Statistics"],"content":"Useful resource for visualizing the relationship between distributions.\nURL: &lt;http://www.math.wm.edu/~leemis/chart/UDR/UDR.html&gt;\n"},"Unrestricted-Grammar":{"title":"Unrestricted Grammar","links":[],"tags":["Computing/Formal-Languages"],"content":"\n\ndef. If G is an Unrestricted Grammar, then L(G) is recursively enumerable [=TM equivalent]. This is equivalent to all productions following the form:\n\n\n(V∪T)+→(V∪T)∗\ndef. G is a Context-Sensitive Grammar if its productions follow the form:\n(V∪T)+→(V∪T)∗and∣LHS∣≤∣RHS∣andthere is no λ in the RHS"},"Use-value,-Exchange-value":{"title":"Use value, Exchange value","links":["Use-value,-Exchange-value","Fetishization","Emergent-Phenomena"],"tags":["Philosophy/Marxism"],"content":"consumption. They constitute the material content of wealth, whatever its social form may be. In the form of society to be considered here they are also the material bearers [Triiger] of … exchange-value.\nThis common element exchange value cannot be a geometrical, physical, chemical or other natural property of commodities.\nBut clearly, the exchange relation of commodities is characterized precisely by its abstraction from their use-values.\nIf then we disregard the use-value of commodities, only one property remains, that of being products of labour.\nIf we make abstraction from its use-value, we abstract also from the material constituents and forms which make it a use-value. It is no longer a table, a house, a piece of yam or any other useful thing.\nWith the disappearance of the useful character of the products of labour, the useful character of the kinds of labour embodied in them also ~sappears; this in tum entails the disappearance of the different toncrete forms of labour.\netc. This is their plain, homely, natural form. However, they are only commodities because they have a dual nature, because they are at the same time objects of utility and bearers of value. Therefore they only appear as commodities, or have the form of commodities, in so far as they possess a double form, i.e. natural form and value form\nFetishization\nthing. But as soon as it emerges as a commodity, it changes into a thing which transcends sensuousness. It not only stands with its feet on the ground, but, in relation to all other commodities, it stands on its head, and evolves out of its wooden brain grotesque ideas far more wonderful than if it were to begin dancing of its own free will. 21\nThe mysterious character of the commodity-form consists therefore simply in the fact that the commodity reflects the social characteristics of men’s own labour as objective characteristics of the products of labour themselves,\nreligion. There the products of the human brain appear as autonomous figures endowed with a life of their own, which enter into relations both with each other and with the human race. So it is in the world of commodities with the products of men’s hands. I call this the fetishism which attaches itself to the products of labour as soon as they are produced as commodities, and is therefore inseparable from the production of commodities.\nformulas, which bear the unmistakable stamp of belonging to a social formation in which the process of production has mastery over man, instead of the opposite, appear to the political economists’ bourgeois consciousness to be as much a self-evident and nature-imposed necessity as productive labour itself.\nvalues. When they thus assume the shape of values, commodities strip off every trace of their natural and original use-value, and of the particular kind of useful labour to which they owe their creation, in order to pupate into the homogeneous social materialization of undifferentiated human labour.\ncrisis. There is an antithesis, immanent in the commodity, between use-value and value, between private labour which must simultaneously manifest itself as directly social labour, and a particular concrete kind of labour which simultaneously counts as merely abstract universal labour, between the conversion of things into persons and the conversion of persons into things*; the antithetical phases of the metamorphosis of the commodity are the developed forms of motion of this immanent contradiction.\nAs against this, the circulation of money as capital is an end in itself, for the valorization of value takes place only within this constantly renewed movement.\nCooperation\nmanufacture [Manufaktur] can hardly be distinguished, in its earliest stages, from the handicraft trades [Handwerksindustrie] of the guilds, except by the greater number of workers simultaneously employed by the same individual capital.\nWhen consumed in common, they give up a smaller part of their value to each single product; partly because the total value they part with is spread over a greater number of products, and partly because their value, although it is greater in absolute terms, is relatively Jess, looked at from the point of view of their sphere of action, than the value of separate means of production.\nJust as the offensive power of a squadron of cavalry, or the defensive power of an infantry regiment, is essentially different from the sum of the offensive or defensive powers of the individual soldiers taken separately, so the sum total of the mechanical forces exerted by isolated workers differs from the social force that is developed when many hands co-operate in the same undivided operation, such as raising a heavy weight, turning a winch or getting an obstacle out of the way. 4 In such cases the effect of the combined labour could either not be produced at all by isolated individual labour, or it could be produced only by a great expenditure of time, or on a very dwarf-like scale. Not only do we have here an increase in the productive power of the individual, by means of co-operation, but the creation of a new productive power, which is intrinsically a collective one. 5\nIf the labour process is complicated, then the sheer number of the co-operators permits the apportionment of various operations to different hands, and consequently their simultaneous performance. The time necessary for the completion of the whole work is thereby shortened. 9\nWhether the combined working day, in a given case, acquires this increased productivity because it heightens the mechanical force of labour, or extends its sphere of action over a greater space, or contracts the field of production relatively to the scale of production, or at the critical moment sets large masses of labour to work, or excites rivalry between individuals and raises their animal spirits, or impresses on the similar operations carried on by a number of men the stamp of continuity and manysidedness, or performs different operations simultaneously, or economizes the means of production by use in common, or lends to individual labour the character of average social labour - whichever of these is the cause of the increase, the special productive power of the combined working day is, under all circumstances, the social productive power of labour, or the productive power of social labour. This power arises from co-operation itself. When the worker co-operates in a planned way with others, he strips off the fetters of his his individuality, and develops the capabilities of his species.13 As a general rule, workers cannot co-operate without being brought together: their assembly in one place is a necessary condition for their co-operation. Hence wage-labourers cannot cooperate unless they are employed simultaneously by the same capital, the same capitalist, and therefore unless their labourpowers are bought simultaneously by him.\nHence, concentration of large masses of the means of production in the hands of individual capitalists is a material condition for the co-operation of wage-labourers, and the extent of co-operation, or the scale of production, depends on the extent of this concentration.\nproduction. That a capitalist should command in the field of production is now as indispensable as that a general should command on the field of battle.\nAs the number of the co-operating workers increases, so too does their resistance to the domination of capital, and, necessarily, the pressure put on by capital to overcome this resistance.\nwage-labourer. An industrial army of workers under the command of a capitalist requires, like a real army, officers (managers) and N.C.O.s (foremen, overseers), who command during the labour process in the name of capital.\nprocess.17 It is not because he is a leader of industry that a man is a capitalist; on the contrary, he is a leader of industry because he is a capitalist.\nHe pays them the value of 100 independent labour-powers, but he does not pay for the combined labour-power of the 100. Being independent of each other, the workers are isolated. They enter into relations with the capitalist, but not with each other.\nJust as the social productive power of labour that is developed by co-operation appears to be the productive power of capital, so co-operation itself, contrasted with the process of production carried on by isolated independent workers, or even by small masters, appears to be a specific form of the capitalist process of production.\nJameson\nAt this point, then, the quality of the various forms of human activity, their unique and distinct “ends” or values, has effectively been bracketted or suspended by the market system, leaving all these activities free to be ruthlessly reorganized in efficiency terms, as sheer means or instrumentality.\nWhat is unsatisfactory about the Frankfurt School position is not its negative and critical apparatus, but rather the positive value on which the latter depends, namely the valorization of traditional modernist high art as the locus of some genuinely critical and subversive, “autonomous” aesthetic production.\nFor all these reasons, it seems to me that we must rethink the opposition high culture/mass culture in such a way that the emphasis on evaluation to which it has traditionally given rise, and which-however the binary system of value operates (mass culture is popular and thus more authentic than high culture, high culture is autonomous and therefore utterly incomparable to a degraded mass culture)-tends to function in some timeless realm of absolute aesthetic judgment, is replaced by a genuinely historical and dialectical approach to these phenomena. S\nIndeed, this view of the emergence of mass culture obliges us historically to respecify the nature of the “high culture” to which it has conventionally been opposed: the older culture critics indeed tended loosely to raise comparative issues about the “popular culture” of the past.\nThe above reflections by no means raise, let alone address, all the most urgent issues which confront an approach to mass culture today. In particular, we have neglected a somewhat different judgment on mass culture, which also loosely derives from the Frankfurt School position on the subject, but whose adherents number “radicals” as well as “elitists” on the Left today. This is the conception of mass culture as sheer manipulation, sheer commercial brainwashing and empty distraction by the multinational corporations who obviously control every feature of the production and distribution of mass culture today. If this were the case, then it is clear that the study of mass culture would at best be assimilated to the anatomy of the techniques of ideological marketing and be subsumed under the analysis of advertising. R\nRather, class struggle, and the slow and intermittent development of genuine class consciousness, are themselves the process whereby a new and organic group constitutes itself, whereby the collective breaks through the reified atomization (Sartre calls it the seriality) of capitalist social life.\nmaterial. To rewrite the concept of a management of desire in social terms now allows us to think repression and wish-fulfillment together within the unity of a single mechanism, which gives and takes alike in a kind of psychic compromise or horse-trading, which strategically arouses fantasy content within careful symbolic containment structures which defuse it, gratifying intolerable, unrealizable, properly imperishable desires only to the degree to which they can again be laid to rest. This model seems to me to permit a far more adequate account of the mechanisms of manipulation, diversion, degradation, which are undeniably at work in mass culture and in the media. In particular it allows us to grasp mass culture not as empty distraction or “mere” false consciousness, but rather as a transformational work on social and political anxieties and fantasies which must then have some effective presence in the mass cultural text in order subsequently to be “managed” or repressed.\nNow the content of the partnership between Hooper and Brody projected by the film may be specified socially and politically, as the allegory of an alliance between the forces of law-and-order and the new technocracy of the multinational corporations: an alliance which must be cemented, not merely by its fantasized triumph over the ill-defined menace of the shark itself, but above all by the indispensable precondition of the effacement of that more traditional image of an older America which must be eliminated from historical consciousness and social memory before the new power system takes its place. T\nThis is the context in which the ideological function of the myth of the Mafia can be understood, as the substitution of crime for big business, as the strategic displacement of all the rage generated by the American system onto this mirror-image of big business provided by the movie screen and the various tv series, it being understood that the fascination with the Mafia remains ideological even if in reality organized crime has exactly the importance and influence in American life which such representations attribute to it. T\nThompson\n? If the transition to mature industrial society entailed a severe restructuring of working habits—new disciplines, new incentives, and a new human nature upon which these incentives could bite effectively—how far is this related to changes in the inward notation of time?\nIn a similar way labour from dawn to dusk can appear to be “natural” in a farming community, especially in the harvest mo\nClearly hunters must employ certain hours of the night to set their snares. Fishing and seafaring people must integrate their lives with the tides.\nLet us return from the timepiece to the task. Attention to time in labour depends in large degree upon the need for the synchronization of labour\nThe work pattern was one of alternate bouts of intense labour and. of idleness, wherever men were in control of their own working lives (The pattern persists among some self-employed—artists, writers, small farmers, and perhaps also with students—today, and provokes the question whether it is not\nimposition. This remains true to this day, and, despite school times and television times, the rhythms of women’s work in the home are not wholly attuned to the measurement of the clock.\nWe are entering here, already in 1700, the familiar landscape of disciplined industrial capitalism, with the time-sheet, the timekeeper, the informers and the finest\nPowell, in 1772, also saw education as a training in the “habit of industry”; by the time the child reached six or seven it should become “habituated, not to say naturalized to Labour and Fati\nIn all these ways - by the division of labour; the supervision of labour; fines; bells and clocks; money incentives; preachings and schoolings; the suppression of fairs and sports—.new labour habits were formed, and a new time-discipline was imposed.) It sometimes took several generations (as in the Potteries), and we may doubt how far it was ever fully accomplished: irregular labour rhythms were perpetuated (and even institutionalized) into the present century, notably in London and in the great ports.114"},"Utilitarianism":{"title":"Utilitarianism","links":[],"tags":["Economics","Philosophy/Political-Philosophy"],"content":""},"Utility-Function":{"title":"Utility Function","links":["Indirect-Utility-Function","Rationality-(Economics)","Cobb-Douglas-Utility-(Two-Goods)","Monotonic-Transformation","Marginal-Rate-of-Substitution-(MRS)","Utility-Function","Budget-Lines","assets/Untitled-2-10.png"],"tags":["Economics"],"content":"for mapping (p1​,p2​,I)↦u, see the Indirect Utility Function\ndef. Utility Function. a utility function maps n goods to utility (happiness) that satisfies the assumption of Rational Taste including convexity.\nU(x): tuple of good quantity Rn​​↦ utility R​​\n⇒ such that if x1​​≻x2​​ then u(x1​​)&gt;u(x2​​)\nTypes of Utility Functions §\nLet Si​ be the allocation bundle to agent i. Si​ contains items (”j”) where xij​ defines what percentage of that item is allocated to j. Then i’s utility μi​ can take various forms.\n\ndef. Additive Utility. μi​=∑j​μij​xj​ \ndef. Cobb-Douglas Utility. μi​:=∏j​μij​xjαij​​ where ∑αij​=1\n(See the case for two goods)\ndef. Leontief Utility. μi​:=minj​{μij​xj​}\n\nHomothetic, Quasilinear, Perfect Substitutes §\nIn addition to the constrains of Rational Taste we can also have these particular tastes that characterize a utility function.\ndef. Quasilinear Tastes. If a utility function has quasilinear tastes against good x1​, then the function (or a Monotonic Transformation of the function) is linear against x1​, e.g.:\nu(x1​,x2​)=x1​+f(x2​)\n\nSubstitutability §\ndef. Perfect Substitutes. If a utility function means goods x1​ and x2​ are perfect substitutes, then every indifference curve of the utility function is a linear function, e.g.\n\\displaylinesu(x1​,x2​)=x1​+x2​⟹setting u=uˉ,uˉ=x1​+x2​ is linear between x1​,x2​\n\ne.g. [[Untitled 7 2.png|5billsand10 bills]]\nSee Marginal Rate of Substitution (MRS)\n\nPerfect Compliments.\n\ne.g. ![[Untitled 6 2.png|Right shoe and left shoe|380]]\nFormula looks something like: u(x1​,x2​)=min(x1​,2x2​)\n\nIndifference Curves §\n\ndef. Indifference Curve. A set of bundles that an agent with rational taste is indifferent about\n\nIndifference curves are horizontal slices of a utility function.\n\ni.e. a level field of a scalar field defined by the Utility Function of two goods—u(x1​,x2​).\nIndifference curves from the same utility function cannot cross (obviously)\n\n\nNorth-east side is always better.\n\n∵ monotonic taste—if this is not the case, change the direction of the curve.\n\n\nThe convexity assumption causes ICs to bend to the origin\nMultiple indifference curves form one indifference map =[Utility Function].\nIndifference maps are considered the same when\n1. the order of the indifference curves is the same\n2. MRS at every point for every curve is the same\nBudget Lines are drawn on the same graph as ICs\nWhen tastes are strictly convex (a taste for variety) then: ^ny6ceu\n\n⇒ (A≿B)∧(B≿C) in the Graph\n\n\n\nAnalyzing Indifference Curve Shapes §\nu(x1​,x2​)=x1α​+x2​\n"},"Utility-Maximization-with-Endowments":{"title":"Utility Maximization with Endowments","links":["Positive-Leisure","Cobb-Douglas-Utility-(Two-Goods)","Uncompensated-Demand-curve","Labor-Supply","Laffer-Curve"],"tags":["Economics/Micro-Economics"],"content":"Budget constraint with endowment\np1​x1​+p2​x2​=Endowmentsp1​e1​+p2​e2​​​+I\n\n⇒ Whether the change in price leads to a utility increase or decrease depends on where your original optimum is; i.e.\n\nWhether you are a net seller or net buyer of the good (optimum is left/right of endowment)\nWhich way the price ratio changes\n\nLeisure—Consumption Utility §\nSame equation as above, but with Consumption-Positive Leisure tradeoff\nConsumptionwL+pc​=Exogenous Income+Endowment=I+wT+pe​​\n…where…\n\nLabor quantities:\n\nw: wage (=price of leisure)\nL: quantity of leisure, in unit hours\nT: total time endowment, in unit hours\n\n\nConsumption quantities:\n\np: price of composite consumption good\nc: quantity of composite consumption good\ne: consumption endowment\n\n\n\nCobb-Douglas:\nmaxL,c​ u=Lαc(1−α) such that wL+pc=I+wT+pe\ntherefore we can plot the Ordinary Demand of Leisure (=Labor Supply)\n\nc=pα(I+wT+pe)​ Ordinary Demand of Consumption\nL=wα(I+wT+pe)​ Ordinary Demand of Leisure\n\nN:=T−L=T−wα(I+wT+pe)​ ==Ordinary Supply of Labor==\n\n\n\nTaxation §\nPercentage tax t on wage (=price of leisure)\n⇒ removing t from the wage\nw(1−t)L+pc=I+w(1−t)+pe\n\nTax Revenue (T−L)​Time worked​×wtHourly tax\nLaffer Curve\n"},"Utility-Maximization":{"title":"Utility Maximization","links":["Utility-Function","Budget-Lines","Lagrangian-Optimization","Monotonic-Transformation","Uncompensated-Demand-curve","Expenditure-Minimization","Homogenous-Function"],"tags":["Economics"],"content":"max u(x1​,x2​) such that I=p1​x1​+p2​x2​\nMaximization of the Utility Function against the Budget Constraints.\n\nUses Lagrangian Optimization\n&amp; If it makes it more convenient, do a Monotonic Transformation of the utility function. See Example.\nOptimal is where the budget line is the tangent to the Indifference Curve. This implies both gradients are same, i.e. −P2​P1​​=MRS\nThe result is the Ordinary Demand functions (one for each good)\nSee Expenditure Minimization for the opposite case\n! Always check if the resulting x1​,x2​ are positive. If not, it’s piecewise function to keep both positive.\n\nPerfect Substitutes §\nWith the form: u=(ax1​k+bx2​l)α\n\n⇒ Go to the corner that gives the highest utility (=buy only one good!)\nx1​={p1​I​0​if u(p1​I​,0)≥u(p2​I​,0)else​, x2​={p2​I​0​if u(0,p2​I​)≥u(0,p1​I​)else​\nPerfect Compliments §\nmaxx1​,x2​​ u(x1​,x2​)=min(x1​,2x2​)  such that I=p1​x1​+p2​x2​\n\n\nGet the formula that is the set of all kinked points in the utility function ⇒ x1​=2x2​\nGet the Constraint formula ⇒ I=p1​x1​+p2​x2​\nSolve for the two equations (=get the intersection)\n\nx1​=2p1​+p2​I​ , x2​=2p1​+p2​2I​\nQuasilinear Optimization §\nUse Lagrangian optimization, but beware that the blue line might happen (=maximum point lies outside x1​,x2​&gt;0):\n\n⇒ In this case, go to the red (*) corner solution.\nKinked Budget Constraint §\n\nCase 1: Inner kink\n\nLagrangian for blue section\nLagrangian for red section\nChoose the better one\n\n\nCase 2: Outer Kink\n\nLagrangian for blue and red section\nIf both solutions are unaffordable (=ICA​,ICB​ in the graph i.e. x2​&lt;0 in graph,), go to the kink.\n\n\n\nMore than Two Goods §\n\nCase 1: u(x1​,x2​,x3​)=x1α​x2β​x31−α−β​ ← Pure three-var cobb-douglas\n\n⇒ Use 3-var lagrangian\n\n\n==Case 2: u(x1​,x2​,x3​)=x1α​x21−α​+x3​==\n\nCheck that each term is Homogenous Degree of 1.\nTry the two-var lagrangian on the first term x1α​x21−α​ (assumping x3​ isn’t consumed)\nTry to maximize x3​ (assuming x1​,x2​ isn’t used)\nChoose the higher of the two utilities\n\nIf there is no concrete number, the cases differ on the conditions u(x1​,x2​,0)≶u(0,0,x3​)\n\n\n\n\n\nSolution for Case 2: \nmaxx1​,x2​,x3​​u=x1α​x21−α​+x3​  such that  I=p1​x1​+p2​x2​\n⎩⎨⎧​⎩⎨⎧​x1​=p1​αI​x2​=p2​1−αI​​x1​=x2​=0 and max x3​​If p1α​p21−α​αα(1−α)1−α​≥p3​1​Otherwise​\nGraphing a budget line with an indifference map, we can see that the bundle B1​ is where the consumer can achieve the most possible utility; where what is affordable = most possible utility\nTo find the bundle(=point) of maximum utility that is affordable, you can rephrase the problem as…\nWorked Example: Constrained Optimization Problem §\nUsing the Lagrange Method for max u(x1​,x2​) s.t. (budget line),\n∇u(x1​,x2​)=λ∇(p1​x1​+p2​x2​)p1​x1​+p2​x2​=I\nTo simplify further: ∇u(x1​,x2​)=λ∇(p1​x1​+p2​x2​−I) and thus let:\nL=∇u(x1​,x2​)−λ∇(p1​x1​+p2​x2​−I)\nto construct a set of equations where:\nδx1​δL​=0δx2​δL​=0p1​x1​+p2​x2​=I\nand solve the three equations. Note that the Lagrange method doesn’t work when:\n\nOne or more goods are non-essential, meaning that the budget line crosses the axes\n→ It’s a corner solution; i.e. the maximum point is at one of the intercepts, or at points where quantities of goods are negative\nTastes are non-convex, where there will be multiple solutions\nUtility Function are kinked or otherwise non-differentiable\n"},"Utility":{"title":"Utility","links":["Consumer-Surplus"],"tags":["Economics/Game-Theory"],"content":"Money. Utility. Consumer Surplus."},"VCG-Auction":{"title":"VCG Auction","links":[],"tags":["Economics/Game-Theory"],"content":"thm. VCG Mechanism Auction. The following mechanism is DSIC and Welfare Maximizing.\nCompute the optimum allocation S=(S1∗​,…,Sn∗​). Then for an agent i, let\n\nV∗:=∑j=in​vj​(Sj∗​) Optimal Social Welfare\nV−i∗​:=maxi=j​∑j=1n​vj​(Sj,−i​)\n\nwhere sj−1​ is the optimal allocation to j when i is simply removed from auction\nThus v−i∗​ is the optimal social welfare without i\n\n\nThus V∗≥V−i∗​\n\nbecause adding more bidders can only increase social welfare\n\n\n\nthm. The following allocation rule is DSIC and welfare-maximizing:\nAllocate by max∑j=1n​vj​(Sj​)\nCharge price to i:\npi​= re-run auction without i V−i∗​​​−auction’s welfare − i’s welfare(V∗−vi​(Si∗​))​​\nIntuition. This is similar to charging i the externality of joining the auction;\nProof.\nFirst, 0≤pi​&lt;vi​(Si​), i.e. we don’t charge negative prices, or a price higher than the bid:\n\n without i V−i∗​​​&gt; excluding i V∗−vi​(Si∗​)​​ because, thus 0≤pi​\npi​=vi​(Si∗​)+&lt;0V−i∗​−V∗​​ because V∗&gt;V−i∗​, i.e. adding more bidders cannot decrease welfare.\n\nSecond, fixing other’s bids b−i​, i will be incentivized to tell the truth of bi​. Consider the situation where the auctioneer took these bids and allocated S^=(S1​^​,…,Sn​^​).\npi​​=V−i∗​−(V∗−vi​(Si∗​)):=maxS​j=i∑​bj​(Sj​^​)−j=i∑​bj​(Sj∗​)​​\nNow, the goal of bidder i is to maximize their utility:\nμi​(b)​=vi​(Si​^​)−pi​(b)=vi​(Si​^​)−​maxS​j=i∑​bj​(Sj​^​)−j=i∑​bj​(Sj∗​)​= i can maximize vi​(Si​^​)+j=i∑​bj​(Sj∗​)​​− i cannot control maxS​j=i∑​bj​(Sj​^​)​​​​\nThus, the problem of i maximizing their utility μi​ is equivalent, by design of the pricing mechanism, as maximizing vi​(Si​^​)+∑j=i​bj​(Sj∗​), and this is equivalent to social welfare of the original auction.\nmaxbi​​μi​(bi​,b−i​​)⟺maxS​i=0∑n​bj​(Sj​)\nThus i is incentivized to tell the truth. ■"},"VSCode-Extensions":{"title":"VSCode Extensions","links":[],"tags":["Computing"],"content":"\nangular language service\ncode spell checker\ncompare folders\ncss property sorter\ncss var complete\nerror lens\neslint\ngit lens\nhtml scss support\nindent rainbow\nintellicode\nintellicode api usage examples\nintellicode completions\njapanese language pack for visual studio code\nprettier - code formatter\nscss intellisense\nstylelint\ntodo highlight\nvim\nvscode-icons\npowershell\n"},"Valorization,-Surplus-Value":{"title":"Valorization, Surplus Value","links":[],"tags":["Philosophy/Marxism"],"content":"Valorization is the addition of surplus value into capital.\n\nBegins with the curiosity:\n\nWhere did this M-C-M’ increase come from? ← magic! (actually capital?)\n\nPedagogical process of explaining valorization\n\nThe evolution of the money-owner to the capitalist\ncapitalist’s rant\n\n\n\n                  \n                  \\Delta M is the capitalist’s focus. C, or commodity, doens’t matter → overproduction, labor commoditization\n                  \n                \n\n\nHere is the answer:\n\nlabor’s use-val &gt; ex-val [=wages]\nCapitalist pays ex-val to buy the labor-hours. But what the laborer gives [=capitalist gets] is use-val (The distinguished character) for the amount of hours bought (by-hours)\nSurplus value is this difference\nA→B→C. C can be lengthened for abs. S-V. A-B can be shorted for rel. S-V\n\nImportance to marx’s pedagogy\n\npol-econ’s idea that wage = value of labor is an illusory description (Ch.19), they confuse the value of labor-power and value of labor i.e., the commiditized ex-val labor and the actual use-val of labor.\nTo pol-econ, wage = price ⇒ det. by demand/supply; if not the “natural price [=necessary price]”\n→ clarifies the source of M-C-M’, why is M&lt;M’ ← capital making capital\n→ reveals the justification/lawful pursuit of surplus-value, but why it doesn’t make sense on closer examination\n"},"Valuing-a-Firm":{"title":"Valuing a Firm","links":["Initial-Public-Offering","Investment-Bank","Comparable-Company-(Comps)-Analysis","Free-Cashflow","Balance-Sheet"],"tags":["Economics/Finance"],"content":"During an Initial Public Offering (IPO) the underwriting Investment Bank will value the firm using:\n\nComparable Company (Comps) Analysis\nDiscounted Cash Flow Analysis\nBook Value, using the Balance Sheet\netc.\n"},"Variance":{"title":"Variance","links":["Covariance-&-Correlation"],"tags":["Math/Probability"],"content":"def. Variance of a random variable X:\nVar[X]=E[(X−μ)2]=E(X2)−[E(X)]2\nVar(X):=E((X−μ)2)≡∫−∞∞​(x−μ)2fX​(x)dx=E(X2)−[E(X)]2\nthm. (Variance Identities.)\n\nFor a single random variable X and constant n\n\nVar(n⋅X)=n2⋅Var(X) Quadratic Scaling\nVar(X+n)=Var(X) Translation Invariance\n\n\nFor any two random variables X,Y:\n\nVar(X+Y)=Var(X)+Var(Y)+2⋅E[(X−μx​)(Y−μy​)]\n\n\n…if X⊥Y (i.e. two are independent:\n\nVar(X+Y)=Var(X)+Var(Y)\n\n\n….if Independent and Identically distributed (i.i.d) random variables I1​,…,In​\n\nVar(I1​+⋯+In​)=n⋅Var(Ii​)\n\n\n\nSee also Covariance &amp; Correlation.\ndef. Standard Deviation of a random variable is:\nSD(X):=Var(X)​=E(X2)−[E(X)]2​\n\nFrom the properties of variance we have SD(aX+b)=∣a∣SD(X)+b\n\nStandardization. §\nMotivation. Sometimes it’s nice to have random variables to have E(X)=0 and Var(X)=0. Using translation invariance and quadratic scaling we can take any random variable and standardize it.\nthm. (Standardization) For random variable X with E(X)=μ and Var(X)=σ2,\nY=σX−μ​\n\nIts expected value (mean) is 0; i.e. E(Y)=0\nIts standard deviation is 1; i.e. Var(Y)​=1\n"},"Variational-Autoencoders":{"title":"Variational Autoencoders","links":["Autoencoders"],"tags":["Computing/Maching-Learning","Math/Statistics"],"content":"Motivation. Autoencoders have the problem of unstructured latent space, i.e. when sampling purely randomly in the latent space h and decoding it to x, the generated data x is not very good—there are only a few ‘locations’ in the latent space that represents realistic data.\nTo solve this, we must ‘structure’ the latent space such that sampling it would almost always give real data. The simplest idea would be to force the latent space to be a standard normal distribution.\n\ndef. Variational Autoencoder (VAE).12 Let a probability distribution of data p(x) and latent distribution of p(z)=N(0,1).\n\nThe encoder encodes the data into variables μ,Σ. This forms the conditional latent distribution, p(z∣xobserved)=N(μ,Σ).\n\nNote that the latent representation is not a vector like autoencoders, but instead a full probability distribution\n\n\nThe decoder then takes a sampled latent point zrealized, and decodes it into the datapoint to get a distribution for the data p(x∣zrealized).\n\nObjective. The loss, also known as Evidence Lower Bound (ELBO) can be shown to be as follows:\n\nL()=Eq(z∣x)​[lnp(x∣z)]​ L2 Norm ​− Make sure it becomes std. norm. DKL​[q(z∣x)​N(μq​,Σq​)​∥p(z)​N(0,1)​]​​\nwhere the terms:\n3. Expectation term is equivalent to the L2 Norm between the original image xobserved and the reconstruction xreconstructed∼p(x∣zrealized)\n4. KL-divergence term is simply to make sure that q(z∣x) resembles a standard normal distribution.\nFootnotes §\n\n\nVariational Autoencoders | Generative AI Animated - YouTube ↩\n\n\nVariational AutoEncoders (VAE) with PyTorch - Alexander Van de Kleut ↩\n\n\n"},"Veil-of-Ignorance":{"title":"Veil of Ignorance","links":["John-Rawls"],"tags":["Economics/Game-Theory"],"content":"Introduced by John Rawls in thought experiment."},"Vickery-Auction":{"title":"Vickery Auction","links":["Ascending-Price-Auction","Sponsored-Search-Auction"],"tags":["Economics/Game-Theory"],"content":"Motivation. A first-price auction (=give the highest bidder the item, then charge them their bid) is a bad idea because, assuming the highest bidder was truthful (vi​=bi​) then their utility is vi​−bi​=0. Thus this is not DISC and not a good auction. It’s also very hard to reason about. Therefore we have… \ndef. Second Price Auction. (=Vickery Auction) Give to the highest bidder; charge them the second highest price. \n\nAn ascending bid auction (=an e-bay auction) is actually a second price auction if you think about it. The last two bidders (v1​,b1​)≥(v2​,b2​) will increase their bids continually until they reach b2​, and then bidder 1 bids just slightly above (b1​=v2​+δ) to get the item.\n! Not to be confused with Ascending Price Auction; they’re ascending two different things.\n\nthm. Second Price Auction is DSIC.\nproof. There are only two cases where a bidder may lie.\n\nCase one: a bidder lies to reduce the price of the item. But pi​(b) does not depend on v. Thus this is impossible\nCase two: a bidder lies to get the item. But lying to get the item results in negative utility. ◆\n\nSponsored Search Auction"},"Vision-(Neuroscience)":{"title":"Vision (Neuroscience)","links":[],"tags":["Biology/Neuroscience"],"content":"Anatomy §\n\nPhotoreceptors in Retina\n\nCones (color) and Rods (Sensitivity)\n\n\nOptic Chasm\nBoth:\n\nLateral Geniculate Nucleaus (LGN) ⊂ Thalamus\nSuperior Colliculus (unconscious brain movements)\n\n\nfrom LGN → Visual cortex\n\n\nBiological built-in hardware feature:\nLateral inhibition. Sharpens contrast in the border regions. Happens in the retina itself\n\nThe bipolar cells (the final signal) get input from photoreceptors\nHorizontal cells inhibit, but this inhibition decays with distance\nFinal signal emphasizes edges\n\n\nProcessing §\nVisual Cortex: V1, V2\n\nSimple cells: Simple kernel, looks like the shape itself\nComplex cells: orientation of shape\nHypercomplex cells: Endstopped\n→ Feature detectors\n\nDevelopment §\nParallel Processing §\nfrom V2:\n\nV3, V4, …\nVentral Stream: object recognition,\nDorsal Stream: 공간지각력,\n\nAt Inferior-Temporal Cortex\nIncludes fusiform gyrus: specialized for face-recognition + similar things\n\n\nMotion Detection by\n\nV5: Middle Temporal Cortex (MT)\nMedial Superior Temporal Cortex (MST)\nDuring Saccades, they stop working\n\n\n"},"Warrants-(Finance)":{"title":"Warrants (Finance)","links":["Black-Scholes-European-Option-Pricing-Formula","No-Arbitrage"],"tags":["Economics/Finance"],"content":"Warrants (=stock options) are a method that companies use to compensate employees (in addition to salary.) They can be priced using the BSM formula.\nSituation §\nAt time t=0, company issues warrants each with strike price K and expiration T.\n\nNout​: number of outstanding shares\nNw​: number of warrants issued\nSt​: stock value at time t\nV(0)=Nout​S0​+Nw​W(0): Company value (=MCAP)\n\nWe currently don’t know W(0)\nAt time t=T, suppose all warrant holders exercise their warrants:\n\n\nMCAPbefore​(T)=Nout​ST,before​ before the warrants exercised\nMCAPafter​(T)=(Nout​+Nw​)ST,after​ after the warrants exercised\nWe don’t know ST,after​=Nout​+Nw​MCAPbefore​(T)+Nw​K​=Nout​+Nw​MCAPafter​​\n\nPricing §\nConsider two portfolios A and B.\n\nA is just a simple warrant that expires at time T with strike K\n\ntime t=0, VA​(0)=W(0)\ntime t=T, VA​(T)=max{Nout​+nw​MCAPafter​​−K,0}\n\n\nB is a long of Nout​+Nw​Nout​​ units of call option on the stock.\n\ntime t=0, VB​(0)=Nout​+Nw​Nout​​C0E​\ntime t=T, VB​(T)=Nout​+Nw​Nout​​max{Nout​MCAPbefore​​−K,0}\nNow,\n\n\n\nVA​(T)​=max{Nout​+Nw​MCAPafter​​−K,0}=max{Nout​+Nw​MCAPbefore​+Nw​K​​−Nout​+Nw​K(Nout​+Nw​​)​,0}=max{Nout​+Nw​MCAPbefore​​−Nout​+Nw​KNout​​,0}=Nout​+Nw​Nout​​max{Nout​MCAPbefore​​−K,0}=VB​(T)​​\nBy the Law of One Price VA​(0)=VB​(0), meaning that:\nW(0)=Nout​+Nw​Nout​​C0E​\n■"},"Wealth-Effect-(WE)":{"title":"Wealth Effect (WE)","links":[],"tags":["Economics/Micro-Economics"],"content":"The Wealth effect is same as the income effect, except this time it is with endowments, so the math is a little different"},"Web-Dev":{"title":"Web Dev","links":[],"tags":["Computing"],"content":"Web Dev §\nThe Twelve-Factor App\nTop 5 Frontend Development Topics To Learn in 2019\ngetify/You-Dont-Know-JS\nTech stack rebuild for a new Facebook.com - Facebook Engineering\nLet’s Define Exactly What Atomic CSS is | CSS-Tricks\nQuick Start\nMonthly Getting Started / Web Dev Career Thread"},"Welfare-Theorems":{"title":"Welfare Theorems","links":["Pareto-Efficiency","Rationality-(Economics)"],"tags":["Economics/Micro-Economics"],"content":"thm. First Welfare Theorem. Market Equilibria ⊆ Pareto Efficiency\nthm. Second Welfare Theorem. If Preferences are convex, the Pareto Efficient Point can be supported as a market equilibrium."},"Wilcoxon-Rank-Sum-Test":{"title":"Wilcoxon Rank Sum Test","links":[],"tags":["Math/Statistics"],"content":""},"Wilcoxon-Signed-Rank-Test":{"title":"Wilcoxon Signed Rank Test","links":[],"tags":["Math/Statistics"],"content":""},"Worker-vs-Machine":{"title":"Worker vs Machine","links":["Proletatriat-(Marxism)","External/Industrial-Revolution"],"tags":["Philosophy/Marxism"],"content":"Factories ← → Workers inversion/subject-object inversion, a moral indignation\n\nHumans as organs of the machine → machine dominates, is the agent; a quantity → qualitative change\n\nDifference between tool and machine [= the factory]\n\n\ntrained since childhood as functions of organs; transforms the person as an appendage\n\nEffects on the human body/soul\n\nDeprives work of all content ← extended alienation, abstraction of labor\n\nRevolts &amp; Machines\n\nMachine → labor revolt → suppressed by machinery (labor is easily replacable, putting machine v. worker against each other)\n\n\n\n                  \n                  industrial machine”, or is it more abstract “technological systems/organization systems”\n                  \n                \n\n\nit is the system not the machine that is at fault. Less of the literal machines, more of the productivity advantage/disadvantage\n"},"Working-Memory":{"title":"Working Memory","links":[],"tags":["Psychology"],"content":"7 ± 2 items can be stored in working memory."},"Writing-101-Myth-of-Meritocracy":{"title":"Writing 101 Myth of Meritocracy","links":["assets/101---Self-aware-Social-Capital.pdf","assets/101---Myth-of-Meritocracy-in-Korean-College-Admissions.pdf"],"tags":["Courses"],"content":"Writing Assignments §\n\n101 - Self-aware Social Capital.pdf\n101 - Myth of Meritocracy in Korean College Admissions.pdf\n\nReadings §\n\n(DevonThink) Alim, (De)Occupying Language Social Justice volume\n(DevonThink) Alim, Critical Language Awareness in the United States\n(DevonThink) Edday, Hogan, How and for Whoem Does Increasing Course Structure Work?\n(DevonThink) Garcia-Sanchez, Public School and the Politics of Inclusion (Ch. 5)\n(DevonThink) Gonzales, College-Goers\n(DevonThink) Haviland, Ideologies of Language and U.S. Law\n(DevonThink) Heath, What No Bedtime Story Means: Narrative Skills at Home and School\n(DevonThink) Invited Forum - Journal of Lingisutic Anthropology\n(DevonThink) Johnson, A critical interrogation of the “language gap”\n(DevonThink) Kendall, White Privilege\n(DevonThink) King, Gods of the Upper Air\n(DevonThink) Kroskrity, Language Ideologies (Ch. 22)\n(DevonThink) Lake, An Indian Father’s Plea\n(DevonThink) Maguire, Kelly Hogan\n(DevonThink) McElhinny, White Police Officers on Race and Affirmative Action Journal_of_Linguistic_Anthropology\n(DevonThink) Philips, Language and social inequality\n(DevonThink) Reeves, Hoarding the Dream\n(DevonThink) Welji, Reclaiming a Diverse Ummah for Writing 101.docx\n"},"Writing-271-Internship-Reflection":{"title":"Writing 271 Internship Reflection","links":["Writing-271-Multimodel-Reflection","Writing-271-Reading-1-Analysis","Writing-271-Prose-Reflection","Writing-271-09-27-Quote","Writing-271-09-22-Enjoyable-Moment","Writing-271-09-15-Prompt","Writing-271-Extra-Credit","Writing-271-Curated-Portfolio"],"tags":["Courses"],"content":"\nWriting 271 Multimodel Reflection\nWriting 271 Reading 1 Analysis\nWriting 271 Prose Reflection\nWriting 271 09-27 Quote\nWriting 271 09-22 Enjoyable Moment\nWriting 271 09-15 Prompt\nWriting 271 Extra Credit\nWriting 271 Curated Portfolio\n"},"Writing-271-Prose-Reflection":{"title":"Writing 271 Prose Reflection","links":[],"tags":[],"content":"What knowledge of your field was most important? In what ways were you able to apply what you have learned in your academic coursework to your internship? How might you apply what you learned during your internship to your academic coursework?\nAfter I recieved my offer for the summer internship in the fall of the year before, I chose to take my first graduate-level course, mathematical finance, in order to prepare and gain knowledge about my role, risk engineer at a finance firm. At the"},"XPath-and-XQuery":{"title":"XPath and XQuery","links":["Extensible-Markup-Language","Regular-Expressions"],"tags":["Computing/Data-Science"],"content":"Query language for XML\nXPath §\n\nConsists of node (=element) and attribute\nuses and, or for conditionals (not |, &amp;, etc.)\n{xpath}/EMPS/EMP/PHONE[@type] checks for the existence of an attribute\n/bibliography/book/@ISBN returns all attribute values (ISBNs)\nfunctions:\n\n{xquery}x+y,x–y,x*y,x div y,x mod y\n{xquery}contains(a,b): a contains b\n{xquery}count($nodeset): number of child nodes in node set\n{xquery}position(): n-th node\nRegular Expressions matching {xquery}matches($string, $regex) e.g. {xquery}matches($input, &#039;H.*o W.*d&#039;)\n\n\nconditionals:\n\n{xquery}x=y note that one equals is fine, {xquery}x!=y\n{xquery}x&gt;y, {xquery}x &lt; y\n{xquery}and or not() works. (but &amp;,|,! does not!)\n\n\nAccess parent node as {xquery}child/.. without trailing backslash\nAccessing Attributes: {xquery}data($person/@name)\nFrequently used in conditions:\n\n{xquery}x + y\n{xquery}x - y\n{xquery}x * y\n{xquery}x div y\n{xquery}x mod y\n\n\nfunctions\n\n{xquery}contains(x, y): true if string x contains string y\n{xquery}count(node-set): counts the number of nodes in node-set\n{xquery}position(): returns the “context position” (roughly, the position of the current node in the node-set containing it)\n{xquery}last(): returns the size of the node-set containing the current node\n{xquery}name(): returns the tag name of the current element\n{xquery}sum(): returns sum of all matches\n\n\n\nXQuery §\nStandard format:\n&lt;result&gt;{\n\tfor $var in XPATH-QUERY (: Comments :)\n\tlet $var2 ...\n\twhere [condition]\n\tstable order by,\n\t\t$variable ascending,\n\t\t$variable descending\n\treturn &lt;elem&gt;{xquery}&lt;/elem&gt;\n}&lt;/result&gt;\nSort §\n\nConditionals §\n\nAxis Test §\n\n/: shorthand for child-or-self\n//: shorthand for descendant-or-self\n/..: shorthand for parent\none of self, attribute,\nparent, child, ancestor,† ancestor-or-self,† descendant, descendant-or-self,\nfollowing, following-sibling, preceding,† preceding-sibling,†\nnamespace\n†: These are reverse axes (=produce resulting node-sets in reverse document order)\nUse like: \n\nExistential Conditions (Exists some…) §\n\nDate Operations §\n\nDates will be printed as P dD T hH iM sS\n\nP just a P.\nd: Number of Days. D: constant\nT: constant\nh: Number of Hours, H: constant\ni: Number of Minutes, M: is constant\ns: Number of Seconds S: is constant\n\n\nDates can be\n\nsubtracted: {xquery}xs:date(&quot;1933-06-22&quot;) - xs:date(&quot;2000-01-01&quot;)\ncompared: {xquery}xs:date(&quot;1933-06-22&quot;) &gt;= xs:date(&quot;2000-01-01&quot;)\n\n\n\nDate functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear-from-dateTimeday-from-dateTimeminutes-from-dateTimeyear-from-dateday-from-dateminutes-from-timeyears-from-durationminutes-from-durationdays-from-durationmonth-from-dateTimehours-from-dateTimeseconds-from-dateTimemonth-from-datehours-from-timeseconds-from-timemonths-from-durationhours-from-durationseconds-from-duration\nTips and Tricks §\n\nNaming elements using variables:\n"},"Yield-Maturity-Curve-(Bond)":{"title":"Yield-Maturity Curve (Bond)","links":["Bonds-(Finance)","Monetary-Policy"],"tags":["Economics/Finance"],"content":"Relationship between yield and maturity of a bond.\nInterest Rate Changes §\nThe government can change the federal funds rate, which influences the y-intercept of the yield curve:\n\n\nInflation ⇒ interest rate ↑ ⇒ firm borrowing ↓ ⇒ investment ↓\nRecession ⇒ interest rate ↑ ⇒ firm borrowing ↑ ⇒ investment ↑\n\n\n\n                  \n                  An inverted yield curve occurs as the bond market: \n                  \n                \nexpects a recession ⇒ expectes the government to increase the interest rate ⇒ long-maturity bonds are more appealing.\n\nMaking Money off Bonds §\n\nCredit Curves §\n\nGovernment bonds have the highest credit rating\nWhen comparing other types of bonds (e.g. corporate bonds), any bond with the same maturity has a higher yield than government bonds.\nSpread is the difference in yield between it and government bonds.\n\nSpread ∝ 1/Credit ∝ maturity, and is related to industry\n\n\n\n\n"},"Zero-Sum-Game":{"title":"Zero Sum Game","links":["Equilibria-in-Game-Theory","Linear-Programming","Zero-Sum-Game"],"tags":["Economics/Game-Theory"],"content":"def. Zero Sum Game. When each box in the payoff (cost) matrix sums to zero.\nthm. Min-Max Theorem. The stackleberg solution in a zero-sum game for player one going first, is same for player two going first. Thus this is a mixed nash equilibirum. You can compute this using Linear Programming.\nExample. In the following game:\n\nRow player will aim to: \nmaxx1​,x2​​column’s expected strategy min( column goes left3x1​−2x2​​​, column goes right −x1​+x2​​​)​​​ Given column’s expected strategy, the best response ​\nThis is also known as the Stackleberg solution. Letting z=min(3x1​−2x2​,−x1​+x2​) we have the following linear program: \nzz10​≤3x1​−2x2​≤−x1​+x2​=x1​+x2​≤x1​,x2​​​\nEquivalently, column player will aim to miny1​,y2​​max(3y1​−y2​,−2y1​+y2​). Letting w=max(3y1​−y2​,−2y1​+y2​) We have the following linear program:\nww10​≥3y1​−y2​≥−2y1​+y2​=y1​+y2​≤y1​,y2​​​\nIn a Zero Sum Game like this game, the above two linear programs are dual problems to each other. The solution to both of these programs are:\n{x1​=73​,x2​=74​y1​=72​,y2​=75​​"},"index":{"title":"index","links":["Ascending-Price-Auction","Maximum-Flow-Problem","Black-Scholes-European-Option-Pricing-Formula","assets/361-Final---Game-Theory,-Financial-Markets,-Distributive-Justice.pdf","CS-535-Algorithmic-Game-Theory","CS-675-Deep-Learning","CS-330-Advanced-Algorithms","CS-334-Formal-Languages","CS-250-Architecture","CS-316-Database-Systems","Math-581-&-582-Mathematical-Finance,-Derivatives","Econ-201-Intermediate-Microeconomics-I","Econ-204-Econometrics","Econ-205-Intermediate-Microeconomics-II","Econ-210-Macroeconomics","Econ-371-Assets-&-Risk,-Finance","Stat-432-Statistics","Stat-230-Probability","Writing-101-Myth-of-Meritocracy","Econ-361-Distributive-Justice","Lit-380-Marxism","GSF-386-Politics-of-Sexuality","Lit-285-Existentialism","English-190FS-Renaissance-Literature","Music-190FS-Music-and-Medicine-in-European-Renaissance","Sociology-110D","Neuroscience-102","Chinese-101","Chinese-102","Chinese-203","International-Baccalaureate-(IB)"],"tags":["Courses"],"content":"\n\n                  \n                  Highlights \n                  \n                \n\nAscending Price Auction\nMaximum Flow Problem\nBlack Scholes European Option Pricing Formula\nGame Theory, Finance, and Distributive Justice\n\n\nCurrently Learning: LLL Tracker - Google Sheets\nSpecializations §\nComputer Science §\n\n🌟 CS 535 Algorithmic Game Theory\n🌟 CS 675 Deep Learning\nCS 330 Advanced Algorithms\nCS 334 Formal Languages\nCS 250 Architecture\nCS 316 Database Systems\nCS 230 Discrete Math for CS\n\nEconomics §\n\n🌟 Math 581 &amp; 582 Mathematical Finance, Derivatives\nEcon 201 Intermediate Microeconomics I\nEcon 204 Econometrics\nEcon 205 Intermediate Microeconomics II\nEcon 210 Macroeconomics\nCS 535 Algorithmic Game Theory\nEcon 371 Assets &amp; Risk, Finance\n\nMath §\n\n🌟 Stat 432 Statistics\nStat 230 Probability\nMath 221 Linear Algebra\nMath 212 Multivariable Calculus\n\nOther §\nLiberal Arts §\n\nWriting 101 Myth of Meritocracy\nEcon 361 Distributive Justice\nLit 380 Marxism\nGSF 386 Politics of Sexuality\nLit 285 Existentialism\nEnglish 190FS Renaissance Literature\nMusic 190FS Music and Medicine in European Renaissance\n\nSocSci, Neuro, Languages, IB §\n\nSociology 110D\nNeuroscience 102\nChinese 101, Chinese 102, Chinese 203\nInternational Baccalaureate (IB)\n"},"main-diagonal":{"title":"main diagonal","links":[],"tags":["Math/Linear-Algebra"],"content":"Main Diagonal\n\nAnti-diagonal\n"},"paradoxes-of-EUT":{"title":"paradoxes of EUT","links":[],"tags":["Philosophy/Epistemology"],"content":"von Neumann Expected Utility theory\nAlles Paradox: misunderstanding expected value\nEllsberg Paradox (urn): change in belief\nSt. Petersberg paradox (2n)"},"pininfo":{"title":"pininfo","links":[],"tags":[],"content":""},"歌舞伎町":{"title":"歌舞伎町","links":["Tenderloin"],"tags":["Sociology"],"content":"The Tenderloin of Tokyo"},"甘え":{"title":"甘え","links":[],"tags":["Sociology"],"content":"def. 甘えとは「嫌いになってほしくない」と「好き勝手やりたい」の混ざった感覚。"}}