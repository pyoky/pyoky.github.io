---
aliases: 
tags: 
origins:
  - "[[CS 675 Deep Learning]]"
---
Motivation. In the input space, draw a line to classify into two categories. This is achievable using [[Ordinary Least Squares Regression|linear regression]]

### Discriminant Function
def. **Discriminant Function** takes an input vector $\mathbf{x}$ and assigns into one of $K$ classes, $\mathcal{C}_{k}$.
- def. **Decision Surface** is the boundary that splits the classes in input space.
- def. **Decision Region** is the regions generated by the decision surfaces that correspond to a single class.
When there is two classes, $\mathcal{C}_{1},\mathcal{C}_{2}$ then:
$$
y(\mathbf{x})=\underbrace{ \mathbf{w}^\top }_\text{ weight=params} \mathbf{x}+\underbrace{ w_{0} }_\text{ bias }
$$
And assign:
$$
\mathbf{x} \in  \begin{cases}
\mathcal{C_{1}}  & \text{if } y(\mathbf{x})\geq 0 \\
\mathcal{C_{2}}  & \text{if } y(\mathbf{x})< 0 \\
\end{cases}
$$
- $\mathbf{w}$ is orthogonal to the decision surface hyperplane (via linalg; green in image)
- $w_{0}$ determines the distace from the origin of the decision surface![[Discriminant Function-20240904121858270.png|358]]
When there are $K>2$ classes then we cant just split into $K$ regions naively because:
![[Discriminant Function-20240904122016627.png]]
Therefore we instead introduce a discriminant function for every pair of classes, in total $\frac{K(K-1)}{2}$ DFs for each $k \in \{0\dots K\}$:
$$
y_{k}(\mathbf{x})=\mathbf{w}^{\top}_{k}\mathbf{x}+w_{k,0}
$$
and then assign to: $\mathcal{C}_{arg\max_{k}\{ y_{k}(\mathbf{x}) \}}$ i.e. the maximum of all discriminant functions. The pairwise decision surface for pair $j,k$ is:
$$
\underbrace{ (\mathbf{w}_{k}-\mathbf{w}_{j}) }_\text{ orthogonal }^{\top}\mathbf{x}+\underbrace{ (w_{k,0}-w_{j,0}) }_\text{ distance from origin }=0
$$
thm. *Such decision regions are singly connected and convex.*[^1]

## Linear Discriminnant Models

def. **Perceptron Algorithm.** (only works for $K=2$, but is illustrative.)
$$
y(\mathbf{x})=f(\mathbf{w}^{\top}\phi(\mathbf{x}))
$$
where:
- a fixed (=non-trained) function $\phi(\mathbf{x})$. Includes bias component.
- an activation function $f(x)$ where:
$$
f(x)=\begin{cases}
1  & \text{if } x\geq0 \\
0  & \text{if } x<0
\end{cases}
$$
### Optimizing the Perceptron Algorithm
def. **Perceptron Error Function.**
$$
E_{P}(\mathbf{w})=-\sum_{n \in \mathcal{M}}\underbrace{ \mathbf{w}^{\top}\cdot\phi_{n}(\mathbf{x})\cdot t_{n} }_\text{ always >0 }
$$
where
- Sums over $\mathcal{M}$, all the misclassified $\mathbf{x}$'s
- $t_{n} \in \{ -1,+1 \}$ indicates which class it is in, $\mathcal{C_{1}},\mathcal{C}_{2}$. 
Intuition. In this error function:
1. Correct classification has error $0$
2. Incorrect classification has error $\mathbf{w}^{\top}\cdot\phi_{n}(\mathbf{x})\cdot t_{n}$
Optimizing this globally using $\nabla E_{P}=0$ is too computationally intensive. We instead use:
$$
\begin{align}
\mathbf{w}^{(\tau+1)} & =\mathbf{w}^{(\tau)}-\eta \nabla E_{P}(\mathbf{w}) \\
 & =\mathbf{w}^{(\tau)}+\eta \cdot\phi_{n}t_{n}
\end{align}
$$
- $\tau$ is simply the learning step index
- $\eta$ is the learning rate. Adjustable
*Visualization.* Following are two steps of a perceptron optimization.![[Linear Discriminant Model-20240904125254152.png]]
1. Top Left: decision boundary (=black line) is initialized, defined by the orthogonal vector (=black arrow). Green point is misclassified with error $\mathbf{w}^{\top}\cdot\phi_{n}(\mathbf{x})\cdot t_{n}$ (=red arrow).
2. Top Right: error is added to the parameters to obtain a new decision boundary and new orthogonal vector.
3. Bottom Left: Green point is misclassified with error (=red arrow).
4. Bottom Right: error is added to the parameters to obtain a new decision boundary and new orthogonal vector.
*Motivation.* We are updating for every single new data $\mathbf{x}$ that comes in. Doesn't this mean that the error for other data may go up? No; the math says otherwise:
thm. **Perceptron Convergence Theorem.** If the training data is linearly separable, the perceptron learning algorithm is guaranteeed to find a "exact solution" in finite steps.[^2]

[^1]: [proof](x-devonthink-item://2A987E7E-2FEC-4396-97A4-080F916E5523?page=203&start=697&length=39&search=Thus%20Rk%20is%20singly%20connected%20and%20convex.)
[^2]: [proof](x-devonthink-item://2A987E7E-2FEC-4396-97A4-080F916E5523?page=213&start=2310&length=8&search=learning)