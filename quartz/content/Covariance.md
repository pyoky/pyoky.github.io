---
aliases: []
tags:
  - Math/Statistics
---
Motivation. Let us imagine a tuple of data generated by two independent RVs: $(x_{1},y_{1}),\dots,(x_{4},y_{4})$, as graphed below. ![[Covariance-20240910155403511.png]]
1. $\text{Var}(X)=2$ for both
2. $\text{Var}(Y)=1$ for both
3. …but the 'spread' of these two are clearly different!
There, we observe that what matters is the 'quadrant' or 'direction' of the spread that matters, and that is captured by multiplication of the two coordinates. Thus:
*Intuition*. Covariance is the "average quadrant" of the direction of spread.
[The covariance matrix from 5:06](https://youtu.be/WBlnwvjfMtQ?feature=shared&t=306)

def. **Covariance.** Covariance measures the joint variability of two R.V.s; let $X,Y$.
- When $X,Y$ show similar behavior, $\text{Cov}(X,Y) >0$
- When $X,Y$ show opposite behavior, $\text{Cov}(X,Y) <0$

$$
\begin{aligned}
\text{Cov}(X,Y):&= \mathbb{E}((X-\mu_X)(Y-\mu
_Y))\\[.5em]&=\mathbb{E}(X\cdot Y)-\mathbb{E}(X)\cdot \mathbb{E}(Y)

\end{aligned}
$$

- When $X\perp Y$, then $\text{Cov}(X,Y)=0$; but $\text{Cov}(X,Y)$ does **not** imply $X\perp Y$
- Covariance is a generalization of [[Variance]]: $\text{Var}(X)=\mathbb{E}((X-\mu_X)^2)=\text{Cov}(X,X)$

thm. **Relationship between Covariance and Variance.** let $X,Y$. then:

$$
\begin{align}
\text{Var}(X+Y)&=\text{Var}(X)+\text{Var}(Y)-2\cdot \mathbb{E}((X-\mu_X)(Y-\mu_Y)) \\
&= Var(X)+Var(Y)-2Cov(X,Y)
\end{align}
$$

- When $X\perp Y$ then $\text{Var}(X,Y)=\text{Var}(X)+\text{Var}(Y)$

thm. **Bilinearity of Covariance.**

- $\text{Cov}(aX,aY)=ab \cdot \text{Cov}(X,Y)$
- $\text{Cov}(X,Y+Z)=\text{Cov}(X,Y)+\text{Cov}(X,Z)$

thm. **Summed Variance.** let $X_1,…,X_n$. Then

$$
\text{Var}(\sum_i
X_i)=\sum_i \text{Var}(X_i)+2\sum_{\forall j,k \text{ s.t.} j<k} \text{Cov}(X_j,X_k)
$$

E.G. second summation has $3 \choose 2$ terms:

$$
\begin{aligned}
\text{Var}(X_1+X_2+X_3)&=\text{Var}(X_1)+\text{Var}(X_2)+\text{Var}(X_3)\\
&+2\cdot \text{Cov}(X_1,X_2)\\
&+2\cdot \text{Cov}(X_2,X_3)\\
&+2\cdot \text{Cov}(X_1,X_3)
\end{aligned}
$$

## Covariance Matrix

def. Covariance Matrix is a collection of covariances for $X_{1},\dots,X_{n}$ RVs:
$$
\Sigma =
\begin{bmatrix}
\sigma_{1,1} & \sigma_{1,2} & \cdots & \sigma_{1,n} \\
\sigma_{2,1} & \sigma_{2,2} & \cdots & \sigma_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{n,1} & \sigma_{n,2} & \cdots & \sigma_{n,n}
\end{bmatrix}
$$
where $\sigma_{i,j}$ is the covariance $\text{Cov}(X_{i},X_{j})$
- Always symmetric
- Determinant gives

